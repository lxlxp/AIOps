id,phase,category,post_id,title,body,accepted_answer_id,answer_count,comment_count,community_owned_date,creation_date,favorite_count,last_activity_date,last_edit_date,last_editor_display_name,last_editor_user_id,owner_display_name,owner_user_id,parent_id,post_type_id,score,tags,view_count,hot_score
258805,1,Error,70239567,NodeJS with fastify and dd-trace - Datadog doesn't trace for post with body,"<p>My problem is pretty the same as here: <a href=""https://github.com/DataDog/dd-trace-js/issues/830"" rel=""nofollow noreferrer"">https://github.com/DataDog/dd-trace-js/issues/830</a>.
The issue is marked as resolved, but I still have it.</p>
<p>My post declaration:</p>
<pre><code>  fastify.post&lt;{
    Body: any;
    Response: any;
  }&gt;('/test', function (_request: FastifyRequest, reply: FastifyWrappableReply) {
    console.log(process.env.DD_CONTEXT_PROPAGATION);
    console.log(tracer.scope().active());
    console.log(tracer.scope().active()?.context().toTraceId());
    reply.status(200).send();
  });
</code></pre>
<p>For post without body the result looks like:</p>
<pre><code>[1] true
[1] DatadogSpan {...}
[1] 3780510505200846220
</code></pre>
<p>For post with body scope().active() returns null. TraceId is undefined:</p>
<pre><code>[1] true
[1] null
[1] undefined
</code></pre>
<p>I'm using fastify 3.22.0 and dd-trace 1.5.1. I will be very thankful for any advice how to enable tracing for post with body</p>",,0,0,,2021-12-5 23:49:23,,2021-12-5 23:49:23,,,,,2420030,,1,0,node.js|typescript|npm|datadog|fastify,175,
258806,3,Visualization,70244409,can Datadog be used to display the difference in time between two events or logs?,"<p>I am using Datadog's count API and if I have 2 events emitted with the same tag (like a UUID), I want to plot a distribution graph of the difference in time between these two events. (e.g. I have an API call and corresponding webhook that I receive afterwards, I want to measure latency between API call and receiving the webhook). Is this possible?</p>",,0,0,,2021-12-6 10:59:47,,2021-12-6 10:59:47,,,,,10526594,,1,0,datadog,31,
258807,3,Monitoring,70263323,"For a Datadog monitor with an arithmetic query, how can I make reference to individual metrics from that query?","<p>I have two Datadog gauge metrics, let's say <code>foo.bar</code> and <code>foo.baz</code>.</p>
<p>Got a monitor that must be triggered when there's discrepancy between them, so that the source query looks like:</p>
<pre><code>abs(max:foo.bar{host:XXXX} - max:foo.baz{host:XXXX})
</code></pre>
<p>The alert notification must contain both the final value of the difference between metrics, and the metrics values themselves - like:</p>
<p><em>&quot;There's difference 5.0 between <strong>foo.bar</strong> (value: 25.0) and <strong>foo.baz</strong> (value: 30.0)&quot;</em></p>
<p>For the former, there's <code>{{value}}</code> template variable, but I could not find the way to pass the individual metrics' values to the message.</p>
<p>How is it possible to put values of the metrics to the notification message?</p>",,0,0,,2021-12-7 16:03:18,,2021-12-7 16:03:18,,,,,17582370,,1,0,datadog,33,
258808,1,Parse,70278933,DataDog to OpenTracing IDs - Converting 64-bit unsigned integer to 128-bit unsigned and 64-bit unsigned hex,"<p>I am attempting to correlate traces originating in DataDog's RUM SDK, which are hitting by BE instrumented with OpenTelemetry. Essentially trying to implement the reverse of DD's documentation on <a href=""https://docs.datadoghq.com/tracing/connect_logs_and_traces/opentelemetry/"" rel=""nofollow noreferrer"">Connect OpenTelemetry Traces And Logs</a></p>
<blockquote>
<p>OpenTelemetry TraceId and SpanId properties differ from Datadog conventions. Therefore it’s necessary to translate TraceId and SpanId from their OpenTelemetry formats (a 128bit unsigned int and 64bit unsigned int represented as a 32-hex-character and 16-hex-character lowercase string, respectively) into their Datadog Formats(a 64bit unsigned int).</p>
</blockquote>
<p>The context is being set in my BE in the following way;</p>
<pre class=""lang-js prettyprint-override""><code>(event:any) =&gt; {
      if(event.headers) {
        const traceId = event.headers['x-datadog-trace-id']
        const spanId = event.headers['x-datadog-parent-id']
        if(traceId &amp;&amp; spanId) {
          const convertedTraceId = convertToOtelId(traceId, 32);
          const convertedSpanId = convertToOtelId(spanId, 16);
          const spanContext: SpanContext = {
            traceId: convertedTraceId,
            spanId: convertedSpanId,
            traceFlags: 1,
            isRemote: true,
          };
    
          return propagation.extract(context.active(), spanContext);
        }
      }
      return ROOT_CONTEXT;
    }
</code></pre>
<p>convertToOtelId is producing these results;</p>
<p><code>trace id - In: 2245099779221068633, Out: 00000000000000001f28325aa9c79b59</code></p>
<p><code>span id - In: 4011377516575614177, Out: 37ab46991f7a64e1</code></p>
<p>Obviously the 128-bit uint for the trace_id does not look correct with its leading zeros. Am I understanding this conversion correctly?</p>",,1,0,,2021-12-8 17:02:13,,2021-12-9 14:05:47,,,,,12005780,,1,0,javascript|unsigned|datadog|opentracing|open-telemetry,64,
258809,1,Error,70280057,Why does datadog lambda instrumentation return a `datadog-lambda-js/handler.handler is undefined or not exported`?,"<p>The Datadog AWS Lambda instrumentation seems unreliable to me. Every few invocations, I get the following error:</p>
<pre><code>    &quot;errorType&quot;: &quot;Runtime.HandlerNotFound&quot;,
    &quot;errorMessage&quot;: &quot;/opt/nodejs/node_modules/datadog-lambda-js/handler.handler is undefined or not exported&quot;,
    &quot;stack&quot;: [
        &quot;Runtime.HandlerNotFound: /opt/nodejs/node_modules/datadog-lambda-js/handler.handler is undefined or not exported&quot;,
        &quot;    at HandlerNotFound.ExtendedError [as constructor] (/opt/nodejs/node_modules/datadog-lambda-js/runtime/errors.js:113:28)&quot;,
        &quot;    at new HandlerNotFound (/opt/nodejs/node_modules/datadog-lambda-js/runtime/errors.js:131:42)&quot;,
        &quot;    at load (/opt/nodejs/node_modules/datadog-lambda-js/runtime/user-function.js:151:15)&quot;,
        &quot;    at Object.&lt;anonymous&gt; (/opt/nodejs/node_modules/datadog-lambda-js/handler.js:65:59)&quot;,
        &quot;    at Module._compile (internal/modules/cjs/loader.js:999:30)&quot;,
        &quot;    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1027:10)&quot;,
        &quot;    at Module.load (internal/modules/cjs/loader.js:863:32)&quot;,
        &quot;    at Function.Module._load (internal/modules/cjs/loader.js:708:14)&quot;,
        &quot;    at Module.require (internal/modules/cjs/loader.js:887:19)&quot;,
        &quot;    at require (internal/modules/cjs/helpers.js:74:18)&quot;
    ]
}
</code></pre>
<p>To start the instrumentation, I run the following code.</p>
<pre><code>datadog-ci lambda instrument \
   -f my-lambda-name \
   -r my-region \
   -v 50 -e 15 \
   --service my-service \
   --env my-env \
   --version 1.0
</code></pre>
<p>Any idea what I'm doing wrong?</p>",,1,0,,2021-12-8 18:33:20,,2021-12-9 05:23:40,,,,,2596680,,1,0,node.js|logging|aws-lambda|datadog,71,
258810,3,Monitoring,70281433,Monitor threshold specifications in DataDog,"<p>I'm trying to set up a monitor in DataDog where when the condition is &gt;=5 (5 or more) the monitor will go into a triggered status, and to recover when it is 4 or less (&lt; 5).  Maybe I'm missing something but it feels like there isn't a way to properly set boundaries for discrete numbers without excluding the number as I hit a validation indicating the two numbers can't be the same (see picture).</p>
<p><a href=""https://i.stack.imgur.com/t6Y23.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t6Y23.png"" alt=""monitor threshold"" /></a></p>",,0,1,,2021-12-8 20:39:21,,2021-12-8 20:39:21,,,,,6800897,,1,0,monitoring|datadog|sre,28,
258811,2,Query,70283723,Truncating in DataDog Widget,"<p>In a DataDog Widget, is there a way to configure a query value such that anything below say .01% (but not 0) is represented as &quot; &lt; .01%&quot;? I can live with the display here:</p>
<p><a href=""https://i.stack.imgur.com/s9YvO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s9YvO.png"" alt=""enter image description here"" /></a></p>
<p>But I want to match what they are doing in their default services view:</p>
<p><a href=""https://i.stack.imgur.com/Rvagz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rvagz.png"" alt=""enter image description here"" /></a></p>
<p>I have played around with Cutoff Min and Clamp Min, but that didn't seem right. Here's the base configuration</p>
<p><a href=""https://i.stack.imgur.com/7nYFf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7nYFf.png"" alt=""enter image description here"" /></a></p>
<p>Thanks in advance!</p>",,0,0,,2021-12-9 01:32:07,,2021-12-9 01:32:07,,,,,286485,,1,0,datadog,16,
258812,3,Monitoring,70318922,How to monitor Hashicorp vault with datadog?,"<p>I need to monitor hashicorp vault that is running as a pod in gcp cluster with datadog. There are some docs about this: <a href=""https://docs.datadoghq.com/integrations/vault/?tab=host"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/vault/?tab=host</a>. Following these instructions I tried to use approle auth method. And then I configured auto-auth for agent. Everything worked properly(client token was created). But when I wrote api_url and client_token Datadog didn't received any metrics. I got such errors:<br />
<a href=""https://i.stack.imgur.com/jhHsn.jpg"" rel=""nofollow noreferrer"">error image</a></p>
<p>How do you think how I can fix that?
Or if you already did such tasks can you describe how you configured everything</p>",,0,0,,2021-12-11 20:49:19,1,2021-12-12 18:04:31,2021-12-12 18:04:31,,7695859,,17619771,,1,0,kubernetes|hashicorp-vault|datadog,33,
258813,1,Method,70331061,Does Datadog tracing impact performance? (specifically Node.js),"<p>Does using Datadog tracing to monitor the performance of my Express (Node.js) REST API impact the throughput (requests per second)? In in other words, is there any performance overhead with tracing requests?</p>
<p>If so, what would the reduction in throughput be? (e.g. -10% approx.)</p>",,0,1,,2021-12-13 07:13:47,,2021-12-13 07:18:43,2021-12-13 07:18:43,,11986963,,11986963,,1,0,node.js|trace|datadog,27,
258814,1,Parse,70355492,Parse value from json,"<p>For a log for a particular service my one attribute <code>httpRequest</code> returns a json object:</p>
<pre><code>&quot;httpRequest&quot;: {
    &quot;country&quot;: &quot;DE&quot;,
    &quot;headers&quot;: [
        {
            &quot;name&quot;: &quot;Host&quot;,
            &quot;value&quot;: &quot;mydomain.example.com&quot;
        },
        {
            &quot;name&quot;: &quot;User-Agent&quot;,
            &quot;value&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:94.0) Gecko/20100101 Firefox/94.0&quot;
        },
        {
            &quot;name&quot;: &quot;Accept&quot;,
            &quot;value&quot;: &quot;image/avif,image/webp,*/*&quot;
        },
        {
            &quot;name&quot;: &quot;Accept-Language&quot;,
            &quot;value&quot;: &quot;en,en-US;q=0.5&quot;
        },
        {
            &quot;name&quot;: &quot;Accept-Encoding&quot;,
            &quot;value&quot;: &quot;gzip, deflate, br&quot;
        },
        {
            &quot;name&quot;: &quot;Connection&quot;,
            &quot;value&quot;: &quot;keep-alive&quot;
        },
        {
            &quot;name&quot;: &quot;Referer&quot;,
            &quot;value&quot;: &quot;https://mydomain.example.com/cat.jpg&quot;
        },
        {
            &quot;name&quot;: &quot;Cookie&quot;,
            &quot;value&quot;: &quot;OptanonConsent=isIABGlobal=false&amp;datestamp=Thu+Oct+21+2021+09%3A55%3A11+GMT%2B0200+(South+Africa+Standard+Time)&amp;version=6.22.0&amp;landingPath=NotLandingPage&amp;AwaitingReconsent=false&amp;groups=C0001%3A1%2CC0003%3A1%2CC0002%3A1%2CC0004%3A1&amp;isGpcEnabled=0&amp;hosts=&amp;geolocation=ZA%3BWC; _ga_QQSC9D30C7=GS1.1.1620799882.6.0.1620799882.60; _ga=GA1.2.2133505955.1614865120; WZRK_G=79ce4c41d4194196af7e2846c4baec9b; ajs_anonymous_id=%227acfac50-a8df-4199-921c-eb149f78cd8d%22; OptanonAlertBoxClosed=2021-10-21T07:55:10.937Z&quot;
        },
        {
            &quot;name&quot;: &quot;Sec-Fetch-Dest&quot;,
            &quot;value&quot;: &quot;image&quot;
        },
        {
            &quot;name&quot;: &quot;Sec-Fetch-Mode&quot;,
            &quot;value&quot;: &quot;no-cors&quot;
        },
        {
            &quot;name&quot;: &quot;Sec-Fetch-Site&quot;,
            &quot;value&quot;: &quot;same-origin&quot;
        },
        {
            &quot;name&quot;: &quot;Pragma&quot;,
            &quot;value&quot;: &quot;no-cache&quot;
        },
        {
            &quot;name&quot;: &quot;Cache-Control&quot;,
            &quot;value&quot;: &quot;no-cache&quot;
        }
    ],
    &quot;args&quot;: &quot;&quot;,
    &quot;httpVersion&quot;: &quot;HTTP/1.1&quot;
}
</code></pre>
<p>What I am trying to do is parse the <code>headers</code> section so that I can create a new facet for the <code>headers.Host</code> value. Another thing to note is that the number of objects in this <code>headers</code> array can change, ie sometimes there will be a <code>X-Forwarder-for</code> header as an example.</p>
<p>To create a new attribute I've tried to use the String builder processor - but it looks like that won't work on an array. I've tried having a target value of <code>%{httpRequest.headers.Host}</code> but that returns a null value.</p>",,1,0,,2021-12-14 21:00:16,,2021-12-14 21:08:11,2021-12-14 21:06:20,,10478578,,10478578,,1,0,datadog,28,
258815,1,Method,70363446,How to add Faraday request body into Datadog tracing in a Rails app,"<p>I'm trying to configure Faraday tracing in a Rails application using Datadog.</p>
<p>I've set up the Faraday -&gt; Datadog connection:</p>
<pre><code>require 'faraday'
require 'ddtrace'

Datadog.configure do |c|
  c.use :faraday
end

Faraday.post(&quot;http://httpstat.us/200&quot;, {foo: 1, bar: 2}.to_json)
Faraday.get(&quot;http://httpstat.us/201?foo=1&amp;bar=2&quot;)
</code></pre>
<p>It works well, the requests are being logged to Datadog.</p>
<p>But those logs do not contain any request parameters, nevertheless GET or POST.</p>
<p><img src=""https://i.stack.imgur.com/BjQtO.png"" alt=""GET request log"" />  <img src=""https://i.stack.imgur.com/izvZO.png"" alt=""POST request log"" /></p>
<p>Any adviсe on how to get the request params/body logged to Datadog?</p>",70369006,1,2,,2021-12-15 12:07:59,1,2021-12-17 17:01:24,2021-12-17 17:01:24,,6226962,,6226962,,1,2,ruby-on-rails|trace|datadog|faraday|apm,213,
258816,3,Visualization,70387904,"Showing ""Last Deploy"" in DataDog Dashboard","<p>In the Datadog Services View, there's a notion of last deploy:</p>
<p><a href=""https://i.stack.imgur.com/0C8d2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0C8d2.png"" alt=""enter image description here"" /></a></p>
<p>Anyone know how I canshow that value in a Dashboard for a given service?</p>
<p>Thanks!</p>",,0,0,,2021-12-17 02:52:38,,2021-12-17 02:52:38,,,,,286485,,1,0,datadog,9,
258817,1,Method,70400632,Is there an API for datadog's agent to send series?,"<p>I can send data to datadog directly or through their agent. However I can't find how to use the agent to send time series (datadog's <a href=""https://docs.datadoghq.com/api/latest/metrics/#submit-metrics"" rel=""nofollow noreferrer"">doc</a> - api/v1/series endpoint)</p>
<p>Is there a way to do this through the agent? (The agent adds some extra tags, and I'd like to reuse that rather than a direct API call)</p>",,0,0,,2021-12-18 02:44:56,,2021-12-18 02:44:56,,,,,2210667,,1,0,datadog,16,
258818,3,Visualization,70458324,Datadog display logs after extracting,"<p>so I have a pipeline with an ngork key value parse rule and it successfully parses out <code>key=value</code> from the logs into the metdata. However, how do I now display the rest of the log message after all of that has been parsed out in the main log view? It's still showing the original log messages with all the kv pairs making it super hard to scan/read.</p>",,0,0,,2021-12-23 06:15:55,,2021-12-23 06:15:55,,,,,798055,,1,0,datadog,9,
258819,0,Integration,70462760,golang gofiber framework demo in k8s with Datadog APM integration --how to add tracer and profiler?,"<p>I have a small proof-of-concept project to add DataDog APM/tracing capabilities to a gofiber (<a href=""https://github.com/gofiber"" rel=""nofollow noreferrer"">https://github.com/gofiber</a>) web app. The app is up and running in an EKS environment which already has strong DataDog integration (agent, APM enabled for entire cluster, etc).</p>
<p>I am still learning the ropes with gofiber.  My question is, what is the simplest and most efficient way to add the tracer and profile to my project?</p>
<p>DataDog is recommending these two packages:</p>
<pre><code>go get gopkg.in/DataDog/dd-trace-go.v1/ddtrace/tracer
go get gopkg.in/DataDog/dd-trace-go.v1/profiler
</code></pre>
<p>Currently I have a simple <code>main.go</code> file serving &quot;Hello World&quot; at /, using one of the gofiber recipes.</p>
<p>Can I add the tracer and profile as separate functions in the same file or should I have separate files for these in my project?</p>
<p>Definitely trying to avoid running an entirely separate container in my pod for this tracing capability.  Thanks for any advice or suggestions.</p>",,0,1,,2021-12-23 13:25:45,,2021-12-23 13:25:45,,,,,9820773,,1,2,go|kubernetes|amazon-eks|datadog|go-fiber,72,
258820,1,Method,70487976,Datadog: How to get a user's user_id using Datadog python library?,"<p>I'd like to run through a list of Datadog users and change their role from Standard role to Read-only.</p>
<p>Both when I run:</p>
<pre><code>api.User.get('some users email')
</code></pre>
<p>and</p>
<pre><code>api.User.get_all()
</code></pre>
<p>The user_id of a user is not displayed and that is the parameter that I need in order to run my script which will run through and change 95 users role.</p>
<p>The only way I found to display a user's user_id, is by going in the UI to Organization settings -&gt; Users -&gt; select a specific user and then the URL displays <code>user_id=47f261a0-fff9-11ab-bccc-da7ad0904453</code></p>
<p>Does anybody know a possibly api-related way to extract a user's user_id from Datadog?</p>",,0,0,,2021-12-26 16:16:40,,2021-12-26 16:16:40,,,,,1702942,,1,0,python|datadog,13,
258821,0,Integration,70507276,Integrating Apache beam with Datadog,<p>I want to monitor an apache beam pipeline through the Datadog. How can I integrate Datadog with the apache beam?</p>,,0,1,,2021-12-28 12:45:22,,2021-12-28 14:10:47,2021-12-28 14:10:47,,13447,,8279787,,1,0,apache-beam|datadog,31,
258822,0,Configuration,70520035,Terraform-Datadog: Locate index inside a map element,"<p>I have created an monitors and slo resources that iterate through a map and creates several similar objects.
Now I am trying to create a dashboard and would like to pin point to the specific map index to find the slo resource. Example:</p>
<p>I have the bellow map:</p>
<pre><code>example_map = {
    example_key0 = &quot;example_value0&quot;
    example_key1 = &quot;example_value1&quot;
    example_key2 = &quot;example_value2&quot;
}
</code></pre>
<p>I created the bellow slo's(monitors created in similar way):</p>
<pre><code>resource &quot;datadog_service_level_objective&quot; &quot;example_SLO&quot; {
  count = length(var.example_map)
  name               = &quot;example_slo_${keys(var.example_map)[count.index]}&quot;
  type               = &quot;monitor&quot;
  description        = &quot;${keys(var.example_map)[count.index]} metric SLO&quot;
  monitor_ids        = [&quot;${datadog_monitor.example_monitor[count.index].id}&quot;]
  thresholds {
    timeframe = &quot;90d&quot;
    target = 99
    warning = 99.9
  }
  tags = [&quot;type:slo_alerts&quot;]
}
</code></pre>
<p>Each of these resources is then identified by the index(example_SLO[0], example_SLO[1],etc..)
On the dashboard i would like to pin point to the example key to get it's index. The bellow fails but I hope this example explains what i am to achieve:</p>
<pre><code>widget {
      service_level_objective_definition {
        title = &quot;example SLO&quot;
        view_type = &quot;detail&quot;
        slo_id = datadog_service_level_objective.example_SLO[${index(keys(var.example_map), example_key2)}].id
        show_error_budget = true
        view_mode = &quot;overall&quot;
        time_windows = [
          &quot;7d&quot;,
          &quot;30d&quot;,
          &quot;previous_month&quot;]
      }
    }
</code></pre>
<p>Else is it possible to create a <code>for_each</code> or <code>count</code> argument for a widget? Create several objects from the same widget.</p>
<p>Thank you very much in advance for any help as this has become a roadblock in my attempt to reduce the configurations through map iteration. Which brings several levels of complexity.</p>",,1,0,,2021-12-29 13:21:34,,2021-12-29 14:15:29,,,,,17788915,,1,1,terraform|datadog,42,
258823,0,Configuration,70536077,How to enable StatsD in Spring Boot?,"<p>I have Spring Boot 2.5.2 and I am trying to send StatsD metrics to Datadog. However, that didn't work for me. So I wrote a single python script to check if my application is sending any data or not.</p>
<p>application.properties</p>
<pre><code>management.metrics.export.statsd.enabled=true
management.metrics.export.statsd.host=localhost
management.metrics.export.statsd.port=8125
</code></pre>
<p>Python script</p>
<pre><code>from socket import *
serverName = 'localhost'
serverPort = 12000
serverSocket = socket(AF_INET, SOCK_DGRAM)
serverSocket.bind((serverName, serverPort))
print(&quot;Server is ready.&quot;)


while True:
    msg, clientAddr = serverSocket.recvfrom(2048)
    word = msg.decode()
    print(word)
    msg = ''
    serverSocket.sendto(msg.encode(), clientAddr)
</code></pre>
<p>I tested this script with the following command:</p>
<pre><code>$ echo &quot;foo:1|c&quot; | nc -u -w0 127.0.0.1 12000
</code></pre>
<p>and it was logged by the script.</p>
<p>I changed <code>management.metrics.export.statsd.port</code> to 12000, but nothing was being sent. Is there a way to diagnose this?</p>",,0,3,,2021-12-30 19:50:22,,2021-12-30 19:50:22,,,,,4777670,,1,1,spring-boot|datadog|micrometer|statsd|spring-micrometer,67,
261329,1,Method,62042278,How to determine the time Node.js spends to send an HTTP response body?,"<p>My current setup involves a Node.js web application using Express.js.<br>
I am using DataDog's dd-tracer to measure the time Node.js spends for particular method invocations as part of my APM solution.</p>

<p>I would like to know if it is possible to measure the portion of time an incoming HTTP request is busy sending data back to the client as HTTP response body.</p>

<p>Are there any pitfalls or inaccuracies involved when trying to do this kind of instrumentation?<br>
Does anybody know why this is not measured by APM client libraries by default?</p>",62047980,4,0,,2020-5-27 11:42:39,,2020-6-5 05:01:37,,,,,10473469,,1,7,node.js|express|instrumentation|datadog|apm,992,29.586
261330,2,Query,63650568,How to search Datadog logs by Attribute,"<p>Question about searching logs in Datadog.</p>
<p>Search works on regular strings in the CONTENT portion of the log.  However, if JSON is passed to the CONTENT portion, the JSON elements are automatically parsed into Attributes.  But the Attributes are NOT searchable.</p>
<p>How do I search for logs by Attribute?</p>
<p>It seems like a step backwards to supply log data in JSON to improve indexing, but then LOSE the ability to search on those elements.</p>",,2,0,,2020-8-29 18:39:32,,2021-4-1 18:13:01,,,,,155963,,1,7,datadog,4789,26.521
261331,2,Query,57918254,Datadog: Use a tag value in an alias,"<p>I have a timeseries graph in a time board that displays data for one metric that has multiple tags called ""page"". The graph has one line for each tag and I'm running functions on the values, so the query for my data is ""ewma_5(avg:client.load_time{env:prod}) by {page}"". This query means the tooltip values when I hover on the graph are things like ""ewma_5(avg:client.load_time{env:prod})"".</p>

<p>I want to know if there is anyway to use the alias function with the tag value in it, so something like ""alias"": ""{page}""?</p>",59791963,1,3,,2019-9-13 06:20:08,3,2020-1-17 17:12:16,,,,,1787315,,1,8,statsd|datadog,1140,24.8276
261332,1,Parse,62092243,Datadog Grok Parsing - extracting fields from nested JSON,"<p>Is it possible to extract json fields that are nested inside a log?</p>

<p>Sample I've been work on:</p>

<pre><code>thread-191555 app.main - [cid: 2cacd6f9-546d-41ew-a7ce-d5d41b39eb8f, uid: e6ffc3b0-2f39-44f7-85b6-1abf5f9ad970] Request: protocol=[HTTP/1.0] method=[POST] path=[/metrics] headers=[Timeout-Access: &lt;function1&gt;, Remote-Address: 192.168.0.1:37936, Host: app:5000, Connection: close, X-Real-Ip: 192.168.1.1, X-Forwarded-For: 192.168.1.1, Authorization: ***, Accept: application/json, text/plain, */*, Referer: https://google.com, Accept-Language: cs-CZ, Accept-Encoding: gzip, deflate, User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko, Cache-Control: no-cache] entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }
</code></pre>

<p>what I wanted to achieve was:</p>

<pre><code>{
""extract"": ""text"",
""duration"": ""451""
}
</code></pre>

<p>I tried to combine a sample regex (<code>""(extract)""\s*:\s*""([^""]+)"",?</code>) with <code>example_parser %{data::json}</code> (using the JSON as a log sample data, for starters) but I haven't managed to get anything working.</p>

<p>Thanks in advance!</p>",62096791,1,0,,2020-5-29 18:10:52,,2020-5-30 01:32:36,,,,,7827582,,1,4,json|logging|datadog,4125,21.2617
261333,1,Error,57402172,How to log output from Airflow DAG for debugging?,"<p>I am writing a Airflow DAG and having some problems with a function. I am trying to debug by printing data to stdout and using the <code>logging</code> library.</p>
<p>My example DAG is:</p>
<pre class=""lang-py prettyprint-override""><code>    from datetime import timedelta
    
    import airflow
    import logging
    
    from airflow.models import DAG
    from airflow.operators.dummy_operator import DummyOperator
    from airflow.contrib.hooks.datadog_hook import DatadogHook
    
    def datadog_event(title, text, dag_id, task_id):
        hook = DatadogHook()
        tags = [
            f'dag:{dag_id}',
            f'task:{task_id}',
        ]
    
        hook.post_event(title, text, tags)
    
    def datadog_event_success(context):
        dag_id = context['task_instance'].dag_id
        task_id = context['task_instance'].task_id
        text = f'Airflow DAG failure for {dag_id}\n\nDAG: {dag_id}\nTasks: {task_id}'
        title = f'Airflow DAG success for {dag_id}'
    
        logging.info(title)
        logging.info(text)
        logging.info(dag_id)
        logging.info(task_id)
    
        datadog_event(title, text, dag_id, task_id)
    
    args = {
        'owner': 'airflow',
        'start_date': airflow.utils.dates.days_ago(2),
    }
    
    dag = DAG(
        dag_id='example_callback',
        default_args=args,
        schedule_interval='*/5 * * * *',
        dagrun_timeout=timedelta(minutes=60),
        on_success_callback=datadog_event_success,
    )
    
    my_task = DummyOperator(
        task_id='run_this_last',
        dag=dag,
    )
</code></pre>
<p>During a run I get an error:</p>
<pre><code>airflow[9490]: Process DagFileProcessor4195-Process:
airflow[9490]: Traceback (most recent call last):
airflow[9490]:   File &quot;/usr/lib/python3.6/multiprocessing/process.py&quot;, line 258, in _bootstrap
airflow[9490]:     self.run()
airflow[9490]:   File &quot;/usr/lib/python3.6/multiprocessing/process.py&quot;, line 93, in run
airflow[9490]:     self._target(*self._args, **self._kwargs)
airflow[9490]:   File &quot;/home/airflow/virtualenv/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py&quot;, line 148, in _run_file_processor
airflow[9490]:     result = scheduler_job.process_file(file_path, pickle_dags)
airflow[9490]:   File &quot;/home/airflow/virtualenv/lib/python3.6/site-packages/airflow/utils/db.py&quot;, line 74, in wrapper
airflow[9490]:     return func(*args, **kwargs)
airflow[9490]:   File &quot;/home/airflow/virtualenv/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py&quot;, line 1542, in process_file
airflow[9490]:     self._process_dags(dagbag, dags, ti_keys_to_schedule)
airflow[9490]:   File &quot;/home/airflow/virtualenv/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py&quot;, line 1239, in _process_dags
airflow[9490]:     self._process_task_instances(dag, tis_out)
airflow[9490]:   File &quot;/home/airflow/virtualenv/lib/python3.6/site-packages/airflow/utils/db.py&quot;, line 74, in wrapper
airflow[9490]:     return func(*args, **kwargs)
airflow[9490]:   File &quot;/home/airflow/virtualenv/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py&quot;, line 732, in _process_task_instances
airflow[9490]:     run.update_state(session=session)
airflow[9490]:   File &quot;/home/airflow/virtualenv/lib/python3.6/site-packages/airflow/utils/db.py&quot;, line 70, in wrapper
airflow[9490]:     return func(*args, **kwargs)
airflow[9490]:   File &quot;/home/airflow/virtualenv/lib/python3.6/site-packages/airflow/models/dagrun.py&quot;, line 318, in update_state
airflow[9490]:     dag.handle_callback(self, success=True, reason='success', session=session)
airflow[9490]:   File &quot;/home/airflow/virtualenv/lib/python3.6/site-packages/airflow/utils/db.py&quot;, line 70, in wrapper
airflow[9490]:     return func(*args, **kwargs)
airflow[9490]:   File &quot;/home/airflow/virtualenv/lib/python3.6/site-packages/airflow/models/dag.py&quot;, line 620, in handle_callback
airflow[9490]:     callback(context)
airflow[9490]:   File &quot;/home/airflow/analytics/etl_v2/airflow_data/dags/example_bash_operator_andy.py&quot;, line 68, in datadog_event_success
airflow[9490]:     datadog_event(title, text, dag_id, task_id)
airflow[9490]:   File &quot;/home/airflow/analytics/etl_v2/airflow_data/dags/example_bash_operator_andy.py&quot;, line 45, in datadog_event
airflow[9490]:     hook.post_event(title, text, tags)
airflow[9490]:   File &quot;/home/airflow/virtualenv/lib/python3.6/site-packages/airflow/contrib/hooks/datadog_hook.py&quot;, line 157, in post_event
airflow[9490]:     self.validate_response(response)
airflow[9490]:   File &quot;/home/airflow/virtualenv/lib/python3.6/site-packages/airflow/contrib/hooks/datadog_hook.py&quot;, line 58, in validate_response
airflow[9490]:     if response['status'] != 'ok':
airflow[9490]: KeyError: 'status'
</code></pre>
<p>But none of my logged into is before or after the error in the scheduler, webserver, worker, or task logs.</p>
<p>I have tested the <code>datadog_event</code> call on my Airflow worker by manually importing the code and it logs properly when I run it that way:</p>
<pre><code>airflow@airflow-worker-0:~/analytics$ /home/airflow/virtualenv/bin/python -i /home/airflow/analytics/etl_v2/airflow_data/dags/example_bash_operator_andy.py
[2019-08-07 20:48:01,890] {settings.py:213} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=29941
[2019-08-07 20:48:02,227] {__init__.py:51} INFO - Using executor DaskExecutor

&gt;&gt;&gt; datadog_event('My title', 'My task', 'example_bash_operator_andy', 'run_this_last')
[2019-08-07 20:51:17,542] {datadog_hook.py:54} INFO - Setting up api keys for Datadog
[2019-08-07 20:51:17,544] {example_bash_operator_andy.py:38} INFO - My title
[2019-08-07 20:51:17,544] {example_bash_operator_andy.py:39} INFO - My task
[2019-08-07 20:51:17,544] {example_bash_operator_andy.py:40} INFO - example_bash_operator_andy
[2019-08-07 20:51:17,545] {example_bash_operator_andy.py:41} INFO - run_this_last
[2019-08-07 20:51:17,658] {api_client.py:139} INFO - 202 POST https://api.datadoghq.com/api/v1/events (113.2174ms)
</code></pre>
<p>My <code>airflow.cfg</code> is posted at <a href=""https://gist.github.com/andyshinn/d743ddc61956ed7440c500fca962ce92"" rel=""nofollow noreferrer"">https://gist.github.com/andyshinn/d743ddc61956ed7440c500fca962ce92</a> and I am using Airflow 1.10.4.</p>
<p>How can I output logging or messages from the DAG itself to better debug what might be happening?</p>",,1,8,,2019-8-7 20:54:34,1,2021-4-6 14:38:55,2021-4-6 14:38:55,,5151861,,684908,,1,7,airflow|datadog,6667,20.6957
261334,0,Configuration,51474223,Kubernetes pods restart issue anomaly,"<p>My Java microservices are running in k8s cluster hosted on AWS EC2 instances.</p>

<p>I have around 30 microservice(a good mix of nodejs and Java 8) running in a K8s cluster. I am facing a challange where my java application pods gets restart unexpectedly which leads to increase in application 5xx count.</p>

<p>To debug this, I started a newrelic agent in pod along with application and found the following graph:</p>

<p><a href=""https://i.stack.imgur.com/lSUfE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lSUfE.png"" alt=""enter image description here""></a></p>

<p>Where I can see that, I have Xmx value as 6GB and my uses is max 5.2GB.</p>

<p>This clearly stats that JVM is not crossing the Xmx value.</p>

<p>But when I describe the pod and look for last state it says ""Reason:Error"" with ""Exit code: 137""</p>

<p><a href=""https://i.stack.imgur.com/PPHVw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PPHVw.png"" alt=""enter image description here""></a></p>

<p>Then on further investigation I find that my Pod average memory uses is close to its limit all the time.(Allocated 9Gib, uses ~9Gib). I am not able to understand why memory uses is so high in Pod even thogh I have only one process running((JVM) and that too is restricted with 6Gib Xmx.</p>

<p><a href=""https://i.stack.imgur.com/Aw06D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Aw06D.png"" alt=""enter image description here""></a> </p>

<p>When I login to my worker nodes and check the status of docker containers I can see the last container of that appriction with Exited state and says ""Container exits with non-zero exit code 137""</p>

<p>I can see the wokernode kernel logs as:</p>

<p><a href=""https://i.stack.imgur.com/LtDL0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LtDL0.png"" alt=""enter image description here""></a></p>

<p>which shows kernel is terminitaing my process running inside container.</p>

<p>I can see I have lot of free memory in my worker node.</p>

<p><a href=""https://i.stack.imgur.com/R9uLY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R9uLY.png"" alt=""enter image description here""></a></p>

<p>I am not sure why my pods get restart again and again is this k8s behaviour or something spoofy in my infrastructure. This force me to move my application from Container to VM again as this leades to increase in 5xx count.</p>

<p>EDIT: I am getting OOM after increasing memory to 12GB.</p>

<p><a href=""https://i.stack.imgur.com/EWPrR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EWPrR.png"" alt=""enter image description here""></a></p>

<p>I am not getting sure why POD is getting killed because of OOM th
ough JVM xmx is 6 GB only.</p>

<p>Need help!</p>",,3,3,,2018-7-23 08:08:06,0,2019-4-24 15:31:21,2018-7-26 10:18:54,,4774816,,4774816,,1,3,docker|linux-kernel|kubernetes|newrelic|datadog,3505,19.9788
261335,2,Query,61521576,DataDog Log Search empty string facet,"<p>On DataDog log search, I want to search for logs with empty string for a specific facet, e.g. logs with userId is empty. <code>@userId:''</code> , <code>@userId:""""</code>, <code>-@userId:*</code> non worked.</p>",,1,0,,2020-4-30 10:43:22,1,2020-4-30 13:28:56,,,,,944768,,1,4,datadog,1078,19.9305
261336,0,Integration,53880368,How to use Datadog agent in Azure App Service?,"<p>I'm running web apps as Docker containers in Azure App Service. I'd like to add Datadog agent to each container to, e.g., read the log files in the background and post them to Datadog log management. This is what I have tried:</p>

<p>1) Installing Datadog agent as extension as described in <a href=""https://www.datadoghq.com/blog/monitor-azure-vms-using-datadog/#install-the-datadog-agent-on-an-azure-vm"" rel=""nofollow noreferrer"">this post</a>. This option does not seem to be available for App Service apps, only on VMs.</p>

<p>2) Using multi-container apps as described <a href=""https://docs.microsoft.com/en-us/azure/app-service/containers/tutorial-multi-container-app"" rel=""nofollow noreferrer"">in this post</a>. However, we have not found a simple way to integrate this with <a href=""https://docs.microsoft.com/en-us/azure/devops/pipelines/apps/cd/deploy-docker-webapp?view=vsts"" rel=""nofollow noreferrer"">Azure DevOps release pipelines</a>. I guess it might be possible to create a custom deployment task wrapping Azure CLI commands?</p>

<p>3) Including Datadog agent into our Dockerfiles by following how Datadog Dockerfiles <a href=""https://github.com/DataDog/datadog-agent/tree/master/Dockerfiles/agent"" rel=""nofollow noreferrer"">are built</a>. The process seems quite complicated and add lots of extra dependencies to our Dockerfile. We'd also not like to inherit our Dockerfiles from Datadog Dockerfile with <code>FROM datadog/agent</code>.</p>

<p>I'd assume this must be a pretty standard problem for Azure+Datadog users. Any ideas what's the cleanest option?</p>",54194264,5,0,,2018-12-21 06:50:36,1,2019-10-24 23:03:41,,,,,10561443,,1,4,azure|azure-devops|azure-web-app-service|datadog,1703,19.9249
261337,2,Query,55848522,query metrics on tag value with regex in datadog,"<p>I want to filter metrics on tag value with a regex. I can do it in Prometheus but I could not find an equivalent way in Datadog. </p>

<p>For example, to select the following metric whose <code>status</code> tag value starts with <code>2</code>, I can use the query <code>http.server.requests.count{status=~""^2..$""}</code></p>

<p>I have the same metric with the same tags in Datadog too, but couldn't find a way to have the same query.</p>",,1,6,,2019-4-25 11:42:47,2,2021-3-29 13:43:12,,,,,672798,,1,16,monitoring|datadog,4438,19.7887
261338,1,Error,63814864,"Micrometer-springboot: Hikaricp , Tomcat and jdbc metrics are not exported to DataDog","<p>Hikaricp , Tomcat and jdbc metrics are not being exported to DataDog</p>
<p>we have setup springboot app to push the metrics to datadoghq, it does export 60 metrics, however the metrics like hikaricp, tomcat and jdbc are missing.</p>
<p>hikaricp, tomcat and jdbc - these mertics are listed under <code>/actuator/metrics</code> endpoint, but not exported to datadog.</p>
<pre><code>springBootVersion = '2.3.3.RELEASE'
springCloudVersion = 'Hoxton.SR7'
implementation 'io.micrometer:micrometer-registry-datadog:latest.release'
</code></pre>
<p>Is there any additional settings required to push hikaricp, tomcat and jdbc metrics ?</p>",63835825,2,4,,2020-9-9 15:28:38,,2020-9-10 19:25:48,,,,,8534030,,1,2,spring-boot|spring-boot-actuator|datadog|micrometer|spring-micrometer,1285,19.2356
261339,0,Configuration,62436021,Unable to detect the kubelet URL automatically : datadog_checks.base.errors.CheckException,"<p>I set up datadog trace client in my kubernetes cluster to monitor my deployed application. It was working fine with the kubernetes version 1.15x but as soon as I upgraded the version to 1.16x, the service itself is not showing in the Datadog Dashboard.</p>

<p>Currently using:</p>

<ol>
<li><p>Kubernetes 1.16.9 </p></li>
<li><p>Datadog 0.52.0</p></li>
</ol>

<p>When checked for agent status. It is giving following exception :</p>

<pre><code>Instance ID: kubelet:xxxxxxxxxxxxx [ERROR]
      Configuration Source: file:/etc/datadog-agent/conf.d/kubelet.d/conf.yaml.default
      Total Runs: 12,453
      Metric Samples: Last Run: 0, Total: 0
      Events: Last Run: 0, Total: 0
      Service Checks: Last Run: 0, Total: 0
      Average Execution Time : 5ms
      Last Execution Date : 2020-06-19 15:18:19.000000 UTC
      Last Successful Execution Date : Never
      Error: Unable to detect the kubelet URL automatically.
      Traceback (most recent call last):
        File ""/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/base/checks/base.py"", line 822, in run
          self.check(instance)
        File ""/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/kubelet/kubelet.py"", line 297, in check
          raise CheckException(""Unable to detect the kubelet URL automatically."")
      datadog_checks.base.errors.CheckException: Unable to detect the kubelet URL automatically.
</code></pre>

<p>This looks like a version issue to me. If it is which Datadog version I need to use for monitoring?</p>",62680072,2,4,,2020-6-17 18:33:30,1,2021-4-15 08:08:06,2020-7-1 15:48:34,,11921495,,11921495,,1,3,java|azure|docker|kubernetes|datadog,1654,19.0741
261340,1,Method,58657063,How do I get TotalStorageSpace or UsedStorageSpace metric from AWS RDS?,"<p>I see that AWS RDS provides a <code>FreeStorageSpace</code> metric for monitoring disk usage. Now I am trying to create a <strong>generic</strong> pre-emptive alert for all my RDS but setting up an ideal threshold on <code>FreeStorageSpace</code> is not making sense. </p>

<p>For example, 20G might be a good threshold with RDS having total disk space as 100G but might be misleading for a RDS with total disk space of 40G.</p>

<p>So I was wondering if there is a way to get <code>TotalStorageSpace</code> or <code>UsedStorageSpace</code> metric from RDS (<s>directly</s> or indirectly).</p>

<p><strong>Update</strong></p>

<p>Since the fact is established that <code>FreeStorageSpace</code> is the only metric RDS provides related to disk storage, any ideas on if / how we can we build a custom metric for <code>TotalStorageSpace</code> or <code>UsedStorageSpace</code>?</p>

<p>p.s.: Creating separate alarms for each RDS for evaluating disk usage percentage seems such waste of time and resource. </p>",,3,0,,2019-11-1 09:45:42,1,2020-6-24 16:25:51,2019-11-1 11:57:44,,1060337,,1060337,,1,5,amazon-web-services|alert|amazon-rds|datadog,1852,19.0706
261341,3,Visualization,53435248,Can I export Datadog dashboards via Datadog REST API?,"<p>Is it possible to export or download Datadog dashboards via Datadog REST API? </p>

<p>Export and update of Datadog Monitors works fine. I need the same functionality for dashboards.</p>",53449880,1,1,,2018-11-22 16:42:39,1,2021-10-23 21:02:53,2019-7-28 09:02:44,,992887,,1099819,,1,4,export|backup|datadog,2064,19.0588
261342,1,Parse,61492210,How to stringify JSON using JQ,"<p>Using JQ I would like to take a complex JSON object that includes JSON embedded as strings and then turn it all into a valid string I can easily embed in other JSON objects. </p>

<p>For example, lets say I have this json object:</p>

<pre><code>{
  ""region"": ""CA"",
  ""waf_rule_tags"": ""{\""RULEID:942100\"":[\""application-multi\"",\""language-multi\"",\""platform-multi\"",\""attack-sqli\"",\""OWASP_CRS/WEB_ATTACK/SQL_INJECTION\"",\""WASCTC/WASC-19\"",\""OWASP_TOP_10/A1\"",\""OWASP_AppSensor/CIE1\"",\""PCI/6.5.2\""]}""
}
</code></pre>

<p>I need to turn this all into the following string:</p>

<pre><code>""{\""region\"": \""CA\"",\""waf_rule_tags\"": \""{\\\""RULEID:942100\\\"":[\\\""application-multi\\\"",\\\""language-multi\\\"",\\\""platform-multi\\\"",\\\""attack-sqli\\\"",\\\""OWASP_CRS/WEB_ATTACK/SQL_INJECTION\\\"",\\\""WASCTC/WASC-19\\\"",\\\""OWASP_TOP_10/A1\\\"",\\\""OWASP_AppSensor/CIE1\\\"",\\\""PCI/6.5.2\\\""]}\""}""
</code></pre>

<p>That way I can take this string and insert it exactly under the <code>text</code> field of another JSON object to create the following. </p>

<pre><code>{
      ""title"": ""12345-accesslogs"",
      ""text"": ""{\""region\"": \""CA\"",\""waf_rule_tags\"": \""{\\\""RULEID:942100\\\"":[\\\""application-multi\\\"",\\\""language-multi\\\"",\\\""platform-multi\\\"",\\\""attack-sqli\\\"",\\\""OWASP_CRS/WEB_ATTACK/SQL_INJECTION\\\"",\\\""WASCTC/WASC-19\\\"",\\\""OWASP_TOP_10/A1\\\"",\\\""OWASP_AppSensor/CIE1\\\"",\\\""PCI/6.5.2\\\""]}\""}"",
      ""priority"": ""normal"",
      ""tags"": [""environment:test""],
      ""alert_type"": ""info""
}
</code></pre>",61492758,2,0,,2020-4-29 00:25:32,,2021-7-17 09:44:20,,,,,5314903,,1,3,json|jq|datadog,2005,18.4085
261343,3,Monitoring,55794663,How to set APM service name in DataDog for an application using Nodejs,"<p>I am not able to see traces for my application under APM --> Service in Datadog. I found some sample code from Datadog docs but don't know exactly where it should go inside my application. Please let me know if anyone has any idea regarding it. </p>

<p>I have already tried with following code in my js file. My application is based on node js which is serverless.</p>

<pre><code>const tracer = require('dd-trace').init();
tracer.use('http', {
    service: 'test'
});
</code></pre>

<p>I have also added dependencies for dd-trace in package.json as <code>""dd-trace"": ""^0.11.0""</code></p>

<p>I expected to list my application with proper name in APM Services in Datadog.</p>",,1,0,,2019-4-22 12:45:56,,2019-4-25 13:15:12,2019-4-22 16:35:20,,9348748,,7945853,,1,1,datadog,1939,18.3503
261344,3,Monitoring,55187016,Datadog - Monitoring multiple applications in the same site hosted by IIS,"<p>I'm trying to monitor several applications within the same site in IIS.
With just running the <code>msi</code> of the tracer <a href=""https://github.com/DataDog/dd-trace-dotnet/releases/tag/v0.7.1-beta"" rel=""nofollow noreferrer"">dd-trace-dotnet</a>, I started to see the events, but these are registered as <code>[site name]/[application]</code> e.g <code>default_web_site/docs_webhook</code><br>
I would love to be able to logs them under a custom service name for each application, but according to the <a href=""https://docs.datadoghq.com/tracing/languages/dotnet/?tab=netframeworkonwindows"" rel=""nofollow noreferrer"">documentation</a>, this is only possible at the site level.<br>
Manual instrumentation is described for windows services, setting the environment variable <code>DD_SERVICE_NAME</code> in the registry entry <code>HKLM\System\CurrentControlSet\Services\{service name}\Environment</code> is enough, but does not apply to IIS applications.</p>

<p>NOTE: Creating separate sites for each application is not an option right now.</p>",55188839,2,0,,2019-3-15 16:34:10,,2019-4-5 22:12:12,2019-4-5 22:12:12,,24231,,553029,,1,0,asp.net|iis|trace|datadog|apm,1828,18.0479
261345,0,Configuration,52390678,Sending ddtrace from docker,"<p>I'm trying to learn how to use docker and am having some troubles. I'm using a <code>docker-compose.yaml</code> file for running a python script that connects to a mysql container and I'm trying to use <code>ddtrace</code> to send traces to datadog. I'm using the following image from <a href=""https://github.com/DataDog/dd-trace-py/blob/master/docker-compose.yml"" rel=""noreferrer"">this github page from datadog</a></p>

<pre><code>ddagent:
      image: datadog/docker-dd-agent
      environment:
          - DD_BIND_HOST=0.0.0.0
          - DD_API_KEY=invalid_key_but_this_is_fine
      ports:
          - ""127.0.0.1:8126:8126""
</code></pre>

<p>And my <code>docker-compose.yaml</code> looks like</p>

<pre><code>version: ""3""
services:
    ddtrace-test:
        build: .
        volumes:
          - "".:/app""
        links:
          - ddagent

    ddagent:
          image: datadog/docker-dd-agent
          environment:
              - DD_BIND_HOST=0.0.0.0
              - DD_API_KEY=&lt;my key&gt;
          ports:
              - ""127.0.0.1:8126:8126""
</code></pre>

<p>So then I'm running the command <code>docker-compose run --rm ddtrace-test python test.py</code>, where <code>test.py</code> looks like</p>

<pre><code>from ddtrace import tracer

@tracer.wrap('test', 'test')
def foo():
    print('running foo')

foo()
</code></pre>

<p>And when I run the command, I'm returned with</p>

<pre><code>Starting service---reprocess_ddagent_1 ... done
foo
cannot send spans to localhost:8126: [Errno 99] Cannot assign requested address
</code></pre>

<p>I'm not sure what this error means. When I use my key and run from local instead of over a docker image, it works fine. What could be going wrong here?</p>",52392480,1,0,,2018-9-18 16:11:08,,2018-9-18 18:18:14,,,,,4498684,,1,5,python|docker|datadog,3087,17.9581
261346,0,Configuration,59177043,How to set cluster name in DataDog Helm chart,"<p>I'm using the DataDog Helm chart to install the DataDog agent on my EKS Kubernetes clusters (<a href=""https://github.com/helm/charts/tree/master/stable/datadog"" rel=""nofollow noreferrer"">https://github.com/helm/charts/tree/master/stable/datadog</a>). The problem I'm having now is that I am not able to filter logs by cluster name. I have also set the <code>DD_CLUSTER_NAME</code> environment variable but it does not seem to do anything.</p>

<p>I have set the following in my values.yml file:</p>

<pre><code>datadog:
  site: datadoghq.com
  logLevel: ERROR

  logsEnabled: true
  logsConfigContainerCollectAll: true
  processAgentEnabled: true
  apmEnabled: true
  nonLocalTraffic: true
  leaderElection: true
  collectEvents: true

  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 500Mi

  nodeLabelsAsTags:
    beta.kubernetes.io/instance-type: aws_instance_type
    kubernetes.io/role: kube_role
  podAnnotationsAsTags:
    iam.amazonaws.com/role: kube_iamrole
  podLabelsAsTags:
    app: kube_app
    release: helm_release

clusterAgent:
  enabled: true
</code></pre>",,2,0,,2019-12-4 13:20:57,,2019-12-4 14:24:18,,,,,2277437,,1,0,kubernetes|kubernetes-helm|amazon-eks|datadog,954,17.9182
261347,0,Configuration,60491872,"Error: UPGRADE FAILED: failed to create resource: ConfigMap in version ""v1"" cannot be handled as a ConfigMap","<p>Per <a href=""https://github.com/DataDog/integrations-core/blob/master/kube_scheduler/datadog_checks/kube_scheduler/data/conf.yaml.example"" rel=""nofollow noreferrer"">this spec</a> on github and these <a href=""https://helm.sh/docs/intro/using_helm/#the-format-and-limitations-of-set"" rel=""nofollow noreferrer"">helm instructions</a>  I'm trying to upgrade our Helm installation of datadog using the following syntax: </p>

<pre><code>helm upgrade datadog-monitoring --set datadog.confd.""kube_scheduler\.yaml"".instances[0].prometheus_url=""http://localhost:10251/metrics"",datadog.confd.""kube_scheduler\.yaml"".init_config= stable/datadog
</code></pre>

<p>However I'm getting the error below regardless of any attempt at altering the syntax of the <code>prometheus_url</code> value (putting the url in quotes, escaping the quotes, etc): </p>

<blockquote>
  <p>Error: UPGRADE FAILED: failed to create resource: ConfigMap in version ""v1"" cannot be handled as a ConfigMap: v1.ConfigMap.Data: ReadString: expects "" or n, but found {, error found in #10 byte of ...|er.yaml"":{""instances|..., bigger context ...|{""apiVersion"":""v1"",""data"":{""kube_scheduler.yaml"":{""instances"":[{""prometheus_url"":""\""<a href=""http://localhost|"" rel=""nofollow noreferrer"">http://localhost|</a>...</p>
</blockquote>

<p>If I add the <code>--dry-run --debug</code> flags I get the following yaml output: </p>

<pre><code>REVISION: 7
RELEASED: Mon Mar  2 14:28:52 2020
CHART: datadog-1.39.7
USER-SUPPLIED VALUES:
datadog:
  confd:
    kube_scheduler.yaml:
      init_config: """"
      instances:
      - prometheus_url: http://localhost:10251/metrics
</code></pre>

<p>The Yaml output appears to mesh with the integration as specified on this <a href=""https://github.com/DataDog/integrations-core/blob/master/kube_scheduler/datadog_checks/kube_scheduler/data/conf.yaml.example"" rel=""nofollow noreferrer"">github page</a>. </p>",,1,0,,2020-3-2 15:22:07,,2020-11-12 11:39:23,2020-3-2 19:33:18,,1489378,,1489378,,1,3,kubernetes-helm|datadog,1795,17.6163
261348,2,Query,61108009,How to get the number of different values of a metric's tag in Datadog,"<p>I have a metric which has a tag with lots of different values (the value is a file name). How can I create a query that determines the number of different values of that tag exist on a metric?</p>

<p>For example if 4 metrics are received during a time frame, with the following tags ""file_name:dir/file1"", ""file_name:dir/file2"", ""file_name:dir/file3"", ""file_name:dir/file1""</p>

<p>I want the query to return the value 3, since of all the metrics received during this timeframe there were 3 distinct values for the file_name tag.</p>",,1,1,,2020-4-8 18:55:30,1,2020-4-15 18:52:15,,,,,13262459,,1,3,datadog,1773,17.5948
261349,3,Monitoring,58414654,How can I monitor AWS cloudwatch customized metric on datadog?,"<p>I am using datadog for monitoring my services on AWS. On my python application, I use below code to send a data to a metric on Cloudwatch:</p>

<pre><code>aws.put_metric_data(Namespace='Org/Test', MetricData=[{
            'MetricName': 'Failure', 'Value': 1}])
</code></pre>

<p>I can see this data on Cloudwatch -> Metric. But I don't know how I can create a monitor on Datadog to listen on this metric. </p>",58421772,1,0,,2019-10-16 13:34:39,,2019-10-16 21:11:59,,,,,5421539,,1,4,amazon-web-services|amazon-cloudwatch|datadog,872,17.5621
261350,1,Parse,60516923,Logging unhandled Golang panics,"<p>I have a logrus log handler in my Golang application. Logs are formatted with JSONFormatter and are submitted as a single line to Datadog, which aggregates them and displays them nicely. However, I recently discovered a case where there's an unhandled panic, and this is <em>not</em> captured with the logrus logger. This results in the actual panic and stack trace being spread across multiple output lines, which Datadog collects individually. This costs us money and makes the logs very difficult to read.</p>

<p>I'm going to fix the issue, but in the event that any further unhandled panics happen, I'd like to be able to capture them using the logrus JSONFormatter.</p>

<p>Something like this:</p>

<pre><code>package main

import (
    ""os""
    ""sync""

    ""github.com/sirupsen/logrus""
)

var (
    loggerInstance *logrus.Entry
    once           sync.Once
    logger  = GetLogger()
)

// GetLogger initializes and returns a reference to a CustomLogger object.
func GetLogger() *logrus.Entry {
    once.Do(func() {
        logrus.SetFormatter(&amp;logrus.JSONFormatter{})

        // We'll just pipe everything to stdout. It's json so Datadog will parse the level regardless
        logrus.SetOutput(os.Stdout)
        logrus.SetLevel(logrus.TraceLevel)                            // We'll leave this as trace and just filter out logs when we index them
        loggerInstance = logrus.WithFields(logrus.Fields{""env"": ""local""}) // Add env tag for easy log parsing
    })
    return loggerInstance
}

func main() {
    logger.Info(""Logrus using json logging"")
    logger.Warn(""We are about to panic"")

    var things = []string{
        ""hi"",
        ""yes"",
        ""thing""}

    print(""%s"", things[len(things)])
}
</code></pre>

<p>This produces the following output. As you can see, the first two logs use logrus, but the unhandled panic does not.</p>

<pre><code>{""env"":""local"",""level"":""info"",""msg"":""Logrus using json logging"",""time"":""2020-03-03T15:12:30-08:00""}
{""env"":""local"",""level"":""warning"",""msg"":""We are about to panic"",""time"":""2020-03-03T15:12:30-08:00""}
panic: runtime error: index out of range

goroutine 1 [running]:
main.main()
        /Users/nathan.deren/Library/Preferences/GoLand2019.3/scratches/scratch.go:38 +0xbe

Process finished with exit code 2
</code></pre>

<p>Is it possible to get those last several lines to log using logrus?</p>",60517002,1,0,,2020-3-3 23:17:31,,2020-3-3 23:27:32,,,,,13002451,,1,4,go|logging|datadog,1397,17.3808
261351,1,Method,58505214,How to Add DataDog trace ID in Logs using Spring Boot + Logback,"<p>OK, I spent quiet some time figuring out how to configure stuff to have DataDog trace ID in logs but couldn't get it working. To be clear what I'm looking for is to see trace IDs in logs message, the same way that adding <code>spring-cloud-starter-sleuth</code> to the classpath, automatically configure Slf4j/Logback to show trace IDs in log messages.</p>

<p>Where I've started:  </p>

<ol>
<li>We've got a simple web spring boot application running as a Docker container deployed as an AWS Elastic BeansTalk, whose logs go to CloudWatch and we read them there.  </li>
<li>We have DataDog as a Java agent (thus no dependencies in pom.xml)</li>
<li>We have SLF4J/Logback in our dependencies list.</li>
<li>There's no other related depndencies (like <code>dd-trace-ot</code> or any <code>opertracing</code> libs)</li>
</ol>

<p>What I did so far:  </p>

<ol>
<li>I found on SO that adding <code>opentracing-spring-cloud-starter</code> will add log integration automatically. But I couldn't get it working.</li>
<li>On DD website, it says configuring the pattern is enough to see the IDs, but in our case it didn't work. (is it because we don't have our logs a JSON?). Also, adding <code>dd-trace-ot</code> didn't help.</li>
</ol>

<p>Notes:  </p>

<ol>
<li>We can't switch to JSON logs.</li>
<li>We can't switch to any other library (e.g. Slueth).</li>
<li>We can't go away from CloudWatch.</li>
</ol>

<p>Can someone tell me how exactly I need to configure the application to see trace IDs in log messages? Is there any documentation or samples I can look at?</p>",,2,0,,2019-10-22 13:17:33,,2020-5-21 07:55:37,,,,,2194119,,1,2,spring-boot|logback|datadog|opentracing|distributed-tracing,3855,17.1441
261352,1,Method,51124961,Send data to Datadog using Go,"<p>i'm collect data using Go and want to visualize it, i chose Datadog, but didn't find examples or live projects where Go used for sending metrics to Datadog. But in offical site says that Go is supported.</p>",,2,2,,2018-7-1 16:13:48,1,2018-7-3 01:00:57,,,,,9699047,,1,2,go|datadog,3637,17.043
261353,0,Configuration,64482767,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,"<p>Using:</p>
<pre><code> Kubernetes: 1.18.8
 Helm: 3.3.4
 Datadog DaemonSet agent: 7.23.0
 Datadog cluster-agent: 1.9.0
 Azure Database for PostgreSQL 11.x (i.e. external postgres-service)
</code></pre>
<p>I am deploying Datadog as a DaemonSet and with the cluster-agent enabled to a Kubernetes cluster using the instructions provided <a href=""https://docs.datadoghq.com/agent/kubernetes/?tab=helm"" rel=""nofollow noreferrer"">here</a>.</p>
<pre><code>helm install my-kubernetes -f values.yaml --set datadog.apiKey=&lt;DATADOG_API_KEY&gt; datadog/datadog --set targetSystem=linux
</code></pre>
<p>I'm configuring Datadog using the <code>values.yaml</code> file as specified.</p>
<p>I want to do some custom metrics, specifically using the integration formerly known as <code>postgres.yaml</code>. I have tried to do this as specified in the <code>values.yaml</code> template found <a href=""https://github.com/DataDog/helm-charts/blob/master/charts/datadog/values.yaml"" rel=""nofollow noreferrer"">here</a>, like this (putting it in the cluster-agent, since these are cluster-wide metrics):</p>
<pre class=""lang-yaml prettyprint-override""><code># clusterAgent.confd -- Provide additional cluster check configurations
## Each key will become a file in /conf.d
## ref: https://docs.datadoghq.com/agent/autodiscovery/
confd:
  postgres.yaml: |-
    init_config:

    instances:
      - host: my-postgres-host.com
        port: 5432
        username: my-user
        password: some-password
        dbname: some-database
        ssl: True
        tags:
        - some_tag
        custom_queries:
        - metric_prefix: some.prefix
          query: SELECT COUNT(*) FROM bla WHERE timestamp &gt; NOW() - INTERVAL '1 hour';
          columns:
          - name: countLastHour
            type: count
</code></pre>
<p>As per the documentation, I can confirm that using the <code>|-</code> prefix this indeed creates a file in the path <code>/etc/datadog-agent/conf.d/postgres.yaml</code> on the node, where I would expect it to. The file correctly has all the contents in the block, i.e. starting with <code>init_config:...</code></p>
<p>Now, when starting the node I see this in the logs (DEBUG):</p>
<blockquote>
<p>'/conf.d/postgres.yaml' -&gt; '/etc/datadog-agent/conf.d/postgres.yaml'
/conf.d/..2020_10_22_10_22_27.239825358 -&gt;
/etc/datadog-agent/conf.d/..2020_10_22_10_22_27.239825358
'/conf.d/..2020_10_22_10_22_27.239825358/postgres.yaml' -&gt;
'/etc/datadog-agent/conf.d/..2020_10_22_10_22_27.239825358/postgres.yaml'</p>
<p>2020-10-22 10:22:29 UTC | CLUSTER | DEBUG |
(pkg/autodiscovery/providers/file.go:196 in collectEntry) | Found
valid configuration in file: /etc/datadog-agent/conf.d/postgres.yaml</p>
<p>2020-10-22 10:22:29 UTC | CLUSTER | DEBUG |
(pkg/collector/scheduler.go:154 in getChecks) | Unable to load a check
from instance of config 'postgres': Core Check Loader: <strong>Check postgres
not found in Catalog</strong></p>
<p>2020-10-22 10:22:29 UTC | CLUSTER | ERROR |
(pkg/collector/scheduler.go:201 in GetChecksFromConfigs) | <strong>Unable to
load the check: unable to load any check from config 'postgres'</strong></p>
</blockquote>
<p>The documentation <a href=""https://docs.datadoghq.com/integrations/faq/postgres-custom-metric-collection-explained/"" rel=""nofollow noreferrer"">here</a> states, that the postgres yaml-contents in agents v7.x should actually be in <code>/etc/datadog-agent/conf.d/postgres.d/conf.yaml</code> and not in <code>/etc/datadog-agent/conf.d/postgres.yaml</code>. It is not possible to create a subfolder / use forward slashes in the config key (internally, the file is created using ConfigMap).</p>
<p>I'm not even sure if the problem is the yaml-file path or if a core integration is missing. So my broad quest is: how do I enable Datadog postgres-integration correctly in my setup?</p>",,2,2,,2020-10-22 12:58:40,1,2021-10-19 07:14:03,2020-10-22 15:23:04,,430885,,430885,,1,2,kubernetes-helm|datadog,1148,17.0398
261354,2,Query,52828258,does else exist on datadog is_match,"<p>I am trying to set up slack monitors with datadog, based on the environment.
For e.g. if the environment is production got to slack channel A and if it is uat go to slack channel B and all other environments should go to slack channel C.</p>

<pre><code> message = &lt;&lt;EOF
{{#is_match ""environment.name"" ""production""}}
   {{#is_alert}} @slack-datadog-production {{/is_alert}}
{{/is_match}}
{{#is_match ""environment.name"" ""uat""}}
   {{#is_alert}} @slack-datadog-uat {{/is_alert}}
{{/is_match}}
else
  {{#is_alert}} @slack-datadog {{/is_alert}}
EOF
</code></pre>

<p>But I can't find a way to do the last part where all others should go to slack channel B. 
Looked at the documentation in <a href=""https://docs.datadoghq.com/monitors/notifications"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/monitors/notifications</a> and googled but couldn't find anything that can do an else condition. </p>",,2,0,,2018-10-16 05:06:28,1,2020-4-17 23:32:29,,,,,5571493,,1,3,datadog,884,16.9858
261355,1,Method,61900949,Adding tags for Datadog browser logs,"<p>I am using Datadog to monitor my browser console logs. I need different tags in for datadog logs. The only option I fount is to add attributes to my logger using,</p>

<p><code>DD_LOGS.addContext('referrer', document.referrer);</code></p>

<p>Is there any way for the frontend client application to have tags in datadog? Or is the attribute and tags are same in Datadog</p>",,1,0,,2020-5-19 21:19:16,,2020-5-23 12:46:33,,,,,4124267,,1,3,datadog,689,16.9529
261356,2,Query,61010490,Is it possible to use tags for excluding instances in DataDog while creating a graph?,"<p>I have DataDog with Amazon AWS RDS integration configured.</p>

<p>Is it possible to create a graph and use a tag to exclude some hosts from the result. I have let's say 100 hosts with tag <em>environment:live</em> and 10 of them are also tagged with tag <em>importance:ignore</em>. So I need to create a graph which will include metrics for 90 hosts that are tagged with first tag but don't tagged with a second one. Is it possible?</p>",,1,0,,2020-4-3 11:06:52,,2020-4-4 00:48:57,2020-4-4 00:48:57,,174777,,1215756,,1,0,amazon-web-services|amazon-rds|datadog,1651,16.871
261357,3,Visualization,66282520,How to set line color on a timeseries in DataDog,"<p>I'm setting up a timeseries to monitor system problems with the standard levels: Critical, Error, Warn, etc.. I want to set the colors as follows:</p>
<ul>
<li>Critical: Red</li>
<li>Error: Orange</li>
<li>Warn: Yellow</li>
</ul>
<p>I can't seem to do this. There is a color selection drop-down, but options aren't colors... they're themes, like &quot;Classic,&quot; &quot;Cool,&quot; &quot;Warm,&quot; etc.</p>
<p>How do I set the color on a line in a time series?</p>",,2,0,,2021-2-19 17:45:55,,2021-2-24 17:43:53,2021-2-23 10:02:30,,3001761,,1526115,,1,1,datadog,706,16.7952
261358,1,Method,59549130,GraphQL + Spring Boot: how to collect (error) metrics?,"<p>I've been working on adding monitoring metrics in our GraphQL gateway recently.</p>

<p>We're using <a href=""https://github.com/graphql-java-kickstart/graphql-spring-boot"" rel=""nofollow noreferrer"">graphql-spring-boot</a> starter for the gateway.</p>

<p>After reading the following documentations, I manage to send the basic graphql.timer.query.* metrics to Datadog</p>

<ul>
<li><a href=""https://www.baeldung.com/spring-boot-actuators"" rel=""nofollow noreferrer"">https://www.baeldung.com/spring-boot-actuators</a></li>
<li><a href=""https://docs.spring.io/spring-boot/docs/2.0.x/actuator-api/html/#metrics"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-boot/docs/2.0.x/actuator-api/html/#metrics</a></li>
<li><a href=""https://github.com/graphql-java-kickstart/graphql-spring-boot#tracing-and-metrics"" rel=""nofollow noreferrer"">https://github.com/graphql-java-kickstart/graphql-spring-boot#tracing-and-metrics</a></li>
</ul>

<p>What I've achieved so far is, when I send a GraphQL query/mutation, I'd collect the request count and time accordingly. e.g. sending the query below</p>

<pre><code>query HelloWorldQuery {
  greeting(
    name: ""Bob""
  ) {
    message
  }
}
</code></pre>

<p>I'll see metrics <code>graphql.timer.query.count</code> / <code>graphql.timer.query.sum</code> with tags <code>operationName=HelloWorldQuery</code></p>

<p>It works like perfectly, until I want to test a query with errors. I realise there is no metrics/tags related to a failed query. For example, if I the above query returns null data and some GraphQL errors, I'd still collect <code>graphql.timer.query.count (operationName=HelloWorldQuery)</code>, but there's no additional tags for me to tell there is an error for that query.</p>

<p>In the gateway, I have implemented a custom <code>GraphQLErrorHandler</code>, so I was thinking maybe I should add error counter (via MeterRegistry) in that class, but I am unable to get the <code>operationName</code> simply from GraphQLError type. the best I can get is error.getPath() which gives the method name (e.g. <code>greeting</code>) rather than the custom query name (<code>HelloWorldQuery</code> - to be consistent with what <code>graphql.timer.query.*</code> provides).</p>

<p>My question is, how to solve the above problem? 
And generally what is the best way of collecting GraphQL query metrics (including errors)?</p>

<p>------------------- <strong>Update</strong> -------------------</p>

<p><strong>2019-12-31</strong>
I read a bit more about GraphQL Instrumentation <a href=""https://github.com/graphql-java/graphql-java-page/blob/master/content/documentation/v10/instrumentation.md"" rel=""nofollow noreferrer"">here</a> and checked the <a href=""https://github.com/graphql-java-kickstart/graphql-spring-boot/blob/master/graphql-spring-boot-autoconfigure/src/main/java/com/oembedler/moon/graphql/boot/metrics/MetricsInstrumentation.java"" rel=""nofollow noreferrer"">MetricsInstrumentation</a> implementation in graphql-spring-boot repo, the I have an idea of extending the MetricsInstrumentation class by adding error metrics there.</p>

<p><strong>2020-01-02</strong>
I tried to ingest my CustomMetricsInstrumentation class, but with no luck. There is internal AutoConfiguration wiring, which I cannot insert my auto configuration in the middle.</p>",59605166,1,0,,2019-12-31 23:32:27,,2020-1-5 23:37:29,2020-1-2 21:19:04,,1264461,,1264461,,1,2,spring-boot|graphql|spring-boot-actuator|datadog|spring-micrometer,1923,16.5359
261359,1,Method,51443070,Collecting Python logs to DataDog in ECS,"<p>I am having a hard time to collect logs from an python app deployed in ECS using DataDog Agent. I have a dockerized Flask app deployed in ECS. The app spits logs to stdout. I now need to monitor them in DataDog. </p>

<p>I've added a new DataDog agent container (Fargate compatible, since I am using Fargate), which runs as part of the same task as the app. I can see the CPU and memory metrics for both container in app.datadoghq.com/containers, so that means that DataDog Agent is working.</p>

<p>I now need the app logs. I went through the documentation in <a href=""https://app.datadoghq.com/logs/onboarding/container"" rel=""nofollow noreferrer"">https://app.datadoghq.com/logs/onboarding/container</a>, added</p>

<pre><code>  ""dockerLabels"": {
    ""com.datadoghq.ad.logs"": ""[{\""source\"": \""python\"", \""service\"": \""flask\""}]""
  },
</code></pre>

<p>to the app container and the following env.vars to the DataDog container :</p>

<pre><code>  ""environment"": [
    {
      ""name"": ""DD_API_KEY"",
      ""value"": ""&lt;key&gt;""
    },
    {
      ""name"": ""DD_LOGS_ENABLED"",
      ""value"": ""true""
    },
    {
      ""name"": ""DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL"",
      ""value"": ""true""
    },
    {
      ""name"": ""SD_BACKEND"",
      ""value"": ""docker""
    },
    {
      ""name"": ""ECS_FARGATE"",
      ""value"": ""true""
    }
  ]
</code></pre>

<p>But that seems to be insufficient.
Am I going in the right direction ? What am I missing ?</p>",51451079,1,1,,2018-7-20 12:57:38,,2018-7-20 22:20:42,,,,,155722,,1,1,logging|flask|amazon-ecs|datadog|aws-fargate,1815,16.2355
261360,1,Error,52176297,"Cannot send ""tags"" via datadog in Amazon ECS, Fargate","<p>I have configured datadog agent on Amazon ECS, Fargate. I can send all the intended metrics but I cannot send ""tags"".</p>

<p>I've set Environment variables in ECS task definitions.</p>

<pre><code>DD_API_KEY  xxxxxxxxxxxxxxxxxxxxxxx
DD_TAGS     env:stg
ECS_FARGATE true
</code></pre>

<p>I think most of the settings are all right because I can see the metrics which I want to see.
But tags, especially env:stg is missing in datadog UI and because of this weired error, some metrics is missing.</p>

<p>Does anyone know the reason of this error and the way to solve this?</p>

<p>Thanks.</p>",,1,0,,2018-9-5 02:38:01,,2018-12-2 23:28:12,,,,,6228435,,1,2,amazon-ecs|datadog|aws-fargate,490,16.1608
261361,0,Configuration,60662893,How do you pass file_system_blacklist arg to Datadog Docker Agent run command?,"<p>I want to exclude a path to avoid getting my logs spammed like so:</p>

<pre><code> (disk.py:75) | Unable to get disk metrics for /host/proc/sys/fs/binfmt_misc:
  [Errno 40] Too many levels of symbolic links: 
    '/host/proc/sys/fs/binfmt_misc'\n"",""stream"":""stdout"",""time"":""2020-03-12T23:01:38.424330408Z""}
</code></pre>

<p>I'm running datadog as a docker agent using the command here:
<a href=""https://docs.datadoghq.com/agent/docker/?tab=standard#installation"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/docker/?tab=standard#installation</a></p>

<p>how do I specify files to exclude in the docker run command? is it an environment variable?</p>",60677166,2,0,,2020-3-12 23:04:26,,2020-3-13 20:56:50,2020-3-13 11:19:07,,2051454,,1363715,,1,3,docker|datadog,919,16.0533
261362,2,Query,63888511,Terraform Datadog Query Is Invalid,"<p>I'm trying to test out creating a monitor for google pub sub and am getting an &quot;Invalid Query&quot; error.  This is the query text when i view source of another working monitor, so i'm confused as to why this isn't working.</p>
<p>Error:  <code>Error: error creating monitor: 400 Bad Request: {&quot;errors&quot;:[&quot;The value provided for parameter 'query' is invalid&quot;]}</code></p>
<p>Terraform:</p>
<pre><code>resource &quot;datadog_monitor&quot; &quot;bad_stuff_sub_monitor&quot; {
  name               = &quot;${var.customer_name} Bad Stuff Monitor&quot;
  type               = &quot;metric alert&quot;
  message            = &quot;${var.customer_name} Bad Stuff Topic getting too big. Notify: ${var.datadog_monitor_notify_list}&quot;
  escalation_message = &quot;Escalation message @pagerduty&quot;

  query = &quot;avg:gcp.pubsub.subscription.num_undelivered_messages{project_id:terraform_gcp_test}&quot;

  thresholds = {
    ok                = 0
    warning           = 1
    warning_recovery  = 0
    critical          = 2
    critical_recovery = 1
  }

  notify_no_data    = false
  renotify_interval = 1440

  notify_audit = false
  timeout_h    = 60
  include_tags = true

  # ignore any changes in silenced value; using silenced is deprecated in favor of downtimes
  lifecycle {
    ignore_changes = [silenced]
  }

  tags = [var.customer_name, var.project_name]
}
</code></pre>",,1,1,,2020-9-14 16:40:28,,2020-9-16 02:05:14,,,,,1190203,,1,2,terraform|google-cloud-pubsub|datadog|terraform0.12+,1408,15.9944
261363,0,Integration,58267953,DataDog GKE NESTJS integration using DataDog's helm chart,"<p>Im trying to deploy my service and read my local logfile from the inside pod.</p>

<p>Using DataDog's helm chart values with the following configs : </p>

<pre><code>## Default values for Datadog Agent
## See Datadog helm documentation to learn more:
## https://docs.datadoghq.com/agent/kubernetes/helm/

## @param image - object - required
## Define the Datadog image to work with.
#
image:

  ## @param repository - string - required
  ## Define the repository to use:
  ## use ""datadog/agent"" for Datadog Agent 6
  ## use ""datadog/dogstatsd"" for Standalone Datadog Agent DogStatsD6
  #
  repository: datadog/agent

  ## @param tag - string - required
  ## Define the Agent version to use.
  ## Use 6.13.0-jmx to enable jmx fetch collection
  #
  tag: 6.13.0

  ## @param pullPolicy - string - required
  ## The Kubernetes pull policy.
  #
  pullPolicy: IfNotPresent

  ## @param pullSecrets - list of key:value strings - optional
  ## It is possible to specify docker registry credentials
  ## See https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  #
  # pullSecrets:
  #   - name: ""&lt;REG_SECRET&gt;""

nameOverride: """"
fullnameOverride: """"

datadog:

  ## @param apiKey - string - required
  ## Set this to your Datadog API key before the Agent runs.
  ## ref: https://app.datadoghq.com/account/settings#agent/kubernetes
  #
  apiKey: ""xxxxxxxx""

  ## @param apiKeyExistingSecret - string - optional
  ## Use existing Secret which stores API key instead of creating a new one.
  ## If set, this parameter takes precedence over ""apiKey"".
  #
  # apiKeyExistingSecret: &lt;DATADOG_API_KEY_SECRET&gt;

  ## @param appKey - string - optional
  ## If you are using clusterAgent.metricsProvider.enabled = true, you must set
  ## a Datadog application key for read access to your metrics.
  #
  appKey: ""xxxxxx""

  ## @param appKeyExistingSecret - string - optional
  ## Use existing Secret which stores APP key instead of creating a new one
  ## If set, this parameter takes precedence over ""appKey"".
  #
  # appKeyExistingSecret: &lt;DATADOG_APP_KEY_SECRET&gt;

  ## @param securityContext - object - optional
  ## You can modify the security context used to run the containers by
  ## modifying the label type below:
  #
  # securityContext:
  #   seLinuxOptions:
  #     seLinuxLabel: ""spc_t""

  ## @param clusterName - string - optional
  ## Set a unique cluster name to allow scoping hosts and Cluster Checks easily
  #
  # clusterName: &lt;CLUSTER_NAME&gt;

  ## @param name - string - required
  ## Daemonset/Deployment container name
  ## See clusterAgent.containerName if clusterAgent.enabled = true
  #
  name: datadog

  ## @param site - string - optional - default: 'datadoghq.com'
  ## The site of the Datadog intake to send Agent data to.
  ## Set to 'datadoghq.eu' to send data to the EU site.
  #
  # site: datadoghq.com

  ## @param dd_url - string - optional - default: 'https://app.datadoghq.com'
  ## The host of the Datadog intake server to send Agent data to, only set this option
  ## if you need the Agent to send data to a custom URL.
  ## Overrides the site setting defined in ""site"".
  #
  # dd_url: https://app.datadoghq.com

  ## @param logLevel - string - required
  ## Set logging verbosity, valid log levels are:
  ## trace, debug, info, warn, error, critical, and off
  #
  logLevel: INFO

  ## @param podLabelsAsTags - list of key:value strings - optional
  ## Provide a mapping of Kubernetes Labels to Datadog Tags.
  #
  # podLabelsAsTags:
  #   app: kube_app
  #   release: helm_release
  #   &lt;KUBERNETES_LABEL&gt;: &lt;DATADOG_TAG_KEY&gt;

  ## @param podAnnotationsAsTags - list of key:value strings - optional
  ## Provide a mapping of Kubernetes Annotations to Datadog Tags
  #
  # podAnnotationsAsTags:
  #   iam.amazonaws.com/role: kube_iamrole
  #   &lt;KUBERNETES_ANNOTATIONS&gt;: &lt;DATADOG_TAG_KEY&gt;

  ## @param tags  - list of key:value elements - optional
  ## List of tags to attach to every metric, event and service check collected by this Agent.
  ##
  ## Learn more about tagging: https://docs.datadoghq.com/tagging/
  #
  # tags:
  #   - &lt;KEY_1&gt;:&lt;VALUE_1&gt;
  #   - &lt;KEY_2&gt;:&lt;VALUE_2&gt;

  ## @param useCriSocketVolume - boolean - required
  ## Enable container runtime socket volume mounting
  #
  useCriSocketVolume: true

  ## @param dogstatsdOriginDetection - boolean - optional
  ## Enable origin detection for container tagging
  ## https://docs.datadoghq.com/developers/dogstatsd/unix_socket/#using-origin-detection-for-container-tagging
  #
  # dogstatsdOriginDetection: true

  ## @param useDogStatsDSocketVolume - boolean - optional
  ## Enable dogstatsd over Unix Domain Socket
  ## ref: https://docs.datadoghq.com/developers/dogstatsd/unix_socket/
  #
  # useDogStatsDSocketVolume: true

  ## @param nonLocalTraffic - boolean - optional - default: false
  ## Enable this to make each node accept non-local statsd traffic.
  ## ref: https://github.com/DataDog/docker-dd-agent#environment-variables
  #
  nonLocalTraffic: true

  ## @param collectEvents - boolean - optional - default: false
  ## Enables this to start event collection from the kubernetes API
  ## ref: https://docs.datadoghq.com/agent/kubernetes/event_collection/
  #
  collectEvents: true

  ## @param leaderElection - boolean - optional - default: false
  ## Enables leader election mechanism for event collection.
  #
  # leaderElection: false

  ## @param leaderLeaseDuration - integer - optional - default: 60
  ## Set the lease time for leader election in second.
  #
  # leaderLeaseDuration: 60

  ## @param logsEnabled - boolean - optional - default: false
  ## Enables this to activate Datadog Agent log collection.
  ## ref: https://docs.datadoghq.com/agent/basic_agent_usage/kubernetes/#log-collection-setup
  #
  logsEnabled: true

  ## @param logsConfigContainerCollectAll - boolean - optional - default: false
  ## Enable this to allow log collection for all containers.
  ## ref: https://docs.datadoghq.com/agent/basic_agent_usage/kubernetes/#log-collection-setup
  #
  logsConfigContainerCollectAll: true

  ## @param containerLogsPath - string - optional - default: /var/lib/docker/containers
  ## This to allow log collection from container log path. Set to a different path if not
  ## using docker runtime.
  ## ref: https://docs.datadoghq.com/agent/kubernetes/daemonset_setup/?tab=k8sfile#create-manifest
  #
  containerLogsPath: /var/lib/docker/containers


  ## @param apmEnabled - boolean - optional - default: false
  ## Enable this to enable APM and tracing, on port 8126
  ## ref: https://github.com/DataDog/docker-dd-agent#tracing-from-the-host
  #
  apmEnabled: true

  ## @param processAgentEnabled - boolean - optional - default: false
  ## Enable this to activate live process monitoring.
  ## Note: /etc/passwd is automatically mounted to allow username resolution.
  ## ref: https://docs.datadoghq.com/graphing/infrastructure/process/#kubernetes-daemonset
  #
  processAgentEnabled: true

  ## @param env - list of object - optional
  ## The dd-agent supports many environment variables
  ## ref: https://github.com/DataDog/datadog-agent/tree/master/Dockerfiles/agent#environment-variables
  #
  # env:
  #   - name: &lt;ENV_VAR_NAME&gt;
  #     value: &lt;ENV_VAR_VALUE&gt;

  ## @param volumes - list of objects - optional
  ## Specify additional volumes to mount in the dd-agent container
  #
  # volumes:
  #   - hostPath:
  #     path: &lt;HOST_PATH&gt;
  #     name: &lt;VOLUME_NAME&gt;

  ## @param volumeMounts - list of objects - optional
  ## Specify additional volumes to mount in the dd-agent container
  #
  # volumeMounts:
  #   - name: &lt;VOLUME_NAME&gt;
  #     mountPath: &lt;CONTAINER_PATH&gt;
  #     readOnly: true

  ## @param confd - list of objects - optional
  ## Provide additional check configurations (static and Autodiscovery)
  ## Each key becomes a file in /conf.d
  ## ref: https://github.com/DataDog/datadog-agent/tree/master/Dockerfiles/agent#optional-volumes
  ## ref: https://docs.datadoghq.com/agent/autodiscovery/
  #
  confd:
    conf.yaml: |-
      init_config:
      instances:
      logs:
        - type: ""file""
          path: ""/app/logs/service.log""
          service: nodejs
          source: nodejs
          sourcecategory: sourcecode
  #   kubernetes_state.yaml: |-
  #     ad_identifiers:
  #       - kube-state-metrics
  #     init_config:
  #     instances:
  #       - kube_state_url: http://%%host%%:8080/metrics

  ## @param checksd - list of key:value strings - optional
  ## Provide additional custom checks as python code
  ## Each key becomes a file in /checks.d
  ## ref: https://github.com/DataDog/datadog-agent/tree/master/Dockerfiles/agent#optional-volumes
  #
  # checksd:
  #   service.py: |-

  ## @param criSocketPath - string - optional
  ## Path to the container runtime socket (if different from Docker)
  ## This is supported starting from agent 6.6.0
  #
  # criSocketPath: /var/run/containerd/containerd.sock

  ## @param dogStatsDSocketPath - string - optional
  ## Path to the DogStatsD socket
  #
  # dogStatsDSocketPath: /var/run/datadog/dsd.socket

  ## @param livenessProbe - object - optional
  ## Override the agent's liveness probe logic from the default:
  ## In case of issues with the probe, you can disable it with the
  ## following values, to allow easier investigating:
  #
  # livenessProbe:
  #   exec:
  #     command: [""/bin/true""]

  ## @param resources - object -required
  ## datadog-agent resource requests and limits
  ## Make sure to keep requests and limits equal to keep the pods in the Guaranteed QoS class
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  #
  resources: {}
  # requests:
  #   cpu: 200m
  #   memory: 256Mi
  # limits:
  #   cpu: 200m
  #   memory: 256Mi

## @param clusterAgent - object - required
## This is the Datadog Cluster Agent implementation that handles cluster-wide
## metrics more cleanly, separates concerns for better rbac, and implements
## the external metrics API so you can autoscale HPAs based on datadog metrics
## ref: https://docs.datadoghq.com/agent/kubernetes/cluster/
#
clusterAgent:

  ## @param enabled - boolean - required
  ## Set this to true to enable Datadog Cluster Agent
  #
  enabled: false

  containerName: cluster-agent
  image:
    repository: datadog/cluster-agent
    tag: 1.3.2
    pullPolicy: IfNotPresent

  ## @param token - string - required
  ## This needs to be at least 32 characters a-zA-z
  ## It is a preshared key between the node agents and the cluster agent
  ## ref:
  #
  token: """"

  replicas: 1

  ## @param metricsProvider - object - required
  ## Enable the metricsProvider to be able to scale based on metrics in Datadog
  #
  metricsProvider:
    enabled: true

  ## @param clusterChecks - object - required
  ## Enable the Cluster Checks feature on both the cluster-agents and the daemonset
  ## ref: https://docs.datadoghq.com/agent/autodiscovery/clusterchecks/
  ## Autodiscovery via Kube Service annotations is automatically enabled
  #
  clusterChecks:
    enabled: true

  ## @param confd - list of objects - optional
  ## Provide additional cluster check configurations
  ## Each key will become a file in /conf.d
  ## ref: https://docs.datadoghq.com/agent/autodiscovery/
  #
  # confd:
  #   mysql.yaml: |-
  #     cluster_check: true
  #     instances:
  #       - server: '&lt;EXTERNAL_IP&gt;'
  #         port: 3306
  #         user: datadog
  #         pass: '&lt;YOUR_CHOSEN_PASSWORD&gt;'

  ## @param resources - object -required
  ## Datadog cluster-agent resource requests and limits.
  #
  resources: {}
#    requests:
#      cpu: 200m
#      memory: 256Mi
#    limits:
#      cpu: 200m
#      memory: 256Mi

  ## @param priorityclassName - string - optional
  ## Name of the priorityClass to apply to the Cluster Agent

  # priorityClassName: system-cluster-critical

  ## @param livenessProbe - object - optional
  ## Override the agent's liveness probe logic from the default:
  ## In case of issues with the probe, you can disable it with the
  ## following values, to allow easier investigating:
  #
  # livenessProbe:
  #   exec:
  #     command: [""/bin/true""]

  ## @param podAnnotations - list of key:value strings - optional
  ## Annotations to add to the cluster-agents's pod(s)
  #
  # podAnnotations:
  #   key: ""value""

  ## @param readinessProbe - object - optional
  ## Override the cluster-agent's readiness probe logic from the default:
  #
  # readinessProbe:

rbac:

  ## @param created - boolean - required
  ## If true, create &amp; use RBAC resources
  #
  create: true

  ## @param serviceAccountName - string - required
  ## Ignored if rbac.create is true
  #
  serviceAccountName: default

tolerations: []

kubeStateMetrics:

  ## @param enabled - boolean - required
  ## If true, deploys the kube-state-metrics deployment.
  ## ref: https://github.com/kubernetes/charts/tree/master/stable/kube-state-metrics
  #
  enabled: true

kube-state-metrics:
  rbac:
    ## @param created - boolean - required
    ## If true, create &amp; use RBAC resources
    #
    create: true

  serviceAccount:
    ## @param created - boolean - required
    ## If true, create ServiceAccount, require rbac kube-state-metrics.rbac.create true
    #
    create: true
    ## @param name - string - required
    ## The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the fullname template
    #
    name: coupon-service-account

  ## @param resources - object - optional
  ## Resource requests and limits for the kube-state-metrics container.
  #
  # resources:
  #   requests:
  #     cpu: 200m
  #     memory: 256Mi
  #   limits:
  #     cpu: 200m
  #     memory: 256Mi

daemonset:

  ## @param enabled - boolean - required
  ## You should keep Datadog DaemonSet enabled!
  ## The exceptional case could be a situation when you need to run
  ## single DataDog pod per every namespace, but you do not need to
  ## re-create a DaemonSet for every non-default namespace install.
  ## Note: StatsD and DogStatsD work over UDP, so you may not
  ## get guaranteed delivery of the metrics in Datadog-per-namespace setup!
  #
  enabled: true

  ## @param useDedicatedContainers - boolean - optional
  ## Deploy each datadog agent process in a separate container. Allow fine-grained
  ## control over allocated resources and better isolation.
  #
  # useDedicatedContainers: false

  containers:

    agent:
      ## @param env - list - required
      ## Additionnal environment variables for the agent container.
      #
      # env:

      ## @param logLevel - string - optional
      ## Set logging verbosity, valid log levels are:
      ## trace, debug, info, warn, error, critical, and off.
      ## If not set, fall back to the value of datadog.logLevel.
      #
      logLevel: INFO

      ## @param resources - object - required
      ## Resource requests and limits for the agent container.
      #
      resources: 
       requests:
         cpu: 200m
         memory: 256Mi
       limits:
         cpu: 200m
         memory: 256Mi

    processAgent:
      ## @param env - list - required
      ## Additionnal environment variables for the process-agent container.
      #
      # env:

      ## @param logLevel - string - optional
      ## Set logging verbosity, valid log levels are:
      ## trace, debug, info, warn, error, critical, and off.
      ## If not set, fall back to the value of datadog.logLevel.
      #
      logLevel: INFO

      ## @param resources - object - required
      ## Resource requests and limits for the process-agent container.
      #
      resources: 
       requests:
         cpu: 100m
         memory: 200Mi
       limits:
         cpu: 100m
         memory: 200Mi

    traceAgent:
      ## @param env - list - required
      ## Additionnal environment variables for the trace-agent container.
      #
      # env:

      ## @param logLevel - string - optional
      ## Set logging verbosity, valid log levels are:
      ## trace, debug, info, warn, error, critical, and off.
      ## If not set, fall back to the value of datadog.logLevel.
      #
      logLevel: INFO

      ## @param resources - object - required
      ## Resource requests and limits for the trace-agent container.
      #
      resources: 
       requests:
         cpu: 100m
         memory: 200Mi
       limits:
         cpu: 100m
         memory: 200Mi

  ## @param useHostNetwork - boolean - optional
  ## Bind ports on the hostNetwork. Useful for CNI networking where hostPort might
  ## not be supported. The ports need to be available on all hosts. It Can be
  ## used for custom metrics instead of a service endpoint.
  ##
  ## WARNING: Make sure that hosts using this are properly firewalled otherwise
  ## metrics and traces are accepted from any host able to connect to this host.
  #
  useHostNetwork: true

  ## @param useHostPort - boolean - optional
  ## Sets the hostPort to the same value of the container port. Needs to be used
  ## to receive traces in a standard APM set up. Can be used as for sending custom metrics.
  ## The ports need to be available on all hosts.
  ##
  ## WARNING: Make sure that hosts using this are properly firewalled otherwise
  ## metrics and traces are accepted from any host able to connect to this host.
  #
  useHostPort: true

  ## @param useHostPID - boolean - optional
  ## Run the agent in the host's PID namespace. This is required for Dogstatsd origin
  ## detection to work. See https://docs.datadoghq.com/developers/dogstatsd/unix_socket/
  #
  # useHostPID: true

  ## @param podAnnotations - list of key:value strings - optional
  ## Annotations to add to the DaemonSet's Pods
  #
  # podAnnotations:
  #   &lt;POD_ANNOTATION&gt;: '[{""key"": ""&lt;KEY&gt;"", ""value"": ""&lt;VALUE&gt;""}]'

  ## @param tolerations - array - optional
  ## Allow the DaemonSet to schedule on tainted nodes (requires Kubernetes &gt;= 1.6)
  #
  # tolerations: []

  ## @param nodeSelector - object - optional
  ## Allow the DaemonSet to schedule on selected nodes
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  #
  # nodeSelector: {}

  ## @param affinity - object - optional
  ## Allow the DaemonSet to schedule using affinity rules
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  #
  # affinity: {}

  ## @param updateStrategy - string - optional
  ## Allow the DaemonSet to perform a rolling update on helm update
  ## ref: https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/
  #
  # updateStrategy: RollingUpdate

  ## @param priorityClassName - string - optional
  ## Sets PriorityClassName if defined.
  #
  # priorityClassName:

  ## @param podLabels - object - optional
  ## Sets podLabels if defined.
  #
  # podLabels: {}

  ## @param useConfigMap - boolean - optional
  #  Configures a configmap to provide the agent configuration
  #
  # useConfigMap: false

deployment:
  ## @param enabled - boolean - required
  ## Apart from DaemonSet, deploy Datadog agent pods and related service for
  ## applications that want to send custom metrics. Provides DogStasD service.
  #
  enabled: false

  ## @param replicas - integer - required
  ## If you want to use datadog.collectEvents, keep deployment.replicas set to 1.
  #
  replicas: 1

  ## @param affinity - object - required
  ## Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  #
  affinity: {}

  ## @param tolerations - array - required
  ## Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  #
  tolerations: []

  ## @param dogstatsdNodePort - integer - optional
  ## If you're using a NodePort-type service and need a fixed port, set this parameter.
  #
  # dogstatsdNodePort: 8125

  ## @param traceNodePort - integer - optional
  ## If you're using a NodePort-type service and need a fixed port, set this parameter.
  #
  # traceNodePort: 8126

  ## @param service - object - required
  ##
  #
  service:
    type: ClusterIP
    annotations: {}

  ## @param priorityClassName - string - optional
  ## Sets PriorityClassName if defined.
  #
  # priorityClassName:

clusterchecksDeployment:

  ## @param enabled - boolean - required
  ## If true, deploys agent dedicated for running the Cluster Checks instead of running in the Daemonset's agents.
  ## ref: https://docs.datadoghq.com/agent/autodiscovery/clusterchecks/
  #
  enabled: false

  rbac:
    ## @param dedicated - boolean - required
    ## If true, use a dedicated RBAC resource for the cluster checks agent(s)
    #
    dedicated: false
    ## @param serviceAccountName - string - required
    ## Ignored if rbac.create is true
    #
    serviceAccountName: default

  ## @param replicas - integer - required
  ## If you want to deploy the cluckerchecks agent in HA, keep at least clusterchecksDeployment.replicas set to 2.
  ## And increase the clusterchecksDeployment.replicas according to the number of Cluster Checks.
  #
  replicas: 2

  ## @param resources - object -required
  ## Datadog clusterchecks-agent resource requests and limits.
  #
  resources: {}
  # requests:
  #   cpu: 200m
  #   memory: 500Mi
  # limits:
  #   cpu: 200m
  #   memory: 500Mi

  ## @param affinity - object - optional
  ## Allow the ClusterChecks Deployment to schedule using affinity rules.
  ## By default, ClusterChecks Deployment Pods are forced to run on different Nodes.
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  #
  # affinity:

  ## @param nodeSelector - object - optional
  ## Allow the ClusterChecks Deploument to schedule on selected nodes
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  #
  # nodeSelector: {}

  ## @param tolerations - array - required
  ## Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  #
  # tolerations: []

  ## @param livenessProbe - object - optional
  ## Override the agent's liveness probe logic from the default:
  ## In case of issues with the probe, you can disable it with the
  ## following values, to allow easier investigating:
  #
  # livenessProbe:
  #   exec:
  #     command: [""/bin/true""]

  ## @param env - list of object - optional
  ## The dd-agent supports many environment variables
  ## ref: https://github.com/DataDog/datadog-agent/tree/master/Dockerfiles/agent#environment-variables
  #
  # env:
  #   - name: &lt;ENV_VAR_NAME&gt;
  #     value: &lt;ENV_VAR_VALUE&gt;
</code></pre>

<p>as you can see I expect my logs to be available at /app/logs/service.log and thats what Im supplying to my conf.d :</p>

<pre><code>confd:
    conf.yaml: |-
      init_config:
      instances:
      logs:
        - type: ""file""
          path: ""/app/logs/service.log""
          service: nodejs
          source: nodejs
          sourcecategory: sourcecode
</code></pre>

<p>In my service, I use WinstonLogger using file transport with the JSON format.</p>

<pre><code>transports: [
            new transports.File({
              winston.format.json(),
              filename: `${process.env.LOGS_PATH}/service.log`,
            }),
]
</code></pre>

<p>process.env.LOGS_PATH = '/app/logs'</p>

<p>After all, exploring my pod and tail -f my service.log in the expected /app/logs folder I see that the application actually writes the logs in a JSON format as expected.</p>

<p>My DataDog doesn't pick up the logs and they are not showing in the log section .. </p>

<p>NOTE:: I do not mount any volume to and from my service .. </p>

<p>What am I missing? 
Should I mount my local log to /var/log/pods/[service_name]/   ?</p>",,2,0,,2019-10-7 10:42:32,,2019-10-7 13:41:01,,,,,3599936,,1,0,node.js|kubernetes|datadog,966,15.9399
261364,0,Configuration,53702226,Best practice to create a file in AWS EC2 instance using cloud formation,"<p>There are a lot of options to do that and I don't know which is the best one. I tried in the beginning to do is as following:</p>
<pre><code>ServiceInstance:
Type: &quot;AWS::EC2::Instance&quot;
Properties:
  ImageId: !Ref AmiId, !Ref LatestOnescoutAmi ]
  InstanceType: !Ref InstanceType
  SubnetId: !ImportValue vpc-stack-PublicASubnet
  SecurityGroupIds:
    - !Ref ServiceSecurityGroup
  KeyName: !Ref KeyName
  UserData:
    'Fn::Base64': !Sub |
      #cloud-config
      write_files:
        - path: /etc/sysconfig/cloudformation
          permissions: 0644
          owner: root
          content: |
            STACK_NAME=${AWS::StackName}
            AWS_REGION=${AWS::Region}
        - path: /etc/datadog-agent/conf.d/mysql.d/conf.yaml
          permissions: 0644
          owner: dd-agent
          content: |
            init_config:
            instances:
            - server: some-db-host
              user: some-admin
              pass: some-password
              port: 3306
              tags:
              - dbinstanceidentifier:someide

      runcmd:
        ## enable datadog agent
        - systemctl start datadog-agent
        - systemctl start application.service
</code></pre>
<p>but then my <code>/etc/datadog-agent/conf.d/mysql.d/conf.yaml</code> grew and I have around 13 blocks and it's not good to put them hardcoded inside the template. Better to keep the template generic and pass the config file as a parameter.
But, according to this <a href=""https://stackoverflow.com/a/53675005/2650254"">answer here</a>, it's not possible to pass a file or content of file to cloud formation.</p>
<p>The way above is the simplest I see among other two options I can think about.</p>
<ol>
<li><p>Store the config in SSM and then get it back at the ec2 start.</p>
</li>
<li><p>Create an Autoscaling and launch group which accepts the file path but it's more complex than I need:</p>
<pre><code>LaunchConfig:
    Type: AWS::AutoScaling::LaunchConfiguration
    Metadata:
      AWS::CloudFormation::Init:
        configSets:
          service_configuration:
          - datadog_setup
        datadog_setup:
          files:
            /etc/datadog-agent/conf.d/mysql.d/conf.yaml:
              content: &quot;@file://./config/conf-${Env}.yaml&quot;
              mode: &quot;000644&quot;
              owner: &quot;root&quot;
              group: &quot;root&quot;
          commands:
            start_datadog:
              command: service datadog-agent start
</code></pre>
</li>
</ol>
<p>Any idea how to do this in a simple, generic and secure way?
An example given would be appreciated.
Thanks in advance.</p>",53759105,1,0,,2018-12-10 08:54:12,,2018-12-13 09:53:53,2020-6-20 09:12:55,,-1,,2650254,,1,1,amazon-web-services|amazon-ec2|amazon-cloudformation|user-data|datadog,3751,15.4966
261365,1,Method,61817555,How to send log to Datadog without Datadog agent,"<p>In my understand, usual case is using Datadog agent to send error to Datadog.
However, I'd like to know there are some ways to send error to Datagog without Datadog agent.</p>

<p>For example, can we send by using Datadog webhooks?</p>",,1,0,,2020-5-15 10:55:47,,2020-5-15 15:27:58,,,,,12781011,,1,0,datadog,1277,15.4248
261366,1,Method,65438638,How to correctly requests.put in Python 3.7,"<p>I'm trying to communicate with datadog's api using the PUT method but failing with a &quot;400&quot; response. I've looked into the documentation and I'm confident my headers are set up correctly and access keys have been specified. Below is the function I'm working with:</p>
<pre><code>def editMonitor(monitor_Data):

api_url = 'https://api.datadoghq.com/api/v1/monitor/' + str(monitor_Data['id'])

response = requests.put(api_url, monitor_Data, headers=headers)

print(response)

if response.status_code == 200:
    return json.loads(response.content.decode('utf-8'))
else:
    return None
</code></pre>
<p>Below is what the header is made of:</p>
<pre><code>headers = {'Content-Type': 'application/json',
           'DD-API-KEY': '**********************',
           'DD-APPLICATION-KEY': '***********************'
           }
</code></pre>
<p>Other articles I've seen so far don't seem to answer my question.</p>",65440360,1,2,,2020-12-24 12:58:25,0,2020-12-30 14:38:36,2020-12-24 15:41:25,,2308683,,5950637,,1,2,python|python-3.x|python-requests|datadog,459,15.0473
261367,3,Monitoring,66113635,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,"<p>My current organization is migrating to DataDog for Application Performance Monitoring. I am deploying a Python Flask web application using docker to Azure Container Registry. After the deployment to Azure the app should be listed/available on Datadog portal.</p>
<p>Please note I just started learning Docker containers. There is a high chance I could do completely wrong. Please bear with me</p>
<p>Steps followed</p>
<p><strong>Option 1: Create a docker container on local machine and push to ACR</strong></p>
<ol>
<li><p>Added <code>dd-trace</code> python library to the docker image</p>
</li>
<li><p>Added dd-trace run command the docker file</p>
</li>
<li><p>build the image</p>
</li>
<li><p>run the container on local</p>
<p><strong>Getting OSError: [Errno 99] Cannot assign requested address</strong></p>
<pre><code>FROM python:3.7
ENV VIRTUAL_ENV=/opt/venv
RUN python -m venv $VIRTUAL_ENV
ENV PATH=&quot;$VIRTUAL_ENV/bin:$PATH&quot;

ENV DD_API_KEY=apikeyfromdatadoghq
ENV DD_ENV=safhire-dev
ENV DD_LOGS_ENABLED=true
ENV DD_LOGS_INJECTION=true
ENV DD_SERVICE=dev-az1-pythonbusinessservice
ENV DD_TAGS=products:myprojects
ENV DD_TRACE_DEBUG=true
ENV DD_TRACE_ENABLED=true
ENV DOCKER_ENABLE_CI=true

COPY /app /app


COPY requirements.txt /
RUN pip install --no-cache-dir -U pip
RUN pip install --no-cache-dir -r /requirements.txt
CMD ddtrace-run python app/main.py runserver 127.0.0.1:3000
</code></pre>
</li>
</ol>
<p><strong>Option 2: Forward logs to Azure Blob Storage but a heavy process</strong></p>
<ol>
<li>Deploy Python using Code base Linux</li>
<li>Forward the logs to a Azure Blob storage</li>
<li>Create a BlobTrigger Azure Function to forward the logs to DataDogAPI</li>
<li>I believe with this approach we can not capture APM logs but, we can capture application and console logs</li>
</ol>
<p><strong>Option 3: using Serilog but, my organization does not want to use third party logging framework, we have our own logging framework</strong></p>
<p>Any help is highly appreciated, I am looking for a solution using <strong>Option 1</strong>. I went through the Microsoft articles, Datadog documentation but, no luck.</p>
<p>I setup app registrations, Manage reader permissions on Subscription, created ClientID and app secrets on Azure portal. none of them helped</p>
<p>Could you confirm whether is there a way to collect the APM logs on datadog with out installing agent on Azure.</p>
<p>Thank you in advance.</p>",,1,4,,2021-2-9 05:46:36,,2021-2-19 19:45:06,2021-2-9 14:16:16,,2032722,,2032722,,1,2,python|azure|docker|datadog,696,14.7704
261368,1,Error,58579323,Custom metrics not being sent to datadog,"<p>I am running the datadog agent using docker</p>

<pre><code>    DOCKER_CONTENT_TRUST=1 \
    docker run -d -v /var/run/docker.sock:/var/run/docker.sock:ro \
          -v /proc/:/host/proc/:ro \
          -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro \
          -e DD_API_KEY=&lt;my_api_key&gt; \
          -e DD_DOGSTATD_NON_LOCAL_TRAFFIC=true \
          -e DD_LOG_LEVEL=debug \
          -p 127.0.0.1:8125:8125/udp \
          datadog/agent:latest
</code></pre>

<p>I want to send custom metrics using dogstatsd. When I run</p>

<pre><code>    echo -n ""custom_metric:60|g|#shell"" | nc -4u -w0 127.0.0.1 8125
</code></pre>

<p>I can see in wireshark that the udp packet was successful from the source to the destination but this metric is not being submitted to datadog. Am I missing some configuration?</p>",,1,0,,2019-10-27 12:13:35,,2021-2-4 22:01:35,,,,,6493646,,1,0,datadog,1510,14.7159
261369,1,Method,51975736,How to send jmeter test results to datadog?,"<p>I wanted to ask if anyone has ever saved jmeter test results (sampler names, duration, pass/fail) to Datadog? Kinda like the backend listener for influx/graphite... but for Datadog. Jmeter-plugins has no such plugin. Datadog seems to offer something called ""JMX integration"" but I'm not sure whether that is what I need.</p>",52615071,1,1,,2018-8-22 21:58:14,,2018-10-4 01:20:39,,,,,4386440,,1,2,jmeter|jmeter-plugins|datadog,2014,14.6162
261370,0,Configuration,62549173,Logging application logs in DataDog,"<p>Using datadog official docs, I am able to print the K8s <code>stdout/stderr</code> logs in DataDog UI, my motive is to print the app logs which are generated by spring boot application at a certain location in my pod.</p>
<p>Configurations done in cluster :</p>
<ol>
<li>Created ServiceAccount in my cluster along with cluster role and cluster role binding</li>
<li>Created K8s secret to hold DataDog API key</li>
<li>Deployed the DataDog Agent as daemonset in all nodes</li>
</ol>
<p>Configurations done in App :</p>
<ol>
<li>Download datadog.jar and instrument it along with my app execution</li>
<li>Exposed ports 8125 and 8126</li>
<li>Added environment tags <code>DD_TRACE_SPAN_TAGS</code>, <code>DD_TRACE_GLOBAL_TAGS</code> in deployment file</li>
<li>Changed pattern in <code>logback.xml</code></li>
<li>Added logs config in deployment file</li>
<li>Added env tags in deployment file</li>
</ol>
<p>After doing above configurations I am able to log <code>stdout/stderr</code> logs where as I wanted to log application logs in datadog UI</p>
<p>If someone has done this please let me know what am I missing here.
If required, I can share the configurations as well. Thanks in advance</p>",,1,4,,2020-6-24 06:41:38,1,2020-6-25 02:28:16,2020-6-25 02:28:16,,11921495,,11195889,,1,2,kubernetes|monitoring|datadog,1113,14.586
261371,0,Configuration,62298190,Is there a way to rotate logs in AWS ECS through task definition configuration?,"<p>I'm running datadog agent container in EC2 by configuring task definition in AWS ECS.</p>

<p>But at this time, the huge amount of logs is stored in  /var/lib/docker/containers/<strong>ContainerID</strong>/<strong>ContainerID</strong>.json so that I want to rotate it.</p>

<p>In Docker documents, I saw this <a href=""https://docs.docker.com/config/containers/logging/configure/"" rel=""nofollow noreferrer"">link</a>.</p>

<p>There are </p>

<pre><code>  ""log-opts"": {
    ""max-size"": ""10m"",
    ""max-file"": ""3""
</code></pre>

<p>Now I want to config these options through task definition but I don't know the convention of them.
Did anyone have any ideas?</p>",,1,0,,2020-6-10 07:39:43,1,2020-8-21 04:12:56,2020-6-15 06:21:14,,13329963,,10558160,,1,3,docker|amazon-ecs|datadog,894,14.4053
261372,2,Query,63779194,"In datadog, how do I query a json formatted log line, WITHOUT adding a facet?","<p>When a message is formatted as json, it is automatically turned into attributes. It seems like attributes cant be queried without first being turned into facets (which only applies to new log lines, and means you sometimes have to see something show up, then facetize it, then debug it).</p>
<p>Is there a way to query the message directly, bypassing the attribute facet requirement?</p>",63779195,2,0,,2020-9-7 14:06:53,1,2021-11-30 11:09:19,,,,,285601,,1,3,datadog,1045,14.2765
261373,0,Configuration,67281698,"Datadog export logs more than 5,000","<p>I want to export more than 5000 logs in csv from datadog, is there any configuration I need to do in datadog so that I can download 10k,20k logs at a time.</p>
<p>I also checked on <a href=""https://docs.datadoghq.com/logs/explorer/"" rel=""noreferrer"">official web page logs explorer</a> but it doesn't helped out.</p>",,1,0,,2021-4-27 10:55:45,3,2021-7-2 05:59:13,,,,,13527633,,1,7,datadog,891,14.1995
261374,2,Query,59357130,datadog replace or manually assign log value,"<p>I have a message like 'Service is running' that i'm not able to change, so in log Grok Parser I want to replace it to 'INFO | Service is running' or manually or somehow manually assign like <code>%{level=INFO}</code> .
Please kindly advice.</p>",59359866,1,0,,2019-12-16 12:58:03,,2019-12-16 15:42:44,,,,,8205634,,1,2,logging|monitoring|datadog,499,14.1924
261375,1,Method,66554063,Spring cloud sleuth adding tag,"<p>I'm trying to implement distributed tracing in my kotlin app using spring cloud sleuth.
I'm sending those data to the datadog. Now I'm able to trace my logs but I want to add some extra data to spans. Let's say I want to add info about user and be able to see it in datadog. Am I right that span tags are good for it? I'm sending the logs in json format to datadog but I cannot add tags here. (traceId and spanId are injected).
Logback config:</p>
<pre><code>        &lt;encoder class=&quot;net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder&quot;&gt;
            &lt;providers&gt;
                &lt;timestamp/&gt;
                &lt;version/&gt;
                &lt;message/&gt;
                &lt;loggerName/&gt;
                &lt;threadName/&gt;
                &lt;logLevel/&gt;
                &lt;logLevelValue/&gt;
                &lt;callerData/&gt;
                &lt;stackTrace/&gt;
                &lt;rootStackTraceElement/&gt;
                &lt;context/&gt;
                &lt;mdc/&gt;
                &lt;tags/&gt;
                &lt;logstashMarkers/&gt;
                &lt;arguments/&gt;
            &lt;/providers&gt;
        &lt;/encoder&gt;
</code></pre>
<p>gradle:</p>
<pre><code>implementation(&quot;org.jetbrains.kotlin:kotlin-reflect&quot;)
implementation(&quot;org.jetbrains.kotlin:kotlin-stdlib-jdk8&quot;)
implementation(&quot;org.springframework.cloud:spring-cloud-starter-netflix-eureka-server&quot;)
implementation(&quot;org.springframework.cloud:spring-cloud-starter-vault-config&quot;)
implementation(&quot;org.springframework.cloud:spring-cloud-starter-sleuth&quot;)
implementation(&quot;org.springframework.boot:spring-boot-starter-actuator&quot;)
implementation(&quot;ch.qos.logback:logback-classic:1.2.3&quot;)
implementation(&quot;net.logstash.logback:logstash-logback-encoder:6.6&quot;)
implementation(&quot;org.zalando:logbook-spring-boot-starter:2.4.2&quot;)
developmentOnly(&quot;org.springframework.boot:spring-boot-devtools&quot;)
runtimeOnly(&quot;io.micrometer:micrometer-registry-datadog&quot;)
testImplementation(&quot;org.springframework.boot:spring-boot-starter-test&quot;)
</code></pre>
<p>and to add the tag I'm trying</p>
<pre><code>@NewSpan
fun newSpanTest() {
    tracer.currentSpan()!!.tag(&quot;user&quot;, &quot;123&quot;)
    log.info(&quot;other span&quot;)
    otherTestService.sameSpanTest()
}
</code></pre>
<p>example log:</p>
<pre><code>{&quot;@timestamp&quot;:&quot;2021-03-09T21:04:46.953+01:00&quot;,&quot;@version&quot;:&quot;1&quot;,&quot;message&quot;:&quot;other span&quot;,&quot;logger_name&quot;:&quot;com.microservices.text.rpg.servicediscovery.TestService&quot;,&quot;thread_name&quot;:&quot;http-nio-8761-exec-3&quot;,&quot;level&quot;:&quot;INFO&quot;,&quot;level_value&quot;:20000,&quot;caller_class_name&quot;:&quot;com.microservices.text.rpg.servicediscovery.TestService&quot;,&quot;caller_method_name&quot;:&quot;newSpanTest&quot;,&quot;caller_file_name&quot;:&quot;TestService.kt&quot;,&quot;caller_line_number&quot;:25,&quot;traceId&quot;:&quot;e61fd165d7c84776&quot;,&quot;spanId&quot;:&quot;5e61f9b51b51619b&quot;}
</code></pre>
<p>shouldn't be that 'user' injected into MDC and then into logs?</p>",,1,0,,2021-3-9 20:09:45,1,2021-3-9 21:11:36,,,,,9824872,,1,0,java|spring|kotlin|datadog|spring-cloud-sleuth,1109,14.1797
261376,1,Method,64847038,How to log using NLog directly to Datadog,"<p>Using Nlog to log from my asp.net Core application, I would like to review the logs in Datadog.</p>
<p>Datadog allows me to visualize the log data, and slice, search, select and sort logs in a convenient way to provide support to my customers.</p>
<p>I was looking for a way to use NLog to directly post to the Datadog API, so I do not need to use the Windows Agent to collect the logs.</p>
<p>Below how to do this, as I could not find the answer anywhere.</p>",,1,0,,2020-11-15 16:43:15,1,2020-11-15 16:43:15,,,,,252340,,1,1,logging|nlog|datadog,550,14.1615
261377,1,Method,67574619,Datadog logs with C# and .NET,"<p>I am trying to post Datadog-logs from my C# application. I managed to send my logs with the desired structure using Postman, but I just can't figure out how to achieve the same goal from the code.
What I tried:</p>
<ol>
<li>Using DogStatsD - but I don't want to install an agent, I'd much rather use the Datadog REST API to just post my logs.</li>
<li>Using Serilog.Sinks.Datadog.Logs - Which seems to be pretty easy to use, but I can't figure out how this works, and whether it is possible to change the log structure or not. By default, there are MessageTemplate and Properties fields in the resulting json. I'd like to be able to send my own structure in one message, rather then use the MessageTemplate. Is that possible?</li>
</ol>
<p><a href=""https://i.stack.imgur.com/HPDjN.png"" rel=""nofollow noreferrer"">The desired log to be seen in Datadog UI Logs Section</a>:</p>
<pre><code>{
    hostname: myHost
    myStuff {   
    item1: item_val1
    item2: item_val2
    }
    otherStuff: oh wow this is cool
    service: MyService
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/EHqsa.png"" rel=""nofollow noreferrer"">Here's what I sent using Postman to achieve this result</a>:</p>
<p>URL: <code>https://http-intake.logs.datadoghq.com/v1/input</code></p>
<p>Headers:</p>
<pre><code>DD-API-KEY: my_api_key
Content-Type: application/json
</code></pre>
<p>Body:</p>
<pre><code>{
    &quot;ddsource&quot;: &quot;mySource&quot;,
    &quot;ddtags&quot;: &quot;myTag: myVal, myValWithoutTag&quot;,
    &quot;hostname&quot;: &quot;myHost&quot;,
    &quot;message&quot;: {
        &quot;myStuff&quot;:
        {
            &quot;item1&quot;: &quot;item_val1&quot;,
            &quot;item2&quot;: &quot;item_val2&quot;
        },
        &quot;otherStuff&quot;: &quot;oh wow this is cool&quot;
    },
    &quot;service&quot;: &quot;MyService&quot;
}
</code></pre>
<p>Is it possible to achieve the same (or even similar) result using datalog serilog sinks? If not, how can I achieve this result in C#?</p>
<p>Here is what I tried from the code:</p>
<pre><code>var config = new DatadogConfiguration(url: &quot;intake.logs.datadoghq.com&quot;, port: 443, useSSL: true, useTCP: true);
using (var log = new LoggerConfiguration().WriteTo.DatadogLogs(
        &quot;&lt;myApiKey&gt;&quot;,
        source: &quot;mySource&quot;,
        service: &quot;myService&quot;,
        host: &quot;myHost&quot;,
        tags: new string[] {&quot;myTag:myVal&quot;, &quot;myValWithoutTag&quot;},
        configuration: config
    ).
    CreateLogger())
{
    var messageTemplate = &quot;{message}&quot;;
    var message = new
    {
        myStuff = new
        {
            item1 = &quot;item_val1&quot;,
            item2 = &quot;item_val2&quot;
        }
    };

    log.Information(messageTemplate, message);
}
</code></pre>
<p>With the undesired result in Datadog UI Logs section:</p>
<pre><code>{
    host: myHost
    level: Information
    MessageTemplate: {message}
    Properties: {   
    message: { myStuff = { item1 = item_val1, item2 = item_val2 } }
    }
    service: myService
    Timestamp: 2021-05-17T00:13:14.2614896+03:00
}
</code></pre>
<p>The tags part did work, and also the host and service parts are the same. I don't mind the level and Timestamp additions,
but I'd love to change the body to behave like in the Postman example (just the message as JSON).
So my questions are:</p>
<ul>
<li>Is it possible to control the message body format using Datadog Serilog sinks?</li>
<li>Is there any good alternative that I didn't try? (except for writing my own client, which is what I'm leaning towards)</li>
<li>Can anyone explain to me how it works? I can't figure out the concept of the sink. Can anyone explain how it works? And why is there no actual REST HTTP client for this task?</li>
</ul>
<p>Thanks!</p>",68671082,1,2,,2021-5-17 17:44:47,,2021-8-5 17:29:07,2021-5-18 09:10:41,,9894041,,9894041,,1,1,c#|.net|datadog,546,14.1488
261378,0,Integration,61576615,Datadog Agent Error : Unable to collect configurations from provider docker: temporary failure in dockerutil,"<p>Datadog installation: using helm was successful </p>

<ul>
<li><p><a href=""https://docs.datadoghq.com/agent/kubernetes/?tab=helm"" rel=""nofollow noreferrer"">Docs used</a></p></li>
<li><p>Agent version (7.19.0)</p></li>
</ul>

<p>Agent Status</p>

<pre class=""lang-sh prettyprint-override""><code>$ kubectl exec -it datadog-release-7jttj agent status | egrep ""OK|ERROR""        
Defaulting container name to agent.
Use 'kubectl describe pod/datadog-release-7jttj -n default' to see all of the containers in this pod.
      Instance ID: cpu [OK]
      Instance ID: disk:e5dffb8bef24336f [OK]
      Instance ID: docker [ERROR]
      Instance ID: file_handle [OK]
      Instance ID: io [OK]
      Instance ID: kubelet:d884b5186b651429 [OK]
      Instance ID: kubernetes_apiserver [OK]
      Instance ID: load [OK]
      Instance ID: memory [OK]
      Instance ID: network:e0204ad63d43c949 [OK]
      Instance ID: ntp:d884b5186b651429 [OK]
      Instance ID: uptime [OK]

</code></pre>

<p>Error in Logs</p>

<pre class=""lang-sh prettyprint-override""><code>$ kubectl logs -f datadog-release-7jttj agent | grep ""ERROR""
2020-05-03 14:49:17 UTC | CORE | ERROR | (pkg/collector/runner/runner.go:292 in work) | Error running check docker: temporary failure in dockerutil, will retry later: try delay not elapsed yet
2020-05-03 14:49:18 UTC | CORE | ERROR | (pkg/autodiscovery/config_poller.go:123 in collect) | Unable to collect configurations from provider docker: temporary failure in dockerutil, will retry later: try delay not elapsed yet
2020-05-03 14:49:19 UTC | CORE | ERROR | (pkg/autodiscovery/config_poller.go:123 in collect) | Unable to collect configurations from provider docker: temporary failure in dockerutil, will retry later: try delay not elapsed yet
2020-05-03 14:49:20 UTC | CORE | ERROR | (pkg/autodiscovery/config_poller.go:123 in collect) | Unable to collect configurations from provider docker: temporary failure in dockerutil, will retry later: try delay not elapsed yet
2020-05-03 14:49:21 UTC | CORE | ERROR | (pkg/autodiscovery/config_poller.go:123 in collect) | Unable to collect configurations from provider docker: temporary failure in dockerutil, will retry later: try delay not elapsed yet
2020-05-03 14:49:22 UTC | CORE | ERROR | (pkg/autodiscovery/config_poller.go:123 in collect) | Unable to collect configurations from provider docker: temporary failure in dockerutil, will retry later: try delay not elapsed yet
2020-05-03 14:49:23 UTC | CORE | ERROR | (pkg/autodiscovery/config_poller.go:123 in collect) | Unable to collect configurations from provider docker: temporary failure in dockerutil, will retry later: try delay not elapsed yet

</code></pre>

<p>ERROR which i see in log agent and the same is true for all pods</p>

<pre class=""lang-sh prettyprint-override""><code> default/datadog-release-7jttj/init-config
  -----------------------------------------
    Type: file
    Path: /var/log/pods/default_datadog-release-7jttj_b373c792-b5da-46b9-a906-1fd71e9a41bc/init-config/*.log
    Status: Error: could not find any file matching pattern /var/log/pods/default_datadog-release-7jttj_b373c792-b5da-46b9-a906-1fd71e9a41bc/init-config/*.log, check that all its subdirectories are executable
      0 files tailed out of 0 files matching
</code></pre>

<p>Not sure where I am going wrong. Any help is highly appreciated.</p>",,0,6,,2020-5-3 15:00:34,,2020-5-3 15:00:34,,,,,5761011,,1,1,kubernetes|monitoring|datadog|typhoon-kubernetes,3419,14.1356
261379,1,Method,60828990,Datadog get logs without facet,"<p>I want to filter logs that either don't have a facet, say half of my logs have some <code>@facet</code> but I want the other half</p>

<p>I tried <code>-@facet</code>, <code>@facet:""""</code> and <code>NOT @facet</code>  but doesn't work and google doesn't help</p>

<p>Feels like there is an easy way for doing this, halp</p>",60829558,1,0,,2020-3-24 10:16:54,,2020-3-24 16:07:21,2020-3-24 10:25:55,,4409319,,4409319,,1,1,logging|datadog,298,14.0969
261380,3,Visualization,53441210,Stacked bar chart — top X values,"<p>I wonder if this is possible to achieve in Datadog. I have a data collected under 1 metric <code>entity.count</code> - now the data are being posted to Datadog with multiple tags, for example <code>entity.count.visits</code>, <code>entity.count.payment</code> and probably another 10 different tags.</p>

<p>I'm trying to create Datadog chart in a dashboard, which would display top 5 tags of the entity counts in a stacked bar chart. I know about the option of adding more queries, but since I'm not sure what entities will be available in the future, I would like datadog to always just display dynamically the top 5 entities in the dashboard (Insted of me specifying in the queries what tag to display). This is what I currently have (and it does the job, it's just not dynamic):</p>

<p><a href=""https://i.stack.imgur.com/tdIah.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tdIah.png"" alt=""enter image description here""></a></p>",53449811,1,0,,2018-11-23 05:45:48,,2018-11-23 16:04:50,2018-11-23 05:47:40,,107625,,6206313,,1,0,datadog,1872,14.0892
261381,3,Monitoring,59500017,How to monitor an ElasticSearch Cluster on the Elastic Cloud with Datadog?,"<p>We have an elasticsearch cluster deployed to the Elastic Cloud and would like to send monitoring/health metrics to Datadog. What is the best way to do that? </p>

<p>It seems like our options are:
* Installing the datadog agent binary via the plugins upload
* Using metric beat -> logstash -> datadog_metrics output</p>",61276030,2,0,,2019-12-27 11:18:42,1,2020-4-17 16:13:15,,,,,3869978,,1,4,elasticsearch|logstash|datadog|metricbeat,1239,13.9723
261382,3,Monitoring,62545185,Creating Datadog alerts for when the percentage difference between two custom metrics goes over a specified percentage threshold,"<p>My current situation is that I have two different data feeds (Feed A &amp; Feed B) and I have created custom metrics for both feeds:</p>
<ul>
<li>Metric of Order counts from Feed A</li>
<li>Metric Order counts from Feed B</li>
</ul>
<p>Next steps is to create alert monitoring for the agreed upon threshold of difference between the two metrics. Say we have agreed that it is acceptable for Order Counts from Feed A to be within ~5% of Order Counts from Feed B. How can I go about creating that threshold and comparison between the two metrics that I have already developed in Datadog?</p>
<p>I would like to send alerts to myself when the % difference between the two data feeds is &gt; 5 % for a daily validation.</p>",,1,0,,2020-6-23 23:16:03,,2020-6-23 23:43:21,,,,,12822559,,1,2,triggers|threshold|alerts|datadog,1380,13.9595
261383,1,Method,55588976,Forwarding the containers stdout logs to datadog without datadog agents,"<p>We re trying to eliminate Datadog agents from our infrastructure. I am trying to find a solution to forward the containers standard output logs to be visualised on datadog but without the agents and without changing the dockerfiles because there are hundreds of them.</p>

<p>I was thinking about trying to centralize the logs with rsyslog but I dont know if its a good idea. Any suggestions ?</p>",55661412,1,4,,2019-4-9 09:04:51,,2019-4-13 02:13:40,2019-4-9 09:30:14,,704848,,9563249,,1,2,docker|kubernetes|datadog,1323,13.8862
261384,0,Configuration,60064427,Proper Setup of Datadog Log Ingestion on Kubernetes,"<p>I have been working with Datadog log ingestion for about a year now. It's been (mostly) great to work with. The documentation around running it inside of Kubernetes is a bit lacking though. Their documentation covers Docker thoroughly, but Kubernetes less so. </p>

<p>When I installed Datadog into our Kubernetes clusters a year ago, there were two ways to do it, you could use a DaemonSet to ensure at least 1 Pod of Datadog runs on every Node. Or you could install it as a Deployment. I went with the DaemonSet option and used Helm to install it. That worked quite well!</p>

<p>Then we wanted to start using DogStatsD to ingest metrics about our applications, and it seemed at the time like this required the ""cluster-agent"" to run. I have serious doubts about this part. If I get all of the Datadog-related objects in my cluster I see the DaemonSet (<code>daemonset.apps/dd-agent-datadog</code>) and I also see a Deployment (<code>daemonset..apps/dd-agent-datadog</code>) on my cluster. </p>

<p>Is this right? Do I really need to run both of those things to get log ingestion and metrics?</p>",60213083,1,0,,2020-2-4 19:35:51,1,2020-2-14 07:17:50,,,,,13800,,1,3,kubernetes|datadog,361,13.83
261385,1,Error,52891518,Datadog and kubernetes running check kubelet,"<p>I set up datadog and kubernetes to test to out monitoring, although in datadog i can see some logs and metrics, in the agent in kubernetes I have the following errors:</p>

<pre><code> TRACE ] trace-agent exited with code 0, disabling
[ AGENT ] 2018-10-17 08:18:24 UTC | WARN | (datadog_agent.go:149 in LogMessage) | (base.py:212) | DEPRECATION NOTICE: device_name is deprecated, please use a device: tag in the tags list instead
[ AGENT ] 2018-10-17 08:18:26 UTC | ERROR | (kubeutil.go:50 in GetKubeletConnectionInfo) | connection to kubelet failed: temporary failure in kubeutil, will retry later: try delay not elapsed yet
[ AGENT ] 2018-10-17 08:18:26 UTC | ERROR | (runner.go:289 in work) | Error running check kubelet: [{""message"": ""Unable to detect the kubelet URL automatically."", ""traceback"": ""Traceback (most recent call last):\n File ""/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/checks/base.py"", line 352, in run\n self.check(copy.deepcopy(self.instances[0]))\n File ""/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/kubelet/kubelet.py"", line 107, in check\n raise CheckException(""Unable to detect the kubelet URL automatically."")\nCheckException: Unable to detect the kubelet URL automatically.\n""}]
[ AGENT ] 2018-10-17 08:18:28 UTC | ERROR | (autoconfig.go:604 in collect) | Unable to collect configurations from provider Kubernetes: temporary failure in kubeutil, will retry later: try delay not elapsed yet

image:
repository: datadog/agent
tag: 6.4.2
</code></pre>

<p>As the logs state the agent cannot connect to Kubectl, has anyone come across this? </p>",,1,0,,2018-10-19 11:31:11,1,2018-11-13 23:03:16,2018-10-20 13:16:47,,1555190,,1555190,,1,1,kubernetes|datadog,1427,13.8177
261386,0,Integration,61091978,Quarkus logging with Datadog,"<p>In Quarkus, the default logging library is JBoss and using the <code>quarkus-logging-json</code> allows you to encode your logs as JSON. </p>

<p>However, Datadog integration requires custom fields such as <code>service</code>, <code>dd.span_id</code> and <code>dd.trace_id</code> to have the logs associated with the correct syntax.</p>

<p>Currently, I've tried adding this in <code>application.properties</code>: </p>

<pre><code>quarkus.log.console.format=%d{yyyy-MM-dd HH:mm:ss.SSS} %-5p service=%X{myServiceName}, traceId=%X{dd.trace_id}, spanId=%X{dd.span_id} [%c{2.}] (%t) %s%e%n
</code></pre>

<p>However, this seems not to show up in Datadog as expected. </p>

<p>When we use log4j2, we simply configure it like so.</p>

<pre class=""lang-yaml prettyprint-override""><code>Appenders:
  Console:
    name: Console_Appender
    target: SYSTEM_OUT
    JSONLayout:
      KeyValuePair:
          - key: service
          value: myServiceName
</code></pre>

<p>Again, cannot find any documentation how to accomplish the same result in Quarkus configs. </p>

<p>Does anyone know how I could inject these custom properties into the JSON logs with Quarkus or how to integrate it with Datadog properly?</p>",,1,0,,2020-4-8 01:29:32,,2021-10-15 13:57:28,2020-4-14 03:28:13,,5305936,,8686989,,1,1,logging|quarkus|datadog|jboss-logging,766,13.7369
261387,0,Configuration,61148607,Spring boot micro meter datadog socket connection error,"<p>I am working on to create some custom metrics for my spring boot 2 rest api. I have added the required micro meter and datadog dependency. My office machine works behind a proxy. I have setup proxy through spring boot plugin.</p>

<pre><code>-Dhttp.proxyHost=xxxx.proxy.com
-Dhttp.proxyPort=xxxx
</code></pre>

<p>below are in my application.properties file. 
    management.metrics.export.datadog.apiKey=mykey</p>

<pre><code>management.metrics.export.datadog.uri=https://app.datadoghq.com

management.metrics.export.datadog.enabled=true

management.metrics.export.datadog.step=10s
</code></pre>

<p>But I am getting the socket connection timeout. </p>

<pre><code>   [datadog-metrics-publisher] 10 Apr 2020 16:51:39,552 WARN  DatadogMeterRegistry [{}]: java.net.SocketTimeoutException: connect timed out
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:606)
    at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:666)
    at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:463)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:558)
    at sun.net.www.protocol.https.HttpsClient.&lt;init&gt;(HttpsClient.java:264)
    at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367)
    at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191)
    at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1162)
    at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)
    at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177)
    at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1340)
    at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1315)
    at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:264)
    at io.micrometer.core.ipc.http.HttpUrlConnectionSender.send(HttpUrlConnectionSender.java:96)
    at io.micrometer.core.ipc.http.HttpSender$Request$Builder.send(HttpSender.java:284)
    at io.micrometer.datadog.DatadogMeterRegistry.publish(DatadogMeterRegistry.java:141)
    at io.micrometer.core.instrument.push.PushMeterRegistry.publishSafely(PushMeterRegistry.java:48)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
</code></pre>

<p>As far as I debugged the io.micrometer.core.ipc.http.HttpUrlConnectionSender.send method is failing and I dont under how the micro meter data dog takes the proxy details. </p>

<p>The micrometer doc says </p>

<pre><code>management.metrics.export.datadog.uri=https://app.datadoghq.com # URI to ship metrics to. If you need to publish metrics to an internal proxy en-route to Datadog, you can define the location of the proxy with this.
</code></pre>

<p>But I dont understand what it means? should I replace this url with my proxy url or is there any specific uri pattern with the proxy?  I am using spring boot 2.2.4.RELEASE</p>",61247522,1,2,,2020-4-10 20:59:55,,2020-4-16 10:00:50,,,,,4850377,,1,2,spring-boot|datadog|spring-micrometer,1154,13.6488
261388,1,Method,60170609,How to post process JSON logs with Datadog?,"<p>Our applications log in JSON format. According to Datadog's documentation JSON logs are not processed by pipelines. How can I enrich the JSON logs with an additional field that is based on a different value of that same log line?</p>

<p>I have this line:</p>

<pre><code>{""requestUri"":""/customers/2934ht8/users""}
</code></pre>

<p>And I want this line:</p>

<pre><code>{""requestUri"":""/customers/2934ht8/users"",""customerId"":""2934ht8""}
</code></pre>

<p>Is this possible with Datadog? I do not want to change our loggers to a the <code>customerId</code> to the log output.</p>",,1,0,,2020-2-11 14:01:42,,2020-2-11 14:50:51,,,,,1191261,,1,2,datadog,2004,13.6076
261389,3,Monitoring,52989371,Datadog - monitor that alert you if the value of the metric does not change for three days,"<p>I am trying to add new monitor to datadog.<br>
I added the metric to my code.<br>
And I can see this metric on datadog (goto Metrics -> explorer -> Graph).<br>
Now I am trying to create monitoring on datadog that will alert me if the value of metric don't change for three days in a row.<br>
Is it possible to create this kind of monitoring?</p>

<p>Thanks.  </p>",,1,0,,2018-10-25 12:32:48,,2018-10-28 07:39:30,2018-10-25 14:07:17,,4742614,,4742614,,1,1,datadog,1183,13.4919
261390,0,Configuration,51524214,Best practices for using NLOG with a multi-user based system?,"<p>We need to separate logs generated by a REST web-service on a user-specific basis and eventually import these logs into an aggregation framework like Datadogs.com. </p>

<p>There are several ways to approach this and I’m interested in getting feedback before selecting an approach. </p>

<p>The basics would go something like this:</p>

<ul>
<li>Rest service is called with UserID as an argument.</li>
<li>Set the nlog LogManager.Configuration.Variable[""userid""] = userid, or use MappedDiagnosticContext.  </li>
<li>Write the log normally until the REST call completes.</li>
</ul>

<p>Depending on the stage</p>

<ul>
<li><p>For development, use the NLOG File Logger, so the developer can simply “tail” the log file. Use the variable in the nlog filename target as<br>
<code>&lt;target filename=""/path/file-${var:userid}.log""/&gt;.</code>  Or use the, it in the nlog target as:
<code>layout=""${var:userid}-${OtherLayout}</code>"", and have developers do a <code>tail -f masterFile.log | grep USERID.</code></p></li>
<li><p>In production switch to using the nlog JsonLayout, so a system like DataDog can read Time, Threadid, Userid and message data. Use the JsonFormat and specify a userid attributes.   I see that the JsonFormat has supports for MappedDiagnosticsLogicalContext but I would prefer the simplicity of the ${var:xxx} format to specify the value. </p></li>
</ul>

<p>Problems:</p>

<p>I’ve used the MappedDiagnosticContext in the past and then specified it in the target filename.  I just attempted to use the ${var} approach and it did not appear to work??</p>

<p>Concerns: </p>

<p>I like the filename target approach for development.  It should work well with 10-100 of users but not scale with 1000's of users.   Clearly with 1000’s of users we would need to close the log file after each line is written, as we don’t want to keep 1000’s of files open.  </p>

<p>A major concern is threading.  It’s possible that each webservice is called multiple times by different users and all approaches require Nlog MappedDiagnosticContext or ${var} capability’s to be thread safe.  Is it?  Any issues to consider? </p>

<p>Eventually we would like to introduce some structured logging into the system, but the majority of the code base was built using standard logging techniques.  If the objects being logged in the structured logging included userid then much of this complexity could be avoided, but that would require a lot of work rewriting for structured logging. </p>

<p>Clearly, there is a lot to think about and I know I’m not the first to ponder this. </p>

<p>Your input will be appreciated. </p>",51552249,1,4,,2018-7-25 17:00:10,1,2018-7-27 07:22:49,,,,,682877,,1,0,logging|nlog|datadog,1321,13.4836
261391,1,Error,55087322,DataDog docker agent doesn't recieve logs from application container,"<p>I have Golang app, it writes logs to Stdout with Logrus.
I was trying to recreate this <a href=""https://github.com/DataDog/docker-compose-example"" rel=""nofollow noreferrer"">https://github.com/DataDog/docker-compose-example</a> scenario, and replace python app with my app.
But logs aren't coming to Datadog dashboad
This is docker-compose I'm trying to make work</p>

<pre><code>version: ""3""
services:
  gos:
    build: goapp
    stdin_open: true
    ports:
      - ""6000:6000""
    volumes:
      - /tmp/goapp:/tmp/goapp
      - ./goapp:/code
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - DATADOG_HOST=datadog
  web:
    build: web
    command: python app.py
    ports:
     - ""5000:5000""
    volumes:
     - ./web:/code # modified here to take into account the new app path
    links:
     - redis
    environment:
     - DATADOG_HOST=datadog # used by the web app to initialize the Datadog library
  redis:
    image: redis
  # agent section
  datadog:
    build: datadog
    links:
     - redis # ensures that redis is a host that the container can find
     - web # ensures that the web app can send metrics
    environment:
     - DD_API_KEY=34f-------63c
    volumes:
     - /var/run/docker.sock:/var/run/docker.sock
     - /proc/:/host/proc/:ro
     - /sys/fs/cgroup:/host/sys/fs/cgroup:ro
</code></pre>

<p>I also tried non-compose, but simple docker container installation for the agent by this <a href=""https://docs.datadoghq.com/logs/log_collection/docker/?tab=containerinstallation"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/log_collection/docker/?tab=containerinstallation</a> instructions.</p>

<p>I run my golang app container with</p>

<pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock:rw -d testgo
</code></pre>

<p>with Dockerfile</p>

<pre><code>FROM golang:1.7.3
WORKDIR /go/src/github.com/alexellis/href-counter/
RUN go get -d -v github.com/Sirupsen/logrus
COPY app.go .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .

FROM alpine:latest  
RUN apk --no-cache add ca-certificates
WORKDIR /root/
COPY --from=0 /go/src/github.com/alexellis/href-counter/app .
EXPOSE 6000
LABEL ""com.datadoghq.ad.logs""='[{""source"": ""goapp"", ""service"": ""webapp""}]'
CMD [""./app""] 
</code></pre>

<p>and DD agent can see app container ups and downs but receive no logs</p>",,1,3,,2019-3-10 11:47:35,1,2019-3-13 22:27:40,2019-3-10 18:17:00,,1398083,,1398083,,1,1,docker|docker-compose|datadog,2013,13.4154
261392,3,Visualization,51521838,show last last_autovacuum/anaylze etc based on the 20 biggest tables,"<p>I'm trying to monitor our postgresql DB and identify the 20 largest tables and than see when was the last vacuum and analyse took place.</p>

<p>I have this query that shows me the largest 20 schema name/relname which is good and that's what I was looking for:</p>

<pre><code>SELECT schema_name, relname, pg_size_pretty(table_size) AS size, table_size 
FROM ( SELECT pg_catalog.pg_namespace.nspname  AS schema_name,relname, 
pg_relation_size(pg_catalog.pg_class.oid) AS table_size FROM 
pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON relnamespace = 
pg_catalog.pg_namespace.oid ) t WHERE schema_name NOT LIKE 'pg_%' ORDER BY 
table_size DESC LIMIT 20;
</code></pre>

<p>I also have this query that shows me all the analysis I want to see with schema name and relname:</p>

<pre><code>select relname, schemaname, last_vacuum, last_autovacuum, last_analyze, last_autoanalyze from pg_stat_user_tables;
</code></pre>

<p>But I'm having a real hard time combining them together to one query that will show me when those analysis only for those 20 tables.</p>

<p>Once this is done I'm looking to view the results in some sort of a graphic view in datadog, so If anyone have a good idea how to run this query as a datadog posgres query it will be amazing as well.</p>",52038935,2,0,,2018-7-25 14:48:20,,2018-8-27 12:09:41,,,,,10134087,,1,1,sql|postgresql|datadog,1688,13.3095
261393,1,Method,52390804,Datadog how to implement ddtrace on Flask application?,"<p>I was able to follow these instructions carefully and thoroughly <a href=""https://docs.datadoghq.com/tracing/setup/python/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/tracing/setup/python/</a>,</p>

<p>I successfully installed DataDog Agent following this guide <a href=""https://docs.datadoghq.com/tracing/setup/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/tracing/setup/</a>,</p>

<p>I was also able to install MacOS tracer since it is required for mac user: <a href=""https://github.com/DataDog/datadog-trace-agent#run-on-osx"" rel=""nofollow noreferrer"">https://github.com/DataDog/datadog-trace-agent#run-on-osx</a>,</p>

<p>I enabled apm_config in the configuration file found here: <a href=""https://docs.datadoghq.com/agent/faq/agent-configuration-files/?tab=agentv6#agent-main-configuration-file"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/faq/agent-configuration-files/?tab=agentv6#agent-main-configuration-file</a></p>

<pre><code>apm_config:
  enabled: true
</code></pre>

<p>I leave the <code>env: none</code> since I only need to run it in on development/debug mode.</p>

<p>Now Im currently on the step 4: <strong>Instrument your application</strong> guide for Flask and here the steps I took:</p>

<ol>
<li><code>$ pip install ddtrace</code></li>
<li><p>Add integration for flask:</p>

<pre><code>import blinker as _

from ddtrace import tracer
from ddtrace.contrib.flask import TraceMiddleware

app = Flask(__name__, static_folder='../public/', static_url_path='')

traced_app = TraceMiddleware(app, tracer, service=""my-app"", distributed_tracing=True)
</code></pre></li>
</ol>

<p>And also my application runs in a docker container and this is what I get from the output log:</p>

<blockquote>
  <p>ERROR:ddtrace.writer:cannot send services to localhost:8126</p>
</blockquote>

<p><strong>Additional Information</strong></p>

<p>On the tracer agent:</p>

<pre><code>INFO (main.go:161) - trace-agent running on host CPUZ124.local
INFO (receiver.go:140) - listening for traces at http://localhost:8126
</code></pre>",,1,0,,2018-9-18 16:19:02,,2019-6-16 15:14:27,,,,,6143656,,1,3,python|datadog,1478,13.2787
261394,3,Monitoring,62474317,Datadog metric monitor notification on Slack with link to the log explorer,"<p>I'm trying to setup a notification message on Slack for a monitor on a custom metric that we created. 
I would like the message to include a <strong>timestamp</strong> of the event, and also a link that redirect to the log, to analyze it immediately. Are there any template variable like {{var}} that let me insert the timestamp and the link to the log, or maybe that let me build the log search query string
dynamically like: 
<strong><a href=""https://app.datadoghq.com/logs"" rel=""nofollow noreferrer"">https://app.datadoghq.com/logs</a>?....</strong>
(so I will need the timestamp at least)? </p>

<p>At the moment we only have this in the message: </p>

<h1>There’s a  {{result.name}} in Registration Service</h1>

<ul>
<li><p>CHANNEL: {{channel.name}} </p></li>
<li><p>ENVIRONMENT: {{environment.name}}.</p></li>
</ul>

<h1>Please investigate!</h1>",,2,0,,2020-6-19 16:08:58,,2020-7-1 08:29:39,2020-6-19 16:20:32,,3327131,,3327131,,1,0,logging|slack|metrics|monitor|datadog,1170,13.2727
261395,3,Monitoring,60326963,Datadog AWS EBS Monitoring - Drive Space - Exclude /dev/loop* devices,"<p>Okay, here is my setup:</p>

<ul>
<li>Platform: AWS </li>
<li>Monitoring: DataDog </li>
<li>Metric: system.disk.in_use</li>
</ul>

<p>Question: So I am running Ubuntu 18.04LTS instances and as time goes on, it seems to spawn additional devices periodically:</p>

<p>device:/dev/loop1, /dev/loop2 and so on.</p>

<p>When I first spun up these instances, there were only 3 /dev/loop(1-3) devices, however, over time, a /dev/loop4 showed up and our drive space alert paged me since these are 100% utilized when created.</p>

<p>So, I have to go into each of the monitors (one per environment) and add an exclusion for the new /dev/loop4, but I cannot set the exclusion until it has been created by at least one of the monitored instances.</p>

<p>Is there a way in DataDog that you can just add a blanket exclusion like:</p>

<p>device:/dev/loop*?</p>

<p>I have been combing through documentation and have not been able to find anything, so I thought I would ask here.</p>",,2,0,,2020-2-20 19:07:44,,2021-10-16 02:39:27,,,,,4579896,,1,0,amazon-web-services|ubuntu|ubuntu-18.04|datadog|aws-ebs,657,13.2703
261396,1,Error,60940393,Datadog spans lost in python thread pool,"<p>I have a function that runs in a thread pool, but it only shows up in the Datadog tracing UI when I run it outside of my threadpool. In the screenshot below you can see it show up in <code>sync_work</code> but not in <code>async_work</code>.</p>

<p><a href=""https://i.stack.imgur.com/t2uNn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t2uNn.png"" alt=""enter image description here""></a></p>

<p>Here is my code, contained in a script called <code>ddtrace_threadpool_example.py</code>:</p>

<pre><code>from concurrent.futures import ThreadPoolExecutor
from ddtrace import tracer
from random import random
import time


def perform_work(input_value):
    with tracer.trace('do_something') as _:
        seconds = random()
        time.sleep(seconds)
        return input_value**2


def sync_work(input_values):
    with tracer.trace('sync_work') as _:
        results = []
        for input_value in input_values:
            result = perform_work(input_value=input_value)
            results.append(result)
        return results


def async_work(input_values):
    with tracer.trace('async_work') as _:
        thread_pool = ThreadPoolExecutor(max_workers=10)
        futures = thread_pool.map(
            lambda input_value:
            perform_work(input_value=input_value),
            input_values
        )
        results = list(futures)
        return results


@tracer.wrap(service='ddtrace-example')
def start_work():
    input_values = list(range(15))
    sync_results = sync_work(input_values=input_values)
    print(sync_results)
    async_results = async_work(input_values=input_values)
    print(async_results)


if __name__ == '__main__':
    start_work()
</code></pre>

<p>I run the script like this: <code>python ddtrace_threadpool_example.py</code>. I'm using Python 3.7, and <code>pip freeze</code> shows <code>ddtrace==0.29.0</code>.</p>",,1,0,,2020-3-30 22:13:17,1,2021-4-13 00:24:18,,,,,554481,,1,1,python|datadog,568,13.2174
261397,2,Query,56172624,Can you perform equality matching on message template variables in data dog?,"<p>I'm setting up datadog monitors/alerts and want to have alerts routed to slack or pagerduty depending on if the issue is in our production environment or not. I've created multi-alert monitors that alert correctly, but I can't figure out how to make only ones where <code>environment.name</code> is equal to <code>prod</code> send an alert to pagerduty, and always send them to Slack.</p>

<p>I was hoping to be able to do something like the following in the alert message but haven't been able to figure out a syntax that works:</p>

<pre><code>[...alert message...]

{{#environment.name==prod}}@pagerduty{{/environment.name}}
@slack
</code></pre>

<p>For now, I've found a work around of having two monitors that are duplicates of each other where one has is scoped to production only and alerts pagerduty only and the second is for all environments and alerts slack only. However, I know this is going to become a maintenance nightmare as we grow and I'd like to know if there's a better solution.</p>",56173299,1,0,,2019-5-16 16:07:50,,2019-5-16 16:55:13,,,,,1608327,,1,0,alert|datadog,610,13.1413
261398,1,Error,62739543,NodeJS not posting POST body to DataDog logs,"<p>I'm trying to integrate DataDog with my NodeJS/Express application, however it appears that when a POST request is sent to my app the body of the POST is not being passed along to datadog, how can I fix this?</p>
<p>I have a file called <code>Winston.js</code> which looks like so:</p>
<pre><code>let appRoot = require('app-root-path');
let winston = require('winston');

// define the custom settings for each transport (file, console)
let options = {
    file: {
        level: 'info',
        filename: `${appRoot}/logs/app.log`,
        handleExceptions: true,
        json: true,
        maxsize: 52428800,
    },
    console: {
        level: 'debug',
        handleExceptions: true,
        json: false,
        colorize: true,
    },
};

// instantiate a new Winston Logger with the settings defined above
let logger = winston.createLogger({
    transports: [
        new winston.transports.File(options.file),
        new winston.transports.Console(options.console)
    ],
    exitOnError: false, // do not exit on handled exceptions
});

// create a stream object with a 'write' function that will be used by `morgan`
logger.stream = {
    write: function(message, encoding) {
        // use the 'info' log level so the output will be picked up by both transports (file and console)
        logger.info(message);
    },
};

module.exports = logger;
</code></pre>
<p>And then I'm attaching them to my app using the following:</p>
<pre><code>app.use(morgan('combined', { stream: winston.stream }));
</code></pre>
<p>Currently my logs in DataDog look like this:
<a href=""https://i.stack.imgur.com/SPtAR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SPtAR.png"" alt=""enter image description here"" /></a></p>",62747357,1,0,,2020-7-5 10:34:24,,2020-7-5 23:24:28,,,,,2505093,,1,1,node.js|datadog,543,13.1392
261399,3,Monitoring,62787931,How do I set a separate message for warning vs alert in terraform for Datadog?,"<p>I'm setting up a monitor that looks like this:</p>
<pre><code>resource &quot;datadog_monitor&quot; &quot;queue_size_critical&quot; {
  message = &quot;High Priority&quot;
  name    = &quot;too many messages in queue&quot;
  query   = &quot;max(last_10m):max:aws.sqs.approximate_number_of_messages_visible{aws_account:&lt;account&gt;,queuename:&lt;queuename&gt;} &gt; 10&quot;
  type    = &quot;metric alert&quot;
  tags    = my_tags
  thresholds = {
    ok = 0
    warning = 1
    critical = 10
  }
  renotify_interval = 1440
}
</code></pre>
<p>I've also got a widget that looks like this:</p>
<pre><code>widget {
  alert_value_definition {
    alert_id = datadog_monitor.queue_size_critical.id
    title    = datadog_monitor.queue_size_critical.name
  }
}
</code></pre>
<p>I'd like to define two different messages, one of which will be sent when the &quot;warning&quot; threshold is crossed, and the other which will be sent when the &quot;critical&quot; threshold is crossed.</p>
<p>How can I do this?</p>
<p>Is this correct?</p>
<pre><code>resource &quot;datadog_monitor&quot; &quot;queue_size_critical&quot; {
      message = &quot;{{#is_alert}}High Priority{{/is_alert}}
                 {{#is_warning}}Low priority{{/is_warning}}
                 This gets sent every time, in every message.&quot;
      name    = &quot;too many messages in queue&quot;
      query   = bla bla bla
      ...etc...
}
</code></pre>",62798229,1,0,,2020-7-8 05:08:21,,2020-7-8 18:49:02,2020-7-8 18:49:02,,10898235,,10898235,,1,1,terraform|monitor|threshold|datadog,515,13.0472
261400,0,Integration,53231545,How to install Datadog agent in AWS lambda,<p>We want to collect metrics from machines running AWS lambda in AWS. How can I get access to these machines and get DD agent installed on them.</p>,,1,1,,2018-11-9 18:36:29,,2018-11-9 19:54:22,,,,,2307671,,1,0,aws-lambda|datadog,573,13.0326
261401,2,Query,52096873,Unique tag count in a DataDog top list,"<p>I've a metric which has 2 tags (it has more but this is for simplicity), <em>client</em> and <em>rule</em>, and its value of course. With it I can see the total count of the values for each client, each rule and each ruleXclient.</p>

<p>Now I want to create a top list which would tell me the amount of unique clients per rule, so if the metric is reporting that 2 clients (2 values of the tag client) have 4 hits each for a rule (single value for the tag rule) I'd like the top list to show me:</p>

<p><em>2: RuleA</em></p>

<p>Is that even possible? How could I approach this if it isn't?</p>

<p>I'm using dogstreams to report the metric.</p>",,0,1,,2018-8-30 12:16:39,,2021-6-24 13:07:39,,,,,2336267,,1,6,datadog,1789,13.0104
261402,1,Method,63314162,How to define source-tag for datadog when sending logs agentless with logback from Spring Boot?,"<p>How can logback be configured to add tags,so that datadog can recognize the source?</p>
<p>I have the following <code>logback.xml</code>:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;!DOCTYPE configuration&gt;
&lt;configuration&gt;
    &lt;springProperty scope=&quot;local&quot; name=&quot;DATADOG_API_KEY&quot; source=&quot;datadog.api-key&quot; /&gt;
    &lt;appender name=&quot;datadog&quot; class=&quot;net.logstash.logback.appender.LogstashTcpSocketAppender&quot;&gt;
        &lt;destination&gt;intake.logs.datadoghq.com:10514&lt;/destination&gt;
        &lt;keepAliveDuration&gt;1 minute&lt;/keepAliveDuration&gt;
        &lt;includeCallerData&gt;true&lt;/includeCallerData&gt;
        &lt;encoder class=&quot;net.logstash.logback.encoder.LogstashEncoder&quot;&gt;
            &lt;includeCallerData&gt;true&lt;/includeCallerData&gt;
            &lt;includeTags&gt;true&lt;/includeTags&gt;
            &lt;customFields&gt;{&quot;ddtags&quot;: &quot;source:java&quot;}&lt;/customFields&gt;
            &lt;prefix class=&quot;ch.qos.logback.core.encoder.LayoutWrappingEncoder&quot;&gt;
                &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt;
                    &lt;pattern&gt;${DATADOG_API_KEY} %mdc{weJustNeedSthEmptyHereSoTheXMLParserWillKeepAWhitespace}&lt;/pattern&gt;
                &lt;/layout&gt;
            &lt;/prefix&gt;
        &lt;/encoder&gt;
    &lt;/appender&gt;
    &lt;root&gt;
        &lt;appender-ref ref=&quot;datadog&quot;/&gt;
    &lt;/root&gt;
&lt;/configuration
</code></pre>
<p>Where the custom field <code>ddtags</code> is supposed to set tags for datadog.</p>
<p>The logs show up in datadog and everything works as expected, despite the <code>source</code>-tag. The log messages sent from my service show up with two tags in datadog: <code>source:java</code> and <code>source:undefined</code>:</p>
<p><a href=""https://i.stack.imgur.com/D3mB3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D3mB3.png"" alt=""tags as they show up in datadog"" /></a></p>
<p>How do I get rid of the <code>source:undefined</code> tag so that datadog correctly recognizes the source?</p>",65920537,1,1,,2020-8-8 10:06:44,,2021-1-27 13:50:53,2020-8-8 17:16:32,,834309,,834309,,1,1,spring-boot|datadog|logstash-logback-encoder,495,12.9784
261403,1,Error,61405563,"NUnit, Serilog and Datadog","<p>we are facing a problem using NUnit, Serilog, and Datadog.</p>

<h1>Our configuration</h1>

<p>We are working with:</p>

<ul>
<li>.NET Core 3.1 (Visual Studio 16.5.4)</li>
<li>NUnit + NUnit3TestAdapter</li>
<li>Serilog + Serilog.Sinks.Datadog.Logs</li>
</ul>

<p>All packages are NuGet lastest.</p>

<h1>Serilog configuration</h1>

<p>This is the Serilog configuration we are using:</p>

<pre><code>DatadogConfiguration datadogConfiguration = new DatadogConfiguration()
{
  Url = ""https://http-intake.logs.datadoghq.eu"",
  Port = 443,
  UseSSL = true,
  UseTCP = false
};

ServiceProvider = new ServiceCollection()
  .AddSingleton&lt;IMyService, MyService&gt;()
  .AddLogging(configure =&gt; configure.AddSerilog(
    new LoggerConfiguration()
    .MinimumLevel.Verbose()
    .Enrich.WithMachineName()
    .Enrich.WithProcessName()
    .Enrich.WithProcessId()
    .Enrich.WithThreadName()
    .Enrich.WithThreadId()
    .Enrich.WithProperty(""Application"", ""Serilog Test Application"")
    .Enrich.WithExceptionDetails()
    .WriteTo.Debug(
      outputTemplate: ""[{Timestamp:o} {Level:u3}] [{MachineName}] [{ProcessName}:{ProcessId}] [{ThreadName}:{ThreadId}] [{Application}] [{SourceContext}] {Message:lj}{Exception}{Properties:j}{NewLine}"")
    .WriteTo.File(
      path: ""D:\\Temp\\LogFile.txt"",
      outputTemplate: ""[{Timestamp:o} {Level:u3}] [{MachineName}] [{ProcessName}:{ProcessId}] [{ThreadName}:{ThreadId}] [{Application}] [{SourceContext}] {Message:lj}{Exception}{Properties:j}{NewLine}"")
    .WriteTo.DatadogLogs(
      ""API_KEY"",
      source: ""csharp"",
      service: ""test service (by code)"",
      host: ""Serilog test application"",
      tags: new string[] { ""TAG_1:VALUE_1"", ""TAG_2:VALUE_2"" },
      configuration: datadogConfiguration)
    .CreateLogger()))
  .BuildServiceProvider();
</code></pre>

<h1>The test</h1>

<p>We are testing this configuration both in debug run (F5 key in Visual Studio) and under the NUnit test environment (in Visual Studio).</p>

<h1>The problem</h1>

<p>The problem we are facing is that while in debug run all work fine:</p>

<ul>
<li>Logs correctly arrive on Visual Studio Debug Output.</li>
<li>Logs correctly arrive on file.</li>
<li>Logs correctly arrive on Datadog.</li>
</ul>

<p>when we run this code in the NUnit environment:</p>

<ul>
<li>Logs correctly arrive on Visual Studio Debug Output.</li>
<li>Logs correctly arrive on file.</li>
</ul>

<p>but <strong>no logs arrive at Datadog</strong>.</p>

<p>Checking the network stream with Fiddler we notice that while in debug run, logs are sent to Datadog, under the NUnit environment <strong>logs are NOT sent to Datadog</strong>.</p>

<p>Any ideas or suggestions?</p>

<p>Thank you</p>",,2,2,,2020-4-24 09:48:46,,2020-12-30 08:52:27,,,,,5666309,,1,0,.net-core|nunit|serilog|datadog,943,12.898
261404,0,Configuration,58564144,Datadog Configuration for Postgres 9.6,"<p>I have a question about the configuration setting for datadog for postgres 9.6.</p>

<p>(1) How do I get all databases monitored in datadog?
(2) How do I get all table level metrics from each database/schema?</p>

<p>Here is conf file.</p>

<pre><code>init_config:

instances:
- host: host_name_goes_here
port: port_number_goes_here
username: datadog
password: password_goes_here
dbname: db_name1

  relations:
  - relation_name: table_1 --This will only give you metrics for table specified here.
  - relation_regex: '.*' --This will give you metrics for all the tables in the database.
</code></pre>

<p>Datadog documents are not really helpful. Instead of listing all the dbs, I want all databases, so if we add a new db, we don't have to change the conf file and same goes for table_name.</p>

<p>According to datadog docs, the table level metrics are collected using pg_stat_user_tables, pg_statio_user_tables etc. And these postgres tables are database specific unlike pg_stat_activity or pg_stat_statements.</p>",,1,0,,2019-10-25 18:48:57,2,2019-11-20 21:00:46,2019-10-25 20:33:22,,10913713,,10913713,,1,1,postgresql|config|datadog|postgres-9.6,463,12.8623
261405,1,Method,60615996,How to import Datadog JSON template in terraform DSL?,"<pre><code>{
   ""title"":""xxxx"",
   ""description"":""xxx"",
   ""widgets"":[
      {
         ""id"":0,
         ""definition"":{
            ""type"":""timeseries"",
            ""requests"":[
               {
                  ""q"":""xxxxxx{xxxx:xx}"",
                  ""display_type"":""bars"",
                  ""style"":{
                     ""palette"":""cool"",
                     ""line_type"":""solid"",
                     ""line_width"":""normal""
                  }
               }
           ]
        }
    ]
}
</code></pre>

<p>I have the above datadog json template with me which I have to just import in terraform instead of recreating it as terraform dsl. </p>",,1,7,,2020-3-10 10:46:31,,2020-3-12 19:36:18,,,,,1482709,,1,0,terraform|datadog,918,12.8514
261406,3,Monitoring,62123685,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,"<p>I’m looking to enable JMX to allow datadog to monitor our java JBoss wildfly systems but keep hitting runtime errors</p>

<p>I have set up the standalone.xml with</p>

<pre><code>&lt;subsystem xmlns=""urn:jboss:domain:jmx:1.3""&gt;              
            &lt;expose-resolved-model/&gt;                     
            &lt;expose-expression-model/&gt;                           
            &lt;remoting-connector use-management-endpoint=""true""/&gt;                       
&lt;/subsystem&gt;
</code></pre>

<p>And</p>

<pre><code>&lt;interfaces&gt;
    &lt;interface name=""management""&gt;
    &lt;inet-address value=""${jboss.bind.address.management:127.0.0.1}""/&gt;
&lt;/interface&gt;
</code></pre>

<p>As well as </p>

<pre><code>&lt;socket-binding name=""management-http"" interface=""management"" port=""${jboss.management.http.port:9990}""/&gt;
</code></pre>

<p>Then in my startup.sh i have added</p>

<pre><code>JAVA_OPTS=""$JAVA_OPTS -Dcom.sun.management.jmxremote""
</code></pre>

<p>But this gives me </p>

<blockquote>
  <p>java.lang.IllegalStateException: The LogManager was not properly
  installed (you must set the ""java.util.logging.manager"" system
  property to ""org.jboss.logmanage r.LogManager"")</p>
</blockquote>

<p>This seems to be fairly common if I look at both here and on google but there seem to be different solutions depending on the version of wildfly.</p>

<p>I think I need to do something like
Set at the start of the standalone.conf</p>

<pre><code>JBOSS_MODULES_SYSTEM_PKGS=""org.jboss.logmanager""
</code></pre>

<p>And then</p>

<pre><code>JBOSS_HOME=""/opt/wildfly""
JAVA_OPTS=""$JAVA_OPTS -Djava.util.logging.manager=org.jboss.logmanager.LogManager -Xbootclasspath/p:$JBOSS_HOME/modules/system/layers/base/org/jboss/logmanager/main/jboss-logmanager-2.1.11.Final.jar -Xbootclasspath/p:$JBOSS_HOME/modules/system/layers/base/org/jboss/log4j/logmanager/main/log4j-jboss-logmanager-1.2.0.Final.jar""
</code></pre>

<p>At the end.</p>

<p>But I still get errors “Could not load Logmanager ""org.jboss.logmanager.LogManager""”</p>

<p>Any advice would be appreciated.</p>",,2,3,,2020-5-31 23:30:53,,2021-10-21 18:14:42,,,,,10701833,,1,2,java|jboss|wildfly|fix-protocol|datadog,985,12.7737
261407,1,Method,52066057,Easiest way to rename a metric in datadog?,"<p>I'm using a <code>statsd.timed</code> to send some time metrics to datadog. These metrics are being used in a few Datadog dashboards. Changing the metric name being sent is straightforward and can be done by updating the name of the metric in the statsd.timed call/decorator in the code, however, the old metric name may already be in use in existing datadog dashboards. </p>

<p>Is there a quick and easy way to rename a metric in Datadog so that all dependencies such as Dashboards using the metric are also updated, without having to go through each dashboard and updating them independently?</p>",,1,0,,2018-8-28 20:53:40,,2018-8-28 21:28:17,,,,,1819254,,1,0,statsd|datadog,1558,12.7703
261408,0,Configuration,61644174,Permission denied to read datadog.yaml file,"<p>I've installed the datadog app on my linux vm but i can't seem to read the datadog.yaml agent file. </p>

<p>[ Error reading /etc/datadog-agent/datadog.yaml: Permission denied ]</p>

<p>My linux box is hosted on GCP, do i need to configure permissions?</p>",,1,2,,2020-5-6 19:54:45,,2020-5-20 12:05:31,,,,,13485796,,1,1,google-cloud-platform|datadog,765,12.7346
261409,0,Configuration,60715172,Datadog instrumentation on Spring boot container application - class not found,"<p>I'm currently running a spring boot application as container into a kubernetes cluster.
Datadog agent is running as containers on the cluster.</p>

<p>I have modified the container image build to include the datadog agent before running the application:</p>

<pre><code>CMD [""/bin/sh"",""-c"",""java -javaagent:dd-java-agent.jar -Ddd.trace.config=datadog.properties -Dlogging.config=/logback.xml -jar service.jar""]
</code></pre>

<p>I also setup the environment variable to indicate the HOST IP of the agent to my container via the Deployment file.</p>

<p>The problem now is i'm getting this class not found exception when the application starts:</p>

<pre><code>java.lang.NoClassDefFoundError: datadog/trace/instrumentation/springscheduling/SpringSchedulingInstrumentation
    at java.lang.Class.getDeclaringClass0(Native Method)
    at java.lang.Class.getDeclaringClass(Class.java:1235)
    at java.lang.Class.getEnclosingClass(Class.java:1277)
    at org.springframework.core.annotation.AnnotationsScanner.processClassHierarchy(AnnotationsScanner.java:233)
    at org.springframework.core.annotation.AnnotationsScanner.processClassHierarchy(AnnotationsScanner.java:194)
    at org.springframework.core.annotation.AnnotationsScanner.processClass(AnnotationsScanner.java:128)
    at org.springframework.core.annotation.AnnotationsScanner.process(AnnotationsScanner.java:107)
    at org.springframework.core.annotation.AnnotationsScanner.scan(AnnotationsScanner.java:97)
    at org.springframework.core.annotation.AnnotationsScanner.scan(AnnotationsScanner.java:78)
    at org.springframework.core.annotation.TypeMappedAnnotations.scan(TypeMappedAnnotations.java:242)
    at org.springframework.core.annotation.TypeMappedAnnotations.get(TypeMappedAnnotations.java:149)
    at org.springframework.core.annotation.TypeMappedAnnotations.get(TypeMappedAnnotations.java:131)
    at org.springframework.core.annotation.AnnotationUtils.findAnnotation(AnnotationUtils.java:581)
    at org.springframework.cloud.sleuth.DefaultSpanNamer.annotation(DefaultSpanNamer.java:71)
    at org.springframework.cloud.sleuth.DefaultSpanNamer.name(DefaultSpanNamer.java:58)
    at org.springframework.cloud.sleuth.instrument.async.TraceRunnable.&lt;init&gt;(TraceRunnable.java:59)
    at org.springframework.cloud.sleuth.instrument.async.TraceRunnable.&lt;init&gt;(TraceRunnable.java:51)
    at org.springframework.cloud.sleuth.instrument.async.LazyTraceThreadPoolTaskScheduler.scheduleWithFixedDelay(LazyTraceThreadPoolTaskScheduler.java:265)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.cloud.sleuth.instrument.async.ExecutorMethodInterceptor.invoke(ExecutorBeanPostProcessor.java:328)
</code></pre>

<p>Quite straigtforward, i need to include some dependencies into the application package. But i could not find anything useful on datadog website nor maven central repository.
Including the agent itself or the api libraries fix nothing. This class is present on the agent but under a different path.</p>

<p>Does anybody know which dependencies should be included in the classpath of the application to fix that ?</p>",,1,0,,2020-3-17 01:33:22,,2020-4-8 21:50:55,2020-3-17 01:49:37,,1687162,,1687162,,1,1,java|kubernetes|instrumentation|datadog,1328,12.6928
261410,0,Integration,54920625,When installed datadog APM ddtrace php then my app not working,"<p>I was installed the ""datadog-php-tracer_0.14.1-beta_amd64.deb"" on my server and after installed my application return 500 error.</p>

<p>Below is the things which I have configured or my server related information:</p>

<p>I am using Ubuntu, NGINX and php-fpm 7.0.</p>

<p>I have installed datadog agent v6.</p>

<pre><code>For FPM I have set below configuration:
Installed : ""datadog-php-tracer_0.14.1-beta_amd64.deb""
NGINX config: fastcgi_param DD_TRACE_DEBUG true;
Set ddtrace.log_backtrace=1 in file /etc/php/7.0/cli/conf.d/98-ddtrace.ini.

In Agent datadog.yaml:
apm_config:
  enabled: true
</code></pre>

<p>When I am checking my php-fpm log file, it shows the PDO error about ""Slim\PDO\Statement\StatementContainer->execute()"". But when I disabled the Datadog Agent or APM trace then my application working normally. In short when I am enable ddtrace my app not working and return 500 error.</p>

<p>Can you please look in it and let me know how can resolved the issue and APM work well with my app.</p>",54938650,1,0,,2019-2-28 07:37:13,1,2019-3-1 05:49:07,,,,,11043786,,1,0,php|datadog|apm,1484,12.6857
261411,2,Query,60369412,Datadog - group by substring of logs,"<p>I am trying to create a ""Top List"" visualization in DataDog and I would like to graph my data which should be grouped by error code. This error code is a substring in logs. An example of a line in the log is given below. I have tried to group my data by message but this is not working, I would like to group my data by substring of the message. Can someone guide me on this?</p>

<blockquote>
  <p>...Server Error {""error"":{""code"":1001,""type"":""MATCH"",""message"":""Invoke
  failed: Failed...
  ...Server Error {""error"":{""code"":2001,""type"":""MATCH"",""message"":""Invoke
  failed: Failed...</p>
</blockquote>

<p>Currently I get the visualization as follows</p>

<pre><code>1.0 .....error:{""code"":1001.....
1.0 .....error:{""code"":1001.....
1.0 .....error:{""code"":2001.....
</code></pre>

<p>1.0 is the number of occurrence</p>

<p>Rather I want the visualization as follows</p>

<pre><code>2.0 .....error:{""code"":1001.....
1.0 .....error:{""code"":2001.....
</code></pre>

<p>2.0 will be total 2 occurrences of error 1001 and 1.0 will be the occurrences of error 2001</p>",,1,0,,2020-2-24 04:07:50,,2020-2-24 17:41:28,2020-2-24 04:21:19,,3553292,,3553292,,1,1,datadog,711,12.6075
261412,1,Method,65766741,Include test run id in k6 metrics sent to Datadog,"<p>I use k6 on my local machine to perform load-testing as well as a <a href=""https://k6.io/docs/results-visualization/datadog"" rel=""nofollow noreferrer"">Datadog agent</a> to visualize the metrics in Datadog.</p>
<p><strong>I'd like to filter k6 metrics in Datadog as the tests aren't distinguishable.</strong></p>
<p>At this point the <code>$test_run_id</code> only shows <code>*</code> (refer to the screenshot below):
<a href=""https://i.stack.imgur.com/1oTX7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1oTX7.png"" alt=""enter image description here"" /></a></p>
<p>I followed <a href=""https://k6.io/docs/cloud/integrations/cloud-apm"" rel=""nofollow noreferrer"">this the official doc</a> that suggests to set <code>include_test_run_id</code> flag to <code>true</code> in k6 config, but I was unsuccessful.</p>
<p>Here's a k6 config I currently use (<code>&lt;YOUR_DATADOG_API_KEY&gt;</code> is replaced with an actual Datadog API key):</p>
<pre><code>export const options = {
  vus: 5,
  duration: &quot;10s&quot;,
  noConnectionReuse: true,
  ext: {
    loadimpact: {
      apm: [
        {
          provider: &quot;datadog&quot;,
          api_key: &quot;&lt;YOUR_DATADOG_API_KEY&gt;&quot;,
          include_test_run_id: true
        }
    ]
    }
  }
};
</code></pre>",65770731,1,0,,2021-1-17 22:41:00,1,2021-1-18 08:05:04,2021-1-17 22:48:52,,3765444,,3765444,,1,2,javascript|load-testing|datadog|k6,332,12.4846
261413,0,Configuration,58597581,Configuring Tracing for Datadog in Spring Boot application,"<p>I have a Spring Boot application and want to configure HTTP request tracing via dependency management without having to deal with setting up the java agent. Can anyone suggest the best way to do this?</p>

<p>I have the <code>micrometer-registry-datadog</code> dependency added to my pom and can see that there are a lot of undocumented <code>com.datadoghq</code> dependencies, but am unsure if any of these will solve my problem. I'm getting all of the JVM metrics, but want some more APM-type metrics now. Ideally I'd like to use the <code>@Timed</code> annotation and various others to get detailed metrics around API calls.</p>",,0,2,,2019-10-28 19:56:46,1,2019-10-28 20:23:05,2019-10-28 20:23:05,,3519838,,3519838,,1,4,java|maven|spring-boot|monitoring|datadog,1319,12.481
261414,1,Method,51868376,sending a udp packet from host works but not from container,"<p>If I run this command on the host(ubuntu)</p>

<pre><code>echo ""PD.file.processing:1|c"" | nc -w 1 -u localhost 8125
</code></pre>

<p>It sends the udp packet fine and the dogstatsd agent running on port 8125 picks it up and I can see it.
But when I run the following command  on the docker container on the same host
Here are the port mappings of the container when I do a docker ps</p>

<pre><code>8125/udp, 0.0.0.0:20019-&gt;8080/tcp, 0.0.0.0:20018-&gt;8443/tcp, 0.0.0.0:20017-&gt;11400/tcp, 0.0.0.0:20016-&gt;11401/tcp, 0.0.0.0:20015-&gt;11402/tcp

echo ""MD.file.returned.success:1|c"" | nc -w 1 -u 172.17.0.1 8125
</code></pre>

<p>This doesn't hit the host and it is not captured by the dogstatsagent running on the host on 8125
Here is the expose line of code in Dockerfile</p>

<pre><code>EXPOSE 8125/udp
</code></pre>

<p>Am I doing something wrong?</p>",,1,0,,2018-8-16 01:16:03,,2018-8-16 04:01:56,,,,,4364069,,1,0,docker|udp|containers|statsd|datadog,1309,12.4678
261415,1,Method,64345103,"How do I set custom ""trace_id"" for Datadog tracing?","<p>How do I set custom &quot;trace_id&quot; for Datadog tracing? I searched high and low but can't find an answer to this. I suspect it's not supported. Would really appreciate it if I can get some help here.</p>
<p>As an example, if I can do the following in multiple files, then I can view these spans together in the Datadog UI since they all have the same trace ID:</p>
<pre><code>@tracer.wrap(service='foo', resource='bar')
def bar(self, ttt):
    span = tracer.current_span()
    span.set_trace_id(&quot;my_customer_trace_id&quot;)
</code></pre>",64396016,2,0,,2020-10-14 00:52:21,,2021-6-22 09:12:43,2020-10-14 17:38:23,,1253272,,1253272,,1,1,python|api|trace|datadog|apm,1003,12.4052
261416,0,Configuration,63909996,Deleting Datadog logs after 7 days time frame,"<p>i am using datadog to monitor my cloud infrastructure(AWS). at present, i am sending aws-logs to datadog and datadog keeping those log data for some default timeframe.</p>
<p><strong>How i can set some limit so that after that particular limit logs will be deleted from datadog?</strong></p>
<p><strong>I want to delete datadog logs after 7 days</strong></p>
<p><strong>Can anyone suggest a solution for this.</strong></p>",63971375,1,2,,2020-9-15 21:19:44,,2020-9-19 17:06:40,,,,,13615987,,1,0,amazon-web-services|amazon-cloudwatch|amazon-cloudwatchlogs|datadog,706,12.3952
261417,0,Integration,60216953,Anyone have experience integrating datadog monitoring with Snowflake?,"<p>Does anyone know if Datadog agent works on snowflake? We want to use Datadog to collect snowflake metrics, traces and logs and create dashboards, graphs, and monitors.</p>",,1,0,,2020-2-13 21:46:18,1,2020-2-19 19:07:06,,,,,5602489,,1,0,snowflake-cloud-data-platform|datadog,1206,12.3254
261418,0,Configuration,51866333,"Datadog with JMX, datadog docker image does not have java installed","<p>I am attempting to use Datadog to monitor my application via JMX... I have successfully deployed my app in a docker container, and exposed the JMX port and confirmed I can indeed attach to the port from anywhere and get information.</p>

<p>So I am attempting to set up the datadog docker image to use JMX and connect to the server... I have it all configured, but at runtime the datadog image attempts to start utilizing JMX, but fails saying it can't find Java on its image... I log into the image and sure enough it has no java installed.</p>

<p>From the datadog documentation:</p>

<pre><code> Java Path
The agent does not come with a bundled JVM, but will use the one installed on 
your system. Therefore you must make sure that the Java home directory is 
present in the path of the user running the agent.

Alternatively, you can specify the JVM path in the integration’s configuration 
file:

java_bin_path: /path/to/java
</code></pre>

<p>Well that's all nice and well, but if I attempt to expose my host machine java to the image via a volume mount, it doesn't work, as the host machine is Apple and if the image attempts to run the java binary it throws an invalid format for the binary file.. not surprising since its a MACOS binary not a Debian Linux Binary (which the datadog image is)....</p>

<p>So, I have been attempting to take the datadog image and build a new image with it as the base with Java... but I have been completely unsuccessful, every attempt to install java during docker build fails.. I have tried every example of how to install java into a debian docker image, but none work... Every one dies with apt-get line returned a non zero</p>

<p>How the heck do I get JAVA installed on a debian image?  </p>

<p>Or better yet, how do I get the datadog image with JMX to run properly?</p>",51933718,1,0,,2018-8-15 20:58:17,,2018-8-20 15:11:48,,,,,282172,,1,1,java|docker|jmx|datadog,337,12.3105
261419,3,Monitoring,62056153,How to Inspect the Queue Processing a Celery Task,"<p>I'm currently leveraging celery for periodic tasks. I am new to celery. I have two workers running two different queues. One for slow background jobs and one for jobs user's queue up in the application.</p>

<p>I am monitoring my tasks on datadog because it's an easy way to confirm my workers a running appropriately. </p>

<p>What I want to do is after each task completes, record which queue the task was completed on.</p>

<pre><code>@after_task_publish.connect()
def on_task_publish(sender=None, headers=None, body=None, **kwargs):
    statsd.increment(""celery.on_task_publish.start.increment"")

    task = celery.tasks.get(sender)
    queue_name = task.queue

    statsd.increment(""celery.on_task_publish.increment"", tags=[f""{queue_name}:{task}""])
</code></pre>

<p>The following function is something that I implemented after researching the celery docs and some StackOverflow posts, but it's not working as intended. I get the first statsd increment but the remaining code does not execute. </p>

<p>I am wondering if there is a simpler way to inspect inside/after each task completes, what queue processed the task. </p>",,1,0,,2020-5-28 02:46:11,,2020-5-28 16:38:33,,,,,7113209,,1,2,python|django|heroku|celery|datadog,946,12.3036
261420,1,Method,64395922,DataDog metric for Kubernetes PersistentVolume usage or remaining space,"<p>Is there a DataDog metric to report the space used or remaining in a GCP PersistentVolume. I have found disk use metrics for the container itself, but not for a PersistentVolume.</p>
<p>I am working in GoogleCloudPlatform.</p>",,1,2,,2020-10-16 20:21:54,,2020-10-20 15:05:44,,,,,9053059,,1,2,kubernetes|google-cloud-platform|datadog|persistent-volumes,297,12.291
261421,3,Visualization,58630932,How to display correct monetary value in Datadog Dashboard Widget,"<p>I've created a custom Datadog metric in a Springboot Java App, and turned on the management end-points.</p>

<p>I am incrementing a MeterRegistry Counter with a double value (relating to the monetary value of an order)
When I use the /management/metrics end-point, I can see the correct value being stored.</p>

<p>However, when I create a widget in my Datadog dashboard, it is only displaying the pre-decimal point value of the data. e.g the order value is 61.67 and in Datadog it is displaying 61, so it's not even doing any rounding !</p>

<p>Is there any way to display the raw value of the counter in a Datadog Dashboard widget?</p>

<p>Thanks in advance</p>",58689757,1,0,,2019-10-30 17:37:51,,2019-11-4 09:05:43,,,,,11936102,,1,0,java|metrics|datadog,1177,12.2831
261422,1,Method,66606637,How do I create a custom metric with additional information from a custom SQL query in DataDog?,"<p>I am currently working on setting up a monitor to monitor slow queries in the Cloud SQL DB. I built a custom query to get the processes running on the SQL server, because currently slow query monitoring doesn't report until the process is completed. To get a check every 15-20 seconds (or whatever is configured in DD) of currently running queries over 5 minutes I have this in my DD agent's config.</p>
<pre><code>    custom_queries:
      - query: SELECT COUNT(*) as processes FROM INFORMATION_SCHEMA.PROCESSLIST WHERE TIME &gt; 
        (5 * 60 * 1000);
        columns:
        - name: mysql.processlist.processes
          type: count
        tags:
        - staging:mysql
</code></pre>
<p>And my results in DD are:
<a href=""https://i.stack.imgur.com/h9DTY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h9DTY.png"" alt=""mysql.processlist.processes"" /></a></p>
<p>As you can see it shows the count of queries that have been running for over 5 minutes. How would I be able to get more information about each query. For example I would like to see the exact query statement that is being executed. I know I can use the query:
<code>SELECT INFO as QUERY FROM INFORMATION_SCHEMA.PROCESSLIST WHERE COMMAND='Query';</code>
to get the Query statement, but Id like to be able to click on the graph in DD and dig further into each process to see the statements. Is there a way to add this information or another feature in Datadog where I can  the processes being queried to each process individually?</p>
<p>My first thought was to change the custom query to this:</p>
<pre><code>  - query: SELECT COUNT(*) as PROCESSES, ID, INFO as QUERY FROM 
    INFORMATION_SCHEMA.PROCESSLIST WHERE COMMAND='Query';
    columns:
    - name: mysql.processlist.ID
      type: tag
    - name: mysql.processlist.PROCESSES
      type: count
    - name: mysql.proccesslist.QUERY
      type: tag
</code></pre>
<p>But this only returns one row with the count number of all process and the ID and Query of the first result.</p>
<pre><code># PROCESSES, ID, QUERY
'3', '61550', 'SELECT ....'
</code></pre>",66632382,1,0,,2021-3-12 20:15:00,,2021-3-15 04:27:00,,,,,10709519,,1,2,mysql|sql-server|google-cloud-sql|metrics|datadog,520,12.264
261423,2,Query,51323878,Datadog: PostgreSQL custom_metrics returns a single row,"<p>I wanted to create a graph in Datadog to display iddle connections per user. </p>

<p>Following this example: <a href=""http://www.miketheman.net/tag/postgres/"" rel=""nofollow noreferrer"">http://www.miketheman.net/tag/postgres/</a> I changed my postgres.yaml configuration to:</p>

<pre><code>init_config:
instances:
- host: 127.0.0.1
  port: 5432
  username: datadog
  password: '**************'
  tags:
  - environment:qa
  - role:db
  custom_metrics:
    - # Postgres Connection state
      descriptors:
        - [datname, database]
        - [usename, user]
        - [state, state]
      metrics:
        COUNT(state): [postgresql.connection_state, GAUGE]
      query: &gt;
        SELECT datname, usename, state, %s FROM pg_stat_activity
        GROUP BY datname, usename, state HAVING COUNT(state) &gt; 0;
      relation: false
</code></pre>

<p>I can see the metric is appearing in Datadog, but I can just see one of the rows that should appear (has I have several databases in my PostgreSQL).</p>

<p><a href=""https://i.stack.imgur.com/NPsBL.png"" rel=""nofollow noreferrer"">Here is the datadog connection graph</a>
Am I missing any step? Is postgres.yaml missing any configuration?</p>

<p>Running that query in psql, I get this (modifying names but not data):</p>

<pre><code>    postgres=# SELECT datname, usename, state, COUNT(state) FROM pg_stat_activity GROUP BY datname, usename, state HAVING COUNT(state) &gt; 0;
           datname       |   usename   |        state        | count
    ---------------------+-------------+---------------------+-------
     compan_strawberr_qa | compan      | idle in transaction |     1
     compan_qa           | compan      | idle                |   130
     pineappplee_qa      | compan      | idle                |    10
     compan_strawberr_qa | compan      | idle                |    29
     compan_qa           | watermel    | active              |     1
     pineappplee_qa      | pineappplee | idle                |    10
     pear_qa             | pear        | idle                |     6
     postgres            | postgres    | active              |     1
     postgres            | datadog     | idle                |     1
     apple_qa            | apple_qa    | idle                |     1
     bblluebberrriiess   | compan      | idle                |     3
     compan_ser_qa       | ser_qa      | idle                |     4
     compan_service_qa   | compan      | idle                |    26
    (13 rows)
</code></pre>",,1,0,,2018-7-13 11:15:48,,2018-7-13 11:39:41,,,,,9763778,,1,1,postgresql|datadog,538,12.1231
261424,1,Parse,55137456,Filter datadog logs in the local agent before sending,"<p>I use datadog agent 6.9 that run on my host(not on a docker), 
and i also run several application on my host (docker images).</p>

<p>I try to avoid sending specific logs to the datadoghq from my mongodb. 
So according to <a href=""https://docs.datadoghq.com/logs/log_collection/?tab=tailexistingfiles"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/log_collection/?tab=tailexistingfiles</a>
I create mongo.d directory and conf.yaml inside that look like:</p>

<pre><code>init_config:
instances:
  # Specify the MongoDB URI, with database to use for reporting (defaults to ""admin"")
  # E.g. mongodb://datadog:LnCbkX4uhpuLHSUrcayEoAZA@localhost:27016/my-db
  - server: mongodb://datadog:LnCbkX4uhpuLHSUrcayEoAZA@localhost:27016/my-db
    # Controls connectTimeoutMS, serverSelectionTimeoutMS and socketTimeoutMS (see http://api.mongodb.com/python/3.4.0/api/pymongo/mongo_client.html)
    # Defaults to 30 seconds
    # timeout: 30

    # tags:
    #   - optional_tag1
    #   - optional_tag2

    # Whether or not to read from available replicas (default true).
    # Disable this if any replicas are inaccessible to the agent.
    replica_check: true

    # Optional SSL parameters, see https://github.com/mongodb/mongo-python-driver/blob/2.6.3/pymongo/mongo_client.py#L193-L212
    # for more details
    #
    # ssl: True # Optional (default to False)
    # ssl_keyfile: # Path to the private keyfile used to identify the local
    # ssl_certfile: # Path to the certificate file used to identify the local connection against mongod.
    # ssl_cert_reqs: 0 # Specifies whether a certificate is required from the other side of the connection, and whether it will be validated if provided.
    # Possible values:
    #   * 0 for ssl.CERT_NONE (certificates ignored)
    #   * 1 for ssl.CERT_OPTIONAL (not required, but validated if provided)
    #   * 2 for ssl.CERT_REQUIRED (required and validated)
    # ssl_ca_certs: #  Path to the ca_certs file
    #
    # By default, the check collects a sample of metrics from MongoDB.
    # The (optional) `additional_metrics` parameter instructs the check to collect additional
    # metrics on specific topics.
    # Available options are:
    # * `metrics.commands` - Use of database commands
    # * `tcmalloc` -  TCMalloc memory allocator
    # * `top` - Usage statistics for each collection
    # * `collection` - Metrics of the specified collections
    additional_metrics:
      - metrics.commands
      - tcmalloc
      - top
      - collection
    #
    # Collect metrics on specific collections from the database specified
    # Requires `additional_metrics.collection` to be present
    # Metrics such as `mongodb.collection.count` will be collected for each named collection and tagged as follows:
    #   - `db:&lt;dbname&gt;` e.g. `db:my-db`
    #   - `collection:&lt;collection-name&gt;` e.g. `collection:my_collection`
    # Each collection generates many metrics,
    # up to 8 + the number of indices on the collection for each collection
    collections:
  - apples
  - oranges
# Collect indexes access metrics for every index in every collections in
# the 'collections' list. This is available starting mongo 3.2.
# collections_indexes_stats: false

## Log section (Available for Agent &gt;=6.0)

logs:

    # - type : (mandatory) type of log input source (tcp / udp / file)
    #   port / path : (mandatory) Set port if type is tcp or udp. Set path if type is file
    #   service : (mandatory) name of the service owning the log
    #   source : (mandatory) attribute that defines which integration is sending the logs
    #   sourcecategory : (optional) Multiple value attribute. Can be used to refine the source attribtue
    #   tags: (optional) add tags to each logs collected

    # - type: file
    #   path: /var/log/mongodb/mongodb.log
    #   service: mongo
    #   source: mongodb
    - type: docker
      service: mongo
      source: mongodb
      log_processing_rules:
      - type: exclude_at_match
        name: exclude_mongo
        pattern: (?s).*
</code></pre>

<p>But when i restart my agent it's still send the unwanted logs to my datadoghq.</p>

<p>Thanks in advance for the help,
Baruch  </p>",,0,5,,2019-3-13 08:36:36,,2019-3-17 10:39:12,2019-3-17 10:39:12,,6552837,,6552837,,1,0,datadog,1069,12.1159
261425,0,Configuration,65629898,conf.d/python.d/ is not available in datadog-agent,"<p>I was trying to install <code>data-dog</code> agent in my <code>Ubuntu 20.04</code> for monitoring a python backend with the following command</p>
<pre class=""lang-sh prettyprint-override""><code>DD_AGENT_MAJOR_VERSION=7 DD_API_KEY=xxxxxxxxx DD_SITE=&quot;datadoghq.com&quot; bash -c &quot;$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script.sh)&quot;
</code></pre>
<p>From the <a href=""https://docs.datadoghq.com/logs/log_collection/python/?tab=jsonlogformatter"" rel=""nofollow noreferrer"">official documentation</a> it says</p>
<pre><code>Create a file conf.yaml in the Agent’s conf.d/python.d/ directory with the following
</code></pre>
<p>But haven't found any <code>python.d</code> inside <code>/etc/datadog-agent/conf.d</code>.  If I create the <code>python.d/con.yaml</code> do I need to do anything else for enabling sending logs?</p>",,1,0,,2021-1-8 13:28:43,,2021-1-8 15:15:24,,,,,11584728,,1,1,python|datadog,298,12.0969
261426,1,Method,66064803,What is the difference between the count and the gauge metric type in DataDog?,"<p>What is the difference between the <a href=""https://docs.datadoghq.com/developers/metrics/types/?tab=count"" rel=""nofollow noreferrer"">count</a> and the <a href=""https://docs.datadoghq.com/developers/metrics/types/?tab=gauge"" rel=""nofollow noreferrer"">gauge</a> metric types in DataDog? Or rather, when should I prefer one over the other? The definitions from their website don't help me much:</p>
<p>Count:</p>
<blockquote>
<p>The COUNT metric submission type represents the total number of event occurrences in one time interval. A COUNT can be used to track the total number of connections made to a database or the total number of requests to an endpoint. This number of events can accumulate or decrease over time—it is not monotonically increasing.</p>
</blockquote>
<p>Gauge:</p>
<blockquote>
<p>The GAUGE metric submission type represents a snapshot of events in one time interval. This representative snapshot value is the last value submitted to the Agent during a time interval. A GAUGE can be used to take a measure of something reporting continuously—like the available disk space or memory used.</p>
</blockquote>
<p>The <code>count</code> type seems to be somewhat related to the <a href=""https://docs.datadoghq.com/developers/metrics/types/?tab=rate"" rel=""nofollow noreferrer""><code>rate</code></a> type, but for me it is unclear why or when I should use <code>count</code> instead of <code>gauge</code>. I mean in principle a measurement of &quot;something&quot; could always be presented as a gauge, couldn't it?</p>",,1,0,,2021-2-5 14:11:24,,2021-7-14 03:04:22,,,,,2881414,,1,1,datadog,529,12.0938
261427,1,Method,65951118,Publish Spring MVC Metrics To Multiple Monitoring System Simultaneously Using Micrometer,"<p>I have a use case wherein I want to publish my spring boot API metrics to Datadog &amp; CloudWatch simultaneously</p>
<p>I have added the below dependencies to my pom</p>
<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;io.micrometer&lt;/groupId&gt;
  &lt;artifactId&gt;micrometer-registry-statsd&lt;/artifactId&gt;
  &lt;version&gt;${micrometer.version}&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
  &lt;groupId&gt;io.micrometer&lt;/groupId&gt;
  &lt;artifactId&gt;micrometer-registry-cloudwatch&lt;/artifactId&gt;
  &lt;version&gt;${micrometer.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>Main Application class</p>
<pre><code>@SpringBootApplication
public class MyApplication {
    @Bean
    MeterRegistryCustomizer&lt;MeterRegistry&gt; metricsCommonTags() {
      return registry -&gt; registry.config().commonTags(&quot;my-tag&quot;, &quot;my-common-tag&quot;);
    }
}
</code></pre>
<p>I have added all required properties in the <code>application.properties</code> as well.</p>
<p>I can see metrics are being published to both datadog &amp; CloudWatch with default metrics name <code>http.server.request</code>
But I want the metrics name for datadog to be different &amp; for this, I have added the below property as well</p>
<p><code>management.metrics.web.server.requests-metric-name = i.want.to.be.different</code></p>
<p>But this is changing the name for both CloudWatch &amp; datadog</p>
<p>My question is how can I change the default metrics name for datadog only or keep names different for both</p>",65953918,1,0,,2021-1-29 08:32:51,0,2021-1-30 05:27:46,2021-1-30 05:27:46,,8386267,,8386267,,1,1,spring-boot|metrics|datadog|micrometer,282,12.001
261428,1,Method,54449261,How to import Typescript definition that exports a single variable,"<p>The type definition for the ""dd-trace"" library, in ""@types/dd-trace"", exports a single variable.</p>

<pre><code>declare var trace: TraceProxy;
export = trace;

declare class TraceProxy extends Tracer {
    /**
     * Initializes the tracer. This should be called before importing other libraries.
     */
    init(options?: TracerOptions): this;

    // A bunch of other irrelevant code.
}
</code></pre>

<p>How can I import this in my code? If I incorrectly try assign ddTrace.init() to a boolean, TypeScript tells me the type is 'TraceProxy'. However I have tried every seeming variation:</p>

<pre><code>import { TraceProxy } from ""dd-trace""
</code></pre>

<p>fails with <code>node_modules/@types/dd-trace""' has no exported member 'TraceProxy'.</code></p>

<pre><code>import { init, trace } from ""dd-trace""

const tracer: trace = init()
</code></pre>

<p>The import succeeds there but then the declaration fails: <code>3:15: cannot find name ""trace""</code></p>

<p>All of these variations fail:</p>

<pre><code>const tracer: trace.trace = init()
const tracer: trace.TraceProxy = init()
const tracer: trace.Tracer = init()
const tracer: TraceProxy = init()
const tracer: Tracer = init()
</code></pre>

<p>Importing the module fails:</p>

<pre><code>import * as ddTrace from ""dd-trace""

const tracer: ddTrace = ddTrace.init()
</code></pre>

<p>with <code>Cannot find name 'ddTrace'.</code> on line 3. </p>

<p>These also failed:</p>

<pre><code>import * as ddTrace from ""dd-trace""

const tracer: ddTrace.TraceProxy = ddTrace.init()
</code></pre>

<p>with <code>Cannot find namespace 'ddTrace'.</code></p>

<p>One suggested answer (since deleted) was:</p>

<pre><code>import trace from ""dd-trace""

const tracer: trace = trace.init()
</code></pre>

<p>This fails with: <code>@types/dd-trace/index""' has no default export.</code></p>

<p>How can I declare the type definition there? I'm using the latest version of TypeScript and compiling by running <code>./node_modules/.bin/tsc myfile.ts</code>.</p>",54449721,1,0,,2019-1-30 20:46:40,,2019-2-11 19:32:07,2019-1-30 21:21:26,,329700,,329700,,1,1,typescript|types|datadog,491,11.9643
261429,0,Configuration,51840681,Datadog Agent check cannot find the path specified,"<p>I have written a Datadog Agent check in Python following the instructions on this page: <a href=""https://docs.datadoghq.com/developers/agent_checks/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/developers/agent_checks/</a>.</p>

<p>The agent check is supposed to read all files in a specified network folder and then send certain metrics to Datadog.</p>

<p>The folder to be read is specified like this in the Yaml file:</p>

<pre><code>init_config:
taskResultLocation: ""Z:/TaskResults""
</code></pre>

<p>This is the code used to read the folder, it is Python 2.7 because that is required by Datadog</p>

<pre><code>task_result_location = self.init_config.get('taskResultLocation')
# Loop through all the XML files in the specified folder
for file in os.listdir(task_result_location):
</code></pre>

<p>If I just run the Python script in my IDE everything works correctly.
When the check is added to the Datadog Agent Manager on the same machine that the IDE is on and the check is run an error is thrown in the Datadog Agent Manager Log saying:</p>

<blockquote>
  <p>2018-08-14 14:33:26 EEST | ERROR | (runner.go:277 in work) | Error running check TaskResultErrorReader: [{""message"": ""[Error 3] The system cannot find the path specified: 'Z:/TaskResults/<em>.</em>'"", ""traceback"": ""Traceback (most recent call last):\n File \""C:\Program Files\Datadog\Datadog Agent\embedded\lib\site-packages\datadog_checks\checks\base.py\"", line 294, in run\n self.check(copy.deepcopy(self.instances[0]))\n File \""c:\programdata\datadog\checks.d\TaskResultErrorReader.py\"", line 42, in check\n for file in os.listdir(task_result_location):\nWindowsError: [Error 3] The system cannot find the path specified: 'Z:/TaskResults/<em>.</em>'\n""}]</p>
</blockquote>

<p>I have tried specifying the folder location in multiple ways with single and double quotes, forward and back slashes and double slashes but the same error is thrown.</p>

<p>Would anyone know if this is a Yaml syntax error or some sort of issue with Datadog or the Python?</p>",51894917,2,1,,2018-8-14 11:47:13,,2018-8-17 11:54:34,,,,,831608,,1,0,python|python-2.7|datadog,957,11.9236
261430,1,Error,51341426,DataDog logging image reporting errors in docker logs,"<p>Using datadog docker image, with the following in docker-compos</p>

<pre><code>datadog:
  agent: true
  privileged: true
  environment:
    - DD_API_KEY=${DATADOG_API_KEY}
    - DD_APM_ENABLED=true
    - DD_LOGS_ENABLED=true
    - DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL=true
  image: datadog/agent:latest
  volumes:
    - /var/run/docker.sock:/var/run/docker.sock:ro
    - /proc/:/host/proc/:ro
    - /cgroup/:/host/sys/fs/cgroup:ro
</code></pre>

<p>I am getting the following errors continuously</p>

<blockquote>
  <p>2018-07-14 16:10:04 UTC | ERROR | (runner.go:277 in work) | Error running check disk: [{""message"": ""[Errno 2] No such file or directory: '/host/proc/filesystems'"", ""traceback"": ""Traceback (most recent call last):\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/checks/base.py\"", line 294, in run\n    self.check(copy.deepcopy(self.instances[0]))\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/disk/disk.py\"", line 43, in check\n    self.collect_metrics_psutil()\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/disk/disk.py\"", line 90, in collect_metrics_psutil\n    for part in psutil.disk_partitions(all=True):\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/psutil/<strong>init</strong>.py\"", line 1839, in disk_partitions\n    return _psplatform.disk_partitions(all)\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/psutil/_pslinux.py\"", line 1000, in disk_partitions\n    with open_text(\""%s/filesystems\"" % get_procfs_path()) as f:\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/psutil/_pslinux.py\"", line 194, in open_text\n    return open(fname, \""rt\"", **kwargs)\nIOError: [Errno 2] No such file or directory: '/host/proc/filesystems'\n""}]</p>
</blockquote>

<p>and another</p>

<blockquote>
  <p>2018-07-14 16:10:04 UTC | WARN | (cgroup.go:510 in
  parseCgroupMountPoints) | No mountPoints were detected, current cgroup
  root is: /host/sys/fs/cgroup/</p>
</blockquote>

<p>Any ideas what it means or how to debug it? Expecting to get logs into datadog from other containers sysout so I have all logs in one place. I can see it successfully detects the other containers</p>

<p>Note the docker image is using version 6 of datadog
Thanks</p>",,0,2,,2018-7-14 16:43:41,,2018-7-14 16:43:41,,,,,1414721,,1,1,docker|logging|docker-compose|filesystems|datadog,956,11.9218
261431,1,Error,62667694,Why is Datadog not capturing tags?,"<p>I am using stats-d [ https://www.npmjs.com/package/node-statsd ] and datadog is connected to it. I would see metrics which I sent to stat-d, being captured on the datadog UI.
However I was asked to add tags.</p>
<p>I changed:</p>
<p>client.increment(somemetric);</p>
<p>to</p>
<p>client.increment(somemetric, [incrementTag]);</p>
<p>Soon after I did that nothing showed up on datadog.
Looks like I have followed the stats-d doc.
What would be my next steps to figure out why datadog cannot read it ?</p>",,1,0,,2020-6-30 23:46:24,,2020-7-2 14:24:10,,,,,2458372,,1,0,datadog|statsd,524,11.8773
261432,1,Error,52212173,Kafka Consumer Custom MetricReporter not receiving metrics,"<p>I've created a class that implements <code>org.apache.kafka.common.metrics.KafkaMetric</code> like so:</p>

<pre><code>public class DatadogMetricTracker implements MetricsReporter {

    @Override
    public void configure(Map&lt;String, ?&gt; configs) {
        System.out.println(configs);
    }

    @Override
    public void init(List&lt;KafkaMetric&gt; metrics) {
        System.out.println(metrics);
    }

    @Override
    public void metricChange(KafkaMetric metric) {
        System.out.println(metric.metricName().name() + "": "" + metric.value() + "" tags: "" + metric.metricName().tags());
    }

    @Override
    public void metricRemoval(KafkaMetric metric) {

    }

    @Override
    public void close() {

    }

}
</code></pre>

<p>Then I register the class as a metric reporter when I set-up the Kafka props:         </p>

<p><code>properties.put(ConsumerConfig.METRIC_REPORTER_CLASSES_CONFIG, ""com.myco.utils.DatadogMetricTracker"");</code></p>

<p>When I start my consumer, <code>configure</code> gets called and <code>init</code>, then <code>metricChange</code> is called one time with a batch of metrics for which the values are all 0 or -Infinity, then it never gets called again. How do I get my metric recorder to fire again?</p>

<p>Thanks!</p>",,2,3,,2018-9-6 21:08:10,,2020-3-16 14:43:36,2018-9-7 03:37:00,,236528,,474719,,1,1,java|apache-kafka|metrics|consumer|datadog,711,11.8075
261433,0,Configuration,55632833,Access Denied while running datadog commands,"<p>I installed datadog agent locally on my windows 10  machine. By default it stored data in ProgramData folder in C drive. It does not give option to select different drive while installation. Now when I run any command, it gives me below error. Can we edit permissions to it allow access to ProgramData folder.</p>

<pre><code>C:\Program Files\Datadog\Datadog Agent\embedded&gt;agent.exe status
Getting the status from the agent.

Error: unable to access authentication token: open C:\ProgramData\Datadog\auth_token: Access is denied.
</code></pre>",55806500,2,0,,2019-4-11 12:44:35,,2019-9-5 12:28:29,,,,,5254815,,1,1,windows-10|datadog,702,11.7853
261434,2,Query,61091505,Datadog regex to find a text that has double quote,"<p>I have logs that contain this kind of line:</p>

<pre><code>...""event_artist_id"": 100, ""event_artist_name"": ""Elton John""...
</code></pre>

<p>I want to filter out ones with <code>""event_artist_id"": 100</code></p>

<p>How do I do that?</p>

<p>I tried many options, but no luck yet, for example: <code>\/""event_artist_id"": 100\/*</code></p>",,0,3,,2020-4-8 00:26:49,0,2020-4-8 00:40:08,2020-4-8 00:40:08,,5437911,,5437911,,1,0,regex|datadog,879,11.776
261435,0,Integration,59088171,DataDog how to disable Redis integration,"<p>I've installed the DataDog agent on my Kubernetes cluster using the Helm chart (<a href=""https://github.com/helm/charts/tree/master/stable/datadog"" rel=""nofollow noreferrer"">https://github.com/helm/charts/tree/master/stable/datadog</a>).</p>

<p>This works very well except for one thing. I have a number of Redis containers that have passwords set. This seems to be causing issues for the DataDog agent because it can't connect to Redis without a password.</p>

<p>I would like to either disable monitoring Redis completely or somehow bypass the Redis authentication. If I leave it as is I get a lot of error messages in the DataDog container logs and the redisdb integration shows up in yellow in the DataDog dashboard.</p>

<p>What are my options here?</p>",,1,0,,2019-11-28 11:46:24,,2020-1-15 08:05:28,,,,,2277437,,1,1,kubernetes|redis|datadog,766,11.7369
261436,2,Query,61679780,Terraform output of embedded multiline string joining new lines in output,"<p>I'm using this resource with a here string:</p>
<pre><code>resource &quot;datadog_monitor&quot; &quot;test&quot; {
  name = &quot;test&quot;
  type = &quot;metric alert&quot;

  message = &lt;&lt;EOF
  {{#is_alert}}
  aaaaaaaaa
  {{/is_alert}}

  {{#is_warning}}
  bbbbbbbb
  {{/is_warning}}

  {{#is_recovery}}
  cccccccc
  {{/is_recovery}}
EOF
....
</code></pre>
<p>When I run plan it puts it all on one line</p>
<pre><code>+ resource &quot;datadog_monitor&quot; &quot;test&quot; {
      + evaluation_delay    = 900
      + id                  = (known after apply)
      + include_tags        = true
      + locked              = false
      + message             = &quot;{{#is_alert}}\n aaaaa\n aaaaa\n {{/is_alert}}\n\n  {{#is_warning}}\n    bbbbbbbb\n  {{/is_warning}}\n\n  {{#is_recovery}}\n  cccccccc\n  {{/is_recovery}}\n
      ......
</code></pre>
<p>Is there a way to have output for &quot;message&quot; be multiline instead of all on one line so its easier to read?</p>",,1,1,,2020-5-8 13:00:54,,2021-5-21 23:11:47,2021-5-21 23:11:47,,472495,,1028270,,1,1,terraform|datadog,744,11.6863
261437,2,Query,59622491,How to correct the query statement (string interpolation) in the Datadog dashboard?,"<p>I want to be able to parameterised my datadog dashboard.</p>

<p>I have already introduced a template variable <code>flavor</code> which to indicate if it is <code>dev</code> or <code>prod</code> environment.</p>

<p>What I wish to achieve is to switch data from one environment o another when I select a different environment (e.g. from <code>dev-db-master</code> to <code>prod-db-master</code>). The string interpolation is necessary because I want to display multiple time series within a single chart.</p>

<p><a href=""https://i.stack.imgur.com/KTe59.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KTe59.png"" alt=""enter image description here""></a></p>

<p>However the chart is basically blank</p>

<p><a href=""https://i.stack.imgur.com/Vh1rI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vh1rI.png"" alt=""enter image description here""></a></p>

<p>The Json tab also shows a pink background which indicates either the json is malformed or the query is too complex.</p>

<p>My goal is to be able to, by changing the template variable <code>flavor</code>,</p>

<p>I can change a group of time series from, says, <strong>'dev-db-master', 'dev-db1-master' and 'dev-db2-master'</strong> to <strong>'prod-db-master', 'prod-db1-master' and 'prod-db2-master'</strong>.</p>

<p>Can you suggest a way to construct a string with a template variable?</p>",59625224,1,0,,2020-1-7 05:10:55,,2020-1-8 09:23:01,2020-1-8 03:30:50,,58129,,58129,,1,1,datadog,418,11.6847
261438,1,Method,54960747,send event to local datadog agent via shell,"<p>I'm trying to send events to my local datadog agent by shell through DataStatsD port. The message is sent without errors but doesn't reach the dashboard.</p>

<p>I use datadog agent in version 6.9 and use datadog documentation:</p>

<p><a href=""https://docs.datadoghq.com/developers/dogstatsd/datagram_shell/#send-metrics-and-events-using-dogstatsd-and-the-shell"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/developers/dogstatsd/datagram_shell/#send-metrics-and-events-using-dogstatsd-and-the-shell</a></p>

<p>When I try to send metrics is work fine and I see the metrics in the datadog dashboard but when I send events it's doesn't show in the dashboard.</p>

<p>I also see that when I send event via shell and then check agent status the number of metrics packets go up but the number of events is still 0. </p>

<p>That's the command i run:</p>

<pre><code>~$ title=""Event from the shell""
~$ text=""This was sent from Bash!""
~$ echo “_e{${#title},${#text}}:$title|$text|#shell,bash""  
     &gt;/dev/udp/localhost/8125
</code></pre>

<p><strong>Edit</strong> When I changed the following configuration properties it's work.
         The configuration I changed:
         1) dogstatsd_non_local_traffic: yes
         2) bind_host: localhost  </p>",,0,4,,2019-3-2 16:47:50,,2019-3-4 07:32:42,2019-3-4 07:32:42,,6552837,,6552837,,1,2,shell|datadog,820,11.6553
261439,1,Method,66876280,Datadog Logs from Windows Event Viewer,"<p>I am new to DataDog and getting back into working with Windows Servers. I am trying to push Event Viewer logs (Security, System, etc) to Datadog logs. I have been successful in terms of setting it up (used their documentation - <a href=""https://docs.datadoghq.com/integrations/win32_event_log/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/win32_event_log/</a>). I am getting logs into my DD for that server for my System and Security:</p>
<pre><code>logs:
  - type: windows_event
    channel_path: &quot;System&quot;
    source: &quot;System&quot;
    service: System_Event
  - type: windows_event
    channel_path: &quot;Security&quot;
    source: &quot;Security&quot;
    service: Security_Event
</code></pre>
<p>I know that you can push items from the Event Viewer to Events in DD by using <code>Instances</code> and you can be more granular there. But I want that granularity in the logs sections since we rarely view Events. Right now it is showing me all the items in the logs, success, etc. I am looking to only get the Errors and Warnings piped to the Logs.</p>
<p>Thanks for the help.
D</p>",68525355,1,0,,2021-3-30 18:09:06,,2021-7-26 06:31:29,,,,,2137149,,1,0,windows-server-2012|datadog|event-viewer,444,11.5895
261440,1,Error,64772196,Custom metrics sent datadog using micrometer DatadogRegistry not showing up in Datadog metric summary,"<p>I want to send custom metrics using io.micrometer.datadog.DatadogMeterRegistry to datadog. Below is the code snippet of the method where I am emitting metrics to Datadog.</p>
<pre><code>@Override
    public void emitMetrices(Map&lt;String, String&gt; dataPoints) {
        try {
            logger.info(&quot;inside emitMetrices from monitoring service with enableCustomMetrics: {}&quot;,
                    enableCustomMetrics);
            if (!isEnabled()) {
                logger.warn(&quot;Metrics are diabled&quot;);
                return;
            }

            // user supplied metrics
            Set&lt;Tag&gt; tags = new LinkedHashSet&lt;Tag&gt;();
            Set&lt;Entry&lt;String, String&gt;&gt; dataPointEntries = dataPoints.entrySet();
            for (Entry&lt;String, String&gt; entry : dataPointEntries) {
                String key = entry.getKey() == null ? MetricConstants.UNKNOWN_TEXT : entry.getKey();
                String value = entry.getValue() == null ? MetricConstants.UNKNOWN_TEXT : entry.getValue();
                tags.add(new ImmutableTag(key, value));
            }

            String tenantMoniker = MetricConstants.UNKNOWN_TEXT;
            String stackName = MetricConstants.UNKNOWN_TEXT;

            TenantDescriptor tenant = TenantContextHolder.get();
            if (tenant != null) {
                tenantMoniker = tenant.getTenantMoniker();
                stackName = tenant.getTierName();
            } else {
                logger.warn(&quot;Tenant is not available&quot;);
            }

            Tag tenantTag = new ImmutableTag(MetricConstants.TENANT_MONIKER, tenantMoniker);
            Tag stackNameTag = new ImmutableTag(MetricConstants.STACK_NAME, stackName);
            Tag serviceNameTag = new ImmutableTag(MetricConstants.SERVICE_NAME, serviceName);

            tags.add(tenantTag);
            tags.add(stackNameTag);
            tags.add(serviceNameTag);

            logger.info(&quot;sending metric to datadog&quot;);
            Counter counter = meterRegistry.counter(METRIC_NAME, tags);
            counter.increment();

            logger.info(&quot;metric sent successfully: {}&quot;, METRIC_NAME);

        } catch (Exception e) {
            logger.error(&quot;Error publishing metrics&quot;, e);
        }

    }
</code></pre>
<p>I am able to see logs &quot;metric sent successfully&quot; with no error but this custom metric is not showing up in Datadog UI under metrics summary. Am I missing anything?</p>",,1,2,,2020-11-10 15:50:55,,2020-11-10 21:11:23,,,,,9828969,,1,0,java|spring-boot|datadog|micrometer,788,11.5861
261441,0,Configuration,64317077,Empty variable when using `status.hostIP` as reference field for my env variable in kubernetes,"<p>I'm deploying a kubernetes pod using helm v3, my kubectl client and server are above 1.7 so it should support reference fields. However when i deploy, the value is just empty.</p>
<p>using</p>
<pre><code>environment:
  - name: DD_AGENT_HOST
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
</code></pre>
<p>Where the DD_AGENT_HOST is my env variable that should be given the host ip.</p>
<p>Any idea on why this might be happening?</p>",64329023,1,2,,2020-10-12 11:33:56,,2020-10-13 05:24:31,,,,,1782536,,1,0,kubernetes|yaml|kubernetes-helm|kubernetes-pod|datadog,436,11.5579
261442,1,Method,65580606,How to get Serilog json-formatted logs to appear correctly in Datadog,"<p>I have been asked to implement a centralized monitoring and logging system using DataDog that will receive information from various services and applications, some running as Windows Services on virtual machines and some running inside a Kubernetes cluster.  In order to implement the logging aspect so that DataDog can correctly ingest the logs, I'm using Serilog to do the logging.</p>
<p>My plan is currently to write the logs to the console in json format and have the DataDog agent installed on each server or k8s node capture and ship them to DataDog.  This works, at least for the k8s node where I've implemented it so far.  (I'm trying to avoid using the custom Serilog sink for DataDog as that's discouraged in the DataDog documentation).</p>
<p>My problem is that I cannot get logs ingested correctly on the DataDog side.  DataDog expects the json to contain a property call Message but Serilog names this property RenderedMessage (if I use JsonFormatter(renderMessage: true)) or @m (if I use RenderedCompactJsonFormatter()).</p>
<p>How can I get my logs shipped to DataDog and ingested correctly on the DataDog end?</p>",,1,0,,2021-1-5 14:16:24,,2021-1-5 17:47:26,,,,,180368,,1,1,json|logging|serilog|datadog|log-shipping,386,11.5463
261443,0,Configuration,63599025,Cannot connect from asgi app (FastApi) to Datadog Agent running on Docker,"<p>I am trying to connect to datadog from my fastapi backend. I am currently trying to do this on localhost using a docker-compose file to let both my datadog-agent and my backend-container run in the same network.
Here is a minimal example</p>
<pre><code>dd-minimal
 - docker-compose.yml
 - backend-client
   - Dockerfile
   - app
     - main.py
</code></pre>
<p>docker-compose.yml</p>
<pre><code>
version: &quot;3.7&quot;

networks:
  my_network:

services:
  datadog:
    image: datadog/agent:latest
    environment:
     DD_API_KEY: &lt;my-api-key&gt;
     DD_APM_ENABLED: 'true'
    volumes:
     - /var/run/docker.sock:/var/run/docker.sock
     - /proc/:/host/proc/:ro
     - /sys/fs/cgroup:/host/sys/fs/cgroup:ro
    ports: 
      - &quot;8126:8126/tcp&quot;
    networks:
      - my_network
    
  backend-web-client:
    image: gql-backend-api
    build:
      dockerfile: Dockerfile
      context: ./backend-client
    environment:
      DD_TRACE_ANALYTICS_ENABLED: 'true'
      DD_AGENT_HOST: 172.21.0.2
    ports: 
      - &quot;5555:8080&quot;
    networks:
      - my_network
    depends_on: 
      - datadog
</code></pre>
<p>Dockerfile</p>
<pre><code>FROM tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim

COPY ./app /app
RUN pip install ddtrace==0.41.0

CMD exec ddtrace-run gunicorn --bind :8080 --workers 1 --threads 8 --timeout 0  main:api -k uvicorn.workers.UvicornWorker
</code></pre>
<p>main.py</p>
<pre><code>import os

import uvicorn
from fastapi import FastAPI

api = FastAPI()

if __name__ == &quot;__main__&quot;:
    uvicorn.run(api, host=&quot;127.0.0.1&quot;, port=int(os.environ.get(&quot;PORT&quot;, 8080)))
</code></pre>
<p>I run docker-compose up and then check the ip of my dd-container with</p>
<pre><code>docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' dd-minimal_datadog_1
</code></pre>
<p>and update it in the compose file.</p>
<p>When I then again run docker-compose up, I get the following error</p>
<pre><code>- DATADOG TRACER DIAGNOSTIC - Agent not reachable. Exception raised: [Errno 111] Connection refused.
</code></pre>
<p>Any help would be very appreciated</p>",,1,1,,2020-8-26 13:44:23,,2020-10-8 05:55:03,,,,,6419777,,1,0,docker|docker-compose|fastapi|datadog,743,11.484
261444,1,Error,67335752,Unable to send metrics to datadog from Datadog agent running as ECS container,"<p>I am running my core business service on ECS Fargate. I have added the 'datadog-agent' as the sidecar container to send metrics of the service running on ECS fargate to datadog.</p>
<p>But, the problem is that I am not getting any metrics on the Datadog itself from the ECS container.</p>
<p>Here are the environment variables I am using for datadog-agent:</p>
<ul>
<li>DD_API_KEY</li>
<li>DD_PROXY_HTTP</li>
<li>DD_PROXY_HTTPS</li>
<li>DD_PROXY_NO_PROXY</li>
<li>DD_SITE</li>
<li>ECS_FARGATE</li>
</ul>
<p>I am using these proxy environments variables because I have to pass the metrics through Squid proxy server. I have checked everything possible but still getting the following error:</p>
<blockquote>
<p>2021-04-30 14:30:33 UTC | CORE | ERROR | (pkg/forwarder/worker.go:174
in process) | Too many errors for endpoint
'https://app.datadoghq.us/api/v1/check_run?api_key= {
&quot;DD_API_KEY&quot;: &quot;***************************xxxx&quot; } ': retrying later</p>
<p>2021-04-30 14:30:29 UTC | CORE | ERROR |
(pkg/collector/runner/runner.go:292 in work) | Error running check
consul: [ {
&quot;message&quot;: &quot;400 Client Error: Bad Request for url: http://xx.xx.xx.xx:8500/v1/status/leader&quot;,</p>
</blockquote>
<p>How can I get metrics successfully on Datadog? I have tried different ways and researched a lot, but no luck.</p>",,1,0,,2021-4-30 14:35:56,,2021-5-7 08:21:38,2021-5-7 08:21:38,,472495,,9923849,,1,1,logging|containers|amazon-ecs|aws-fargate|datadog,644,11.4355
261445,1,Error,67364047,Not able to see metrics on datadog sent by statsd,"<p>I am trying to integrate statsd+datadog.</p>
<ul>
<li>I have launched the dd-agent container with -e
DD_DOGSTATSD_NON_LOCAL_TRAFFIC=&quot;true&quot; , and apiKey is also correct
(container logs confirm this)</li>
<li>I am using the <a href=""https://docs.datadoghq.com/developers/metrics/dogstatsd_metrics_submission/#code-examples"" rel=""nofollow noreferrer"">official code example</a> to test the integration</li>
<li>On datadog dashboard, in metrics explorer, I see data coming in for <code>datadog.dogstatsd.client.metrics</code></li>
</ul>
<p>Problem</p>
<ul>
<li>But not able to find the exact metrics I am pushing anywhere on datadog dashboard, how to see the metrics I pushed via statsd?</li>
<li>How can I see this graph as shown from official documentation (image below)</li>
</ul>
<p><a href=""https://i.stack.imgur.com/CE8ND.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CE8ND.png"" alt=""enter image description here"" /></a></p>",67854261,1,0,,2021-5-3 05:49:35,,2021-6-5 22:03:57,,,,,6069796,,1,1,datadog|statsd,360,11.4252
261446,0,Configuration,58094024,Intra-host UDP traffic missing after destination-pod re-created,"<p>I'm sending UDP packets (statsd) from pods on a host to <code>&lt;hostIP&gt;:8125</code>. On the other end, a collector (datadog-agent using <code>hostPort</code>; one per host via DaemonSet) picks up the packets and does it's thing.</p>

<p>Generally this works fine, but if I ever delete + re-create the collector (<code>kubectl delete pod datadog-agent-xxxx</code>; new pod is started on same IP/port a few seconds later), traffic from <em>existing</em> client-sockets stop arriving at the collector (UDP sockets created <em>after</em> the pod-rescheduling works fine).</p>

<p>Re-starting just the agent inside the collector pod (<code>kubectl exec -it datadog-agent-xxxxx agent stop</code>; auto-restarts after ~30s) the same old traffic <em>does</em> show up. So containers somehow must have an impact.</p>

<p>While UDP are (supposedly) stateless, something, somewhere is obviously keeping state around!? Any ideas/pointers?</p>

<p>Each ""client"" pod has something like this in the deployment/pod:</p>

<pre><code>kind: Deployment
...
spec:
  template:
    spec:
      containers:
        - name: webservice
          env:
            # Statsd defaults to localhost:8125, but that's this pod. Use `hostPort` on collector + hostIP here to get around that.
            DD_AGENT_HOST:
              valueFrom:
                fieldRef:
                  fieldPath: 'status.hostIP'

</code></pre>

<p>On the collector (following <a href=""https://docs.datadoghq.com/agent/kubernetes/daemonset_setup/?tab=k8sfile"" rel=""nofollow noreferrer"">datadog's k8s docs</a>):</p>

<pre><code>kind: DaemonSet
...
spec:
  template:
    spec:
      containers:
        - image: datadog/agent:6.140.0
          ports:
            - containerPort: 8125
              hostPort: 8125
              protocol: UDP
          env:
            - name: DD_DOGSTATSD_NON_LOCAL_TRAFFIC
              value: ""true""
            - ...
</code></pre>

<p>This happens on Kubernetes 1.12 on Google Kubernetes Engine.</p>",58132394,1,4,,2019-9-25 08:13:43,,2019-9-27 10:06:51,2019-9-25 08:38:28,,145307,,145307,,1,1,kubernetes|datadog,353,11.3911
261447,3,Visualization,52311463,How to display service-name prefix to all metrics in Kamon(1.x) Datadog dashboard,"<p>I am using Kamon DatadogAgentReporter to record different metrics in my application. After migrating Kamon from 0.6.x to 1.x, I can see only the list of metrics with tags without any service name. I added the reporter like this, Kamon.addReporter(new DatadogAgentReporter()) and the config as given below,</p>

<pre><code>kamon {

    environment {
        service = ""xxx"" //application-name
        host = """"
        instance = """"
    }

    util.filters {
        datadog - tag - filter {
            includes = [""**""]
            excludes = []
        }
    }

    datadog {
        additional - tags {
            service = ""yes""
            host = ""yes""
            instance = ""yes""
            blacklisted - tags = []
        }

        filter - config - key = ""datadog-tag-filter""
    }
}
</code></pre>

<p>Did I miss something? How do I get the display service-name prefix for my metrices? </p>

<p>Thanks in advance!</p>",,1,0,,2018-9-13 10:09:31,,2018-11-2 08:10:01,,,,,3387304,,1,0,akka|datadog|kamon|akka-monitoring,395,11.3864
261448,2,Query,54382680,How to measure execution time of asynchronous process in Datadog?,"<p>I have a microservice based project with <code>kafka</code>, which I used for event bus. I have a business process, which contains multiple microservices. Microservices asynchronous communicate with each other with help kafka. Each instance of business process has unique <code>process_id</code>.</p>

<p>Let's consider a example of some process:</p>

<ol>
<li>User creates a process with <code>request_id == 556bb813-bf77-4f5f-8bb0-1a59d6ba16b4</code> in API gateway service.</li>
<li>API gateway service produce some event to <code>service1_in</code>. Of course, event contains <code>556bb813-bf77-4f5f-8bb0-1a59d6ba16b4</code>.</li>
<li>Microservice_1 consumes event from <code>service1_in</code> and produces event to <code>service2_in</code>.</li>
<li>Microservice_2 consumes event from <code>service2_in</code> and produces event to <code>service1_in</code>.</li>
<li>Microservice_1 consumes event from <code>service1_in</code> and produces event to <code>service1_out</code>.</li>
<li>API gateway service  consumes event from <code>service1_out</code> and eventually returns result to Client.</li>
</ol>

<p>So, I need to measure execution time bwtween differ steps of this process. For example, I want to know, duration between steps <code>3</code> and <code>5</code>, or <code>2</code> and <code>6</code>.</p>

<p>So, I don't know, how to measure it. I have only one workaround solution. I can have share memory (e.g., redis), where I can store points of time for each stem of process. In the end of process, I can calculate all metrics and push it datadog.</p>",,0,2,,2019-1-26 20:56:48,1,2019-1-26 20:56:48,,,,,4167563,,1,2,asynchronous|monitoring|metrics|execution-time|datadog,700,11.3804
261449,1,Method,67844692,Send traces with spring-boot-sleuth to Datadog,"<p>I have spring-boot application and it has the next dependencies:</p>
<pre><code>dependencies {
implementation 'org.springframework.boot:spring-boot-starter-web'
implementation 'org.springframework.cloud:spring-cloud-starter-sleuth'
developmentOnly 'org.springframework.boot:spring-boot-devtools'
implementation &quot;com.datadoghq:dd-java-agent:0.75.0&quot;
annotationProcessor &quot;com.datadoghq:dd-java-agent:0.75.0&quot;
testImplementation 'org.springframework.boot:spring-boot-starter-test'}

bootRun {
   jvmArgs = [&quot;-javaagent:&quot; + configurations.runtimeClasspath.files.find { f -&gt; f.path.contains('dd-java-agent') }.path]
}
</code></pre>
<p>I execute Datadog agent in the container and configure it there(KEY, ENV).</p>
<p>When I use API from <strong>dd-trace</strong> (like <code>datadog.trace.api.Trace</code>), I can see traces in Datadog. But when I use <strong>sleuth</strong> API to create spans/tags/events I cant see traces.</p>
<p>Is it possible to use sleuth API to send traces to Datadog via Datadog agent? If yes, what do I need to do for it?</p>",67854304,1,0,,2021-6-4 22:32:45,,2021-6-5 22:12:11,,,,,16132332,,1,0,spring-boot|datadog|spring-cloud-sleuth,699,11.3779
261450,0,Integration,65619895,Conditional JavaAgent Command for SBT Native Packager,"<p>I'm using scala, sbt, sbt-native-package, and potentially sbt-java-agent to conditionally activate a datadog java agent at runtime w/ kubernetes.</p>
<p>By adding the <code>dd-java-agent</code> as a dependency and adding a script snippet, I'm able to activate datadog only when a specific env. variable is set, but this is also adding the dd-java-agent to the classpath, which I'm trying to avoid:</p>
<pre class=""lang-scala prettyprint-override""><code>val DataDogAgentVersion = &quot;0.70.0&quot;

libraryDependencies += &quot;com.datadoghq&quot; % &quot;dd-java-agent&quot; % DataDogAgentVersion % &quot;runtime&quot;

bashScriptExtraDefines += &quot;&quot;&quot;if [ &quot;$DD_PROFILING_ENABLED&quot; = &quot;true&quot; ]; then addJava &quot;-javaagent:${app_home}/../lib/dd-java-agent-&quot;&quot;&quot; + DataDogAgentVersion + &quot;&quot;&quot;.jar&quot;; fi&quot;&quot;&quot;&quot;
</code></pre>
<p>Is there a way to have sbt manage the downloading of dd-java-agent.jar, include this jar in the <code>lib</code> directory (or a different directory if that's what it takes), but exclude from classpath?</p>
<p>I've tried using <code>sbt-java-agent</code> which puts the jar in a <code>dd-java-agent</code> directory and excludes the jar from the classpath, but I can not figure out how to wrap the <code>addJava</code> statement in an <code>if</code> check when using that plugin.</p>
<p>Thanks for any help you can provide!</p>",65636279,1,0,,2021-1-7 21:02:26,,2021-1-8 20:56:19,,,,,2051074,,1,0,scala|sbt|javaagents|datadog|sbt-native-packager,217,11.3458
261451,3,Monitoring,65151286,Long response time in anonymous middleware,"<p>I'm using NestJS (with Express Server) for a project and trying to optimize the performance on some of the endpoints. Using Datadog I noticed that about 83% of the response time of all endpoints is spent in an anonymous middleware. Does anyone know what middleware this is and why it's taking this long?</p>
<p><a href=""https://i.stack.imgur.com/Z2xLN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z2xLN.png"" alt=""Usage stats"" /></a></p>
<p>I suspect that it has to do with the framework itself due to the similar unanswered question <a href=""https://stackoverflow.com/questions/63778644/annotating-an-annonymus-middleware-in-newrelic-in-a-nestjs-app"">here</a>.</p>",,1,5,,2020-12-4 21:55:50,,2020-12-5 04:52:20,,,,,6095754,,1,1,node.js|express|nestjs|datadog,337,11.3105
261452,3,Monitoring,65396200,Datadog: How to alert if a pod in running in kubernetes,"<p>I'm using datadog to monitor the health of several pods deployed in a kubernetes cluster. I use a query like this to check the pods</p>
<pre><code>avg(last_5m):avg:kubernetes.pods.running{environment:develop,kube_service:service} &lt;= 0
</code></pre>
<p>If I stop the pod, there ins't any data for kubernetes.pods.running (so the value is not zero, I don't have any value) . I don't know if it's possible to check from datadog that no pods has kube_service running.</p>",,1,0,,2020-12-21 16:14:18,,2020-12-21 16:34:58,,,,,4476024,,1,0,kubernetes|datadog,370,11.2728
261453,3,Monitoring,63921139,Different alert thresholds depending on the current time,"<p>I was wondering if I can somehow use different thresholds depending on the current time. For example, if I lose traffic on my website for 10 minutes while I'm on my working hours, I'd like to get notified to fix it quickly.</p>
<p>But if I lose traffic for only 10 mins at night, I feel like sometimes it's not worth to wake up.</p>
<p>I've been trying to search for a way of doing something like that but couldn't find any.</p>",,2,0,,2020-9-16 13:29:19,,2020-9-21 19:23:50,2020-9-19 09:02:44,,472495,,2347708,,1,1,datadog,164,11.2594
261454,3,Monitoring,64579481,How to instrument a controller in nestjs interceptor?,"<p>I want to instrument every method of a nestjs controller for APM purposes.
I wrote the following interceptor in order to instrument the controller invocation.</p>
<p>However, I do not know how to properly wrap the call to <code>next.handle()</code>.<br />
I do not have any experience using RxJS Observables.</p>
<p>Question: Is it possible to wrap the invocation properly and if so how?</p>
<p>The current approach seems to measure the controller's execution time but does not set a correct tracer scope for the controller's method. I guess the issue is that <code>next.handle()</code> must be wrapped too.</p>
<pre class=""lang-js prettyprint-override""><code>import { CallHandler, ExecutionContext, Injectable, NestInterceptor } from &quot;@nestjs/common&quot;;
import { Reflector } from &quot;@nestjs/core&quot;;
import { Observable } from &quot;rxjs&quot;;
import { PATH_METADATA } from '@nestjs/common/constants';
import tracer from &quot;dd-trace&quot;;

@Injectable()
export class ApmInterceptor implements NestInterceptor {
    constructor(private readonly reflector: Reflector) {}
    
    public intercept(context: ExecutionContext, next: CallHandler): Observable&lt;unknown&gt; {
        const request: Request = context.switchToHttp().getRequest();

        const path = this.reflector.get&lt;string[]&gt;(PATH_METADATA, context.getHandler()); 
        const method = request.method;

        const observable = next.handle();

        tracer.trace(`[${method}] ${path}`, () =&gt; new Promise((resolve, reject) =&gt; {
            observable.subscribe({
                complete: resolve,
            });
        }));

        return observable;
    }
}
</code></pre>",,1,9,,2020-10-28 18:50:41,,2021-6-9 09:28:55,2020-10-28 18:55:51,,10473469,,10473469,,1,1,node.js|typescript|rxjs|nestjs|datadog,575,11.2387
261455,1,Parse,56382266,Log JSON to DataDog log message field,"<p><strong>I'd like to be able to send logs to datadog and have the message be a JSON object rather than a string.</strong></p>

<p>The metadata fields aren't searchable unless a facet is created, which I would like to avoid doing.  </p>

<p>I'm currently using <a href=""https://www.npmjs.com/package/winston"" rel=""nofollow noreferrer"">winston</a> + <a href=""https://www.npmjs.com/package/@shelf/winston-datadog-logs-transport"" rel=""nofollow noreferrer"">winston-datadog-logs-transporter</a> to send the logs.</p>

<p>If I do: <code>logger.info(JSON.stringify(message))</code>, datadog records the message as blank and adds the stringified message as metadata.</p>

<p>If I do: <code>logger.info('foo' + JSON.stringify(message)</code>, then the message is interpreted as a string and I can search on it.</p>

<p>If I do: <code>logger.info('foo', message)</code>, the body is set to <code>foo</code> and <code>message</code> is interpreted as metadata, which I cannot search for without creating a facet.</p>

<p>Any help is appreciated, thanks!</p>",,1,1,,2019-5-30 16:40:24,,2020-5-21 04:25:17,,,,,3869978,,1,1,node.js|logging|winston|datadog,571,11.2265
261456,1,Method,55009193,How to report StatsD metrics from a Google Cloud Function written in Node.js?,"<p>Is there a simple way it to report custom defined stats to our statsd / Datadog infrastructure from a Google Cloud Function written in Node.js? </p>

<p>Since it's a high-traffic Javascript Cloud Function, I'd like to avoid heavy initialization of additional libraries every time the cloud function is invoked. </p>

<p>Also, by custom stats I mean stats of our own definition (not boilerplate summary statistics via StackDriver or DataDog GCP integration). </p>",55054296,1,0,,2019-3-5 18:20:12,,2019-3-7 23:11:52,,,,,1181073,,1,1,google-cloud-platform|google-cloud-functions|statsd|datadog,318,11.2097
261457,1,Method,62394798,How Datadog read custom log files in containers kubernetes?,"<p>I have an application running on Kubernetes and this app has log files that I want to stream to datadog log, then set up an alert. Previously, this app run on bare-metal server, I installed datadog agent on that server, and I used custom log collection to retrieve that logs. It worked perfectly well. </p>

<p>Now, I have an obstacle on how to read the log files in the container. I have googled and it said I can use annotations and auto discovery, but I can't see where I am supposed to define the log path.</p>

<pre><code>apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: cardpayment
spec:
  selector:
    matchLabels:
      app: cardpayment
  template:
    metadata:
      annotations:
        ad.datadoghq.com/cardpayment.logs: &gt;-
          [{
            ""source"": ""java"",
            ""service"": ""cardpayment"",
            ""log_processing_rules"": [{
              ""type"": ""exclude_at_match"",
              ""name"": ""exclude_datadoghq_users"",
              ""pattern"" : ""\\w+@datadoghq.com""
            }]
          }]
      labels:
        app: cardpayment
      name: cardpayment
    spec:
      containers:
        - name: cardpayment
          image: cardpayment:latest
</code></pre>

<p>Does anyone have an idea how to resolve this or have a similar case with mine?
Thank you in advance.</p>",,0,1,,2020-6-15 18:38:29,,2020-6-15 18:38:29,,,,,13751595,,1,2,logging|kubernetes|containers|datadog,622,11.1752
261458,0,Integration,61769846,Is there a serverless kubernetes datadog agent?,"<p>I have a unique type of Kubernetes cluster that cannot install the <a href=""https://docs.datadoghq.com/agent/kubernetes/?tab=helm"" rel=""nofollow noreferrer"">Kubernetes Datadog agent</a>. I would like to collect the logs of individual docker containers in my Kubernetes pods similar to how the <a href=""https://docs.datadoghq.com/agent/docker/log/?tab=containerinstallation#one-step-install"" rel=""nofollow noreferrer"">Docker agent</a> works. </p>

<p>I am currently collecting docker logs from Kubernetes and then using a script with the <a href=""https://docs.datadoghq.com/logs/log_collection/?tab=http#custom-log-forwarder"" rel=""nofollow noreferrer"">Datadog custom log forwarder</a> to upload them to Datadog.  I was curious if there is a better way to achieve this serverless collection of docker logs from Kubernetes clusters in datadog?  The ideal situation I want is to plug my kubeconfig somewhere and then let Datadog take care of the rest without deploying anything onto my Kubernetes cluster.  </p>

<p>Is there an option for that outside of creating a custom script?</p>",61783745,1,3,,2020-5-13 08:32:23,,2020-5-13 20:10:16,,,,,5314903,,1,2,docker|kubernetes|datadog,272,11.1383
261459,3,Monitoring,65751475,How to monitor ssl certificates with Datadog?,"<p>I have an nginx-pod which redirects traffic into Kubernetes services and stores related certificates insides its volume. I want to monitor these certificates - mainly their expiration.</p>
<p>I found out that there is a TLS integration in Datadog (we use Datadog in our cluster): <a href=""https://docs.datadoghq.com/integrations/tls/?tab=host"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/tls/?tab=host</a>.</p>
<p>They provide sample file, which can be found here: <a href=""https://github.com/DataDog/integrations-core/blob/master/tls/datadog_checks/tls/data/conf.yaml.example"" rel=""nofollow noreferrer"">https://github.com/DataDog/integrations-core/blob/master/tls/datadog_checks/tls/data/conf.yaml.example</a></p>
<p>To be honest, I am completely lost and do not understand comments of the sample file - such as:</p>
<pre><code>## @param server - string - required
## The hostname or IP address with which to connect.
</code></pre>
<p>I want to monitor certificates that are stored in the pod, does it mean this value should be localhost or do I need to somehow iterate over all the certificates that are stored using this value (such as server_names in nginx.conf)?
If anyone could help me with setting sample configuration, I would be really grateful - if there are any more details I should provide, that is not a problem at all.</p>",,0,0,,2021-1-16 15:49:45,1,2021-1-16 15:49:45,,,,,15019392,,1,3,ssl|kubernetes|certificate|devops|datadog,594,11.0951
261460,1,Method,66326300,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?","<p>Java app, built with Gradle, implementing SLF4J and Logback, exporting with Logstash to Datadog agentless logging.</p>
<p>Can't seem to get the <code>host</code>, <code>service</code>, or <code>source</code> properties to transmit:</p>
<p><a href=""https://i.stack.imgur.com/l9vMk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l9vMk.png"" alt=""I have a Java app, built with Gradle, implementing SLF4J and Logback, exporting with Logstash to Datadog agentless logging. Can't quite seem to get the host or service properties to transmit properly, nor the source tag. Host and Service are empty; Source is undefined"" /></a></p>
<h3>build.gradle</h3>
<pre><code>implementation 'ch.qos.logback:logback-classic:1.2.3'
implementation 'net.logstash.logback:logstash-logback-encoder:6.6'
implementation 'org.slf4j:log4j-over-slf4j:1.7.13'
implementation 'org.slf4j:slf4j-api:1.7.5'
</code></pre>
<h3>logback.xml</h3>
<p>Note where I've included the <code>&lt;host&gt;</code> and <code>&lt;service&gt;</code> tags. I also tried <code>&lt;property name=&quot;..&quot; value=&quot;..&quot;&gt;</code> and <code>&lt;KeyValuePair key=&quot;service&quot; value=&quot;java-app&quot; /&gt;</code> to no avail.</p>
<pre><code>&lt;appender name=&quot;JSON_TCP&quot; class=&quot;net.logstash.logback.appender.LogstashTcpSocketAppender&quot;&gt;
  &lt;remoteHost&gt;intake.logs.datadoghq.com&lt;/remoteHost&gt;
  &lt;service&gt;my-favorite-service&lt;/service&gt;
  &lt;host&gt;${HOSTNAME}&lt;/host&gt;
  &lt;port&gt;10514&lt;/port&gt;
  &lt;keepAliveDuration&gt;20 seconds&lt;/keepAliveDuration&gt;
  &lt;encoder class=&quot;net.logstash.logback.encoder.LogstashEncoder&quot;&gt;
    &lt;prefix class=&quot;ch.qos.logback.core.encoder.LayoutWrappingEncoder&quot;&gt;
      &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt;
        &lt;pattern&gt;abc123abc123abc123abc123abc123 %mdc{keyThatDoesNotExist}&lt;/pattern&gt;
      &lt;/layout&gt;
    &lt;/prefix&gt;
  &lt;/encoder&gt;
&lt;/appender&gt;
</code></pre>
<h3>Docs</h3>
<p>Here are the docs I'm reading from Datadog:</p>
<ul>
<li><p><a href=""https://docs.datadoghq.com/logs/log_collection/java/?tab=log4j#agentless-logging"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/log_collection/java/?tab=log4j#agentless-logging</a></p>
</li>
<li><p><a href=""https://docs.datadoghq.com/tracing/connect_logs_and_traces/java?tab=log4j2"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/tracing/connect_logs_and_traces/java?tab=log4j2</a></p>
</li>
<li><p><a href=""https://www.datadoghq.com/blog/java-logging-guide/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/java-logging-guide/</a></p>
</li>
<li><p><a href=""https://docs.datadoghq.com/logs/log_collection/?tab=host"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/log_collection/?tab=host</a></p>
</li>
<li><p><a href=""https://docs.datadoghq.com/getting_started/tagging/unified_service_tagging/?tab=kubernetes"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/getting_started/tagging/unified_service_tagging/?tab=kubernetes</a></p>
</li>
</ul>
<p>Also, the docs <a href=""https://github.com/logstash/logstash-logback-encoder/"" rel=""nofollow noreferrer"">for logstash-logback-encoder</a> itself states:</p>
<p>By default, each property of Logback's Context (ch.qos.logback.core.Context) will appear as a field in the LoggingEvent.</p>
<blockquote>
<p>By default, each property of Logback's Context <code>(ch.qos.logback.core.Context)</code> will appear as a field in the LoggingEvent.</p>
</blockquote>
<p>So, how do I add a property to Logback's Context?</p>",,1,0,,2021-2-23 02:41:35,,2021-2-24 06:55:54,2021-2-24 06:54:56,,1335245,,1335245,,1,0,java|logstash|logback|slf4j|datadog,592,11.0893
261461,2,Query,59102943,Understanding the search thread pool of elasticsearch,"<h2>Background:</h2>

<p>I am using Datadog integration with elasticsearch to monitor the ES clusters, one important metric which it shows on its dashboard is the no of active and waiting for search threads. Referring to <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html"" rel=""nofollow noreferrer"">this</a> ES docs, I understand that search threads work on a request queue in ES which is of the fixed size of 1000.</p>

<h2>Problem</h2>

<p>I am seeing a lot of waiting for threads as shown in the image, but there is no rejected queue exception explained <a href=""https://discuss.elastic.co/t/rejected-execution-queue-capacity-1000/89954/2"" rel=""nofollow noreferrer"">here</a>. So it means ES is not rejecting the requests but still search threads are not able to execute the request fast enough hence ended up in waiting status for a long time. </p>

<p><strong>Questions</strong></p>

<ol>
<li>How Search request queue works exactly, is new request comes to this queue and removed as soon as it's picked by a thread?</li>
<li>I Know ES rejecting the request is definitely explains that ES is under-pressure but is there is any way to show that metric in Datadog dashboard, I couldn't find the relevant metric mentioned in <a href=""https://docs.datadoghq.com/integrations/elastic/"" rel=""nofollow noreferrer"">Datadog site</a> if not is there is any API which shows the historical count of these.</li>
<li><strong>Our ES cluster CPU usage is below 45 even during peak time%</strong>, Still, we see a lot of waiting for search threads, so is it possible that our ES configuration isn't optimized? if yes, what are the ways to improve it.</li>
</ol>

<p>I know its a board question, hence let me know if any additional information is required.</p>

<p><a href=""https://i.stack.imgur.com/eYDZT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eYDZT.png"" alt=""enter image description here""></a></p>",,1,0,,2019-11-29 10:21:37,,2021-2-22 22:44:34,,,,,4039431,,1,2,multithreading|performance|elasticsearch|metrics|datadog,263,11.0798
261462,1,Method,61191225,How to get the metrics collected on datadog monitor using Java code?,"<p>I need to monitor fifty application. As apart of which I need to perform healthcheck on datadog dashboard to all the application everyday. So, Is it possible to collect the metrics collected in datadog from Java code
..</p>

<p>Thanks in advance.</p>",,2,0,,2020-4-13 15:30:09,,2020-4-15 15:18:07,,,,,9510770,,1,0,java|spring-boot|datadog,331,11.0793
261463,0,Integration,67476472,How to run an export command within the docker entrypoint?,"<p>I'm trying to integrate Datadog APM tracing and log collection to a python application running on Docker.
Within the Dockerfile, I need to activate the conda virtual env and export an env DD_AGENT_HOST.</p>
<pre><code>FROM continuumio/miniconda3

WORKDIR /app

COPY src ./src
COPY application.yaml .
COPY wsgi.py .
COPY gunicorn.conf.py .
COPY logging.ini .

RUN conda env create -f application.yaml

SHELL [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;dd_venv&quot;, &quot;/bin/bash&quot;, &quot;-c&quot;]

ENTRYPOINT [&quot;conda&quot;, &quot;run&quot;, &quot;--no-capture-output&quot;, &quot;-n&quot;, &quot;dd_venv&quot;, &quot;ddtrace-run&quot;, &quot;gunicorn&quot;, &quot;-c&quot;, &quot;gunicorn.conf.py&quot;, &quot;wsgi:app&quot;, &quot;--preload&quot;]
</code></pre>
<p>I'm able to run the application using the above Dockerfile. However, it is required to export the env DD_AGENT_HOST for the application to connect with Datadog and the value of DD_AGENT_HOST has to be retrieved via an HTTP request.</p>
<pre><code>ENTRYPOINT [&quot;conda&quot;, &quot;run&quot;, &quot;--no-capture-output&quot;, &quot;-n&quot;, &quot;dd_venv&quot;, &quot;export DD_AGENT_HOST=$(wget &lt;ip_ddress&gt;)&quot;, &quot;ddtrace-run&quot;, &quot;gunicorn&quot;, &quot;-c&quot;, &quot;gunicorn.conf.py&quot;, &quot;wsgi:app&quot;, &quot;--preload&quot;]
</code></pre>
<p>I tried adding the export command to the ENTRYPOINT as above, which causes Docker run to fail with the error
&quot;export DD_AGENT_HOST=: command not found:.</p>
<p>What is the correct way of accomplishing this?</p>",67498085,2,2,,2021-5-10 19:19:19,,2021-5-12 06:02:50,,,,,3094264,,1,1,python|docker|conda|gunicorn|datadog,143,11.0213
261464,3,Visualization,56097993,Using template variable in Datadog SLO widget,"<p>I have one Datadig dashboard to monitor a particular service. To use the same dashboard for other services, I added a couple of template variables to change the queries in the dashboard. But, I could not use these variables in the query section of Datadog SLO widget. </p>

<p>The following query with variables works in the other type of widgets, but not in the SLO widget.</p>

<pre><code>sum:aws.applicationelb.httpcode_target_2xx{$environment,$service}.as_count()
</code></pre>

<p>Is there a way to use variables in SLO widget too, or not possible because it is in beta version or something?</p>",,0,1,,2019-5-12 09:45:58,,2019-5-12 10:25:58,2019-5-12 10:25:58,,672798,,672798,,1,3,monitoring|datadog,543,10.9392
261465,0,Integration,59571560,AWS Elastic Beanstalk: Deployment failing,"<p>I have a node.js app already deployed on Elastic Beanstalk. The EC2 instance on which the node app is deployed is running Ubuntu 16.04.5 LTS. I am trying to integrate Datadog APM with Elastic Beanstalk using datadog's config file in .ebextensions folder. I am following the instructions given on their docs page(<a href=""https://docs.datadoghq.com/integrations/amazon_elasticbeanstalk/#alternate-datadog-agent-configuration"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/amazon_elasticbeanstalk/#alternate-datadog-agent-configuration</a>)</p>
<p>Even though I am following all the mentioned steps I keep getting the following error on AWS ELB.</p>
<p><a href=""https://i.stack.imgur.com/sZOvF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sZOvF.png"" alt=""ELB error"" /></a></p>
<p>My Datadog config file code:</p>
<pre><code># .ebextensions/99datadog.config
option_settings:
    - namespace:  aws:elasticbeanstalk:application:environment
    option_name:  DD_API_KEY
    value      :  &quot;MY_API_KEY&quot;
    - namespace:  aws:elasticbeanstalk:application:environment
    option_name:  DD_AGENT_VERSION
    value      : &quot;7.16.0&quot;
files:
    &quot;/configure_datadog_yaml.sh&quot;:
        mode: &quot;000700&quot;
        owner: root
        group: root
        content: |
            #!/bin/bash

            DD_KEY=&quot;$(/opt/elasticbeanstalk/bin/get-config environment -k DD_API_KEY)&quot;

            sed 's/api_key:.*/api_key: '$DD_KEY'/' /etc/datadog-agent/datadog.yaml.example &gt; /etc/datadog-agent/datadog.yaml
            echo -e &quot;process_config:\n  enabled: \&quot;true\&quot;\n&quot; &gt;&gt; /etc/datadog-agent/datadog.yaml

    &quot;/datadog/datadog.repo&quot;:
        mode: &quot;000644&quot;
        owner: root
        group: root
        content: |
            [datadog]
            name = Datadog, Inc.
            baseurl = https://yum.datadoghq.com/stable/7/x86_64/
            enabled=1
            gpgcheck=1
            gpgkey=https://yum.datadoghq.com/DATADOG_RPM_KEY_E09422B3.public

   &quot;/datadog/hooks/99start_datadog.sh&quot;:
       mode: &quot;000755&quot;
       owner: root
       group: root
       content: |
           #!/bin/bash
           STATUS=`sudo initctl status datadog-agent`
           if [[ &quot;$STATUS&quot; == *&quot;datadog-agent start/running&quot;* ]]
           then
             echo &quot;Agent already running&quot;
           else
             echo &quot;Agent starting...&quot;
             sudo initctl start datadog-agent
           fi

   &quot;/datadog/hooks/99stop_datadog.sh&quot;:
       mode: &quot;000755&quot;
       owner: root
       group: root
       content: |
           #!/bin/bash
           STATUS=`sudo initctl status datadog-agent`
           if [[ &quot;$STATUS&quot; == *&quot;datadog-agent stop/waiting&quot;* ]]
           then
             echo &quot;Agent already stopped&quot;
           else
             echo &quot;Agent stopping...&quot;
             sudo initctl stop datadog-agent
           fi


container_commands:
    02mkdir_appdeploy_post:
        test: '[ ! -d /opt/elasticbeanstalk/hooks/appdeploy/post ]'
        command: &quot;mkdir /opt/elasticbeanstalk/hooks/appdeploy/post&quot;
    02mkdir_configdeploy_post:
        test: '[ ! -d /opt/elasticbeanstalk/hooks/configdeploy/post ]'
        command: &quot;mkdir /opt/elasticbeanstalk/hooks/configdeploy/post&quot;
    10appdeploy_pre_stop:
        test: '[ -f /datadog/hooks/99stop_datadog.sh ]'
        command: &quot;cp /datadog/hooks/99stop_datadog.sh /opt/elasticbeanstalk/hooks/appdeploy/pre/&quot;
    11appdeploy_post_start:
        test: '[ -f /datadog/hooks/99start_datadog.sh ]'
        command: &quot;cp /datadog/hooks/99start_datadog.sh /opt/elasticbeanstalk/hooks/appdeploy/post/&quot;
    20preinit_stop:
        test: '[ -f /datadog/hooks/99stop_datadog.sh ]'
        command: &quot;cp /datadog/hooks/99stop_datadog.sh /opt/elasticbeanstalk/hooks/preinit&quot;
    21postinit_start:
        test: '[ -f /datadog/hooks/99start_datadog.sh ]'
        command: &quot;cp /datadog/hooks/99start_datadog.sh /opt/elasticbeanstalk/hooks/postinit&quot;
    30configdeploy_pre_stop:
        test: '[ -f /datadog/hooks/99stop_datadog.sh ]'
        command: &quot;cp /datadog/hooks/99stop_datadog.sh /opt/elasticbeanstalk/hooks/configdeploy/pre/&quot;
    31configdeploy_post_start:
        test: '[ -f /datadog/hooks/99start_datadog.sh ]'
        command: &quot;cp /datadog/hooks/99start_datadog.sh /opt/elasticbeanstalk/hooks/configdeploy/post/&quot;
    90install_datadog:
        test: '[ -f /datadog/datadog.repo ]'
        command: 'cp /datadog/datadog.repo /etc/yum.repos.d/datadog.repo; yum -y makecache; DD_AGENT_VERSION=&quot;$(/opt/elasticbeanstalk/bin/get-config environment -k DD_AGENT_VERSION)&quot;; yum -y install datadog-agent${DD_AGENT_VERSION:+-$DD_AGENT_VERSION-1}'
    91setup_datadog:
        test: '[ -x /configure_datadog_yaml.sh ]'
        command: &quot;/configure_datadog_yaml.sh&quot;
</code></pre>
<p>Even after replacing the <em>initctl</em> with <em>systemctl</em> in the start &amp; stop scripts, I am still getting the same error.</p>
<p>Can't understand where I'm going wrong. Please Help!</p>",,0,0,,2020-1-2 23:57:59,,2020-1-2 23:57:59,2020-6-20 09:12:55,,-1,,2195092,,1,2,node.js|amazon-web-services|amazon-elastic-beanstalk|ubuntu-16.04|datadog,536,10.9167
261466,0,Integration,65202464,Can Heroku Postgres dynos talk with Datadog?,"<p>I have a Postgres dyno on Heroku and I use Datadog.</p>
<p>Two postgres dashboards are by default on Datadog: Metrics and Overview.</p>
<p>Metrics is working (CPU usage, memory, I/O,...) but Overview is not (deadlocks, indexes usages)</p>
<p>Are Heroku Postgres dyno and Datadog fully compatible?</p>",67420698,1,2,,2020-12-8 16:04:51,,2021-5-6 14:57:36,,,,,1660655,,1,3,heroku-postgres|datadog,378,10.91
261467,0,Integration,68134668,How to choose an Opentelemetry backend vendor?,"<p>With Opentelemetry becoming the new standard of tracing, and it being vendor-agnostic, how do we then choose a backend vendor for opentelemetry?</p>
<p>For example, there are currently many vendors that supports Opentelemetry like GCP Cloudtrace, Datadog, Dynatrace, Lightstep, Instana. How do you choose a vendor for just opentelemtry? Or it doesn't matter at all since opentelemetry is cloud-agnostic and we can just choose the cheapest one to store our traces</p>",68200489,1,0,,2021-6-25 16:56:24,,2021-6-30 19:15:07,,,,,7455397,,1,6,datadog|dynatrace|open-telemetry|google-cloud-trace|lightstep,146,10.8574
261468,1,Error,59092413,Error 500 when requesting Datadog logs by Python Requests,"<p>I have the following <code>curl</code> command which works fine:</p>

<pre><code>curl -X POST -H 'content-type: application/json' -H ""DD-API-KEY: ${api_key}"" -H ""DD-APPLICATION-KEY: ${app_key}"" \
-d '{ 
        ""query"": ""service:my_service"",
        ""time"": {
            ""from"": ""2019-11-28T00:00:00Z"",
            ""to"": ""2019-11-28T16:00:00Z""
        },
        ""sort"": ""asc"",
        ""limit"": 1000
    }' ""https://api.datadoghq.com/api/v1/logs-queries/list"" -o output3.json5
</code></pre>

<p>Then I convert this requests to Python Requests, and the <code>curl</code> method works but Python returns a 500 error without any details.</p>

<pre><code>import requests

def main():
    headers = {
        'content-type': 'application/json',
        'DD-API-KEY': 'AAA',
        'DD-APPLICATION-KEY': 'XXX',
    }

    data = {
        ""query"": ""service:my_service"",
        ""time"": {
            ""from"": ""now - 1h"",
            ""to"": ""now""
        },
        ""sort"": ""asc"",
        ""limit"": 50
    }
    response=requests.post(""https://api.datadoghq.com/api/v1/logs-queries/list"",headers=headers, data=data)
</code></pre>

<p>I tried it outside my Docker guessing that maybe connection was the key, but it doesn't work either.</p>",59096255,1,2,,2019-11-28 15:49:14,,2019-11-28 21:20:30,2019-11-28 21:20:30,,354577,,5556466,,1,0,python|curl|python-requests|datadog,291,10.8556
261469,1,Error,54811591,Issue in placing DataDog log annotation on deployment via ansible,"<p>I am using ansible version 2.7 for kubernetes deployment. 
For sending logs to datadog on kubernetes one of the way is to configure annotations like below,</p>

<pre><code>template:
    metadata:
      annotations:
        ad.datadoghq.com/nginx.logs: '[{""source"":""nginx"",""service"":""webapp""}]'
</code></pre>

<p>this works fine and I could see logs in DataDog.</p>

<p>However I would like to achieve above configuration via ansible deployment on kubernetes for which I have used below code</p>

<pre><code> template:
        metadata:
           annotations:
             ad.datadoghq.com/xxx.logs: ""{{ lookup('template', './datadog.json.j2')}}""
</code></pre>

<p>and datadog.json.j2 looks like below</p>

<pre><code>'[{{ '{' }}""source"":""{{ sourcea }}""{{ ',' }} ""service"":""{{ serviceb }}""{{ '}' }}]'  **--&gt; sourcea and serviceb are defined as vars**
</code></pre>

<p>However the resulting config on deployment is below</p>

<pre><code>template:
    metadata:
      annotations:
        ad.datadoghq.com/yps.logs: |
          '[{""source"":""test"", ""service"":""test""}]'
</code></pre>

<p>and this config does not allow datadog agent to parse logs failing with below error</p>

<pre><code>[ AGENT ] 2019-xx-xx xx10:50 UTC | ERROR | (kubelet.go:97 in parseKubeletPodlist) | Can't parse template for pod xxx-5645f7c66c-s9zj4: could not extract logs config: in logs: invalid character '\'' looking for beginning of value
</code></pre>

<p>if I use ansible code as below (using replace) </p>

<pre><code>template:
        metadata:
           annotations:
             ad.datadoghq.com/xxx.logs: ""{{ lookup('template', './datadog.json.j2', convert_data=False) | string | replace('\n','')}}""
</code></pre>

<p>it generates deployment config as below</p>

<pre><code>template:
    metadata:
      annotations:
        ad.datadoghq.com/yps.logs: '''[{""source"":""test"", ""service"":""test""}]'''
      creationTimestamp: null
      labels:
</code></pre>

<p>Which also fails,</p>

<p>to configure the working config with ansible, I have to either remove leading pipe (|) or three quotes coming when using replace). </p>

<p>I would like to have jinja variables substitution in place so that I could configure deployment with desired source and service at deployment time.</p>

<p>kindly suggest</p>",,1,0,,2019-2-21 16:14:23,,2019-2-21 18:18:42,2019-2-21 17:56:33,,1958107,,1958107,,1,1,kubernetes|ansible|jinja2|datadog,259,10.8532
261470,1,Method,54508636,Datadog event using micrometer,<p>I am utilizing dogstatsd approach to send metrics to datadog using micrometer. I get the normal metrics like counter and gauge but I am not able to generate events. Is there a way to generate datadog events?</p>,,2,1,,2019-2-3 23:29:07,,2021-4-28 19:13:29,,,,,3335579,,1,2,metrics|datadog|micrometer|spring-micrometer,315,10.7932
261471,2,Query,64661079,String interpolation with the name of metric in the Datadog query,"<p>There are a bunch of custom metrics that looks like:</p>
<pre><code>metric.name.key1
metric.name.key2
...
</code></pre>
<p>There is a template variables dropdown (<a href=""https://docs.datadoghq.com/dashboards/template_variables/"" rel=""noreferrer"">https://docs.datadoghq.com/dashboards/template_variables/</a>) on the dashboard that contains all keys. (Name of the variable is <em>$key</em> and values key1, key2..)</p>
<p>The metrics query like this works fine:</p>
<pre><code>avg:metric.name.key1{*}.as_count()
</code></pre>
<p><strong>But is it possible to parametrize the query with the <em>$key</em> variable by concatenation with the part of the name of metric?</strong>
So it would look like this:</p>
<pre><code>avg:metric.name.$key{*}.as_count()
</code></pre>
<p>As I know, it is possible to something like that by using tags but unfortunately, there are no any metrics with the tag &quot;key&quot;.</p>",,0,2,,2020-11-3 10:32:26,,2020-11-3 10:32:26,,,,,5810648,,1,5,metrics|datadog,471,10.6921
261472,3,Visualization,61511271,What is causing DataDog widgets to display http status code as N/A,"<p>My organization is setting up dashboards for our backend services and after performance testing that we ran, we have noticed that some API calls report http status <code>N\A</code>.</p>

<p><a href=""https://i.stack.imgur.com/PKcpd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PKcpd.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/xCKT4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xCKT4.png"" alt=""enter image description here""></a></p>

<p>It is not very helpful, anyone seen something like that? 
Is that a configuration issue? </p>",61513379,1,0,,2020-4-29 20:40:27,,2020-4-29 23:25:24,,,,,1048185,,1,0,spring-boot|datadog,260,10.6599
261473,0,Configuration,67843569,datadog API python auth,"<p>Is there a basic example of how to authenticate to datadog using the python lib <code>datadog_api_client.v2</code>? I was looking at their documentation <a href=""https://datadoghq.dev/datadog-api-client-python/v2/#apikeyauth"" rel=""nofollow noreferrer"">https://datadoghq.dev/datadog-api-client-python/v2/#apikeyauth</a> and am not seeing any examples I was expecting something along the lines of:</p>
<pre><code>import datadog_api_client.v2
import os
from datadog_api_client.v2 import list_tag_configurations

configuration = datadog_api_client.v2.Configuration(
    host = &quot;https://api.datadoghq.com&quot;
)

configuration.api_key['apiKeyAuth'] = os.getenv('123')

with datadog_api_client.v2.ApiClient(configuration) as api_client:
    api_instance = list_tag_configurations(api_client)
    print(api_instance) 
</code></pre>",67878190,1,1,,2021-6-4 20:26:36,,2021-6-7 20:18:07,2021-6-4 21:07:07,,3953007,,3953007,,1,3,python|datadog,327,10.6582
261474,3,Monitoring,55953321,How to set downtime duration for datadog downtime recuring weekly,"<p>I'm trying to create a datadog monitor that only alerts on Wednesdays and Fridays. I have created the metric and monitor, and I think the best solution is to create a schedualed downtime that repeats for the days I'm not interessted in. </p>

<p>Ive created the downtime window as:</p>

<pre><code>resource ""datadog_downtime"" ""rm_snapshot"" {
  scope = [""*""]
  start = 1556841600

  disabled = false
  monitor_id = ""${module.RM_snapshot.monitor_id}""

  recurrence = {
    period = 1
    type = ""weeks""
    week_days = [""Sun"", ""Mon"", ""Tue"", ""Thu"", ""Sat""]
  }

  lifecycle {
    ignore_changes = [""start"", ""end""]
  }
}
</code></pre>

<p>This creates a window for only 1hr, ideally this should be 24hr</p>",,1,0,,2019-5-2 13:14:08,,2019-5-2 16:18:34,,,,,9497116,,1,0,terraform|datadog,455,10.632
261475,1,Method,59890153,get more metrics with http.server.request metrics using mircrometer and statsd,"<p>How do I get all the metrics including status codes and exceptions using micrometer and statsd with flavor datadog. I am using maven dependency for micrometer-statsd and spring-boot actuator ? </p>

<p>I have added @Timed annotation according to spring boot actuator configuration to a controller. But in the graphite I only see http.server.requests.max BUT no exceptions or status codes. </p>

<p>Can someone point out what config am I missing ?</p>

<pre><code>@RestController
@Timed
@Slf4j
public class AccountsController {

@PostMapping(consumes = MediaType.APPLICATION_JSON_VALUE)
@ResponseStatus(value = HttpStatus.CREATED)
@Timed(value = ""createAccount.timer"", percentiles = {0.9, 0.99}, histogram = true)
@Counted(value = ""compass.createAccount.counter"")
public AccountResponse createAccount(@RequestBody CreateAccountRequest 
createAccountRequest){
// Business logic
}
}
</code></pre>

<pre><code>@Configuration
@EnableAspectJAutoProxy
public class MetricConfiguration {

    @Bean
    public TimedAspect timedAspect(MeterRegistry registry) {
        return new TimedAspect(registry);
    }

    @Bean
    public CountedAspect countedAspect(MeterRegistry registry) {
        return new CountedAspect(registry);
    }
}
</code></pre>",,0,3,,2020-1-24 03:49:40,,2020-1-24 04:25:37,2020-1-24 04:25:37,,9576741,,9576741,,1,0,spring-boot|spring-boot-actuator|datadog|micrometer,454,10.6282
261476,2,Query,62782910,How to calculate duration between logs in Datadog?,"<p>Splunk has <code>transaction</code> command which can produce <code>duration</code> between logs grouped by id:</p>
<pre><code>2020-01-01 12:12 event=START id=1
2020-01-01 12:13 event=STOP  id=1
</code></pre>
<p>as it is decribed on</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/61358636/query-for-calculating-duration-between-two-different-logs-in-splunk"">Query for calculating duration between two different logs in Splunk</a></li>
<li><a href=""https://stackoverflow.com/questions/45551991/splunk-duration-between-two-different-messages-by-guid?rq=1"">Splunk - duration between two different messages by guid</a></li>
<li><a href=""https://community.splunk.com/t5/Splunk-Search/transaction-time-between-events/td-p/50279"" rel=""nofollow noreferrer"">transaction time between events</a></li>
</ul>
<p>How to calculate duration between events in Datadog?</p>",,0,0,,2020-7-7 19:52:50,,2020-7-7 23:11:06,2020-7-7 23:11:06,,2227420,,5962766,,1,4,logging|monitoring|datadog|splunk-query,453,10.6244
261477,2,Query,64992563,Datadog: METRIC.as_rate() vs. per_second(METRIC),"<p>I'm trying to figure out the difference between the <a href=""https://docs.datadoghq.com/developers/metrics/type_modifiers/?tab=count#in-application-modifiers"" rel=""nofollow noreferrer"">in-application modifier</a> <code>as_rate()</code> and the <a href=""https://docs.datadoghq.com/dashboards/functions/rate/#per-second"" rel=""nofollow noreferrer"">rollup function</a> <code>per_second()</code>.</p>
<p>I want a table with two columns: the left column shows the total number of events submitted to a <a href=""https://docs.datadoghq.com/developers/metrics/types/?tab=distribution#metric-types"" rel=""nofollow noreferrer"">Distribution</a> (in query-speak: <code>count:METRIC{*} by {tag}</code>), and the right column shows the average rate of events per second. The table visualization applies a sum rollup on left column, and an average rollup on the right column, so that the left column should equal the right column multiplied by the total number of seconds in the selected time period.</p>
<p>From reading the docs I expected either of these queries to work for the right column:</p>
<p><code>count:DISTRIBUTION_METRIC{*} by {tag}.as_rate()</code></p>
<p><code>per_second(count:DISTRIBUTION_METRIC{*} by {tag})</code></p>
<p>But, it turns out that these two queries are not the same. <code>as_rate()</code> is the only one that finds the expected average rate where <code>left = right * num_seconds</code>. In fact, the <code>per_second()</code> rollup does this extra weird thing where metrics with lower total events have higher average rates.</p>
<p>Is someone able to clarify why these two functions are not synonymous and what <code>per_second()</code> does differently?</p>",,0,0,,2020-11-24 18:21:01,,2020-11-24 18:21:01,,,,,6248782,,1,2,metrics|rate|datadog,451,10.6167
261478,2,Query,56748943,Per second rate values for a metric are coming different for prometheus and datadog,"<p>I am very new to the monitoring of microservices using prometheus and datadog. I am trying to monitor the rate of event callback requests per second using PromQL queries and datadog queries but when i compared the values from both the tools, they came out to be very different. I want to know if the two values need to be same. If yes, then how do i write my queries to obtain the correct values</p>

<p>PromQL query </p>

<pre><code>sum by (service)(rate(service_event_callback_processing_seconds_count[1m]))

</code></pre>

<p>Datadog query(json file)</p>

<pre><code>{
  ""viz"": ""timeseries"",
  ""requests"": [
    {
      ""q"": ""per_second(avg:prod.service_event_callback_processing_seconds.count{$service,team:callai})"",
      ""type"": ""line"",
      ""style"": {
        ""palette"": ""dog_classic"",
        ""type"": ""solid"",
        ""width"": ""normal""
      },
      ""aggregator"": ""avg"",
      ""conditional_formats"": []
    }
  ],
  ""autoscale"": true
}

</code></pre>

<p>Please tell me how can i make the two queries equivalent?</p>",,0,0,,2019-6-25 07:31:34,0,2019-6-25 08:07:16,2019-6-25 08:07:16,,9952376,,9952376,,1,1,rate|datadog,449,10.609
261479,3,Monitoring,66775454,Should 4xx status codes be considered errors in a Frontend/SSR application?,"<p>We're implementing logging &amp; monitoring for a Vue/Node application which is using a REST Api.</p>
<p>Oftentimes the API returns 4xx reponses (401s, 404s) which are currently caught by Axios and returned as &quot;Errors&quot;.</p>
<p>These end up in our logging solutions (Datadog, Sentry) but dont bring much actionable points.</p>
<p>Should in general  status codes like these be considered Errors? Are there any best practices for SPA logging and monitoring? (couldn't find any resources)</p>",,1,0,,2021-3-24 05:56:52,,2021-3-24 06:05:50,,,,,458060,,1,1,javascript|single-page-application|monitoring|sentry|datadog,125,10.5876
261480,3,Monitoring,68108626,How can I detect stuck Airflow jobs?,"<p>My team and I are on Airflow v2.1.0 using the Celery executor with Redis. Recently we’ve noticed some jobs are occasionally running until we kick them (many hours, sometimes days—basically until someone notices). There doesn’t seem to be a particular pattern that we’ve noticed yet.</p>
<p>We also use DataDog and the statsd provider to collect and monitor metrics produces by Airflow. Ideally we could setup a DataDog monitor for this but there doesn’t appear to be an obvious metric for this situation.</p>
<p>How can we detect and alarm on stuck jobs like this?</p>",,2,0,,2021-6-24 01:29:37,,2021-6-24 12:08:35,,,,,1542038,,1,2,airflow|airflow-scheduler|datadog|statsd,87,10.5581
261481,0,Integration,58694140,Checking Datadog agent versions installed on AWS EC2 Instances,"<p>Recent Tenable scan highlighted an issue with certain versions of datadog versions. This is also brought to attention in Datadog monitor.</p>

<p>Critical bug in Windows Agent versions 6.14.0 and 6.14.1. See --> <a href=""http://dtdg.co/win-614-fix"" rel=""nofollow noreferrer"">http://dtdg.co/win-614-fix</a> &lt;-- for steps to fix the issue. </p>

<p>As the bulk of our servers are hosted on AWS - just wondered if I could query this through AWS CLI to list which servers were using the affected versions. </p>",,1,2,,2019-11-4 13:11:29,,2019-11-4 14:13:25,,,,,12320061,,1,0,amazon-web-services|command-line-interface|datadog,243,10.5424
261482,1,Method,59757258,Datadog Create custom metric from another custom metric,"<p>I have a datadog count metric that I want to create a new metric from which shows the difference between two agent points on the metric, so I can see the change between points.</p>

<p>Is there a way to create a metric from another metric using the datadog dashboard.</p>",,1,0,,2020-1-15 18:08:10,,2020-1-15 18:38:33,,,,,47508,,1,0,datadog,243,10.5424
261483,3,Monitoring,58607248,Implement opentracing in Spring Boot for Datadog,"<p>I need to implement tracing opentracing (opentelementary) for datadog in my Spring Boot application with rest controller.</p>

<p>I have a given kubernetes endpoint, to which I should send traces.</p>",,1,0,,2019-10-29 12:12:01,,2019-11-4 08:54:37,2019-10-29 16:11:07,,1000551,,11680916,,1,0,spring-boot|trace|datadog|opentracing,426,10.5176
261484,1,Method,68290539,Getting Kafka Connect JMX metrics reporting into Datadog,"<p>I am working won a project involving Kafka Connect.  We have a Kafka Connect cluster running on Kubernetes with some Snowflake connectors already spun up and working.  The part we are having issues with now is trying to get the JMX metrics from the Kafka Connect cluster to report in Datadog.  From my understanding of the Docs (<a href=""https://docs.confluent.io/home/connect/monitoring.html#using-jmx-to-monitor-kconnect"" rel=""nofollow noreferrer"">https://docs.confluent.io/home/connect/monitoring.html#using-jmx-to-monitor-kconnect</a>) the workers are already emitting metrics by default and we just need to find a way to get it reported to Datadog.</p>
<p>In our K8 Configmap we have these values set:</p>
<pre><code>    CONNECT_KAFKA_JMX_PORT: &quot;9095&quot;
    KAFKA_JMX_PORT: &quot;9095&quot;
    JMX_PORT: &quot;9095&quot;
</code></pre>
<p>I have included this launch script where we are setting the KAFKA_JMX_PORT env var:</p>
<pre><code>export KAFKA_JMX_OPTS=&quot;-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=&lt;redacted&gt; -Dcom.sun.management.jmxremote.rmi.port=${JMX_PORT}&quot;
</code></pre>
<p>I’ve been looking online and all over Stackoverflow and haven’t actually seen an example of people getting JMX metrics reporting to Datadog and standing up a dashboard there so I was wondering if anyone had experience with this.</p>",,1,1,,2021-7-7 17:22:31,,2021-7-9 08:55:11,,,,,16395485,,1,0,kubernetes|apache-kafka|apache-kafka-connect|jmx|datadog,425,10.5136
261485,3,Monitoring,68017937,"Is there a way to export all Datadog Monitors, Alerts and Dashboards?","<p>So far what I got when researching about that question was summarised here (<a href=""https://stackoverflow.com/questions/66001758/export-datadog-monitor-alerts-weekly"">Export Datadog Monitor Alerts Weekly</a>) and here (<a href=""https://docs.datadoghq.com/monitors/faq/how-can-i-export-alert-history/#pagetitle"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/monitors/faq/how-can-i-export-alert-history/#pagetitle</a>)</p>
<p>My actual challenge is to find a way to mass convert several monitors, alerts and dashboards to Terraform scripts.  The reason for that is that the actual DevOps infrastructure and process have already in place an automation that reads from a GIT full of Terraform scripts and once it gets a new Pull Request it imports to the Datadog environment the Monitor, Alert or Dashboard that was in the approved pull request.</p>
<p>Is there a way to export all Datadog Monitors, Alerts and Dashboards into a set of JSON files?</p>",,1,0,,2021-6-17 10:55:30,,2021-6-17 15:21:03,,,,,3490004,,1,0,terraform|export|devops|datadog,419,10.4889
261486,0,Configuration,57758517,Micrometer's Gauge - update ratio,"<p>I'm wondering what's the update ratio of gauge meter in micrometer:</p>

<pre><code>List&lt;String&gt; list = registry.gauge(""listGauge"", Collections.emptyList(), new ArrayList&lt;&gt;(), List::size);
</code></pre>

<p>now let's say that list is changing sometimes often, sometimes stays without a change for a long time. How often <code>aList.size()</code> will be called? </p>

<p><strong><em>More general question:</em></strong></p>

<p>Is gauge a good pick for reporting DB table state every x seconds? I've a table with date column and want to configure alert when date is older than t seconds. That requires DB query each time, so I don't want to be called too often. Appreciate any advice.</p>",,1,0,,2019-9-2 14:17:15,,2021-6-23 15:28:20,,,,,2099376,,1,0,spring|spring-boot|datadog|micrometer|spring-micrometer,414,10.468
261487,2,Query,58873873,Not able to search for $ in datadog logs,<p>I am searching for the occurrence of character <code>$</code> in a log file on datadog log explorer which uses lucene syntax. It's pretty similar to kibana. I have logged a string for testing <code>Testing $ pattern datadog</code> but when I search for <code>$</code> it doesn't show any results. On searching for <code>Testing</code> I get <code>Testing $ pattern datadog</code> in response. Please tell me how can list the occurrences of <code>$</code> any help is much appreciated. </p>,,1,0,,2019-11-15 09:25:26,,2019-11-19 12:49:29,2019-11-15 10:40:42,,10309342,,12377556,,1,0,lucene|datadog,412,10.4596
261488,1,Error,65717686,kafka datadog not sending metrics correctly,"<p>i am trying to send kafka consumer metrics to datadog but its not showing in monitoring when I select the node. The server is giving below check in status</p>
<pre><code>   Instance ID: kafka_consumer:d6........f5 [OK]
      Configuration Source: file:/etc/datadog-agent/conf.d/kafka_consumer.d/conf.yaml
      Total Runs: 567
      Metric Samples: Last Run: 0, Total: 0
      Events: Last Run: 0, Total: 0
      Service Checks: Last Run: 0, Total: 0
      Average Execution Time : 162ms
      Last Execution Date : 2021-01-14 10:49:06.000000 UTC
      Last Successful Execution Date : 2021-01-14 10:49:06.000000 UTC
      metadata:
        version.major: 2
        version.minor: 5
        version.patch: 0
        version.raw: 2.5.0
        version.scheme: semver


JMXFetch
    runtime_version : 11.0.9.1
    version : 0.40.3
  Initialized checks
    kafka
      instance_name : kafka-10.128.0.105-9999
      message : &lt;no value&gt;
      metric_count : 99
      service_check_count : 0
      status : OK
Failed checks
    no checks
</code></pre>
<p>JMX is as above. Please help in finding what could be wrong.</p>",66783806,1,0,,2021-1-14 10:56:11,,2021-3-24 15:02:56,,,,,9063834,,1,-1,apache-kafka|datadog,259,10.4532
261489,2,Query,57189512,How to count the number of metrics sent to Datadog over a 24 hour period?,"<p>I have a situation where I'm trying to count the number of files loaded into the system I am monitoring. I'm sending a ""load time"" metric to Datadog each time a file is loaded, and I need to send an alert whenever an expected file does not appear. To do this, I was going to count up the number of ""load time"" metrics sent to Datadog in a 24 hour period, then use anomaly detection to see whether it was less than the normal number expected. However, I'm having some trouble finding a way to consistently pull out this count for use in the alert.</p>

<p>I can't use the count_nonzero function, as some of my files are empty and have a load time of 0. I do know about .as_count() and count:metric{tags}, but I haven't found a way to include an evaluation interval with either of these. I've tried using .rollup(count, time) to count up the metrics sent, but this call seems to return variable results based on the rollup interval. For instance, if I compare intervals of 2000 and 4000 seconds, I would expect each 4000 second interval to count up about the sum of two 2000 second intervals over the same time period. This does not seem to be what happens at all - the counts for the smaller intervals seem to add up to much more than the count for the larger one. Additionally some rollup intervals display decimal numbers as counts, which does not make any sense to me if this function is doing what I thought it was.</p>

<p>Does anyone have any thoughts on how to accomplish this? I'd really appreciate any new ideas.</p>",,0,2,,2019-7-24 18:52:22,,2019-7-24 19:05:56,2019-7-24 19:05:56,,11736052,,11736052,,1,1,monitoring|dashboard|datadog,406,10.4341
261490,0,Configuration,66761816,How can I add pg_monitor role to a postgresql user on heroku,"<p>I'm trying to set up a Datadog PostgreSQL integration that requires a user with <code>pg_monitor</code> role and <code>SELECT</code> permission on <code>pg_stat_database</code> as described on their own <a href=""https://docs.datadoghq.com/integrations/postgres/?tab=host#configuration"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>My database is currently hosted on Heroku and it seems the default user doesn't have <code>SUPERUSER</code>permissions because, when I try to apply the above role and permission to a &quot;monitor&quot; user I have the following error message:</p>
<blockquote>
<p>ERROR:  must have admin option on role &quot;pg_monitor&quot;</p>
</blockquote>
<p>So I'm looking for some way of:</p>
<ul>
<li>grant the necessary permissions to that user without being a superuser</li>
<li>get superuser access on Heroku Postgres (what I think is <a href=""https://help.heroku.com/IV1DHMS2/can-i-get-superuser-privileges-or-create-a-superuser-in-heroku-postgres"" rel=""nofollow noreferrer"">not possible</a>)</li>
</ul>
<p>Someone has ever faced this issue? There is a way to handle this case?</p>",66871585,1,0,,2021-3-23 10:59:05,1,2021-3-30 13:10:14,2021-3-30 13:10:14,,6301287,,6301287,,1,0,postgresql|heroku|heroku-postgres|datadog,227,10.4241
261491,3,Monitoring,66354474,How can I detect SNAT port exhaustion on Azure using Datadog metrics?,"<p>Here's a <a href=""https://docs.datadoghq.com/integrations/azure_load_balancer/"" rel=""nofollow noreferrer"">list</a> of DataDog metrics of Azure Load Balancer that are available to use. It seems like</p>
<pre><code>azure.network_loadbalancers.allocated_snat_ports (count) - Total number of SNAT ports allocated within time period,
azure.network_loadbalancers.snat_connection_count (count) - Total number of new SNAT connections created within time period,
azure.network_loadbalancers.used_snat_ports (count) - Total number of SNAT ports used within time period
</code></pre>
<p>are the most releveant.</p>
<blockquote>
<p>When SNAT port resources are exhausted, outbound flows fail. You could observe failing outbound connections or are advised by support that you're exhausting SNAT ports.</p>
</blockquote>
<blockquote>
<p>Simply seeing failed connections does not confirm SNAT exhaustion. Seeing the failed connections was a clue we were having an issue but there is no way to confirm SNAT exhaustion w/o a ticket to Microsoft it turns out.</p>
</blockquote>",,1,0,,2021-2-24 16:02:56,1,2021-2-24 19:03:13,,,,,15013270,,1,0,azure|nat|datadog|azure-vm-scale-set|azure-load-balancer,391,10.3687
261492,1,Method,67366237,DB Mocking in one Go test case is interfering with other test case,"<p>I have two Go test cases as shown below that test a gRPC function called <code>MyEndpoint</code>.</p>
<p><code>MyEndpoint</code> is supposed to succeed when the database row it selects has Field1 == &quot;A&quot; and return an error otherwise.</p>
<p>I'm mocking out the database with the <a href=""https://github.com/DATA-DOG/go-sqlmock"" rel=""nofollow noreferrer"">go-sqlmock package from Data-dog</a>.</p>
<pre><code>package mypackage_test

import (
    &quot;github.com/DATA-DOG/go-sqlmock&quot;
    &quot;github.com/stretchr/testify/require&quot;
)


type MyEntity struct {
    Id                     sql.NullInt32 `db:&quot;id&quot;`  
    Field1                 sql.NullString `db:&quot;field1&quot;`
    Field2                 sql.NullString `db:&quot;field2&quot;`
    Field3                 sql.NullString `db:&quot;field3&quot;`
}

var Columns = []string{
    &quot;id&quot;,
    &quot;field_1&quot;,
    &quot;field_2&quot;,
    &quot;field_3&quot;
}



var dbRow = []driver.Value{int32(999), &quot;A&quot;, &quot;B&quot;, &quot;C&quot;]


func TestMyTestA(t *testing.T) {
    t.Run(&quot;Verify MyEndpoint Fails when mocked Database row has Field1 != A&quot;, func(t *testing.T) {
        api, err := getMyAPI()
        require.Nil(t, err)
        defer api.Close()

        api.DBMock.ExpectBegin()
        api.DBMock.MatchExpectationsInOrder(false)

        modifiedDBRow := dbRow
        modifiedDBRow[0] = &quot;Z&quot;
        api.DBMock.ExpectQuery(&quot;SELECT&quot;).
            WithArgs(int32(999)).WillReturnRows(sqlmock.NewRows(Columns).AddRow(modifiedDBRow...))
        api.DBMock.ExpectCommit()
        
        _, err = ... // Call MyEndpoint with parameter Id: int32(999)
        api.DBMock.ExpectClose()
        require.NotNil(t, err)
    })
}

func TestMyTestB(t *testing.T) {
    t.Run(&quot;Verify MyEndpoint succeeds when mocked Database row has Field1 == A&quot;, func(t *testing.T) {
        api, err := getMyAPI()
        require.Nil(t, err)
        defer api.Close()

        api.DBMock.ExpectBegin()
        api.DBMock.MatchExpectationsInOrder(false)

        api.DBMock.ExpectQuery(&quot;SELECT&quot;).
            WithArgs(int32(999)).WillReturnRows(sqlmock.NewRows(Columns).AddRow(dbRow...))
        api.DBMock.ExpectCommit()
        
        _, err = ... // Call MyEndpoint with parameter Id: int32(999)
        api.DBMock.ExpectClose()
        require.Nil(t, err)
    })
}
</code></pre>
<p>When I run these two test cases individually, they both pass.</p>
<p>But when I run them together, <code>TestMyTestB</code> fails because it thinks Field1 == &quot;Z&quot;.
So clearly <code>TestMyTestA</code> is interfering with <code>TestMyTestB</code>. Why?</p>
<p>It seems that the mocking done in <code>TestMyTestA</code> case is still in effect when it gets to <code>TestMyTestB</code> and the mocking I'm doing in <code>TestMyTestB</code> case is completely ignored.</p>
<p>How can I mock these two test cases out independently of one another?</p>",67367129,1,0,,2021-5-3 09:07:22,,2021-5-3 10:16:11,2021-5-3 09:45:57,,1742777,,1742777,,1,0,go|datadog|go-sqlmock,123,10.3596
261493,1,Method,65829133,How to Tag / Label / Filter DataDog logs,"<p>I’m trying to group logs from a source so I can filter them in or out in DataDog logs.</p>
<p>There is already a grok parser that formats the messages, but how can I add a tag to them?</p>
<p>DataDog seem to use a subset of LogStash grok parsing rules: <a href=""https://docs.datadoghq.com/logs/processing/processors/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/processing/processors/</a></p>
<p>Eg from Heroku:</p>
<pre><code>source=worker.1 dyno=heroku.152688202.1b2da7cb-291c-43ea-b5ee-bf9388bc5c6a 
sample#load_avg_1m=0.06 sample#load_avg_5m=0.14 sample#load_avg_15m=0.09
</code></pre>
<p>as</p>
<pre><code>{ source { worker: 1 }...
</code></pre>
<p>What I'd like is to add something like a <code>type</code>, so I know they are not from the app, eg</p>
<pre><code>{ type: 'metrics', source { worker: 1 }...
</code></pre>
<p>Then I guess I could add the same thing to app logs and add <code>type: 'app'</code> to them.</p>",,1,3,,2021-1-21 14:06:29,,2021-1-21 19:29:01,2021-1-21 17:19:14,,119790,,119790,,1,0,logging|logstash|logstash-grok|datadog,387,10.3508
261494,0,Configuration,63454209,Unable to pass the env variable to container,"<pre><code>    FROM mcr.microsoft.com/dotnet/core/aspnet:3.1-alpine

ENV CORECLR_ENABLE_PROFILING=1 \
    CORECLR_PROFILER={846F5F1C-F9AE-4B07-969E-05C26BC060D8} \
    CORECLR_PROFILER_PATH=/opt/datadog/Datadog.Trace.ClrProfiler.Native.so \
    DD_INTEGRATIONS=/opt/datadog/integrations.json \
    DD_DOTNET_TRACER_HOME=/opt/datadog

WORKDIR /app

RUN apk --no-cache update \
    &amp;&amp; apk add bash make curl

ARG TRACER_VERSION=1.19.1

RUN mkdir -p /opt/datadog
RUN curl -L https://github.com/DataDog/dd-trace-dotnet/releases/download/v${TRACER_VERSION}/datadog-dotnet-apm-${TRACER_VERSION}.tar.gz \
    |  tar xzf - -C /opt/datadog
WORKDIR /app
COPY --from=buildcontainer /app/build .

COPY ./Entrypoint.sh /
RUN chmod +x /Entrypoint.sh &amp;&amp; /Entrypoint.sh

ENTRYPOINT [&quot;dotnet&quot;,&quot;testdatadog.dll&quot;]
</code></pre>
<p>Entrypoint.sh</p>
<pre><code>#!/bin/bash
set -e
curl http://169.254.169.254/latest/meta-data/local-ipv4 &gt; temp_var
export DD_AGENT_HOST=$(cat temp_var)
exec &quot;$@&quot;
</code></pre>
<p>When I ssh in to my ec2 and see for environment variables I don't see the DD_AGENT_HOST set. When I am manually trying to set the env it works. Am I missing something? appreciate the inputs.</p>",63460896,2,0,,2020-8-17 15:43:11,,2020-8-18 15:28:07,,,,,5577850,,1,0,docker|shell|amazon-ecs|datadog,384,10.3373
261495,1,Method,66172172,How to send Confluent cloud metrics to Datadog?,"<p>I want to get Confluent cloud metrics into Datadog so I followed the this <a href=""https://medium.com/ni-tech-talk/monitoring-confluent-cloud-kafka-with-datadog-natural-intelligence-e0fed5df535"" rel=""nofollow noreferrer"">instruction</a>. Instead of using CCLOUD_USER: ${CCLOUD_USER} and CCLOUD_PASSWORD: ${CCLOUD_PASSWORD} I used CCLOUD_API_KEY and CCLOUD_API_SECRET as environment variables for the exporter container.</p>
<p>I get a Failed to establish a new connection: [Errno 111] Connection refused error:</p>
<pre><code>2021-02-12 12:29:08 UTC | CORE | ERROR | (pkg/collector/runner/runner.go:292 in work) | Error running check openmetrics: [{&quot;message&quot;: &quot;HTTPConnectionPool(host='localhost', port=2112): Max retries exceeded with url: /metrics (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f227f294b80&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))&quot;, &quot;traceback&quot;: &quot;Traceback (most recent call last):\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/urllib3/connection.py\&quot;, line 159, in _new_conn\n    conn = connection.create_connection(\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/urllib3/util/connection.py\&quot;, line 84, in create_connection\n    raise err\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/urllib3/util/connection.py\&quot;, line 74, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/urllib3/connectionpool.py\&quot;, line 670, in urlopen\n    httplib_response = self._make_request(\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/urllib3/connectionpool.py\&quot;, line 392, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/http/client.py\&quot;, line 1255, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/http/client.py\&quot;, line 1301, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/http/client.py\&quot;, line 1250, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/http/client.py\&quot;, line 1010, in _send_output\n    self.send(msg)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/http/client.py\&quot;, line 950, in send\n    self.connect()\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/urllib3/connection.py\&quot;, line 187, in connect\n    conn = self._new_conn()\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/urllib3/connection.py\&quot;, line 171, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x7f227f294b80&gt;: Failed to establish a new connection: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/requests/adapters.py\&quot;, line 439, in send\n    resp = conn.urlopen(\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/urllib3/connectionpool.py\&quot;, line 726, in urlopen\n    retries = retries.increment(\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/urllib3/util/retry.py\&quot;, line 446, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=2112): Max retries exceeded with url: /metrics (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f227f294b80&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/base/checks/base.py\&quot;, line 876, in run\n    self.check(instance)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/base/checks/openmetrics/base_check.py\&quot;, line 112, in check\n    self.process(scraper_config)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/base/checks/openmetrics/mixins.py\&quot;, line 521, in process\n    for metric in self.scrape_metrics(scraper_config):\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/base/checks/openmetrics/mixins.py\&quot;, line 458, in scrape_metrics\n    response = self.poll(scraper_config)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/base/checks/openmetrics/mixins.py\&quot;, line 764, in poll\n    response = self.send_request(endpoint, scraper_config, headers)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/base/checks/openmetrics/mixins.py\&quot;, line 790, in send_request\n    return http_handler.get(endpoint, stream=True, **kwargs)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/base/utils/http.py\&quot;, line 298, in get\n    return self._request('get', url, options)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/base/utils/http.py\&quot;, line 363, in _request\n    response = request_method(url, **new_options)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/requests/api.py\&quot;, line 75, in get\n    return request('get', url, params=params, **kwargs)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/requests/api.py\&quot;, line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/requests/sessions.py\&quot;, line 533, in request\n    resp = self.send(prep, **send_kwargs)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/requests/sessions.py\&quot;, line 646, in send\n    r = adapter.send(request, **kwargs)\n  File \&quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/requests/adapters.py\&quot;, line 516, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=2112): Max retries exceeded with url: /metrics (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f227f294b80&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n&quot;}]
</code></pre>
<p>When I tried to curl http://ccloudexporter_ccloud_exporter_1:2112/metrics I got no reply but I did with a curl to http://localhost:2112/metrics. So I adjusted the openmetrics.yml to use prometheus url http://localhost:2112/metrics.
Still same error in the DD container. When I go to http://localhost:2112/metrics in my browser I see metrics.</p>
<p>No clue on why DD cannot connect to /metrics.</p>",,1,0,,2021-2-12 12:46:36,,2021-12-1 22:37:52,,,,,7779815,,1,1,prometheus|datadog|confluent-cloud,340,10.3259
261496,1,Error,67383640,java.lang.IllegalStateException: InjectionManagerFactory not found,"<p>I am stuck at this issue where I am using a particular <em><strong>Datadog HTTP API</strong></em> ( <code>&quot;com.datadoghq&quot; % &quot;datadog-api-client&quot; % &quot;1.0.0-beta10&quot; % &quot;compile&quot;</code> ) to send some custom stats to Datadog Agent from within my application. This client depends on <code>jersey</code> dependencies.</p>
<p>I have a few end to end test cases where the entire functionality / flow is working and I am able to send stats successfully.</p>
<p>The issue arises when when I start using the (fat) jar to create a Lambda Function that triggers with any incoming SNS event. I get the below error and I have tried many combinations of dependencies reading some earlier thread, but nothing seems to be working.</p>
<p>Could you please suggest something? Could this be due to the way how Lambda is invoked? As when I run in local, everything is fine. Although I don't think this matters, but the application is written in <code>Scala 2.12</code>.</p>
<p>I have tried the below threads:</p>
<ol>
<li><a href=""https://stackoverflow.com/questions/44088493/jersey-stopped-working-with-injectionmanagerfactory-not-found"">Jersey stopped working with InjectionManagerFactory not found</a></li>
<li><a href=""https://stackoverflow.com/questions/44283802/http-status-500-servlet-init/44536542#44536542"">HTTP Status 500 - Servlet.init()</a></li>
<li><a href=""https://github.com/igniterealtime/REST-API-Client/issues/29"" rel=""nofollow noreferrer"">https://github.com/igniterealtime/REST-API-Client/issues/29</a></li>
</ol>
<pre><code>2021-05-03T23:58:12.412-04:00   java.lang.IllegalStateException: InjectionManagerFactory not found.
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.internal.inject.Injections.lambda$lookupInjectionManagerFactory$0(Injections.java:98)
2021-05-03T23:58:12.412-04:00   at java.util.Optional.orElseThrow(Optional.java:290)
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.internal.inject.Injections.lookupInjectionManagerFactory(Injections.java:98)
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.internal.inject.Injections.createInjectionManager(Injections.java:68)
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.client.ClientConfig$State.initRuntime(ClientConfig.java:432)
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.internal.util.collection.Values$LazyValueImpl.get(Values.java:341)
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.client.ClientConfig.getRuntime(ClientConfig.java:826)
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.client.ClientRequest.getConfiguration(ClientRequest.java:285)
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.client.JerseyInvocation.validateHttpMethodAndEntity(JerseyInvocation.java:143)
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.client.JerseyInvocation.&lt;init&gt;(JerseyInvocation.java:112)
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.client.JerseyInvocation.&lt;init&gt;(JerseyInvocation.java:108)
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.client.JerseyInvocation.&lt;init&gt;(JerseyInvocation.java:99)
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.client.JerseyInvocation$Builder.method(JerseyInvocation.java:445)
2021-05-03T23:58:12.412-04:00   at org.glassfish.jersey.client.JerseyInvocation$Builder.post(JerseyInvocation.java:351)
2021-05-03T23:58:12.412-04:00   at com.datadog.api.v1.client.ApiClient.sendRequest(ApiClient.java:1476)
2021-05-03T23:58:12.412-04:00   at com.datadog.api.v1.client.ApiClient.invokeAPI(ApiClient.java:1435)
2021-05-03T23:58:12.412-04:00   at com.datadog.api.v1.client.api.MetricsApi.submitMetricsWithHttpInfo(MetricsApi.java:533)
2021-05-03T23:58:12.412-04:00   at com.datadog.api.v1.client.api.MetricsApi.submitMetrics(MetricsApi.java:476)
2021-05-03T23:58:12.412-04:00   at com.organization.raw.beacon.metric.statsD.DataDogHttpMetricsApi.$anonfun$send$1(DataDogHttpMetricsApi.scala:115)
2021-05-03T23:58:12.412-04:00   at scala.util.Try$.apply(Try.scala:213)
2021-05-03T23:58:12.412-04:00   at com.organization.raw.beacon.metric.statsD.DataDogHttpMetricsApi.send(DataDogHttpMetricsApi.scala:115)
2021-05-03T23:58:12.412-04:00   at com.organization.raw.beacon.metric.application.EventHandler.pushStats(EventHandler.scala:102)
2021-05-03T23:58:12.412-04:00   at com.organization.raw.beacon.metric.application.SnsEventHandler.handle(SnsEventHandler.scala:67)
2021-05-03T23:58:12.412-04:00   at com.organization.raw.beacon.metric.SnsLambdaHandler.handleRequest(SnsLambdaHandler.scala:27)
2021-05-03T23:58:12.412-04:00   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-03T23:58:12.412-04:00   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-03T23:58:12.412-04:00   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-03T23:58:12.412-04:00   at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-03T23:58:12.412-04:00   at lambdainternal.EventHandlerLoader$PojoMethodRequestHandler.handleRequest(EventHandlerLoader.java:263)
2021-05-03T23:58:12.412-04:00   at lambdainternal.EventHandlerLoader$PojoHandlerAsStreamHandler.handleRequest(EventHandlerLoader.java:180)
2021-05-03T23:58:12.412-04:00   at lambdainternal.EventHandlerLoader$2.call(EventHandlerLoader.java:902)
2021-05-03T23:58:12.412-04:00   at lambdainternal.AWSLambda.startRuntime(AWSLambda.java:340)
2021-05-03T23:58:12.412-04:00   at lambdainternal.AWSLambda.&lt;clinit&gt;(AWSLambda.java:63)
2021-05-03T23:58:12.412-04:00   at java.lang.Class.forName0(Native Method)
2021-05-03T23:58:12.412-04:00   at java.lang.Class.forName(Class.java:348)
2021-05-03T23:58:12.412-04:00   at lambdainternal.LambdaRTEntry.main(LambdaRTEntry.java:150)
</code></pre>
<p>Invoking Handler as below:</p>
<pre class=""lang-java prettyprint-override""><code>class SnsLambdaHandler extends RequestHandler[SNSEvent, Unit] with LazyLogging {

  // base configuration
  lazy val baseConfig: Config = getRootConfig.getConfig(this.getClass.getPackage.getName)

  /**
   * Handles SNS notification triggered by CreateObject Request in S3
   *
   * @param snsEvent
   * @param context
   */
  override def handleRequest(snsEvent: SNSEvent, context: Context): Unit = {

    logger.info(&quot;Request handler invoking for {} messages from the Sns Topic.&quot;, snsEvent.getRecords.size())
    val sqsMessageHandler = SnsEventHandler.apply(baseConfig, snsEvent)

    // handle the event
    sqsMessageHandler.handle()
  }
}

</code></pre>",,0,1,,2021-5-4 11:11:20,,2021-5-4 14:01:59,2021-5-4 14:01:59,,6806763,,6806763,,1,0,java|aws-lambda|jersey|illegalstateexception|datadog,381,10.3237
261497,1,Parse,60465436,"NGINX, Extracting url path from http header","<p>So I'm sending my Nginx access logs to Datadog (an APM solution). </p>

<p>My log format looks like this</p>

<pre><code>  log_format json_custom 
    '{'
      '""http.version"":""$request"",'
      '""http.status_code"":$status,'
      '""http.method"":""$request_method"",'
      '""http.referer"":""$http_referer"",'
      '""http.useragent"":""$http_user_agent"",'
      '""time_local"":""$time_local"",'
      '""remote_addr"":""$remote_addr"",'
      '""remote_user"":""$remote_user"",'
      '""body_bytes_sent"":""$body_bytes_sent"",'
      '""request_time"":$request_time,'
      '""response_content_type"":""$sent_http_content_type"",'
      '""X-Forwarded-For"":""$proxy_add_x_forwarded_for"",'
      '""custom_key"":""custom_value""'
    '}';
</code></pre>

<p>I can extract the url from the <code>referrer</code> field and it looks like this</p>

<pre><code>http://example.com/foo/bar 
</code></pre>

<p>I only want <code>/foo/bar</code> though. Is this something I have to modify in the <code>log_format</code>?</p>

<p>I saw an example from datadog docs where they're able to extract a url path attribute, but there's no example config.</p>

<p><a href=""https://i.stack.imgur.com/1XX2S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1XX2S.png"" alt=""enter image description here""></a></p>",60493178,2,1,,2020-2-29 13:04:00,,2020-3-2 16:42:32,,,,,3709060,,1,0,nginx|webserver|devops|nginx-config|datadog,214,10.3217
261498,1,Method,63603260,Datadog distinct-like custom metrics,"<p>Given following scenario:</p>
<ul>
<li>A lambda receives an event via SQS</li>
<li>The lambda receives a uuid pointing to an entity.</li>
<li>The lambda may fail with an error</li>
<li>SQS will retrial that particular entity several times</li>
<li>The lambda will be called with different entities thousand of times</li>
</ul>
<p>Right now we monitor a custom error-count metric like <code>myService.errorType</code>.
Which gives us an exact number of how many times an error occurred - independent from a specific entity: If an entity can't be processed like 100 times, then the metric value will be <code>100</code>.</p>
<p>What I'd like to have, though, is a distinct metric based on the UUID.
Example:</p>
<ul>
<li>entity with id 123 fails 10 times</li>
<li>entity with id 456 succeeds</li>
<li>entity with id 789 fails 20 times</li>
</ul>
<p>Then I'd like to have a metric with the value of <code>2</code> - because the processes failed for two entities only (and not for 30, as it would be reported right now).</p>
<p>While searching for a solution I found the possibility of using tags. But <a href=""https://docs.datadoghq.com/getting_started/tagging/"" rel=""nofollow noreferrer"">as the docs point</a> out they are not meant for such a use-case:</p>
<blockquote>
<p>Tags shouldn’t originate from unbounded sources, such as epoch timestamps, user IDs, or request IDs. Doing so may infinitely increase the number of metrics for your organization and impact your billing.</p>
</blockquote>
<p>So are there any other possibilities to achieve my goals?</p>",63668170,2,0,,2020-8-26 17:58:55,,2020-9-1 06:21:18,,,,,842302,,1,0,metrics|datadog,377,10.3054
261499,2,Query,66943560,Break down Datadog COUNT/GAUGE without double-counting,"<p>I have a script that queries our CI (Buildkite)'s API once per minute to fetch details of all build agents and emit metrics to Datadog for analysis. Getting an accurate <em>count</em> of these agents in the Datadog UI has proven challenging, however.</p>
<p>If the script emits a COUNT metric for each agent it sees, then agents will be double-counted in the Datadog UI when the interval is longer than a minute, because the script runs once per minute and sees (mostly) the same agents each time. The script could total up the number of agents it sees each run and emit that as a GAUGE, but then I lose the ability to break down the count in the Datadog UI by agent-specific tags (queue, etc).</p>
<p>I suppose I could emit a GAUGE with a value of 1 for each agent on each run, and add an artificial <code>index</code> tag with a value of the numeric index in the agent array, and rely on the Datadog UI to do the summation across <code>index</code> values? I could use the agent ID/host, of course, but Datadog charges by number of tag values and we've got our agents in an auto-scaling group, so hosts change frequently.</p>
<p>This seems hacky - is there a better solution? Am I overthinking this?</p>",,1,0,,2021-4-4 16:40:16,,2021-4-5 15:20:48,,,,,783547,,1,0,datadog,212,10.3053
261500,0,Configuration,59282255,Containers - Failed to get host IPs,"<p>I trying to run 3 containers and get the error for all of them.
I found the source code, but can't find the answer to why this is happening.
Also, tried to look at the internet and can't find anyone with the same error.</p>

<p>Source code:
<a href=""https://github.com/DataDog/datadog-agent/blob/eb35254e9e13165b4148fc9280ef79e2d6bf8235/pkg/util/docker/containers.go"" rel=""nofollow noreferrer"">https://github.com/DataDog/datadog-agent/blob/eb35254e9e13165b4148fc9280ef79e2d6bf8235/pkg/util/docker/containers.go</a></p>

<p>The error from /var/log/syslog:</p>

<blockquote>
  <p>process-agent[30759]: 2019-12-11 08:58:17 UTC | PROCESS | ERROR |
  (pkg/util/docker/containers.go:110 in ListContainers) | Failed to get
  host IPs. Container XXXXX will be missing network info: %!s()</p>
</blockquote>

<p>The containers running on the Host network.</p>

<p>Thanks</p>",,0,6,,2019-12-11 09:13:32,2,2020-1-5 09:51:14,,,,,3994390,,1,3,docker|networking|containers|datadog,369,10.2681
261501,2,Query,67201672,How to query Datadog to return the values of an attribute instead of the overall count of events,"<p>There are logs with &quot;events&quot; that have attributes such as names, statuses, etc as well as &quot;amount&quot; which corresponds to a dollar amount.  I'm sending requests to datadog's timeseries api to get back data about these logs.</p>
<p>I'm starting with a base query:
<code>sum:events{event_name:reload,event_result:success,event_environment:main,event_market:gb}.as_count()</code></p>
<p>This query just returns the total number of these &quot;reload&quot; events that have occurred in the timeframe.  Each of these events has an attribute &quot;amount&quot; which I want returned instead.  I can't figure out how to format the syntax for the query to get it to return the sum of these amounts instead of the sum of the occurrences of events.</p>
<p>Here are some queries I have tried which do not work:</p>
<pre><code>sum:events{event_name:reload,event_result:success,event_environment:main,event_market:gb}.amount.as_count()
sum:events{event_name:reload,event_result:success,event_environment:main,event_market:gb}.amount
sum:events.amount{event_name:reload,event_result:success,event_environment:main,event_market:gb}.as_count()
sum:events{event_name:reload,event_result:success,event_environment:main,event_market:gb} by {amount}.as_count()
</code></pre>
<p>I've had difficulty finding anything about this in the datadog documentation.  If anyone understands how to perform this with datadog's query syntax I would very much appreciate the help.</p>
<p>Thanks</p>",,1,0,,2021-4-21 18:37:06,,2021-4-23 16:14:15,,,,,11501494,,1,1,datadog,326,10.2529
261502,1,Error,69651350,Custom OpenMetrics Not Being Propagated to DataDog,"<p>I am using the <code>prometheus-fastapi-instrumentator</code> package to expose my custom metrics but they don't seem to be picked up by DataDog.</p>
<p>I'm experiencing a lot of trouble getting DataDog to scrape my <code>Counter</code> metrics. Additionally, <code>Histogram</code> buckets don't seem to be going through as distribution metrics.</p>
<p>Does anyone have any clue as to what the issue could be?</p>
<p>Here is my monitoring.py file: <a href=""https://github.com/rileyhun/fastapi-ml-example/blob/main/app/core/monitoring.py"" rel=""nofollow noreferrer"">https://github.com/rileyhun/fastapi-ml-example/blob/main/app/core/monitoring.py</a></p>
<p>Reproducible Example:</p>
<pre><code>git clone https://github.com/rileyhun/fastapi-ml-example.git

docker build -t ${IMAGE_NAME}:${IMAGE_TAG} -f Dockerfile .
docker tag ${IMAGE_NAME}:${IMAGE_TAG} rhun/${IMAGE_NAME}:${IMAGE_TAG}
docker push rhun/${IMAGE_NAME}:${IMAGE_TAG}

minikube start --driver=docker --memory 4g --nodes 2
kubectl create namespace monitoring
helm install prometheus-stack prometheus-community/kube-prometheus-stack -n monitoring

kubectl apply -f deployment/wine-model-local.yaml
kubectl port-forward svc/wine-model-service 8080:80

python api_call.py
</code></pre>",69691332,1,0,,2021-10-20 18:35:57,,2021-10-23 19:24:13,2021-10-21 21:52:39,,5378132,,5378132,,1,2,python|prometheus|metrics|fastapi|datadog,91,10.2362
261503,3,Monitoring,58765810,Datadog event monitor aggregation,"<p>I have created a Multi alert event monitor</p>

<pre><code>events('sources:rds event_source:db-instance').by('dbinstanceidentifier').rollup('count').last('1d') &gt;= 1
</code></pre>

<p>I wanted it to be aggregated by ""dbinstanceidentifier"" but it shows the accumulative count for ""* (Entire Infrastructure)"". Basically it doesn't see any groups. But I can see them in the infrastructure. Is it a problem of datadog? May be it's only available in a kind of ""premium"" subscription?</p>",58771357,1,0,,2019-11-8 11:35:54,,2019-11-8 17:36:03,,,,,4867627,,1,0,events|datadog,202,10.2214
261504,1,Method,64929616,Can DataDog dd-trace-js send trace info to the server via http headers?,"<p>I want to trace a request path that has started in the Web application React JS (frontend), then passed to the backend and returned as a response.</p>
<p>Can <a href=""http://%20https://github.com/DataDog/dd-trace-js/"" rel=""nofollow noreferrer"">dd-trace-js</a> start the span and pass it to the server over HTTP HEADERS?</p>",64973799,1,0,,2020-11-20 12:37:15,,2020-11-23 18:08:08,,,,,469898,,1,1,reactjs|trace|datadog,305,10.1372
261505,1,Method,56116856,How to get route data from Identity Server 4 endpoints,"<p>I have a ResponseTimeMiddleware.cs responsible for getting response time metrics (I am using datadog) for every request made. Which is tagged by controller and action names. However when we hit the ""connect/token"" endpoint, the context.GetRouteData() is null, probably because identity server is doing it behind the scenes. Is there a way I could get this information or some other unique information where I could tag with?</p>

<p>here's my code:</p>

<pre><code>public class ResponseTimeMiddleware
{

    // other code..

    public Task InvokeAsync(HttpContext context)
    {
        var request = context.Request;
        var watch = new System.Diagnostics.Stopwatch();
        watch.Start();
        context.Response.OnStarting(() =&gt;
        {
            watch.Stop();

            var routeData = context.GetRouteData();
            var responseTime = watch.ElapsedMilliseconds.ToString();
            var tags = new[] { $""statusCode:{context.Response.StatusCode.ToString()}"", $""controller:{routeData.Values[""controller""]}"", $""action:{routeData.Values[""action""]}"" };

            context.Response.Headers[ResponseHeaderResponseTime] = responseTime;

            DogStatsd.Timer(""response.time"", responseTime, tags: tags);

            return Task.CompletedTask;
        });

        return nextDelegate(context);
    }
}
</code></pre>

<p>This is my Startup:</p>

<pre><code>public class Startup
{

    // other code..

    public static void Configure(IApplicationBuilder app, IHostingEnvironment env)
    {
        if (env.IsDevelopment())
        {
            app.UseDeveloperExceptionPage();
        }
        else
        {
            app.UseExceptionHandler(""/Error"");
            app.UseHsts();
        }

        app.UseMiddleware&lt;ResponseTimeMiddleware&gt;();
        app.UseMvcWithDefaultRoute();
        app.UseStaticFiles();
        app.UseEndpointRouting();
        app.UseCookiePolicy();
        app.UseCors(""CorsPolicy"");
        app.UseIdentityServer();

    // This method gets called by the runtime. Use this method to add services to the container.
    public async void ConfigureServices(IServiceCollection services)
    {
        services.AddDataDogStatsd(Configuration, ""identity"");

        // other code
    }
}
</code></pre>",,1,2,,2019-5-13 17:02:43,,2019-5-13 18:01:30,,,,,3223737,,1,1,asp.net-core|identityserver4|datadog,171,10.132
261506,3,Monitoring,53382882,How can I alert that a particular process is crashing in Datadog?,"<p>I'm trying to figure out how to create an alert around a process that may be crashing and restarting repeatedly. It might be providing some data to Datadog while it's up, so a ""no data"" alert won't do because the lack of data never hits the duration threshold as the process restarts. I was thinking of alerting on a changing PID, but I cannot for the life of me figure out how to create a PID-based Monitor. Is it possible? And how? Does anyone have any other suggestions for this situation?</p>",,1,0,,2018-11-19 21:27:17,,2018-11-19 21:55:25,,,,,3753348,,1,0,datadog,190,10.115
261507,1,Method,55621848,How to aggregate logs from several Jenkins Jobs\Pipelines in one place?,"<p>Our project is responsible for migrating data from one system to another. We are going to run transformation, validation and migration scripts using Jenkins.</p>

<p>It's unclear for me how to aggregate logs from several Jobs or Pipelines in Jenkins. How it can be done?</p>

<p>We'll rely on logs heavily to identify any issues found during validation etc.</p>

<p>In terms of our planned setup we'll have AWS EC2 instances + we can use Datadog (our company uses it). Can we use Datadog for this purpose?</p>",55661352,1,1,,2019-4-10 21:54:06,,2019-4-13 01:58:31,,,,,3345737,,1,0,jenkins|logging|error-logging|datadog,189,10.1058
261508,3,Monitoring,64476688,Is it possible to take action on an application using datadog or Prometheus?,<p>I want to take some action on the app server based on the server utilisation. The monitoring on the server is done by datadog. So is it possible to take an action on the server using datadog ?</p>,64484665,2,0,,2020-10-22 06:44:42,,2021-5-3 18:09:16,,,,,3749845,,1,0,prometheus|datadog|server-monitoring,336,10.1054
261509,3,Monitoring,58502004,Datadog alert when Amazon RDS is created,"<p>I have an alert in Datadog when CPU Credits are low. The problem is when I create a new RDS in Amazon, initially it has 0 CPU credits and I receive this alert.</p>

<p>How can I avoid this case? I tried to find ""time since creation"" metric, but with no success.</p>",58547485,1,0,,2019-10-22 10:16:11,,2019-11-4 03:49:31,2019-11-4 03:49:31,,174777,,4867627,,1,0,postgresql|amazon-web-services|datadog,187,10.0874
261510,1,Error,68919712,Using the Datadog logs and RUM SDKs sends multiple requests when errors are being thrown,"<p>I have the following issue:</p>
<p><strong>Trigger</strong></p>
<p>An uncaught exception is being thrown and datadog logs SDK sends request to log the incident.</p>
<p><strong>Expected outcome</strong></p>
<p>One request is sent/incident and the incident is logged only once in Datadog UI.</p>
<p><strong>Actual outcome</strong></p>
<p>Datadog logs SDK sends many requests/incident (between 1k-2.5k) and the incident is logged many times in Datadog UI.</p>
<p><strong>Additional Information</strong></p>
<p>When disabling the Datadog RUM SDK, then the Datadog logs SDK behaves as expected. However, I want to run them both, so this is not an option at the moment.</p>
<p>I am using version <code>3.1.3</code> for both <code>@datadog/browser-logs</code> and <code>@datadog/browser-rum</code> packages.</p>
<p>Here's a screenshot to illustrate the issue:</p>
<p><a href=""https://i.stack.imgur.com/8xX8L.png"" rel=""nofollow noreferrer"">Many requests being sent for one uncaught exception example</a></p>
<p>This is the code I am using to initialise both logs and RUM SDKs:</p>
<pre><code>    import { datadogLogs } from '@datadog/browser-logs';
    import { datadogRum } from '@datadog/browser-rum';

    if (process.env.NODE_ENV === 'production' &amp;&amp; process.env.DATADOG_CLIENT_TOKEN) {
            const environment = getEnvironment();
            const config = {
                site: 'datadoghq.eu',
                clientToken: process.env.DATADOG_CLIENT_TOKEN,
                service: typeof DATADOG_SERVICE !== 'undefined' ? DATADOG_SERVICE : undefined,
                env: environment ? `${environment}` : undefined,
                proxyHost: process.env.PROXY_HOST
            };
        
            datadogLogs.init(config);
        
            
                if (process.env.DATADOG_APPLICATION_ID) {
                    datadogRum.init({
                        ...config,
                        trackInteractions: true,
                        applicationId: process.env.DATADOG_APPLICATION_ID
                    });
        
                    datadogRum.setUser({
                        name: service.getName(),
                        email: service.getEmail()
                    });
                }
    }
</code></pre>",69881973,1,0,,2021-8-25 08:49:22,,2021-11-8 10:26:26,2021-8-27 20:55:18,,7143585,,7143585,,1,0,javascript|reactjs|logging|datadog|rum,186,10.0781
261511,1,Method,66316616,Remove particular type of error from Datadog,"<p>Currently, I see error status for all the authentication errors and it feels like a lot of extra noise in the total errors chart. I looked at <code>https://github.com/DataDog/dd-trace-js/pull/909</code> and tried to use the custom execute provided for graphql</p>
<pre><code>import ddTrace from 'dd-trace'
let tracer = ddTrace.init({
  debug: false
}) // initialized in a different file to avoid hoisting.

tracer.use('graphql', {
  hooks: {
    execute: (span, args, res) =&gt; {
        if (res &amp;&amp; res.errors &amp;&amp; res.errors[0] &amp;&amp; res.errors[0].status !== 403) {
            span?.setTag('error', res.errors)
        }
    }
  }
})
export default tracer
</code></pre>
<p>But still, res with only 403 error is going into error status. Please help me with how can I achieve this.</p>",,1,0,,2021-2-22 13:28:18,,2021-2-24 22:49:28,,,,,1958324,,1,0,graphql|datadog,329,10.0688
261512,3,Monitoring,69217202,Micro meter high memory usage,"<p>We are seeing some of our services (Java &lt;&gt; SpringBoot) getting OOM. On checking heap dump, we found the micrometer library is taking 113MB (around 54% of total heap memory).</p>
<pre><code>io.micrometer.statsd.internal.LogbackMetricsSuppressingUnicastProcessor
Jar: io.micrometer:micrometer-core
</code></pre>
<p>I did some research online and found creating a lot of distinct tags can lead to this issue. However, this is not the case with our services. We are pushing a lot of metrics to data dog but reusing tags all the time.</p>",,1,1,,2021-9-17 02:51:59,,2021-9-20 19:41:43,,,,,1619503,,1,1,out-of-memory|spring-boot-actuator|datadog|micrometer|spring-micrometer,89,9.99756
261513,1,Method,66205822,Airflow Datadog hook conn_id format,"<p>I want to use a <a href=""https://airflow.apache.org/docs/apache-airflow/1.10.8/_api/airflow/contrib/hooks/datadog_hook/index.html"" rel=""nofollow noreferrer"">DatadogHook</a> in one of my Airflow DAGs. As documentation says, I need to pass a <code>datadog_conn_id</code> upon hook initialization, and the parameter description says:</p>
<pre><code>datadog_conn_id – The connection to datadog, containing metadata for api keys.
datadog_conn_id – str
</code></pre>
<p>The problem is that I couldn't figure out what the format of <code>datadog_conn_id</code> should be. It's not exactly specified in the documentation. After digging in some of Airflow source code, I <a href=""https://github.com/apache/airflow/blob/master/airflow/providers/datadog/hooks/datadog.py#L42"" rel=""nofollow noreferrer"">found</a> out that it expects to find <code>api_key</code> and <code>app_key</code> from the given <code>datadog_conn_id</code>, but not sure how does it extract them from the given string, what's the expected format.</p>
<p>So, is there any well known format that is meant here that I don't know? I don't believe that documentation is just missing such an important information.</p>
<p><strong>Note</strong>: I do have both API and APP keys ready, just not sure about the format in which I should provide them to the hook.</p>",,1,0,,2021-2-15 09:49:32,,2021-2-15 12:24:21,,,,,8107694,,1,2,python|airflow|hook|datadog,141,9.99688
261514,1,Method,66900989,How to send Airflow Metrics to datadog,"<p>We have a requirement where we need to send airflow metrics to datadog. I tried to follow the steps mentioned here
<a href=""https://docs.datadoghq.com/integrations/airflow/?tab=host"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/airflow/?tab=host</a></p>
<p>Likewise, I included statsD in airflow installation and updated the airflow configuration file (Steps 1 and 2)</p>
<p>After this point, I am not able to figure out how to send my metrics to datadog. Do I follow the Host configurations or containarized configurations? For the Host configurations, we have to update the datadog.yaml file which is not in our repo and for containerized version, they have specified how to do it for Kubernetics but we don't use Kubernetics.</p>
<p>We are using airflow by creating a docker build and running it over on Amazon ECS. We also have a datadog agent running parallely in the same task (not part of our repo). However I am not able to figure out what configurations I need to make in order to send the StatsD metrics to datadog. Please let me know if anyone has any answer.</p>",,0,0,,2021-4-1 08:28:37,1,2021-4-1 08:28:37,,,,,14770558,,1,2,airflow|amazon-ecs|datadog|statsd,315,9.99324
261515,0,Configuration,64108531,How do I configure this PostgreSQL check for DataDog?,"<p>I try to setup a postgres check using DD Agent and i'm getting an error thrown by postgres.py script. As you can see in the screenshot, i'm using this simple query to  get the number of active connections to a db. I've put it inside the /etc/datadog-agent/conf.d/postgres.d/conf.yaml like this :</p>
<pre><code>- metric_prefix: postgresql
     query: SELECT datname as db_name, count(pid) as active_connections FROM pg_stat_activity where state = 'active' group by db_name;
     columns:
       - name: active_connections
         type: gauge
       - name: db_name
         type: tag
</code></pre>
<p>The error i get when i run a config check is the following :</p>
<pre><code>[root@my_box postgres.d]# datadog-agent check postgres | grep -i -A 20 -B 20  active_connections
Error: postgres:953578488181a512 | (postgres.py:398) | non-numeric value `cldtx` for metric column `active_connections` of metric_prefix `postgresql`
</code></pre>
<p>If i understood correctly the conf.yaml file is used to call the postgres.py script with certain parameters. The postgres.py script can be found here :
<a href=""https://github.com/DataDog/integrations-core/blob/master/postgres/datadog_checks/postgres/postgres.py"" rel=""nofollow noreferrer"">https://github.com/DataDog/integrations-core/blob/master/postgres/datadog_checks/postgres/postgres.py</a></p>
<p><img src=""https://i.stack.imgur.com/oIlGv.png"" alt=""Screenshot"" /></p>",64407940,1,2,,2020-9-28 19:35:35,,2020-10-17 22:24:48,2020-9-28 20:25:49,,494631,,8916618,,1,0,python|postgresql|datadog,315,9.99324
261516,0,Integration,67646017,AWS python lambda datadog integration,"<p>I have a lambda written in python and I want to submit some custom metrics from the lambda to datadog</p>
<p>I followed this documentation, (<a href=""https://www.datadoghq.com/blog/datadog-lambda-layer/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/datadog-lambda-layer/</a>) Added datadog dependencies as a lambda layer and added it to my lambda as a layer. When I test the lambda I receive the below error,</p>
<pre><code>Response
{
  &quot;errorMessage&quot;: &quot;Unable to import module 'lambda_function': No module named 'datadog_lambda'&quot;,
  &quot;errorType&quot;: &quot;Runtime.ImportModuleError&quot;,
  &quot;stackTrace&quot;: []
}

Function Logs
START RequestId: 63addc4a-389b-4526-865e-b44bc272f1ab Version: $LATEST
[ERROR] Runtime.ImportModuleError: Unable to import module 'lambda_function': No module named 'datadog_lambda'
</code></pre>
<p>Can anyone help me with figuring out the issue here.</p>",,1,0,,2021-5-22 04:06:03,,2021-5-23 12:32:42,,,,,4859626,,1,0,python|amazon-web-services|aws-lambda|datadog|aws-lambda-layers,310,9.96545
261517,2,Query,64671093,DataDog metric query to PromQL,"<p>I'm creating alerts in Prometheus and migrating from Datadog.</p>
<p>I have two metrics queries that I'm not able to understand yet.</p>
<pre><code>avg(last_1d):anomalies(avg:default.burrow_kafka_consumer_lag_total{*} by {consumer_group,env}, 'robust', 3, direction='above', alert_window='last_30m', interval=300, count_default_zero='true', seasonality='hourly') &gt;= 1
</code></pre>
<p>In this query I understand the <code>avg:default.burrow_kafka_consumer_lag_total{*} by {consumer_group,env}</code> part but not the rest and how to translate it to PromQL.</p>
<p>Second</p>
<pre><code>min(last_2h):derivative(avg:default.burrow_kafka_consumer_lag_total{!consumer_group:connect-analytics-mobile-json} by {env,consumer_group}.rollup(avg, 300)) &gt; 0
</code></pre>
<p>I don't understand the rollup part, how to translate it into PromQL?</p>
<pre><code>pct_change(avg(last_1h),last_1h):avg:default.burrow_kafka_consumer_lag_total{!consumer_group:vdv_trip_aggregator_app,!consumer_group:vdv_trip_normalizer_app-fast,!consumer_group:vdv_trip_normalizer_app-slow,!consumer_group:disruption-consumer-app} by {consumer_group,env} &gt; 300
</code></pre>
<p>how is <strong>pct_change(avg(last_1h),last_1h)</strong> part of the query?</p>
<p>I'm new to this. I have translated other queries but these I don't understand.</p>",,0,0,,2020-11-3 21:39:03,,2020-11-3 21:39:03,,,,,4323514,,1,1,prometheus|datadog|prometheus-alertmanager,309,9.95983
261518,3,Monitoring,67631633,Datadog Monitor via Terraform: Invalid operator,"<p>When I try to roll out my terraform code containing the following <code>datadog_monitor</code></p>
<pre><code>resource &quot;datadog_monitor&quot; &quot;test_alert&quot; {
  name               = &quot;Test alert&quot;
  type               = &quot;log alert&quot;
  message            = &quot;Test alert message @test@example.com&quot;

  query = &quot;logs('service:myservice Some log message').index('main').rollup('count').last('5m') &gt; 0&quot;

  enable_logs_sample  = true
  notify_no_data      = false
  include_tags        = true

  tags = [&quot;tag1:tag1value&quot;, &quot;tag2:tag2value&quot;]
}
</code></pre>
<p>this results in the following error message:</p>
<pre><code>Error: error validating monitor: 400 Bad Request: {&quot;errors&quot;: [&quot;The value provided for parameter 'query' is invalid: invalid operator specified: &quot;]}
</code></pre>
<p>According to the <a href=""https://docs.datadoghq.com/api/latest/monitors/#create-a-monitor"" rel=""nofollow noreferrer"">API description</a>, the query field for log alerts should have this syntax: <code>logs(query).index(index_name).rollup(rollup_method[, measure]).last(time_window) operator #</code>, and <code>&gt;</code> is listed as a valid operator.</p>
<p>Other questions in this field listed solutions that were largely unrelated to the error message, so I assume the issue is unrelated to the operator, but I haven't been able to figure out the issue so far.</p>",,1,1,,2021-5-21 06:12:18,,2021-5-21 10:26:18,,,,,2329895,,1,1,terraform|datadog,270,9.92546
261519,1,Error,64720852,Datadog Real User Monitoring breaks node server on IE11,"<p>Using the @datadog/browser-rum package from <a href=""https://github.com/DataDog/browser-sdk/tree/master/packages/rum"" rel=""nofollow noreferrer"">RUM Browser Monitoring</a>, with Node apollo web client fails on IE11.</p>
<p>Seeing multiple requests to <a href=""https://run-http-intake.logs.datadoghq.com"" rel=""nofollow noreferrer"">https://run-http-intake.logs.datadoghq.com</a> on IE11 that are different than Chrome. Unable to login to the application on IE11. Working theory is that datadogRUM is blocking other application requests on IE11. When datadogRUM is removed, the application is working correctly.</p>
<p>Chrome and IE11 requests:</p>
<blockquote>
<p>Request Method: POST  RequestURL: <a href=""https://rum-http-intake.logs.datadoghq.com/v1/input/pub%7Bid%7D?_dd.application_id=%7Bid%7D&amp;ddsource=browser&amp;&amp;ddtags=sdk_version:1.25.2.env:local&amp;batch_time=%7Btimestamp%7D"" rel=""nofollow noreferrer"">https://rum-http-intake.logs.datadoghq.com/v1/input/pub{id}?_dd.application_id={id}&amp;ddsource=browser&amp;&amp;ddtags=sdk_version:1.25.2.env:local&amp;batch_time={timestamp}</a></p>
</blockquote>
<p>Only IE11 request:</p>
<blockquote>
<p>Request Method: CONNECT RequestURL: <a href=""https://rum-http-intake.logs.datadoghq.com"" rel=""nofollow noreferrer"">https://rum-http-intake.logs.datadoghq.com</a> Proxy-Connection: Keep-Alive</p>
</blockquote>
<pre><code>import { datadogRum } from '@datadog/browser-rum' 

datadogRum.init({
  applicationId: '&lt;DATADOG_APPLICATION_ID&gt;',
  clientToken: '&lt;DATADOG_CLIENT_TOKEN&gt;',
  site: '&lt;DATADOG_SITE&gt;',
  //  service: 'my-web-application',
  //  env: 'production',
  //  version: '1.0.0',
  sampleRate: 100,
  trackInteractions: true,
})
</code></pre>
<p>Please let me know what additional information I should find to help with this issue. IE11 issues are no fun. Thank you!</p>",64825884,1,1,,2020-11-6 19:57:15,,2020-11-24 21:46:04,,,,,872145,,1,0,node.js|internet-explorer-11|datadog,301,9.91427
261520,2,Query,66435705,GORM Test Insert Query [ExpectedBegin => expecting database transaction Begin],"<p>I am testing the insert query using the DATA Dog library. Spend the whole day to figure out</p>
<p>Here is a model</p>
<pre><code>package main

type Students struct {
    Id   string `json:&quot;id&quot;`
    Name string `json:&quot;name&quot;`
}

func (s *Students) TableName() string {
    return &quot;students&quot;
}
</code></pre>
<p>Repository</p>
<pre><code>package main

import (
    &quot;database/sql&quot;
    &quot;github.com/jinzhu/gorm&quot;
    &quot;github.com/stretchr/testify/require&quot;
    &quot;github.com/stretchr/testify/suite&quot;
    &quot;gopkg.in/DATA-DOG/go-sqlmock.v1&quot;
    &quot;testing&quot;
)

type Suite struct {
    suite.Suite
    DB   *gorm.DB
    mock sqlmock.Sqlmock

    repository StudentRepository
    student     *Students
}

func (s *Suite) SetupSuite() {
    var (
        db  *sql.DB
        err error
    )

    db, s.mock, err = sqlmock.New()
    require.NoError(s.T(), err)

    s.DB, err = gorm.Open(&quot;mysql&quot;, db)
    require.NoError(s.T(), err)

    s.DB.LogMode(true)

    s.repository = NewStudentRepository(s.DB)
}



func (s *Suite) AfterTest(_, _ string) {
    require.NoError(s.T(), s.mock.ExpectationsWereMet())
}

func TestInit(t *testing.T) {
    suite.Run(t, new(Suite))
}

func (s *Suite) Test_repository_Create() {
    var (
        id   = &quot;1&quot;
        name = &quot;A&quot;
    )
    s.mock.ExpectBegin()
    s.mock.ExpectQuery(
        &quot;INSERT INTO `students` (`id`,`name`) VALUES (?,?)&quot;).
        WithArgs(id, name)
    s.mock.ExpectCommit()

    err := s.repository.Create(id, name)
    require.NoError(s.T(), err)
}
</code></pre>
<p>RepositoryTest</p>
<pre><code>package main

import (
    &quot;github.com/jinzhu/gorm&quot;
)

type StudentRepo struct {
    DB *gorm.DB
}

type StudentRepository interface {
    Create( id string ,name string) error
}

func NewStudentRepository(db *gorm.DB) *StudentRepo {
    return &amp;StudentRepo{
        DB: db,
    }
}

func (ctx *StudentRepo) Create(id string, name string) error {

    student:= &amp;Students{
        Id: id,
        Name: name,
    }

    return ctx.DB.Create(student).Error
}
</code></pre>
<p>When I tried to run the test getting the following error</p>
<pre><code> Error:          Received unexpected error:
                                there is a remaining expectation which was not matched: ExpectedBegin =&gt; expecting database transaction Begin
                Test:           TestInit/Test_repository_Create

</code></pre>",,0,0,,2021-3-2 08:21:04,1,2021-3-2 09:02:02,2021-3-2 09:02:02,,13860,,902126,,1,2,unit-testing|go|testing|go-gorm|datadog,298,9.89686
261521,1,Error,67372864,instrument datadog agent by the location of logs file with datadog ansible role,"<p>i try to enable logs collecting with datadog ansible role,</p>
<p>but i can't figure out why the logs are not reported to the Datadog ui</p>
<p>i found an example of a playbook in the github repo,</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>- hosts: servers
  roles:
    - { role: datadog.datadog, become: yes }
  vars:
    datadog_api_key: ""&lt;YOUR_DD_API_KEY&gt;""
    datadog_agent_version: ""7.16.0""
    datadog_config:
      tags:
        - ""&lt;KEY&gt;:&lt;VALUE&gt;""
        - ""&lt;KEY&gt;:&lt;VALUE&gt;""
      log_level: INFO
      apm_config:
        enabled: true
      logs_enabled: true  # available with Agent v6 and v7
    datadog_checks:
      process:
        init_config:
        instances:
          - name: ssh
            search_string: ['ssh', 'sshd' ]
          - name: syslog
            search_string: ['rsyslog' ]
            cpu_check_interval: 0.2
            exact_match: true
            ignore_denied_access: true
      ssh_check:
        init_config:
        instances:
          - host: localhost
            port: 22
            username: root
            password: &lt;YOUR_PASSWORD&gt;
            sftp_check: True
            private_key_file:
            add_missing_keys: True
      nginx:
        init_config:
        instances:
          - nginx_status_url: http://example.com/nginx_status/
            tags:
              - ""source:nginx""
              - ""instance:foo""
          - nginx_status_url: http://example2.com:1234/nginx_status/
            tags:
              - ""source:nginx""
              - ""&lt;KEY&gt;:&lt;VALUE&gt;""

        #Log collection is available on Agent 6 and 7
        logs:
          - type: file
            path: /var/log/access.log
            service: myapp
            source: nginx
            sourcecategory: http_web_access
          - type: file
            path: /var/log/error.log
            service: nginx
            source: nginx
            sourcecategory: http_web_access
    # datadog_integration is available on Agent 6.8+
    datadog_integration:
      datadog-elastic:
        action: install
        version: 1.11.0
      datadog-postgres:
        action: remove
    network_config:
      enabled: true</code></pre>
</div>
</div>
</p>
<p>but how can i do it for my case i have just a tomcat server logging to a file and i want datadog to take those logs file to my datadog account</p>
<p>thanks in advance</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code># my code
- name: install the agent on targets
          include_role:
            name: datadog.datadog
          vars: 
            datadog_api_key: ""myApiKey""
            datadog_site: ""datadoghq.com""
            datadog_config:
              log_level: INFO
              apm_config:
                enabled: true
              logs_enabled: true
              logs:
                - type: file
                  path: /home/ubuntu/web-app/tomcatlogs/logs/*.log
                  service: myapp
                  source: tomcat</code></pre>
</div>
</div>
</p>",67382827,1,0,,2021-5-3 16:58:12,0,2021-5-4 10:15:35,,,,,10735209,,1,0,java|ansible|datadog|ansible-role,167,9.89087
261522,3,Monitoring,61971889,How to monitor HTTP status codes with serverless datadog plugin,"<p>I am using <a href=""https://github.com/DataDog/serverless-plugin-datadog"" rel=""nofollow noreferrer"">serverless-plugin-datadog</a>, which uses <a href=""https://www.datadoghq.com/blog/datadog-lambda-layer/"" rel=""nofollow noreferrer"">datadog-lambda-layer</a> under the hood.</p>

<p>The <a href=""https://github.com/DataDog/serverless-plugin-datadog/blob/master/README.md#how-it-works"" rel=""nofollow noreferrer"">docs state</a>, that by using this plugin it is not necessary to wrap a handler anymore. This is, by the way, the main reason why I decided to go for it.</p>

<p>The lambda itself is a REST API, which responds with dedicated status codes.</p>

<p>My question now is, how can I monitor the number of <code>4xx</code> and <code>5xx</code> http status codes? Do I have to define custom metrics in datadog for this to work? I was under the assumption that the plugin comes with those data out-of-the-box, but it looks like I'm missing an important part here.</p>",62008387,1,0,,2020-5-23 12:17:53,,2020-5-25 18:21:06,,,,,842302,,1,0,aws-lambda|serverless-framework|datadog,294,9.87339
261523,3,Monitoring,64453730,Terraform/Datadog Alert Monitoring,"<p>I am trying to create an alert Datadog using Terraform for when multiple hosts (1 or more)  are at &gt;= 95% CPU usage. So far, with the code I have, the alert would trigger anytime a host exceeds the threshold and that is a little too noisy. Would you happen to know how to create the logic to satisfy both conditions before the alert gets triggered? (Alert when Multiple hosts at 95% CPU or higher)</p>
<pre><code>resource &quot;datadog_monitor&quot; &quot;worker_high_disk_usage&quot; {
    type    = &quot;metric alert&quot;
    name    = &quot;worker high disk usage&quot;
    message = &lt;&lt;-EOT
    {{#is_alert}} 
    @slack_channel {{system}} {{env}} host {{host.name}} device {{device}} has had disk usage 
    enter code hereover {{threshold}} of availible disk space for the last 30m
    {{/is_alert}} 
    {{#is_recovery}}
    @pagerduty
    {{system}} {{env}} host {{host.name}} device {{device}} high disk usage resolved.
    {{/is_recovery}}
    EOT
    query   = &quot;min(last_30m):avg:system.disk.in_use{env:prod,system:worker,team:team} by 
    {host,device} &gt; 0.95&quot;

    thresholds = {
    critical = 0.95

    timeout_h           = 1
  
    require_full_window = false
      lifecycle {
        ignore_changes = [silenced]
      }
      tags = [&quot;disk&quot;]
    }
</code></pre>",,1,0,,2020-10-20 21:42:34,,2020-11-19 07:38:42,,,,,10987929,,1,0,terraform|datadog,290,9.84959
261524,1,Parse,58933286,Can I extract Datadog metrics info via Datadog REST API?,"<p>Is there any way to extract the Tags info from DataDog via API for a specific metric?</p>

<p>I need the same info that the <a href=""https://docs.datadoghq.com/graphing/metrics/explorer/"" rel=""nofollow noreferrer"">Metrics Explorer</a> displays (list of hosts and tags), for only one metric.</p>

<p>I can retrieve the Tags filtered with a regular expression pattern:</p>

<pre class=""lang-py prettyprint-override""><code>def _load_metrics(from_time, filter_pattern: str):
    regex = re.compile(filter_pattern)
    all_metrics = api.Metric.list(from_time)['metrics']
    filtered = list(filter(regex.search, all_metrics))
    if not len(filtered):
        print('No metrics found with filter {}'.format(args.filter_pattern))
        return None
    print(""Found {} metrics matching filter '{}'"".format(len(filtered), args.filter_pattern))
    return filtered
</code></pre>

<p>And I can get all the Hosts from a Metric with:</p>

<pre class=""lang-py prettyprint-override""><code>def get_hosts_from_tag(tag_name: str):
    hosts = api.Hosts.search(q='hosts:', filter='tag={}'.format(tag_name))
    for host in hosts['host_list']:
        print(host['name'])
</code></pre>

<p>Using either the <a href=""https://datadogpy.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">datadogpy</a> or the <a href=""https://docs.datadoghq.com/api/?lang=python#tags"" rel=""nofollow noreferrer"">DataDog API</a>, <strong>how can I get all the Tags from a Metric?</strong></p>

<p>Thanks</p>",,0,0,,2019-11-19 11:40:33,2,2019-11-19 11:40:33,,,,,4033879,,1,2,python|tags|metrics|datadog,290,9.84959
261525,1,Error,69375403,Metrics sent via Datadog HTTP API does not get reflected in Metric Explorer,"<p>Referring to <a href=""https://docs.datadoghq.com/api/latest/metrics/#submit-metrics"" rel=""nofollow noreferrer"">Datadog's 'Submit metrics' API documentation</a>, I've tried sending metrics with following payload using Postman:</p>
<ul>
<li>API: <code>POST</code> <a href=""https://api.datadoghq.com/api/v1/series"" rel=""nofollow noreferrer"">https://api.datadoghq.com/api/v1/series</a></li>
<li>Request header: <code>DD-API-KEY</code> and <code>DD-APPLICATION-KEY</code> with valid values</li>
<li>Request body (based on <a href=""https://www.postman.com/datadog/workspace/datadog-s-public-workspace/request/7274195-8e07c48a-7769-4a51-bd64-b161c4e6dd58"" rel=""nofollow noreferrer"">Datadog sample</a>):</li>
</ul>
<pre><code>{
    &quot;series&quot;: [
        {
            &quot;metric&quot;: &quot;IvanPOCMetric&quot;,
            &quot;points&quot;: [
                [
                    &quot;${NOW}&quot;,
                    &quot;1234.5&quot;
                ]
            ]           
        }
    ]
}
</code></pre>
<p>The response was successful (<em>202 Accepted</em>) with following response body:</p>
<pre><code>{
    &quot;status&quot;: &quot;ok&quot;
}
</code></pre>
<p>However, when I try to search for my submitted metrics, via either <a href=""https://docs.datadoghq.com/api/latest/metrics/#search-metrics"" rel=""nofollow noreferrer"">'Search metrics' API</a> or <strong>Datadog Metric Explorer</strong>, I'm unable to find <code>IvanPOCMetric</code>.</p>
<p>Is there any issue with my request payload above? Or is there additional configuration needed to be done on Datadog portal to correctly 'register' the custom metric?</p>
<p>Thanks in advance.</p>",69385543,1,0,,2021-9-29 11:21:34,,2021-9-30 02:57:22,2021-9-29 11:33:29,,5413930,,5413930,,1,0,datadog,151,9.71591
261526,0,Integration,63326724,Can I prevent DataDog from closing OpsGenie incidents?,"<p>The question is about DataDog - OpsGenie integration. Whenever a DataDog monitor triggers an alert an incident is opened in OpsGenie (which is good), but when the monitor recovers back to a healthy state the OpsGenie incident is auto-closed (which is bad).</p>
<p>Is there any way to prevent this behavior? I want to keep incidents open until they are acked and resolved.</p>",63606465,1,0,,2020-8-9 13:09:42,1,2020-8-26 22:15:40,,,,,5414176,,1,0,datadog|opsgenie,268,9.71254
261527,0,Configuration,68298402,Datadog: enabling RUM allowedTracingOrigins raises CORS errors,"<p>I tried to connect <a href=""https://docs.datadoghq.com/real_user_monitoring/connect_rum_and_traces/?tab=browserrum"" rel=""nofollow noreferrer"">RUM with backend traces</a>. In react SPA application I setup a datadog-rum and enabled allowedTracingOrigins option for it, after that our fetch and xhr requests to API started to fail.</p>
<p>How to connect RUM and backend traces properly?</p>",,1,0,,2021-7-8 08:49:40,,2021-7-8 08:49:40,,,,,469898,,1,0,datadog|distributed-tracing|rum,266,9.69953
261528,0,Integration,58725596,Is it possible to migrate existing data (Like in Prometheus and elk )to datadog?,"<p>Is it possible to migrate existing data (Like in Prometheus and elk )to datadog? </p>

<p>There is a setup for live streaming of Prometheus metrics to Datadog by configuring datadog config. But what could be done with the past data?</p>",,1,0,,2019-11-6 08:08:41,,2019-11-8 03:46:31,2019-11-7 12:46:29,,8763847,,10831906,,1,1,datadog,131,9.66909
261529,3,Monitoring,62345348,DataDog events are auto-recovered,"<p>I created an event monitor that catches events with errors and notifies about the alert in a special messenger. Everything worked out for me, but I noticed that such alerts are auto-recovered on their own for some time.</p>

<p>As I understand it is because of this parameter:
<a href=""https://i.stack.imgur.com/SzQPt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SzQPt.png"" alt=""enter image description here""></a></p>

<p>So, datadog catches event, then sets event-monitor in alert status, then wait 5min-48hours and if there are no new events, it is auto-recovered and set status from ""Alert"" to ""OK"". It absolutely does not suit me. Can I somehow configure the monitor that the monitor's status does not change automatically from ""Alert"" to ""OK"" until I change it manually?</p>",,0,1,,2020-6-12 13:33:12,,2020-6-12 13:33:12,,,,,3286108,,1,1,datadog,260,9.65989
261530,0,Integration,52587443,How do I setup Datadog with Google App Engine node.js runtime?,"<p>According to the <a href=""https://github.com/DataDog/gae_datadog"" rel=""nofollow noreferrer""><code>gae_datadog</code> Github repo</a>, the way to setup datadog in app engine is to clone the repo and add the following in <code>app.yaml</code>:</p>

<pre><code>handlers:
# Should probably be at the beginning of the list
# so it's not clobbered by a catchall route
- url: /datadog
  script: gae_datadog.datadog.app

env_variables:
    DATADOG_API_KEY: 'YOURAPIKEY'
</code></pre>

<p>However, this doesn't appear to work with their nodejs runtime. Here is my <code>app.yaml</code>:</p>

<pre><code># Copyright 2017, Google, Inc.
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START app_yaml]
handlers:
    # Should probably be at the beginning of the list
    # so it's not clobbered by a catchall route
    - url: /datadog
      script: gae_datadog.datadog.app

env_variables:
    DATADOG_API_KEY: 'MY_KEY'

runtime: nodejs
env: flex
# [END app_yaml]
</code></pre>

<p>It seems like the datadog url handler isn't used at all, because it 404. I assume the node.js app takes precedence here, but I don't know how to change that.</p>",,0,2,,2018-10-1 08:45:48,,2018-10-1 08:45:48,,,,,1203633,,1,3,node.js|google-app-engine|datadog,254,9.61934
261531,1,Error,65042688,socket: too many open files Error for goroutines in indefinite loop,"<p>I have a requirement in my program to send metrics to datadog indefinitely (for continuous app monitoring in datadog). The program runs for a while and exits with the error &quot;dial udp 127.0.0.1:18125: socket: too many open files&quot;.</p>
<pre><code>    func sendData(name []string, channel chan []string) {
      c, err := statsd.New(&quot;127.0.0.1:18125&quot;)
      if err != nil {
        log.Fatal(err)
      }

      v := versionDetails()
      tag := &quot;tag:&quot; + v
      final_tag := []string{dd_tags}
      appEpochTimeList := epochTime()
      rate := float64(1)

      for i, app := range name {
        e := c.Gauge(app, float64(appEpochTimeList[i]), final_tag , rate)
        if e != nil {
            log.Println(e)
            channel &lt;- name
        }
        channel &lt;- name
        log.Printf(&quot;Metrics Sent !!&quot;)
      }
  }
</code></pre>
<p>The app names are read from a config.toml file</p>",65043190,1,0,,2020-11-27 19:12:03,,2020-11-28 05:37:00,2020-11-28 05:37:00,,9702194,,9702194,,1,-1,for-loop|go|goroutine|datadog,158,9.59463
261532,1,Method,60748902,"In Datadog, is there a JavaScript library that allows you to get existing metric data?","<p>For example, suppose I have a Node library that I could use something like:</p>

<pre><code>var datadog = require('some datadog library for JS');
var currentValue = datadog.getMetric('my.existing.metric');
console.log(currentValue);
//currentValue --&gt; 72

var currentMetrics = datadog.getAllMetrics();
console.log(currentMetrics);
// currentMetrics --&gt; ['my.existing.metric','my.existing.metric.1','my.existing.metric.2']

var currentMonitors = data.getAllMonitors();
console.log(currentMonitors);
// currentMonitors --&gt; [{name:'my.monitor.1', status:'GOOD'},{name:'my.monitor.1', status:'BAD'}]
</code></pre>

<p>The few I've looked at all seem to be good for posting new data to Datadog, however, are there any that can pull data from Datadog? Does anything like this exist? I like the <a href=""https://docs.datadoghq.com/api/"" rel=""nofollow noreferrer"">Datadog API</a> but it seems as though that is only in Curl, Python, and Ruby.</p>",,1,0,,2020-3-18 23:31:14,,2020-3-19 00:12:25,,,,,9933041,,1,0,javascript|node.js|datadog|telemetry,243,9.54243
261533,0,Configuration,68283023,How to connect Datadog java agent to host?,"<p>I set up datadog java agent with this command.</p>
<pre><code>java -javaagent:/path/to/dd-java-agent.jar -jar test.jar
</code></pre>
<p>After running my application, I get this error.
<a href=""https://i.stack.imgur.com/pNYNS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pNYNS.png"" alt=""enter image description here"" /></a></p>
<p>It seems like I should have passed API key when setting up java agent. Although I was looking up the official document, I couldn't find the way to pass it. Can someone help me to solve this problem?</p>",,1,0,,2021-7-7 08:58:28,,2021-7-23 20:11:38,,,,,12833382,,1,0,java|datadog|apm,242,9.53526
261534,1,Method,65423039,Datadog Python log collection from self-hosted Github Runner,"<p>I'm trying to collect logs from cron jobs running on our self hosted Github runners, but so far can only see the actual github-runner host logs.</p>
<p>I've created a self-hosted Github Runner in AWS running on Unbtu with a standard config.</p>
<p>We've also installed the Datadog agent v7 with their script and basic configuration, and added log collection from files using <a href=""https://docs.datadoghq.com/agent/logs/?tab=tailfiles#custom-log-collection"" rel=""nofollow noreferrer"">these instructions</a></p>
<p>Our config for log collection is below.</p>
<pre class=""lang-sh prettyprint-override""><code>curl https://s3.amazonaws.com/dd-agent/scripts/install_script.sh -o ddinstall.sh
export DD_API_KEY=${datadog_api_key}
export DD_SITE=${datadog_site}
export DD_AGENT_MAJOR_VERSION=7
bash ./ddinstall.sh

# Configure logging for GitHub runner
tee /etc/datadog-agent/conf.d/runner-logs.yaml &lt;&lt; EOF
logs:
  - type: file
    path: /home/ubuntu/actions-runner/_diag/Worker_*.log
    service: github
    source: github-worker
  - type: file
    path: /home/ubuntu/actions-runner/_diag/Runner_*.log
    service: github
    source: github-runner
EOF
chown dd-agent:dd-agent /etc/datadog-agent/conf.d/runner-logs.yaml

# Enable log collection
echo 'logs_enabled: true' &gt;&gt; /etc/datadog-agent/datadog.yaml
systemctl restart datadog-agent
</code></pre>
<p>After these steps, I can see logs from our Github runners servers. However, on those runners we have several python cron jobs running in Docker containers, logging to stdout. I can see those logs in the Github Runner UI, but they're not available in Datadog, and those are the logs I'd really like to capture, so I can extract metrics from.</p>
<p>Do the docker containers for the python scripts need some special datadog setup as well? Do they need to log to a file that the datadog agents registers as a log file in the setup above?</p>",,0,2,,2020-12-23 10:49:56,,2020-12-23 10:49:56,,,,,6204542,,1,0,python|docker|github-actions|datadog|github-actions-self-hosted-runners,241,9.52807
261535,0,Integration,62331584,How to import 3rd party python libraries for use with glue python shell script,"<p>I'm trying to import a 3rd party library (datadog) for use with a glue shell script and I'm running into issues. I've packaged the file as a .egg and given the path to it in the glue job, as instructed <a href=""https://docs.aws.amazon.com/glue/latest/dg/add-job-python.html#python-shell-supported-library"" rel=""nofollow noreferrer"">here</a>. This ends up throwing an error saying zipimport.ZipImportError: not a Zip file: '/tmp/glue-python-libs/datadog.egg'. When I try using a zip file instead, it throws ModuleNotFoundError: No module named 'datadog'. How do I go about importing the library?</p>",,1,0,,2020-6-11 18:57:49,,2020-6-11 21:23:58,,,,,6501026,,1,0,python|amazon-web-services|aws-glue|datadog,239,9.51359
261536,2,Query,65368981,Micrometer wrong SLO histogram count with statsd datadog,"<p><strong>Background:</strong></p>
<p>I'm trying SLO feature from micrometer and I expect that I can get the number of requests that fulfill the SLO. For example if I set the SLO to 500ms, then I want to know how many requests &lt;= 500ms. Also I want to know the total requests.</p>
<p><strong>Problem:</strong></p>
<p>http.server.requests.count says 24
http.server.requests.histogram with tag le:_inf says 126</p>
<p>I believe both of them should have the same (or at least similar) value</p>
<p><strong>I'm using:</strong></p>
<p>Spring Boot 2.3.2.RELEASE
Micrometer: 1.5.2</p>
<p><strong>application.properties</strong></p>
<pre><code>management.metrics.enable.all=true
 management.metrics.export.statsd.enabled=true
 management.metrics.export.statsd.flavor=datadog
 management.metrics.export.statsd.host=127.0.0.1
 management.metrics.export.statsd.port=8125
 management.metrics.web.client.request.metricName=acd.http.client.requests
 management.metrics.web.server.request.metricName=acd.http.server.requests
</code></pre>
<p><strong>Meter Filter</strong></p>
<pre><code> @Configuration
 open class AccomWebFluxMeterFilter {
 
   @Bean
   open fun meterFilter(): MeterFilter {
     return object : MeterFilter {
       override fun configure(id: Meter.Id, config: DistributionStatisticConfig): DistributionStatisticConfig? {
         if (id.name.startsWith(&quot;acd.http.server.requests&quot;) || id.name.startsWith(&quot;acd.http.client.requests&quot;)) {
           return DistributionStatisticConfig.builder()
             .percentiles(0.5, 0.75, 0.95, 0.998, 0.9995, 0.9999)
             .serviceLevelObjectives(
               Duration.ofMillis(50).toNanos().toDouble(),
               Duration.ofMillis(100).toNanos().toDouble(),
               Duration.ofMillis(1000).toNanos().toDouble(),
               Duration.ofMillis(5000).toNanos().toDouble()
             )
             .build()
             .merge(config)
         }
         return config
       }
     }
   }
 
 }
</code></pre>
<p><strong>Datadog</strong>
<a href=""https://i.stack.imgur.com/q7uc2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q7uc2.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/RTUtW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RTUtW.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/uY3ZK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uY3ZK.png"" alt=""enter image description here"" /></a></p>
<p>Any clue on what's happening here?
Thanks</p>",,0,0,,2020-12-19 10:48:38,,2020-12-19 23:18:55,2020-12-19 23:18:55,,462565,,462565,,1,1,spring-boot|datadog|statsd|micrometer|spring-micrometer,235,9.48427
261537,0,Configuration,65586123,"Fargate container_definition field ""secretOptions"" not passsing datadog API_KEY to logConfiguration","<p>I'm trying to send my ECS Fargate logs to Datadog. To do this I need to pass my Datadog API_KEY as a field in the <code>logConfiguration</code> object. I need to secure my API_KEY so I am using AWS Secrets Manager via the <code>secretOptions</code> key of the <code>logConfiguration</code> object.</p>
<p>I'm following the steps from AWS laid out <a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data-secrets.html#secrets-logconfig"" rel=""nofollow noreferrer"">here</a>.
The full steps from the Datadog site can be found <a href=""https://docs.datadoghq.com/integrations/ecs_fargate/?tab=fluentbitandfirelens#log-collection"" rel=""nofollow noreferrer"">here</a></p>
<p>For some reason I dont see the logs show up in datadog. Here is the log config section of my Terraform code under the <code>container_definitions</code> object of the <code>aws_ecs_task_definition</code> resource:</p>
<pre><code>&quot;logConfiguration&quot;: {
    &quot;logDriver&quot;: &quot;awsfirelens&quot;,
    &quot;options&quot;: {
        &quot;Name&quot;: &quot;datadog&quot;,
        &quot;Host&quot;: &quot;http-intake.logs.datadoghq.com&quot;,
        &quot;dd_service&quot;: &quot;myservice&quot;,
        &quot;dd_source&quot;: &quot;mysource&quot;,
        &quot;dd_message_key&quot;: &quot;log&quot;,
        &quot;dd_tags&quot;: &quot;env:dev&quot;,
        &quot;TLS&quot;: &quot;on&quot;,
        &quot;provider&quot;: &quot;ecs&quot;
    },
    &quot;secretOptions&quot;: [{
        &quot;name&quot;: &quot;apikey&quot;,
        &quot;valueFrom&quot;: &quot;arn:aws:secretsmanager:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:secret:mysecret&quot;
                }]
}
</code></pre>
<p>If I take out the <code>secretOptions</code> and add the apikey in plaintext, the logs show up on the datadog console:</p>
<pre><code>&quot;logConfiguration&quot;: {
   &quot;logDriver&quot;: &quot;awsfirelens&quot;,
   &quot;options&quot;: {
       &quot;Name&quot;: &quot;datadog&quot;,
       &quot;Host&quot;: &quot;http-intake.logs.datadoghq.com&quot;,
       &quot;dd_service&quot;: &quot;myservice&quot;,
       &quot;dd_source&quot;: &quot;mysource&quot;,
       &quot;dd_message_key&quot;: &quot;log&quot;,
       &quot;dd_tags&quot;: &quot;env:dev&quot;,
       &quot;TLS&quot;: &quot;on&quot;,
       &quot;provider&quot;: &quot;ecs&quot;,
       &quot;apikey&quot;: &quot;myapikey&quot;
   }
}
</code></pre>
<p>I of course cant just send my API_KEY in plaintext. Does the <code>secretOptions</code> just not work for Datadog? Any help is appreciated.</p>",,1,0,,2021-1-5 20:30:07,,2021-5-19 11:31:02,2021-1-5 21:02:44,,9220093,,9220093,,1,2,terraform|aws-fargate|datadog|aws-secrets-manager|firelens,182,9.44029
261538,0,Integration,53226600,For datadog how to check posgresql up and running (on Linuxredhat server),<p>We are not willing to use DD agent. How can we know if PostgresSQL is up and running on my Redhat linux server so that I can create an alert when postgres is down.</p>,,1,2,,2018-11-9 13:27:03,,2018-11-20 19:44:45,2018-11-20 19:26:44,,6458418,,6458418,,1,0,postgresql|datadog,228,9.43174
261539,1,Parse,62699424,spring-cloud-skipper-server - logging in Json format,"<p>We recently started using SCDF on Kubernetes, and we are trying to workout the kinks. One of thing things that i was'nt able to find is whether there is a way to affect logging format,for ex. switch to using Json format. Reason for this is simple, we are using Datadog as our logging platform, and with Json, you don't have to write custom log parsing rules.</p>
<p>With regular log format, you will endup with something like this</p>
<p><a href=""https://i.stack.imgur.com/X71uV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X71uV.png"" alt=""enter image description here"" /></a></p>",62710050,1,0,,2020-7-2 15:05:40,,2020-7-5 16:24:14,,,,,1782683,,1,0,spring|spring-cloud-dataflow|datadog,128,9.42884
261540,1,Method,67550507,Kubernetes HPA based Datadog metrics,"<p>I am last days reading and using HPA based Datadog metrics.
Under I put documentations</p>
<p><a href=""https://www.datadoghq.com/blog/autoscale-kubernetes-datadog/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/autoscale-kubernetes-datadog/</a>
<a href=""https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</a></p>
<p>This is my Datadogmetric</p>
<pre><code>apiVersion: datadoghq.com/v1alpha1
kind: DatadogMetric
metadata:
  name: deliver-rate
  namespace: beta
spec:
  query: rabbitmq.queue.messages.deliver.rate{environment:production,rabbitmq_queue:enqueue.xxx.callable}.as_count()

</code></pre>
<p>and this one is my HPA</p>
<pre><code>apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: test-beta2
  namespace: beta
spec:
  minReplicas: 1
  maxReplicas: 8
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 10
      policies:
      - type: Pods
        value: 2
        periodSeconds: 2
    scaleUp:
      stabilizationWindowSeconds: 10
      policies:
      - type: Pods
        value: 2
        periodSeconds: 2
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: dealroom-xxx
  metrics:
    - type: External
      external:
        metric:
          name: 'datadogmetric@beta:deliver-rate'
        target:
          type: Value
          value: 200
</code></pre>
<p>I got two problems</p>
<p><em>1.</em></p>
<p>And all worked but didn't in the right way scaling up each time when metric value upper than the defined target value. For example, under I will put the  <code>metricvalue/targetvalue</code> and appropriate <code>desiredReplicas</code></p>
<pre><code>desiredReplicas: 4   252/200
desiredReplicas: 6   252/200
desiredReplicas: 6   252/200
desiredReplicas: 5   240/200
desiredReplicas: 6   240/200
desiredReplicas: 6   240/200
desiredReplicas: 8   240/200
</code></pre>
<p>Why the autoscaler adding pods more than 2</p>
<pre><code>  behavior:
    scaleDown:
      stabilizationWindowSeconds: 10
      policies:
      - type: Pods
        value: 2
        periodSeconds: 2
    scaleUp:
      stabilizationWindowSeconds: 10
      policies:
      - type: Pods
        value: 2
        periodSeconds: 2
</code></pre>
<p><em>2.</em></p>
<p>Periodically I saw that datadogmetric's validation is false, but at the same time datadogmetric fetched metric and showed the value</p>
<p><code>deliver-rate   True     False   252     beta/test-beta2   9s</code></p>
<p>this is the error message</p>
<pre><code>HPA controller was able to get the target''s current scale&quot;},{&quot;type&quot;:&quot;ScalingActive&quot;,&quot;status&quot;:&quot;False&quot;,&quot;lastTransitionTime&quot;:&quot;2021-05-15T14:32:01Z&quot;,&quot;reason&quot;:&quot;FailedGetExternalMetric&quot;,&quot;message&quot;:&quot;the
      HPA was unable to compute the replica count: unable to get external metric beta/datadogmetric@beta:deliver-rate/nil:
      unable to fetch metrics from external metrics API: Internal error occurred:
      DatadogMetric is invalid, err: Outdated result from backend, query: rabbitmq.queue.messages.deliver.rate{environment:production,rabbitmq_queue:enqueue.dealroom-php.callable}.as_count()&quot;},{&quot;type&quot;:&quot;ScalingLimited&quot;,&quot;status&quot;:&quot;True&quot;,&quot;lastTransitionTime&quot;:&quot;2021-05-15T14:30:57Z&quot;,&quot;reason&quot;:&quot;TooManyReplicas&quot;,&quot;message&quot;:&quot;the
      desired replica count is more than the maximum replica count&quot;}]'
</code></pre>
<p>Who can help me to solve these 2 problems?</p>",,0,0,,2021-5-15 19:27:36,,2021-5-15 19:39:27,2021-5-15 19:39:27,,9143141,,9143141,,1,0,autoscaling|datadog|hpa,226,9.41643
261541,0,Configuration,65790261,In yaml how to declare multiple value to the single key in tags,"<p>For e.g.<br />
tags:</p>
<pre><code>  - name:instancename
  - component:['component1','component2']
  - environment:uat
  - region:region
</code></pre>
<p>When using above syntax not getting logs in Datadog for component2 only getting logs only for component1 so we want to parse logs for both the component i.e component1 and component2 so how would get it?</p>",65834510,1,2,,2021-1-19 11:04:21,,2021-1-21 19:32:19,2021-1-20 10:19:42,,13346540,,13346540,,1,0,yaml|datadog,224,9.40099
261542,1,Parse,63859370,datadog facet path with special symbols,"<p>I have an index created on the log and the paths have special character :
<strong>for example:</strong></p>
<pre><code>@params.rs:orgId
@params.rs:format
</code></pre>
<p><strong>Sample URL:</strong></p>
<pre><code>10.32.45.56 - user [12/Sep/2020:06:25:51 -0400] &quot;GET /v1/resources/manifestinfo?rs:format=json&amp;rs:orgId=123&amp;rs:correlationId=39e8f697-3549-a142-224b-251fc6672a94-32edb97dc3f8 HTTP/1.1&quot; 200 14002 - -
</code></pre>
<p><strong>grok parser:</strong></p>
<pre><code>rule1 %{ipv4:network.client.ip}\s+-\s+%{word:user}\s+\[%{date(&quot;dd/MMM/yyyy:HH:mm:ss Z&quot;):date}\]\s+\&quot;%{word:http.module}\s+\/v1\/resources\/+%{word:onemds.module}\?+%{data:onemds.params:keyvalue(&quot;=&quot;,&quot;/:&quot;,&quot;&quot;,&quot;&amp;&quot;)}
</code></pre>
<p>when I try to add facet for <code>@params.rs:orgId</code></p>
<p>I am getting error as</p>
<blockquote>
<p>An error occurred while saving the facet: The Facet path must contain
only letters, digits, or the characters - _ . @ $</p>
</blockquote>",,1,0,,2020-9-12 10:30:14,,2020-9-15 22:09:03,2020-9-14 15:47:52,,2199826,,12517930,,1,1,regex|regex-negation|grok|datadog,199,9.39541
261543,3,Monitoring,68954149,Can I redirect alerts from a single DataDog monitor to different Slack channels?,"<p>So I've got a DataDog monitor and its query is roughly similar to:</p>
<pre><code>trace-analytics(&quot;service:foo-service @http.status_code:403&quot;).rollup(&quot;count&quot;).last(&quot;10m&quot;) &gt; 1000
</code></pre>
<p>and under &quot;Notify your team&quot; section of DD monitor I setup my team's channel on Slack and we receive quite a lot of alerts as a result.</p>
<p>That said, we'd be interested to redirect those errors to relevant teams (e.g., if the route <code>/abc</code> throws a <code>403</code> we'd like to post it to another #abc-team Slack channel, <code>/cde</code> to <code>#cde-team</code> etc). How can we do it? I was thinking I could write a Slack bot or something since I didn't find such an option on DataDog.</p>",,2,0,,2021-8-27 13:19:59,,2021-8-27 15:10:37,,,,,16691082,,1,1,slack|slack-api|datadog,175,9.37215
261544,1,Method,66553744,Datadog: how to get redis cpu usage metrics?,"<p>Taking a look at <a href=""https://docs.datadoghq.com/integrations/redisdb/?tab=host"" rel=""nofollow noreferrer"">Redis metrics for Datadog</a> , we can see that <code>redis.cpu.sys</code> refers to <em>System CPU consumed by the Redis server.</em> But this is metric for CPU usage time, not CPU usage.</p>
<p>How do we do if we want to create alerts based on the actual CPU usage?</p>",,0,0,,2021-3-9 19:46:51,,2021-3-9 19:46:51,,,,,7469990,,1,1,database|redis|datadog,215,9.32975
261545,2,Query,62570190,Datadog search by java stacktrace,"<p>Anyway, how can I search for all messages(errors) where stacktrace contains specific piece of code? According to datadog documentation it search only by message attribute(it's infered from the json-like object sent to datadog when you log something). Stacktrace is a separate property and I cannot understand how to search it.</p>",,0,0,,2020-6-25 07:37:07,1,2020-6-25 07:37:07,,,,,2265497,,1,3,java|monitoring|stack-trace|datadog,214,9.32166
261546,3,Monitoring,55698130,"Memory, CPU and IO of an Airflow task",<p>I am working on a problem to identify the intensive Airflow tasks which are using more memory or CPU or IO in an AWS ECS container. I can see the memory and CPU utilization overall from the ECS container. How do I go by specific process and task level? Any help would be appreciated.</p>,,1,0,,2019-4-15 22:35:10,,2019-4-16 18:44:48,,,,,5915171,,1,0,python|amazon-web-services|airflow|amazon-ecs|datadog,214,9.32166
261547,1,Method,59877263,Flink with StatsD counter metrics comparision,"<p>Talking about Counters with respect to StatsD, the way it works is that you keep posting a value of a counter eg. <code>numOfRequests|c:1</code> whenever app get a request to the StatsD Daemon. The daemon has a flush interval set, when it pushes the aggregate of this counter in that period to an external backend. Additionally it also resets the counter to 0.</p>

<p>Trying to map this to Flink Counters. </p>

<ol>
<li>Flink counters only has inc and dec methods so till the reporting time comes, app can call inc or dec to change the value of a counter. </li>
<li>At the time of reporting the latest value of counter is reported to StatsD daemon but the Flink counter value is never reset(Not able to find any code). </li>
</ol>

<p>So should the flink counter be reported as a gauge value to StatsD. Or Flink does reset the counters?</p>",,1,0,,2020-1-23 11:13:47,,2020-1-23 16:01:18,,,,,2818245,,1,0,apache-flink|statsd|datadog,211,9.29713
261548,0,Integration,68599474,Spring Boot integration with Datadog,"<p>I have a simple springboot application and trying to integrate datadog agent with that but couldn't run the application with Javaagent. Have done the following steps.</p>
<p>Added dependency in the pom.</p>
<pre><code>
    &lt;dependency&gt;
    &lt;groupId&gt;com.datadoghq&lt;/groupId&gt;
    &lt;artifactId&gt;dd-java-agent&lt;/artifactId&gt;
    &lt;version&gt;0.83.2&lt;/version&gt;
    &lt;/dependency&gt;

</code></pre>
<p>unpack datadog dependencies and added classes -----------</p>
<pre><code>&lt;plugin&gt;
&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
&lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
&lt;version&gt;3.1.1&lt;/version&gt;
&lt;executions&gt;
&lt;execution&gt;
&lt;phase&gt;prepare-package&lt;/phase&gt;
&lt;goals&gt;
&lt;goal&gt;unpack-dependencies&lt;/goal&gt;
&lt;/goals&gt;
&lt;configuration&gt;
&lt;includeArtifactIds&gt;dd-java-agent&lt;/includeArtifactIds&gt;
&lt;outputDirectory&gt;${project.build.outputDirectory}&lt;/outputDirectory&gt;
&lt;/configuration&gt;
&lt;/execution&gt;
&lt;/executions&gt;
&lt;/plugin&gt;
</code></pre>
<pre><code>added premain class of datadog agent -----

    &lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
    &lt;version&gt;2.5&lt;/version&gt;
    &lt;configuration&gt;
    &lt;archive&gt;
    &lt;manifestEntries&gt;
    &lt;Premain-Class&gt;datadog.trace.bootstrap.AgentBootstrap&lt;/Premain-Class&gt;
    &lt;Can-Redefine-Classes&gt;true&lt;/Can-Redefine-Classes&gt;
    &lt;Can-Retransform-Classes&gt;true&lt;/Can-Retransform-Classes&gt;
    &lt;/manifestEntries&gt;
    &lt;/archive&gt;
    &lt;/configuration&gt;
    &lt;/plugin&gt;

</code></pre>
<p>Added To make spring repackager plugin to find springboot main class-----------</p>
<pre><code>&lt;plugin&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
&lt;version&gt;2.3.4.RELEASE&lt;/version&gt;
&lt;configuration&gt;
&lt;mainClass&gt;com.user.info.userInfoApplication&lt;/mainClass&gt;
&lt;/configuration&gt;
&lt;/plugin&gt;

        
</code></pre>
<pre><code>Finally when issuing below command fails 

    java -javaagent:user-info-0.0.1-SNAPSHOT.jar -jar user-info-0.0.1-SNAPSHOT.jar
    
    Exception in thread &quot;main&quot; java.lang.ClassNotFoundException: datadog.trace.bootstrap.AgentBootstrap
            at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
            at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
            at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
            at java.instrument/sun.instrument.InstrumentationImpl.loadClassAndStartAgent(InstrumentationImpl.java:431)
            at java.instrument/sun.instrument.InstrumentationImpl.loadClassAndCallPremain(InstrumentationImpl.java:525)
    FATAL ERROR in native method: processing of -javaagent failed

```````````````````````````````````````````````````````````````````````````````````````````






</code></pre>",,0,0,,2021-7-31 05:42:20,1,2021-7-31 05:42:20,,,,,16565451,,1,0,java|spring-boot|maven|dependencies|datadog,211,9.29713
261549,1,Error,69431383,ERROR:ddtrace.internal.writer:failed to send traces to Datadog Agent at http://localhost:8126: OSError: [Errno 99] Cannot assign requested address,"<p>I want to set up the datadog agent to monitor my python application running inside a docker container. I have created the docker image tagged as datadog_app:v_1.
Below is my docker file:</p>
<pre><code>FROM ubuntu:18.04
WORKDIR /app
RUN apt-get update --no-install-recommends
RUN apt-get install -y build-essential python3.6 python3.6-dev python3-pip python3-setuptools --no-install-recommends
RUN pip3 install Cython
RUN pip3 install ddtrace
RUN apt-get install -y curl
ADD ./datadog_app.py /app/handler.py
RUN DD_AGENT_MAJOR_VERSION=7 DD_API_KEY=my_api_key DD_SITE=&quot;datadoghq.com&quot; bash -c &quot;$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script.sh)&quot;
CMD ddtrace-run python3 handler.py
</code></pre>
<p>Below is the code for the datadog_app.py file:</p>
<pre><code>from ddtrace import tracer
import time


@tracer.wrap(service=&quot;addition&quot;)
def handler():
    time.sleep(2)
    print(1+2)

handler()
</code></pre>
<p>Now, I am trying to run a docker container from the &quot;datadog_app:v_1: image in interactive and making sure to expose the port 8126 way by using the following command:</p>
<pre><code>docker run -it -p 8126:8126 datadog_app:v_1
</code></pre>
<p>I am getting the following error:</p>
<pre><code>ERROR:ddtrace.internal.writer:failed to send traces to Datadog Agent at http://localhost:8126
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.6/dist-packages/tenacity/__init__.py&quot;, line 407, in __call__
    result = fn(*args, **kwargs)
  File &quot;/usr/local/lib/python3.6/dist-packages/ddtrace/internal/writer.py&quot;, line 356, in _send_payload
    response = self._put(payload, headers)
  File &quot;/usr/local/lib/python3.6/dist-packages/ddtrace/internal/writer.py&quot;, line 332, in _put
    conn.request(&quot;PUT&quot;, self._endpoint, data, headers)
  File &quot;/usr/lib/python3.6/http/client.py&quot;, line 1281, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File &quot;/usr/lib/python3.6/http/client.py&quot;, line 1327, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File &quot;/usr/lib/python3.6/http/client.py&quot;, line 1276, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File &quot;/usr/lib/python3.6/http/client.py&quot;, line 1042, in _send_output
    self.send(msg)
  File &quot;/usr/lib/python3.6/http/client.py&quot;, line 980, in send
    self.connect()
  File &quot;/usr/lib/python3.6/http/client.py&quot;, line 952, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File &quot;/usr/lib/python3.6/socket.py&quot;, line 724, in create_connection
    raise err
  File &quot;/usr/lib/python3.6/socket.py&quot;, line 713, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.6/dist-packages/ddtrace/internal/writer.py&quot;, line 458, in flush_queue
    self._retry_upload(self._send_payload, encoded, n_traces)
  File &quot;/usr/local/lib/python3.6/dist-packages/tenacity/__init__.py&quot;, line 404, in __call__
    do = self.iter(retry_state=retry_state)
  File &quot;/usr/local/lib/python3.6/dist-packages/tenacity/__init__.py&quot;, line 361, in iter
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[&lt;Future at 0x7f32383f0a58 state=finished raised OSError&gt;]
</code></pre>
<p>I have also tried running the following command:</p>
<pre><code>docker run -it --expose=8126 datadog_app:v_1
</code></pre>
<p>But I am getting the same error.
My goal is to run the datadog-agent inside the same container that contains my python application. Can someone explain what is wrong with my code or my run commands?</p>",,0,3,,2021-10-4 05:46:20,,2021-10-5 18:40:49,2021-10-5 18:40:49,,16493383,,16493383,,1,1,python|docker|datadog,206,9.25547
261550,1,Method,68853025,Getting Infrastructure metrics from AWS Batch to DataDog,"<p>Is there a direct integration point or a connector to integrate an AWS Batch job with DataDog? To retrieve the logs and infrastructure metrics?</p>
<p>So far what I came across was the DataDog forwarder which can forward the logs to DataDog from Cloudwatch but nothing for Infrastructure metrics?</p>",,0,5,,2021-8-19 18:36:06,,2021-8-19 18:36:06,,,,,14938081,,1,0,amazon-web-services|amazon-cloudwatch|datadog|aws-batch,202,9.22141
261551,3,Visualization,65179367,How to view statsD events in Datadog?,"<p>I am using statsD (<a href=""https://www.npmjs.com/package/hot-shots"" rel=""nofollow noreferrer"">hot-shots</a>) to try log events in datadog such as when a new item is created and by who but, when I am calling <code>statsD.event('title', 'description')</code> I do not see any events in datadog metrics.</p>
<p>My statsD client is setup like this:</p>
<pre><code>const client = new StatsD({
  port: process.env.STATSD_PORT,
  host: process.env.STATSD_HOST,
  prefix: 'service-name.',
  globalTags: {
    env: process.env.CONFIG_ENV,
    service: 'service_name'
  },
  errorHandler: error =&gt; {
    console.error(error);
  }
});
</code></pre>
<p>and then I call the event method like so:</p>
<pre><code>client.event(ACTION.CREATE, JSON.stringify({
  username: this.username,
  alias: alias,
  //...
}));
</code></pre>
<p>In the metrics section of datadog, I do not appear to be seeing any events come through. The only way I can see some logs there is using <code>increment</code></p>
<pre><code>client.increment(ACTION.CREATE, {
  username: this.username,
  alias: alias,
  ///...
});
</code></pre>
<p>But increment only seems to tell me how many times that action is called and I am unable to log additional data such as username and alias.</p>
<p>I am pretty sure my statsD client and datadog configs are setup correctly as I am able to see data from increment so I suspect it is to do with the way I am trying to use the event method.</p>
<p>Am I using the event method incorrectly? Perhaps I am checking in the wrong place in datadog? Perhaps I should be using increment?</p>
<p>How can I log events along with associated data in datadog using statsD?</p>",,0,3,,2020-12-7 09:47:27,,2020-12-7 09:47:27,,,,,3528590,,1,1,javascript|node.js|metrics|datadog|statsd,201,9.21278
261552,3,Monitoring,66001758,Export Datadog Monitor Alerts Weekly,"<p>Is there a way to easily generate reports of alerts from certain monitors in Datadog, on a weekly or biweekly basis?</p>
<p>Context: At the moment, these alerts go to a Slack channel. Folks have to scroll through the channel to see all the issues and prioritize investigations (during sprint planning).  I am trying to make it easy for sprint planners to pull up the alerts report.</p>
<p>I found only a couple related things after googling:</p>
<ol>
<li><p>Datadog has a CSV with 6 months of alerts, that you can curl to download. I guess I could curl, diff with prior week's csv and filter for interesting monitors. But does not seem like the best solution. <a href=""https://docs.datadoghq.com/monitors/faq/how-can-i-export-alert-history/?tab=us"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/monitors/faq/how-can-i-export-alert-history/?tab=us</a></p>
</li>
<li><p>An old article about Monitor Trends Report which I can't find in the app. <a href=""https://www.datadoghq.com/blog/monitor-alert-status/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/monitor-alert-status/</a></p>
</li>
</ol>",67582183,1,1,,2021-2-1 23:49:07,,2021-5-18 08:02:17,,,,,1325849,,1,1,datadog,174,9.1622
261553,3,Monitoring,52269765,alert condition over variables in datadog,"<p>How can I create a threshold alert by comparing aggregate of 2 metrics ? For example, if m1=[2,3,1,5] and m2=[6,7], I want to create an alert when sum(m1) > sum(m2).  sum method here I assume will add all the data points returned by a query.</p>

<p>From what I have observed, datadog flow of alert creating is like, define the metric  -> set alert condition on metric. That is, it looks like the alert condition will be some condition on metric data type only.</p>

<p>But I am looking for something like storing the aggregate of metrics in a variables and comparing those variables. How can it be done in datadog ?</p>",,0,0,,2018-9-11 06:18:36,,2018-9-11 06:18:36,,,,,3150716,,1,1,datadog,195,9.16014
261554,2,Query,59260837,Datadog explore facets and measures,"<p>We have multiple applications sending logs to <code>Datadog</code> via <code>syslog</code>. Every team has created <code>facets</code> / <code>measures</code> for their respective applications under a particular group.</p>

<p>Is there a direct way to explore the list of <code>facets</code> / <code>measures</code> under a specific group? I am trying to create a document for our support team to include the list of <code>facets</code> and <code>measures</code>. I am able to view them in the Log Search page, but, cannot copy.</p>

<p>I am looking something as export the list of <code>facets</code>/<code>measures</code> to excel / csv.</p>

<p>Note: I am restricted to use datadog api.</p>",,0,2,,2019-12-10 05:34:31,,2019-12-10 05:34:31,,,,,1806481,,1,1,facet|measure|datadog,194,9.15121
261555,2,Query,53459133,How to find average CPU utilization over a period of time on datadog,<p>In our infra CPU utilization is being monitored by datadog SAS. The dashboard shows  CPU utilization over a period time graphically. How do I find the average CPU utilization over that period of time? </p>,53624609,1,0,,2018-11-24 14:25:03,,2018-12-5 03:10:36,,,,,7651428,,1,0,monitoring|datadog,105,9.08476
261556,3,Monitoring,66004286,Python unit test for datadog api monitor creation,"<p>how to write pytest for the below module? I have been trying to write the unit test for the below datadog API monitor creation using python language. Assume the create method is going to send 200 status.</p>
<p>how do I mock <code>json[&quot;monitors&quot;][0]['type']</code>. I get</p>
<pre><code>type=json[&quot;monitors&quot;][0]['type'],
TypeError: 'int' object is not subscriptable
</code></pre>
<p>Datadog API reference - <a href=""https://docs.datadoghq.com/api/latest/monitors/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/latest/monitors/</a></p>
<pre><code>import json
from datadog import api
   
def create_monitor(json):
     resp = api.Monitor.create(
           type=json[&quot;monitors&quot;][0]['type'],
            query=json[&quot;monitors&quot;][0]['query'],
            name=json[&quot;monitors&quot;][0]['name']
        )
       return resp
</code></pre>
<p>json content:</p>
<pre><code>{
  &quot;monitors&quot;: [
    {
      &quot;name&quot;: &quot;Success rate&quot;,
      &quot;type&quot;: &quot;metric alert&quot;,
      &quot;query&quot;: &quot;query here&quot;,
    
    }
  ]
}
</code></pre>",,0,1,,2021-2-2 05:50:03,,2021-2-3 06:59:25,2021-2-3 06:59:25,,1552748,,6223346,,1,0,python|pytest|monkeypatching|datadog|pymock,186,9.07805
261557,2,Query,65163032,Prometheus Percentage Change over time,"<p>I'm trying Migrating from DataDog to prometheus. and this one query i'm having some difficulties with.
I'm unable to find the pct_change functionality available in DD in prometheus.</p>
<p>I tried using (A-B)/B or A/B but they don't work. not appropriate results.</p>
<p>e.g.</p>
<pre><code>DD
pct_change(avg(last_1h),last_1h):avg:default.burrow_kafka_consumer_lag_total{consumer_group IN (connect-.*)} by {consumer_group} &gt; 300

PromQL
avg(avg_over_time(burrow_kafka_consumer_lag_total{consumer_group=~&quot;connect-.*&quot;}[1h]) / (avg_over_time(burrow_kafka_consumer_lag_total{consumer_group=~&quot;connect-.*&quot;}[1h] offset 1h))) by (consumer_group) &gt; 3
</code></pre>
<p>But it's not correct. The alert fires very often. Any leads on this would be appreciated. How can I write the PromQl alternative of DD?</p>",,0,0,,2020-12-5 23:22:23,,2020-12-5 23:22:23,,,,,4323514,,1,0,prometheus|grafana|datadog|promql,186,9.07805
261558,3,Monitoring,64528428,Datadog monitor to alert when one metric hasn't happened within a period of time since another metric,"<p>I'm having trouble setting up monitor that will alert me when an event hasn't happened since some period of time following another event. Basically, for a given task in my application, I have a log that indicates a state of &quot;running&quot; and another log that indicates a state of &quot;finished&quot;. From these logs, I've defined two custom metrics in datadog. I'm trying to set up a monitor that will alert me when a task has not finished within 2 hours of when it started running. So for example, if the running metric is observed at 2:00, the monitor shouldn't alert for the absence of the finished until 4:00. If the finished metric is observed before 4:00, the monitor will not alert for this task.</p>
<p>The way that I've tried implementing this is by using a threshold monitor, and subtracting the count of my running metric from the count of my finished metric. However, what's challenging here is the time-delta piece.</p>
<p>I've tried using the delay evaluation (delaying by 2 hrs), however, at the time when it starts evaluating, it will only take into account the first metric. It basically, just slides the window back.</p>",,0,0,,2020-10-25 20:26:35,,2020-10-25 20:26:35,,,,,12534182,,1,1,datadog,186,9.07805
261559,0,Configuration,62221212,Datadog Terraform (IaC) - directory layout strategy with multiple organizations,"<p>I'm trying to import my whole Datadog environment to the Terraform configuration. My account has access to multiple organizations. I want to import it to the single Monolithics repository. 
Unfortunately, I met the issue with directory layout startegy - I'm not sure how it should look based on <strong>Terraform best practices</strong>. </p>

<p><strong>I suggested:</strong> </p>

<pre><code>datadog-iac-repo:
    organizations/
       org1/
         user/
         monitors/
         dashboards/
       org2/
         user/
         monitors/
         dashboards/
       org3/
         user/
         monitors/
         dashboards/
    tools/
    init.tf
    main.tf

</code></pre>

<p>or </p>

<pre><code>datadog-iac-repo:
    user/
       org1/
       org2/
       org3/
    dashboards/
       org1/
       org2/
       org3/
    monitors/
       org1/
       org2/
       org3/
    tools/
    init.tf
    main.tf

</code></pre>

<p>Does somebody have experience with the issue? What do you think? 
Could you provide me your experience? </p>

<p>Thanks in advance!</p>",,1,0,,2020-6-5 17:39:26,,2020-6-6 08:04:38,,,,,11049682,,1,0,automation|terraform|monitoring|datadog,104,9.06813
261560,3,Monitoring,54735683,Track traffic endpoints with Nginx,"<p>I'm using Nginx and I want to keep track how many hits we get to what endpoint.</p>

<p>We have few services in our website, how we can track the number of hits each one gets (not only number of connections but to what path of the platform)?</p>

<p>That way for example we can see what each point of our API get's the most hits and to improve things there.</p>

<p>If there is a way to get this even further with origin of the request it will be great.</p>

<p>I've installed Datadog agent but didn't installed anything related to NGINX, there is better tool for this task?</p>

<p>Thanks!</p>",,1,0,,2019-2-17 17:18:19,,2019-2-18 16:30:16,,,,,1060664,,1,0,performance|http|nginx|monitor|datadog,184,9.05927
261561,1,Parse,68247751,Datadog parse array of json,"<p>I have array of json`s as log in Datadog.
Example:</p>
<pre><code>[{name:&quot;jon&quot;} , {name:&quot;mike&quot;}]
</code></pre>
<p>How can I parse inner json`s?</p>",,1,0,,2021-7-4 18:35:49,,2021-7-7 17:36:55,,,,,4959129,,1,0,datadog,184,9.05927
261562,1,Method,68902329,Set Prefix to Spring Micrometer Merics using Statsd and Datadog,"<p>I'm trying to Implement Custom Metrics Integration for my App. Using the following setup</p>
<pre><code>// DogStatsd Metrics Integration with MicroMeter
implementation group: 'io.micrometer', name: 'micrometer-registry-statsd', version: '1.7.2'
</code></pre>
<p>Custom Spring Configuration Added for the application</p>
<pre><code>@Configuration
public class MetricConfiguration {

private final MeterRegistry meterRegistry;

@Value(&quot;${management.metrics.export.statsd.prefix}&quot;)
private String prefix;

@Autowired
public MetricConfiguration(MeterRegistry meterRegistry) {
    this.meterRegistry = meterRegistry;
}

@Bean
public MeterRegistryCustomizer&lt;MeterRegistry&gt; metricsCommonTags() {
    return registry -&gt; registry.config().meterFilter(new MeterFilter() {
        @Override
        public Meter.Id map(Meter.Id id) {
            if (!id.getName().startsWith(prefix)) {
                return id.withName(prefix + &quot;.&quot; + id.getName());
            } else {
                return id;
            }
        }
    });
}

@Bean
public TimedAspect timedAspect() {
    return new TimedAspect(meterRegistry);
}
}
</code></pre>
<p>YAML Configuration for  Metrics</p>
<pre><code>management:
  metrics:
   enable:
    jvm: false
    process: false
    tomcat: false
    system: false
    logback: false
   distribution:
    slo:
      http.server.requests: 50ms
    percentiles-histogram:
      http.server.requests: true
    percentiles:
      http.server.requests: 0.99
  export:
    statsd:
      enabled: false
      flavor: datadog
      host: ${DD_AGENT_HOST}
      port: 8125
      prefix: ${spring.application.name}
  endpoints:
    enabled-by-default: true
    web:
      exposure:
        include: &quot;*&quot;
 endpoint:
   metrics:
     enabled: true
   health:
     enabled: true
     show-components: &quot;always&quot;
     show-details: &quot;always&quot;
</code></pre>
<p>Trying to set the prefix to all the custom metrics, but after setting the prefix the excluded metrics are breaking are started showing in the <strong>/actuator/metrics</strong> response.</p>
<p>The response looks like below:</p>
<pre><code>{
 names: [
        &quot;my-service.http.server.requests&quot;,
        &quot;my-service.jvm.buffer.count&quot;,
        &quot;my-service.jvm.buffer.memory.used&quot;,
        &quot;my-service.jvm.buffer.total.capacity&quot;,
        &quot;my-service.jvm.classes.loaded&quot;,
        &quot;my-service.jvm.classes.unloaded&quot;,
        &quot;my-service.jvm.gc.live.data.size&quot;,
        &quot;my-service.jvm.gc.max.data.size&quot;,
        &quot;my-service.jvm.gc.memory.allocated&quot;,
        &quot;my-service.logback.events&quot;,
        &quot;my-service.process.cpu.usage&quot;,
        &quot;my-service.process.files.max&quot;,
        &quot;my-service.process.files.open&quot;,
        &quot;my-service.process.start.time&quot;,
        &quot;my-service.process.uptime&quot;,
        &quot;my-service.system.cpu.count&quot;,
        &quot;my-service.system.cpu.usage&quot;,
        &quot;my-service.system.load.average.1m&quot;,
        &quot;my-service.tomcat.sessions.active.current&quot;,
        &quot;my-service.tomcat.sessions.active.max&quot;,
        &quot;my-service.tomcat.sessions.alive.max&quot;,
        &quot;my-service.tomcat.sessions.created&quot;,
        &quot;my-service.tomcat.sessions.expired&quot;,
        &quot;my-service.tomcat.sessions.rejected&quot;
        ]
 }
</code></pre>",,0,3,,2021-8-24 05:47:49,,2021-8-24 05:54:17,2021-8-24 05:54:17,,1559147,,1559147,,1,0,spring|spring-boot|datadog|micrometer|statsd,183,9.0498
261563,1,Method,64126426,Log ActiveJob APM Traces with DataDog,"<p>my configuration is:</p>
<pre><code>Datadog.configure do |c|
  c.tracer enabled: Rails.env.production? || ENV['DD_TRACE_ENABLED'] == 'true'
  c.tracer analytics_enabled: true

  c.use :rails, service_name: 'rails'
  c.use :resque, workers: ApplicationJob.subclasses + ApplicationWorker.subclasses, service_name: 'resque'
  c.use :http, service_name: 'http'
  c.use :redis, service_name: 'redis'
end
</code></pre>
<p>Where:</p>
<ul>
<li>The <code>ApplicationJob</code> signature is <code>class ApplicationJob &lt; ActiveJob::Base</code></li>
<li>The <code>ApplicationWorker</code> is used as a parent class for vanilla Resque
classes.</li>
</ul>
<p>I'm not getting any errors, but I <strong>am</strong> seeing the subclasses of <code>ApplicationWorker</code> in the DataDog dashboard for the APM traces but <em>not</em> the <code>ApplicationJob</code> subclasses.</p>
<p>From the documentation I've found this is the correct way to configure <code>activeJob</code> tracing when it uses <code>resque</code>, but I haven't found such great documentation on the subject.</p>",,1,1,,2020-9-29 19:33:24,,2021-2-21 22:44:46,,,,,598805,,1,2,resque|rails-activejob|datadog,139,8.97206
261564,3,Monitoring,69026810,Datadog airflow alert when a dag is paused,"<p>We have encountered a scenario recently where someone mistakenly turned off a production dag, and we want to get alert whenever a dag is paused using datadog.</p>
<p>I have checked <a href=""https://docs.datadoghq.com/integrations/airflow/?tab=host"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/airflow/?tab=host</a></p>
<p>But have not got any metric for dag to check if it is paused or not.</p>
<p>I can run a custom script in datadog as well.</p>
<p>One of the method is that I exec into postgres pod and get the list of active dags:</p>
<pre><code>select * from dag where is_paused=true;
</code></pre>
<p>Or is there any other way I can get the unpaused dag list and also when new dag is added what is the best way to handle it.</p>
<p>I want the alert whenever a unpaused dag is paused.</p>",,1,0,,2021-9-2 08:35:22,,2021-9-2 10:18:04,,,,,12965658,,1,1,airflow|devops|datadog,49,8.96078
261565,1,Error,66732666,No custom metrics when posting by HTTP,"<p>There seems to be very limited documentation on this.  I'm looking to post a custom metric (actually from an IoT microcontroller) via HTTP.</p>
<p>According to <a href=""https://docs.datadoghq.com/developers/metrics/"" rel=""nofollow noreferrer"">this page</a> the <a href=""https://docs.datadoghq.com/api/latest/metrics/#submit-metrics"" rel=""nofollow noreferrer"">appropriate endpoint</a> is <code>POST https://api.datadoghq.com/api/v1/series?api_key=xyz</code>, although this does seem more geared to posting a bulk set of time series data and not for individual measurements.  Anyway, I've posted to it in various ways (including the EXAMPLE JSON given on that page) and receiving back <code>HTTP 202</code>s, which leads me to believe the data is sinking in somewhere.</p>
<p>However, nothing appears in the Metrics Explorer section on my account.</p>
<p>Can anyone provide some direction?</p>",,1,0,,2021-3-21 13:07:30,,2021-3-26 13:36:48,,,,,589558,,1,2,datadog,137,8.94688
261566,2,Query,56230928,Recording sales amount in datadog metrics,"<p>I'm trying to record my website sales $ amount in datadog. However I'm getting way more than the actual value.</p>

<p>I'm using java-dogstatsd client and spring. My application is running on 3 hosts. I recorded all metrics (using sendWebOrder method) but no luck.</p>

<pre><code>@EnableConfigurationProperties({DataDogProperties.class})
@Component
public class DDMetrics {
@Autowired
DataDogProperties dataDogProperties;
@Autowired
private NonBlockingStatsDClient statsd;
private Map&lt;TopicPartition,Long&gt; lags = new HashMap&lt;&gt;();
@Bean
private NonBlockingStatsDClient initClient() {
    NonBlockingStatsDClient metricsClient = new NonBlockingStatsDClient(
            dataDogProperties.getServiceName(),
            dataDogProperties.getHostname(),
            dataDogProperties.getPort();
    return metricsClient;
}

public void sendWebOrder(WebOrder webOrder) {
    List&lt;String&gt; tags = new ArrayList&lt;&gt;();
    tags.add(""transactionType:"" + webOrder.getTransactionType());
    tags.add(""dataSourceType:""      + webOrder.getDataSourceType()));
    statsd.count(""amount_count"", webOrder.getAmount(), String.join("","", tags));
    statsd.recordDistributionValue(""amount_dist"", webOrder.getAmount(), String.join("","", tags));
    statsd.recordHistogramValue(""amount_hist"", webOrder.getAmount(), String.join("","", tags));
    statsd.recordGaugeValue(""amount_gauge"", webOrder.getAmount(), String.join("","", tags));
    statsd.incrementCounter(""weborder"", String.join("","", tags));
}
</code></pre>

<p>I'm trying to generate a datadog toplist by transactiontype. I'm not getting the correct amount in any of the metrics (tried mainly count, gauge and histogram.sum). Here is my datadog config:</p>

<pre><code>{
  ""viz"": ""toplist"",
  ""requests"": [
    {
      ""q"": ""top(sum:projecta.webtransactions.amount_histogram.sum{$TransactionType} by {transactiontype}, 10, 'sum', 'desc')"",
      ""type"": ""area"",
      ""style"": {
        ""palette"": ""dog_classic"",
        ""type"": ""solid"",
        ""width"": ""normal""
      },
      ""aggregator"": ""sum"",
      ""conditional_formats"": []
    }
  ],
  ""autoscale"": true
}
</code></pre>

<p>What am I missing? Is this the correct way to record money value? Do I've to do any rollup in config?
Any help is appreciated.</p>",,0,1,,2019-5-21 04:06:13,,2019-5-21 04:06:13,,,,,2666282,,1,1,java|statsd|datadog,171,8.93198
261567,0,Configuration,66443203,Exclude site from Datadog automatic trace instrumentation on IIS,<p>I was wondering to know if there is a way to exclude a site from Datadog automatic tracing on IIS. I've read the docs but didn't find anything about.</p>,66642805,2,2,,2021-3-2 16:24:52,,2021-3-18 14:55:00,2021-3-18 14:55:00,,24231,,12649047,,1,0,.net|iis|trace|datadog|distributed-tracing,170,8.9218
261568,0,Integration,68723146,AWS Datadog integration issue,"<p>I have created AWS Role/Policies but when I am trying to integrate AWS with Datadog APM I entered my AWS account details and role, and I see:</p>
<blockquote>
<p>Access denied. See <a href=""https://docs.datadoghq.com/integrations/amazon_web_services/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/amazon_web_services/</a></p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/9dOpQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9dOpQ.png"" alt=""enter image description here"" /></a></p>
<p>Can someone please help me figure out this issue?</p>",68730508,1,0,,2021-8-10 08:10:10,,2021-10-12 13:50:51,2021-8-10 16:27:49,,1723857,,8446049,,1,0,amazon-web-services|amazon-cloudfront|datadog|apm|aws-roles,94,8.89251
261569,2,Query,60115340,How to calculate max CPU / Mem usage with Datadog API (Python),"<p>I am trying to query the datadog server for some specific metrics ie. ""max mem used"" over some period x and I'm doing the following:</p>

<pre><code>time_period = 7200
end = int(time()) # time period in secodns over which data will be fetched
start = end - time_period

query = ""max:system.mem.used{host:&lt;hostname&gt;}"" 

results = api.Metric.query(start=start - time_period, end=end, query=query)
</code></pre>

<p>I would have expect a single timestamped value to be returned, however I get datapoint for every minute like so:
<code>[
          1581084600000,
          1339840512
        ],
        [
          1581084660000,
          1339883520
        ],
        [
          1581084720000,
          1339740160
        ]</code></p>

<p>Is there a way to get a specific result, i.e. the maximum out of all these results?</p>

<p>Thanks.</p>",,0,0,,2020-2-7 14:24:39,,2020-2-7 14:24:39,,,,,7890683,,1,1,datadog,167,8.89087
261570,3,Monitoring,58873617,Datadog event trigger returns no data instead of 0,"<p>I have created a event monitor( for example</p>

<p><code>events('sources:rds event_source:db-instance').by('dbinstanceidentifier').rollup('count').last('1d') &gt;= 1</code>) </p>

<p>but it returns ""NO DATA"" when there are no any events.</p>

<p>How to make it return 0 when there are no any events?</p>",,0,5,,2019-11-15 09:10:05,1,2019-11-15 10:02:49,2019-11-15 10:02:49,,4867627,,4867627,,1,1,events|datadog,167,8.89087
261571,3,Monitoring,70086193,How to monitor different Spark jobs on the same cluster/SparkContext on Databricks?,"<p>I wanted to have a monitoring and alerting system in place (with a tool such as Datadog) that could fetch metrics and logs from my Spark applications in Databricks. Thing is, for not having to spin up, run and kill hundreds or even thousands of Job-clusters every day, it is  better to re-use existing clusters for similar Data Extraction jobs.</p>
<p>To fetch the metrics from Databricks and Spark in Datadog, I have tried the following:</p>
<ol>
<li>Change the <code>SparkSession.builder.appName</code> within each notebook: doesn't work, since it is not possible to change it after the cluster's started. By default it will always be &quot;Databricks Shell&quot;</li>
<li>Set a cluster-wide tag and unset it after the job has ended -&gt; can lead to mismatch between tags, when concurrency happens. Also, I didn't find a clear way to &quot;append&quot; a tag there.</li>
<li>Somehow fetch the Databricks' Job/Run Id from Datadog: I have no clue on how to do this.</li>
</ol>
<p>Seems to me that it would be feasible, since every spark job on the same SparkSession has the name of my Databricks' Job/Run id. I just have to understand how to identify it on Datadog.</p>
<p>Thoughts? Anything silly i might me missing to achieve this?</p>",70092779,2,0,,2021-11-23 18:40:11,1,2021-11-24 08:25:41,2021-11-23 19:45:00,,8830597,,8830597,,1,1,apache-spark|monitoring|databricks|metrics|datadog,41,8.85114
261572,3,Monitoring,57978390,Figure out the problematic index in ES cluster?,"<p>I have  elastic-search cluster which hosts more than 15 indices, I have a Datadog integration which shows me the below view of my elastic-search cluster.</p>

<p>We have alert integration with DD(datadog) which gives us alert if overall CPU usage goes beyond 60% and also in our application we start getting alerts when <strong>elasticsearch cluster is under stress</strong> as in this case our response time increases beyond a configures threshold.</p>

<p>Now my problem is how to know which indices are consuming the ES cluster resources most, so that we can fine either throttle the request from those indices or optimize their requests.</p>

<p>Some things which we did:</p>

<ol>
<li>Looked at the slow query log: Which doesn't give us the culprit as due to heavy load or CPU usage, we have slow queries log from almost all the big indices.</li>
<li>Like in the DD dashboard there is spike in the <code>bulk</code> queue, but this is overall and not specific to a particular ES indices.</li>
</ol>

<p>So my problem is very simple and all I want some metric from DD or elastic which can easily tell me which indices are consuming the most resources on a elastic-search cluster.</p>

<p><a href=""https://i.stack.imgur.com/3UAJG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3UAJG.png"" alt=""enter image description here""></a></p>",,1,0,,2019-9-17 16:25:15,,2019-9-17 16:52:44,,,,user12056260,,,1,0,performance|elasticsearch|monitoring|datadog,161,8.8273
261573,3,Monitoring,65348502,How to monitor failed AWS EMR Steps using Datadog,"<p>I was reading the <a href=""https://docs.datadoghq.com/integrations/amazon_emr/"" rel=""nofollow noreferrer"">Datadog docs</a> on how to monitor AWS Elastic Map Reduce using Datadog because I need to get metrics for failed EMR steps.
<a href=""https://i.stack.imgur.com/PYQy2.png"" rel=""nofollow noreferrer"">LIKE HERE</a></p>
<p>I think the most accurate metric is <a href=""https://i.stack.imgur.com/Ckxgp.png"" rel=""nofollow noreferrer"">aws.elasticmapreduce.jobs_failed</a> but as the image says, is only available for Hadoop V1, but I'm using Hadoop V2... so I don't see it in my <a href=""https://i.stack.imgur.com/U1Q7c.png"" rel=""nofollow noreferrer"">Datadog Metric Explorer</a></p>
<ul>
<li>Can someone help me?</li>
<li>Is there a replacement for that metric in Hadoop v2?</li>
<li>Is there another way to monitor failed EMR steps with Datadog?</li>
<li>The steps are Sqoop jobs</li>
</ul>",,0,0,,2020-12-17 21:19:53,,2020-12-17 21:19:53,,,,,14622925,,1,1,amazon-web-services|monitoring|amazon-emr|datadog,160,8.81648
261574,1,Parse,64033996,How do I extract a Value from a Key in an Array from a json object using jq?,"<p>Im writing a bash script that needs to make two calls, the second call needs a value from the json response of the first call.
However I am getting an Error when I attempt to retrieve this value.</p>
<blockquote>
<p>jq: error: Could not open file ............. Invalid argument</p>
</blockquote>
<p>I have checked my jq query in a browser and it works fine.
Here is an example of my script.</p>
<pre><code>api_key=&quot;key1&quot;
app_key=&quot;key2&quot;

response=$(curl -X POST \
-H 'Content-Type: application/json' \
-H &quot;DD-API-KEY: ${api_key}&quot; \
-H &quot;DD-APPLICATION-KEY: ${app_key}&quot; \
-d '{
    &quot;tests&quot;: [
        {
            &quot;public_id&quot;: &quot;abc-123-xyz&quot;,
            &quot;locations&quot;: [&quot;aws:eu-west-2&quot;]
         
        }
    ]
}' &quot;https://URL&quot;)

resultId=$(jq '.results[].result_id' $response)

curl -G \
    &quot;https://URL&quot; \
    -H &quot;DD-API-KEY: ${api_key}&quot; \
    -H &quot;DD-APPLICATION-KEY: ${app_key}&quot; \
    -d &quot;result_ids=[$resultId]&quot;
</code></pre>
<p>and this is an example of the json response</p>
<pre><code>{
    &quot;results&quot;: [
        {
            &quot;result_id&quot;: &quot;1234&quot;,
            &quot;public_id&quot;: &quot;abc-123-xyz&quot;,
            &quot;location&quot;: 1
        }
    ],
    &quot;triggered_check_ids&quot;: [
        &quot;abc-123-xyz&quot;
    ],
    &quot;locations&quot;: [
        {
            &quot;is_active&quot;: boolean,
            &quot;region&quot;: &quot;Somewhere&quot;,
            &quot;display_name&quot;: &quot;A1&quot;,
            &quot;id&quot;: 1,
            &quot;name&quot;: &quot;A1&quot;
        },
        {
            &quot;is_active&quot;: boolean,
            &quot;region&quot;: &quot;Somewhere else&quot;,
            &quot;display_name&quot;: &quot;B2&quot;,
            &quot;id&quot;: 2,
            &quot;name&quot;: &quot;B2&quot;
        }
    ]
}
</code></pre>",64034394,1,2,,2020-9-23 18:15:33,,2020-9-23 18:43:28,,,,,4563137,,1,0,bash|shell|jq|datadog,89,8.79756
261575,1,Method,63225377,Can I add tags to parent span Datadog,"<p>I'm working with express + graphql environment.
I want to add tags to express span with the value I derive while resolving the graphql query.
Currently, tags get added to the graphql span with the following code.</p>
<pre><code>let span = tracer.scope().active();
  if (span !== null) {
    span.setTag('queryname', queryName);
  } 
</code></pre>
<p>Let me know if there is a way to add these tags to the parent span instead of the current span.
This is required as I don't want to enable analytics on graphql since I already have it enabled in the express app.</p>
<p>Basically I want tags to root trace(express) instead of current span(graphql.execute).</p>",,0,0,,2020-8-3 07:28:55,1,2020-8-8 14:58:54,2020-8-8 14:57:34,,4386423,,4386423,,1,1,datadog,157,8.7836
261576,0,Integration,61888648,Helm: Datadog Agent with JDBC driver,"<p>I would like to use the <a href=""https://docs.datadoghq.com/integrations/oracle/"" rel=""nofollow noreferrer"">Datadog Oracle Integration</a> via the <a href=""https://github.com/helm/charts/tree/master/stable/datadog"" rel=""nofollow noreferrer"">Helm Chart Datadog</a>. Oracle Integration states <em>To use the Oracle integration, either install the Oracle Instant Client libraries, or download the Oracle JDBC Driver.</em></p>

<p>I do not want to use a custom image to package the JDBC-driver, I want to use a standard image such as tag:7-jmx. Other options that come to mind (e.g. EFS volume with the driver inside) seem to be an overkill also. </p>

<p>Best option to me seems to be an init container that downloads the JDBC driver. But Datadog Helm Chart does not support custom init containers for the agents.</p>

<p>What's the best way to do this? To get an Datadog Agent with a JDBC driver via Helm?</p>",,1,0,,2020-5-19 10:19:57,,2020-5-26 07:07:33,,,,,10300113,,1,0,kubernetes|kubernetes-helm|amazon-eks|datadog,155,8.76133
261577,0,Configuration,62819086,Communicating with a remote server via microsoft Teams,"<p><strong>Scenario:</strong></p>
<p>I have a remote server which is monitored (via DataDog) and sends out a warning when some anomaly is detected. This warning can be fetched via a webhook. Now I want to connect that webhook (<a href=""https://docs.datadoghq.com/integrations/webhooks/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/webhooks/</a>) with MS Teams (probably via Bot), to receive a warning. Then I want to send a command back to the remote server to resolve the warning.</p>
<p><strong>Technology:</strong></p>
<p>MS Teams, Python flask/Django, remote server</p>
<p><strong>Expected Results:</strong></p>
<p>I can receive a warning from my remote server to MS Teams via a bot. Then send a command back to the remote server. My initial plan is doing this using Python Flask/Django but not tied to a specific language.</p>
<p><strong>Environment:</strong></p>
<p>Remote server is a LINUX based system. we have a internal network that is used within our company, so might need to resolve a firewall problem potentially (idk whole lot about it tho).</p>
<p><strong>Things I have tried:</strong></p>
<p>I just want to see if this is possible or not, so i havent coded up any. But I found some information relevant to our problem:</p>
<p><a href=""https://docs.datadoghq.com/integrations/webhooks/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/webhooks/</a></p>
<p><a href=""https://docs.microsoft.com/en-us/microsoftteams/platform/bots/how-to/create-a-bot-for-teams"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/microsoftteams/platform/bots/how-to/create-a-bot-for-teams</a></p>
<p><a href=""https://docs.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/what-are-webhooks-and-connectors"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/what-are-webhooks-and-connectors</a></p>
<p>One last note, I am not also tied down to Teams bot. Our company also uses Azure Devops, so that is another resource I can use to realize the solution.</p>
<p>Btw, this question was posted on <a href=""https://docs.microsoft.com/en-us/answers/questions/44217/communicating-with-a-remote-server-via-teams.html"" rel=""nofollow noreferrer"">here</a> but was told to post on MSDN, but I couldn't find an appropriate forum. Hence, I am posting on stack overflow instead</p>",,1,3,,2020-7-9 16:07:30,,2020-7-12 17:27:19,2020-7-11 21:44:45,,10047933,,10047933,,1,0,webhooks|microsoft-teams|datadog,155,8.76133
261578,0,Integration,53469205,statsd's side effects possibly causing extra latency,"<p>I'm using Datadog's <a href=""https://github.com/DataDog/java-dogstatsd-client"" rel=""nofollow noreferrer"">statsd client</a> to record the duration of a certain server response. I used to pass in quite a few number of custom tags when <code>time</code>-ing these responses. So I'm in the process of reducing the number of custom tags.</p>

<p>However, the problem is that when I reduce the number of tags passed in, there is extra latency of server response, which isn't intuitive because I'm passing in fewer tags and the implementation hasn't changed. </p>

<p>According to Datadog and Etsy (which originally released <a href=""https://github.com/etsy/statsd"" rel=""nofollow noreferrer"">statsd</a>), these methods that record these metrics aren't blocking. However, they must be using some extra threads to perform this. </p>

<p>What could be the issue? Are there possible any side effects associated with using this client?</p>",54224944,2,0,,2018-11-25 15:54:38,,2019-1-16 20:34:23,,,,,5885013,,1,0,latency|side-effects|statsd|datadog,153,8.73877
261579,1,Method,51377442,Manually insert metrics to Datadog,"<p>I have file full of metrics:</p>

<pre><code>response:1|c|#environment:development,region:eu-central-1,version:1.0.15,type:container,rounded:200,exact:200
response.time:980395.597|g|#environment:development,region:eu-central-1,version:1.0.15,type:container
response.size:4632|g|#environment:development,region:eu-central-1,version:1.0.15,type:container
</code></pre>

<p>I would like to send them (once) to DataDog to create dashboard, because our infrastructure team didn't install DogStatsD agents on our containers yet.</p>",,1,0,,2018-7-17 09:11:26,0,2018-7-30 23:53:00,,,,,2446102,,1,0,statsd|datadog,151,8.71591
261580,0,Integration,68417072,AWS EB Multi-container Datadog integration not working properly,"<p>I have the following Dockerrun.aws.json:</p>
<pre><code>{
    &quot;AWSEBDockerrunVersion&quot;: 2,
    &quot;containerDefinitions&quot;: [
        {
            &quot;name&quot;: &quot;dd-agent&quot;,
            &quot;image&quot;: &quot;gcr.io/datadoghq/agent:7.25.1&quot;,
            &quot;environment&quot;: [
                {
                    &quot;name&quot;: &quot;DD_API_KEY&quot;,
                    &quot;value&quot;: &quot;xxxxx&quot;
                },
                {
                    &quot;name&quot;: &quot;DD_ENV&quot;,
                    &quot;value&quot;: &quot;development&quot;
                },
                {
                    &quot;name&quot;: &quot;DD_SERVICE&quot;,
                    &quot;value&quot;: &quot;xxxx&quot;
                },
                {
                    &quot;name&quot;: &quot;DD_VERSION&quot;,
                    &quot;value&quot;: &quot;1.0.0&quot;
                },
                {
                    &quot;name&quot;: &quot;DD_SITE&quot;,
                    &quot;value&quot;: &quot;datadoghq.com&quot;
                },
                {
                    &quot;name&quot;: &quot;DD_TAGS&quot;,
                    &quot;value&quot;: &quot;app:xxxxx&quot;
                },
                {
                    &quot;name&quot;: &quot;DD_LOGS_ENABLED&quot;,
                    &quot;value&quot;: &quot;true&quot;
                },
                {
                    &quot;name&quot; : &quot;DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL&quot;,
                    &quot;value&quot;: &quot;true&quot;
                },
                {
                    &quot;name&quot; : &quot;DD_LOGS_CONFIG_DOCKER_CONTAINER_USE_FILE&quot;,
                    &quot;value&quot;: &quot;true&quot;
                },
                {
                    &quot;name&quot; : &quot;DD_CONTAINER_EXCLUDE&quot;,
                    &quot;value&quot;: &quot;name:datadog-agent&quot;
                },
                {
                    &quot;name&quot;: &quot;DD_APM_ENABLED&quot;,
                    &quot;value&quot;: &quot;true&quot;
                },
                {
                    &quot;name&quot;: &quot;DD_APM_NON_LOCAL_TRAFFIC&quot;,
                    &quot;value&quot;: &quot;true&quot;
                },
                {
                    &quot;name&quot;: &quot;DD_CATEGORY&quot;,
                    &quot;value&quot;: &quot;sourcecode&quot;
                }
            ],
            &quot;portMappings&quot;: [
                {
                    &quot;hostPort&quot;: 8126,
                    &quot;containerPort&quot;: 8126
                }
            ],
            &quot;memory&quot;: 200,
            &quot;mountPoints&quot;: [
                {
                    &quot;sourceVolume&quot;: &quot;docker_sock&quot;,
                    &quot;containerPath&quot;: &quot;/var/run/docker.sock&quot;,
                    &quot;readOnly&quot;: false
                },
                {
                    &quot;sourceVolume&quot;: &quot;proc&quot;,
                    &quot;containerPath&quot;: &quot;/host/proc&quot;,
                    &quot;readOnly&quot;: true
                },
                {
                    &quot;sourceVolume&quot;: &quot;cgroup&quot;,
                    &quot;containerPath&quot;: &quot;/host/sys/fs/cgroup&quot;,
                    &quot;readOnly&quot;: true
                },
                {
                    &quot;sourceVolume&quot;: &quot;awseb-logs-nginx-proxy&quot;,
                    &quot;containerPath&quot;: &quot;/var/log/nginx&quot;,
                    &quot;readOnly&quot;: true
                },
                {
                    &quot;sourceVolume&quot;: &quot;php-logs&quot;,
                    &quot;containerPath&quot;: &quot;/var/log/php-app&quot;,
                    &quot;readOnly&quot;: true
                },
                {
                    &quot;sourceVolume&quot;: &quot;php-log-integration-config&quot;,
                    &quot;containerPath&quot;: &quot;/etc/datadog-agent/conf.d/php.d/conf.yaml&quot;,
                    &quot;readOnly&quot;:true
                }
            ]
        },
        {
            &quot;essential&quot;: true,
            &quot;image&quot;: &quot;649208843984.dkr.ecr.us-west-2.amazonaws.com/php7:2.0.1&quot;,
            &quot;memory&quot;: 2048,
            &quot;mountPoints&quot;: [
                {
                    &quot;containerPath&quot;: &quot;/var/www/html&quot;,
                    &quot;readOnly&quot;: true,
                    &quot;sourceVolume&quot;: &quot;php-app&quot;
                },
                {
                    &quot;containerPath&quot;: &quot;/apns&quot;,
                    &quot;readOnly&quot;: true,
                    &quot;sourceVolume&quot;: &quot;apns&quot;
                },
                {
                    &quot;containerPath&quot;: &quot;/var/log/php-app&quot;,
                    &quot;sourceVolume&quot;: &quot;awseb-logs-php-app&quot;
                },
                {
                    &quot;containerPath&quot;: &quot;/tmp&quot;,
                    &quot;readOnly&quot;: false,
                    &quot;sourceVolume&quot;: &quot;tmp&quot;
                },
                {
                    &quot;containerPath&quot;: &quot;/jwt&quot;,
                    &quot;readOnly&quot;: true,
                    &quot;sourceVolume&quot;: &quot;jwt&quot;
                }
            ],
            &quot;Update&quot;: &quot;true&quot;,
            &quot;name&quot;: &quot;php-app&quot;,
            &quot;links&quot;: [
                &quot;dd-agent:dd-agent&quot;
            ]
        },
        {
            &quot;essential&quot;: true,
            &quot;image&quot;: &quot;nginx&quot;,
            &quot;links&quot;: [
                &quot;php-app&quot;,
                &quot;dd-agent:dd-agent&quot;
            ],
            &quot;memory&quot;: 1536,
            &quot;mountPoints&quot;: [
                {
                    &quot;containerPath&quot;: &quot;/var/www/html&quot;,
                    &quot;readOnly&quot;: true,
                    &quot;sourceVolume&quot;: &quot;php-app&quot;
                },
                {
                    &quot;containerPath&quot;: &quot;/etc/nginx/conf.d&quot;,
                    &quot;readOnly&quot;: true,
                    &quot;sourceVolume&quot;: &quot;nginx-proxy-conf&quot;
                },
                {
                    &quot;containerPath&quot;: &quot;/var/log/nginx&quot;,
                    &quot;sourceVolume&quot;: &quot;awseb-logs-nginx-proxy&quot;
                }
            ],
            &quot;name&quot;: &quot;nginx-proxy&quot;,
            &quot;portMappings&quot;: [
                {
                    &quot;containerPort&quot;: 80,
                    &quot;hostPort&quot;: 80
                }
            ]
        }
    ],
    &quot;volumes&quot;: [
        {
            &quot;name&quot;: &quot;apns&quot;,
            &quot;host&quot;: {
                &quot;sourcePath&quot;: &quot;/apns&quot;
            }
        },
        {
            &quot;name&quot;: &quot;php-logs&quot;,
            &quot;host&quot;: {
                &quot;sourcePath&quot;: &quot;/var/log/containers/php-app&quot;
            }
        },
        {
            &quot;name&quot;: &quot;php-app&quot;,
            &quot;host&quot;: {
                &quot;sourcePath&quot;: &quot;/var/app/current/php-app&quot;
            }
        },
        {
            &quot;name&quot;: &quot;nginx-proxy-conf&quot;,
            &quot;host&quot;: {
                &quot;sourcePath&quot;: &quot;/var/app/current/proxy/conf.d&quot;
            }
        },
        {
            &quot;name&quot;: &quot;tmp&quot;,
            &quot;host&quot;: {
                &quot;sourcePath&quot;: &quot;/tmp&quot;
            }
        },
        {
            &quot;name&quot;: &quot;jwt&quot;,
            &quot;host&quot;: {
                &quot;sourcePath&quot;: &quot;/jwt&quot;
            }
        },
        {
            &quot;name&quot;: &quot;docker_sock&quot;,
            &quot;host&quot;: {
                &quot;sourcePath&quot;: &quot;/var/run/docker.sock&quot;
            }
        },
        {
            &quot;name&quot;: &quot;proc&quot;,
            &quot;host&quot;: {
                &quot;sourcePath&quot;: &quot;/proc/&quot;
            }
        },
        {
            &quot;name&quot;: &quot;cgroup&quot;,
            &quot;host&quot;: {
                &quot;sourcePath&quot;: &quot;/cgroup/&quot;
            }
        },
        {
            &quot;name&quot;: &quot;php-log-integration-config&quot;,
            &quot;host&quot;: {
                &quot;sourcePath&quot;: &quot;/datadog/php.d/conf.yaml&quot;
            }
        }
    ]
}
</code></pre>
<p>Datadog agent logs shows this after deploy:</p>
<pre><code>2021-07-17 01:30:48 UTC | CORE | INFO | (pkg/serializer/serializer.go:356 in sendMetadata) | Sent metadata payload, size (raw/compressed): 1146/310 bytes.
2021-07-17 01:30:49 UTC | PROCESS | INFO | (collector.go:208 in func1) | Delivery queues: process[size=0, weight=0], pod[size=0, weight=0]
2021-07-17 01:30:55 UTC | CORE | INFO | (pkg/serializer/serializer.go:376 in SendJSONToV1Intake) | Sent processes metadata payload, size: 1394 bytes.
2021-07-17 01:31:19 UTC | TRACE | INFO | (pkg/trace/info/stats.go:101 in LogStats) | No data received
2021-07-17 01:31:49 UTC | PROCESS | INFO | (collector.go:208 in func1) | Delivery queues: process[size=0, weight=0], pod[size=0, weight=0]
2021-07-17 01:32:19 UTC | TRACE | INFO | (pkg/trace/info/stats.go:101 in LogStats) | No data received
2021-07-17 01:32:19 UTC | PROCESS | INFO | (collector.go:160 in runCheck) | Finish container check #340 in 27.457254ms
2021-07-17 01:32:48 UTC | CORE | INFO | (cmd/agent/app/run.go:110 in func2) | Received signal 'terminated', shutting down...
2021-07-17 01:32:48 UTC | CORE | INFO | (pkg/collector/runner/runner.go:149 in Stop) | Runner is shutting down...
2021-07-17 01:32:48 UTC | CORE | INFO | (pkg/collector/python/subprocesses.go:48 in TerminateRunningProcesses) | Canceling all running python subprocesses
2021-07-17 01:32:48 UTC | PROCESS | INFO | (pkg/process/util/signal_nowindows.go:21 in HandleSignals) | Caught signal 'terminated'; terminating.
2021-07-17 01:32:48 UTC | CORE | INFO | (pkg/aggregator/aggregator.go:667 in run) | Stopping aggregator
2021-07-17 01:32:48 UTC | PROCESS | INFO | (pkg/forwarder/forwarder.go:389 in Stop) | stopping the Forwarder
2021-07-17 01:32:48 UTC | TRACE | INFO | (main.go:25 in handleSignal) | received signal 15 (terminated)
2021-07-17 01:32:48 UTC | TRACE | INFO | (pkg/trace/agent/agent.go:133 in loop) | Exiting...
2021-07-17 01:32:48 UTC | PROCESS | INFO | (pkg/forwarder/domain_forwarder.go:278 in Stop) | domainForwarder stopped
2021-07-17 01:32:48 UTC | PROCESS | INFO | (pkg/forwarder/forwarder.go:389 in Stop) | stopping the Forwarder
2021-07-17 01:32:48 UTC | TRACE | INFO | (pkg/trace/stats/concentrator.go:100 in Run) | Exiting concentrator, computing remaining stats
2021-07-17 01:32:48 UTC | PROCESS | INFO | (pkg/forwarder/domain_forwarder.go:278 in Stop) | domainForwarder stopped
2021-07-17 01:32:48 UTC | PROCESS | INFO | (pkg/forwarder/forwarder.go:389 in Stop) | stopping the Forwarder
2021-07-17 01:32:48 UTC | CORE | INFO | (pkg/forwarder/forwarder.go:389 in Stop) | stopping the Forwarder
trace-agent exited with code 0, disabling
process-agent exited with code 0, disabling
[cont-finish.d] executing container finish scripts...
[cont-finish.d] done.
[s6-finish] waiting for services.
2021-07-17 01:32:48 UTC | CORE | INFO | (pkg/forwarder/domain_forwarder.go:278 in Stop) | domainForwarder stopped
2021-07-17 01:32:48 UTC | CORE | INFO | (pkg/logs/logs.go:128 in Stop) | Stopping logs-agent
2021-07-17 01:32:48 UTC | CORE | INFO | (pkg/logs/input/docker/tailer.go:84 in Stop) | Stop tailing container: 8bbd4d72395c
2021-07-17 01:32:48 UTC | CORE | INFO | (pkg/logs/input/docker/tailer.go:84 in Stop) | Stop tailing container: 83c7171252ba
2021-07-17 01:32:48 UTC | CORE | INFO | (pkg/logs/input/docker/tailer.go:84 in Stop) | Stop tailing container: c9e5568bb4b2
2021-07-17 01:32:48 UTC | CORE | INFO | (pkg/logs/input/docker/tailer.go:84 in Stop) | Stop tailing container: eeb5f67f9fea
2021-07-17 01:32:49 UTC | CORE | INFO | (pkg/logs/logs.go:140 in Stop) | logs-agent stopped
2021-07-17 01:32:49 UTC | CORE | INFO | (cmd/agent/app/run.go:423 in StopAgent) | See ya!
AGENT EXITED WITH CODE 0, SIGNAL 0, KILLING CONTAINER
[s6-finish] sending all processes the TERM signal.
[s6-finish] sending all processes the KILL signal and exiting.
</code></pre>
<p>I can't see neither APM nor Logs (which are configured both as files and stdout)</p>",,0,0,,2021-7-17 03:00:51,,2021-7-17 03:00:51,,,,,260610,,1,0,amazon-web-services|docker|amazon-elastic-beanstalk|datadog|monolog,149,8.69275
261581,1,Method,69020138,DataDog log collection in AKS Azure Kubernetes containerd,"<p>I have successfully set up the DataDog agent to collect console logs from applications running in my AKS clusters where the Kubernetes version is less than 1.19.  However, I have not been able to get the agent to report logs from my new cluster which is Kubernetes 1.20.  The node agent runs without crashing, and DataDog picks up events and metrics from the cluster, but no logs.</p>
<p>I have followed DataDog's documentation for installing the latest version of DataDog via Helm with containerd support:</p>
<p>values.yaml modifications:</p>
<pre><code># datadog.criSocketPath -- Path to the container runtime socket (if different from Docker)
criSocketPath:  /var/run/containerd/containerd.sock

env:
    - name: DD_KUBERNETES_KUBELET_HOST
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: DD_KUBELET_CLIENT_CA
      value: &quot;/host/etc/kubernetes/certs/kubeletserver.crt&quot;

# agents.volumes -- Specify additional volumes to mount in the dd-agent container
volumes:
  - hostPath:
      path: /host/etc/kubernetes/certs/
    name: certs

# agents.volumeMounts -- Specify additional volumes to mount in all containers of the agent pod
volumeMounts:
- name: certs
  mountPath: /host/etc/kubernetes/certs/
  readOnly: true
</code></pre>
<p>I have explored the node via a privileged debug container to verify the correct location of the kubelet cert.</p>
<p>This is a standard AKS cluster that I just created with the Portal UI.</p>
<p>DataDog support has not been very helpful, acting like each situation is unique.  This AKS cluster is just out-of-the-box, as is the DataDog installation, so why am I the only one having trouble getting my logs in the new Kubernetes version?</p>",,0,0,,2021-9-1 20:20:30,,2021-9-1 20:20:30,,,,,8615710,,1,1,kubernetes|azure-aks|datadog,148,8.68105
261582,1,Error,65511552,Datadog Trace with Grpc Kotlin Coroutines not working,"<p>As the title suggests <code>@Trace</code> annotation is not working with Kotlin Grpc Coroutines. Is there a way to make it work?</p>
<pre><code>import datadog.trace.api.Trace
...
class FaaBarGrpcCoroutineService() : FaaBarServiceCoroutineImplBase()  {
...
    @Trace(resourceName = &quot;Foo&quot;, operationName = &quot;Bar&quot;)
    override suspend fun bar(request: FooRequest): BarResponse = ...
</code></pre>
<p>Unfortunately this gives no error or warning.
Can I construct Trace programatically and would it work for a <code>suspend</code> function?</p>",,0,0,,2020-12-30 18:05:03,,2021-1-4 08:31:50,,,,,944768,,1,0,kotlin|grpc|kotlin-coroutines|datadog|grpc-kotlin,148,8.68105
261583,1,Method,68917956,Read Flink latency tracking metric in Datadog,"<p>I'm following this doc <a href=""https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/ops/metrics/#end-to-end-latency-tracking"" rel=""nofollow noreferrer"">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/ops/metrics/#end-to-end-latency-tracking</a> and enabled metrics.latency.interval in flink-conf.yaml as shown below:</p>
<pre><code>metrics.latency.interval: 60000
metrics.latency.granularity: operator

</code></pre>
<p>Now, I have the following questions:</p>
<ol>
<li><p>how could I know what kind of metrics(a list of metrics name) are enabled? I didn't find any in metrics UI.
<a href=""https://i.stack.imgur.com/s3rFC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s3rFC.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Datadog is my reporter, will the latency metrics send to Datadog just like other system metrics listed here <a href=""https://docs.datadoghq.com/integrations/flink/#data-collected"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/flink/#data-collected</a>? If yes, what's their name? If no, is there anything I need to do to get them in Datadog?</p>
</li>
</ol>
<p>I'm new to the Flink and the Datadog.Many thanks!</p>",,2,0,,2021-8-25 06:37:43,,2021-8-27 01:59:52,,,,,7876879,,1,0,apache-flink|flink-streaming|datadog,81,8.63394
261584,0,Configuration,68269296,How to config a Datadog check for a postgres db running in a docker container?,"<p>How to i create check of postgres running on a docker container for DataDog?</p>
<p>So i have followed the necessary steps to create a datadog user and permissions in the postgres database. I have also set up the config which is below</p>
<pre class=""lang-yaml prettyprint-override""><code>  postgres:
    init_config:
    instanceS:
      - host: localhost
        port: 5432
        password: password
</code></pre>
<p>now when i run the check manually to see if it has worked it return the error</p>
<pre class=""lang-sh prettyprint-override""><code>error: py.loader: could not configure check 'postgres (7.0.2)': could not invoke 'postgres' python check constructor. New constructor API returned:
Traceback (most recent call last):
  File &quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/postgres/postgres.py&quot;, line 42, in __init__
    self._config = PostgresConfig(self.instance)
  File &quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/postgres/config.py&quot;, line 35, in __init__
    raise ConfigurationError('Please specify a user to connect to Postgres.')
datadog_checks.base.errors.ConfigurationError: Please specify a user to connect to Postgres.
Deprecated constructor API returned:
__init__() got an unexpected keyword argument 'agentConfig'
error: Unable to load the check: unable to load any check from config 'postgres'

Error: could not load postgres:
* JMX Check Loader: check is not a jmx check, or unable to determine if it's so
* Python Check Loader: could not configure check instance for python check postgres: could not invoke 'postgres' python check constructor. New constructor API returned:
Traceback (most recent call last):
  File &quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/postgres/postgres.py&quot;, line 42, in __init__
    self._config = PostgresConfig(self.instance)
  File &quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/postgres/config.py&quot;, line 35, in __init__
    raise ConfigurationError('Please specify a user to connect to Postgres.')
datadog_checks.base.errors.ConfigurationError: Please specify a user to connect to Postgres.
Deprecated constructor API returned:
__init__() got an unexpected keyword argument 'agentConfig'
* Core Check Loader: Check postgres not found in Catalog
Error: no valid check found
</code></pre>",,0,0,,2021-7-6 10:56:40,,2021-7-6 13:26:02,2021-7-6 13:26:02,,15267554,,15267554,,1,0,postgresql|docker|datadog,143,8.62134
261585,1,Error,67666070,Datadog could not invoke kafka_consumer python check constructor,"<p>I am getting the Loading Errors while integrating Kafka with <a href=""https://www.datadoghq.com/blog/monitor-kafka-with-datadog/"" rel=""nofollow noreferrer"">Datadog</a>.</p>
<pre><code>could not invoke 'kafka_consumer' python check constructor. New constructor API returned:
Traceback (most recent call last):
  File &quot;/opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/kafka_consumer/kafka_consumer.py&quot;, line 54, in __new__
    kafka_version = cls._determine_kafka_version(init_config, instance)
.
.
.
Python Check Loader:
        could not configure check instance for python check kafka_consumer: could not invoke 'kafka_consumer' python check constructor. New constructor API returned:
Traceback (most recent call last):
</code></pre>
<p>The config file is base on an <a href=""https://github.com/DataDog/integrations-core/blob/master/kafka_consumer/datadog_checks/kafka_consumer/data/conf.yaml.example"" rel=""nofollow noreferrer"">example.config</a></p>
<p>kafka_consumer version is <code>2.6.1</code>.</p>",,1,0,,2021-5-24 02:39:59,,2021-5-24 02:55:59,,,,,3288890,,1,1,kafka-consumer-api|datadog,126,8.60148
261586,1,Parse,63404608,How to filter statsd metrics in the DataDog agent?,"<p>We're using the DataDog agent with some .Net Core micro-services on Linux. We want to send our statsd metrics directly through the DataDog agent but we need to do some filtering before the agent sends the metrics to DataDog.</p>
<p>All I could find was the following: <a href=""https://docs.datadoghq.com/tracing/custom_instrumentation/agent_customization/?tab=mongodb"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/tracing/custom_instrumentation/agent_customization/?tab=mongodb</a></p>
<p>That all appears to be about logging and spans, not metrics.</p>
<p>Is it possible to filter the statsd metrics?</p>",,1,0,,2020-8-13 23:39:00,,2021-5-7 03:56:11,,,,,857025,,1,0,datadog,140,8.58451
261587,0,Integration,69311886,How to setup DataDog alerts via AWS CDK?,<p>Is it possible to create DataDog alerts &amp; notifications in IaC way via AWS CDK? I want to send notifications in case of any HTTP 500 errors my web service emits.</p>,,1,0,,2021-9-24 08:30:38,,2021-9-28 21:02:47,,,,,4238587,,1,1,amazon-web-services|aws-cdk|datadog|infrastructure-as-code,69,8.5554
261588,2,Query,60290897,Is it possible to construct a datadog graph with a metric that matches two values for one tag?,"<p>I <em>think</em> this should be possible, but I can't find documented syntax. I would like to construct a DD graph that displays metrics matching tag_one:A or tag_one:B. Is this possible? If so, what is the syntax?</p>",64168376,1,0,,2020-2-18 23:28:07,,2020-10-2 08:24:59,,,,,2884555,,1,3,datadog,94,8.49251
261589,1,Error,55393658,Error `Cross-device link` when set logrotate mode `create 0644 root root` in kubernetes,"<p>In order to rotate pod logs, <a href=""https://github.com/blacklabelops/logrotate"" rel=""nofollow noreferrer"">logrotate pod</a> is used in kubernetes. By default, logrotate will use <code>copytruncate</code> mode to create a new rotated file, however <a href=""https://www.datadoghq.com/"" rel=""nofollow noreferrer""><code>datadog</code></a> cannot work well in this mode, failed to retrieve pod log occasionally.</p>

<p>So I try to set the mode to <code>create 0644 root root</code>, but the following error comes up</p>

<p><code>Cross-device link</code></p>

<p>And <code>logrotate</code> failed to rotate logs either.</p>

<p>Could someone help me to resolve this issue or is there any better practice on rotate pod logs?</p>",,0,2,,2019-3-28 09:03:41,,2019-3-28 09:03:41,,,,,3011380,,1,0,docker|kubernetes|logrotate|datadog,132,8.4823
261590,1,Method,57524158,datadog: set traceId manually,"<p>I am reading this <a href=""https://docs.datadoghq.com/tracing/advanced/manual_instrumentation/?tab=python"" rel=""nofollow noreferrer"">tutorial</a>:</p>

<p>I would like to set <code>traceId</code> be equal <code>requestId</code>. How can I do it using <code>ddtrace</code> api?</p>",,0,1,,2019-8-16 11:50:53,,2019-8-16 12:20:45,2019-8-16 12:20:45,,3014866,,3014866,,1,1,python|datadog,132,8.4823
261591,0,Integration,63165346,Forward Jaeger traces to Datadog,"<p>Is there a way of getting Jaeger traces to Datadog, whether it be through a proxy, scraping traces from Jager and converting them to DD Traces, etc...</p>
<p>We have a vendor provided backend that only supports Jaeger, but the enterprise APM solution is Datadog.</p>
<p>Thanks!</p>",,0,0,,2020-7-30 02:39:45,,2020-7-30 02:39:45,,,,,3064090,,1,1,trace|datadog|jaeger,132,8.4823
261592,3,Monitoring,57870216,"How to mute Monitor in Datadog, when Windows Service has status Disabled?","<p>Is it possible to automatically mute created Datadog Monitor(which is monitoring Windows Service), when Windows service state was changed from Automatic to Disabled?
How can it be configured
Thanks.</p>",,0,3,,2019-9-10 11:52:10,,2019-9-10 11:52:10,,,,,4582679,,1,0,datadog,131,8.46908
261593,0,Configuration,60413506,Datadog checks not appearing in Datadog console,"<p>This is the installation path of datadog <code>/etc/datadog-agent</code>
Under this we have checks under folder <code>/etc/datadog-agent/conf.d</code></p>

<p>Under this we have defined a service to report disk space alert under disk.d
<code>/etc/datadog-agent/conf.d/disk.d</code></p>

<pre><code>disk.d]# ls *.yaml
disk.yaml
</code></pre>

<p>We have the file ready in the configuration. We did tried to reload the datadog agent to reflect the changes.</p>

<p>The expected scenario is it should reflect in datadog console under the service defined 
The query we are using is </p>

<pre><code>avg(last_5m):max:system.disk.in_use{esb,nonproduction} by {host,device} &gt; 0.9
</code></pre>

<p>But we are unable to establish anything.</p>

<p>Nutshell none of the alerts configured for this host are not reflecting in datadog console.</p>",,0,1,,2020-2-26 12:02:31,,2020-2-26 15:06:39,2020-2-26 15:06:39,,2377164,,12966427,,1,2,datadog,131,8.46908
261594,3,Visualization,66377430,Data Dog: parse logs and create dashboards like SUMO,"<p>During years I used to work with SUMO. I used to do things like:</p>
<pre><code>_sourceCategory=apache 
| parse &quot;GET * &quot; as url 
| count by url
</code></pre>
<p>And from there you would add time slices, to count on time basis. And then you could also integrate the results with some nice charts like time series, pie chart. All within seconds.</p>
<p>Now, I forced to use DataDog. Imagine you have errors like:</p>
<pre><code>Error in house    
Error in car
Error in street
Error in house
Error in house
Error in car
Error in house
Error in street
</code></pre>
<p>I would like to count errors by place. In this case:</p>
<pre><code>PLACE | COUNT
-------------
house |   4
car   |   2
street|   2
</code></pre>
<p>Well I can't do it with Data Dog. I hate it. I read about Pipeline, Processors, Metrics. I can't believe this is so difficult compared with SUMO.</p>
<p>I have a Pipeline, who parses my logs. Then I can see all the fields being parsed. But how can I create a metric from there? My fields are strings, so I can't &quot;measure&quot; them. I just need to count how many &quot;house&quot; errors happened. And so on with some other strings.</p>",,0,0,,2021-2-25 22:28:05,,2021-4-19 21:01:18,2021-4-19 21:01:18,,3149444,,3149444,,1,0,parsing|logging|datadog,130,8.45577
261595,1,Error,67600281,Can not use log4net.Ext.Json to make Json log in Sitecore related project,"<p>As we know that Sitecore has capsulated log4net as default logging functionality(Sitecore.Logging). My task is to use log4net.Ext.Json Layout to create Json log and feed it from Azure to Datadog APM.
Corresponding useful links are as follows:
<a href=""https://docs.datadoghq.com/logs/log_collection/csharp/?tab=log4net"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/log_collection/csharp/?tab=log4net</a>
<a href=""https://docs.datadoghq.com/tracing/connect_logs_and_traces/dotnet?tab=log4net"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/tracing/connect_logs_and_traces/dotnet?tab=log4net</a>
<a href=""https://github.com/DataDog/dd-trace-dotnet/tree/master/samples/AutomaticTraceIdInjection/Log4NetExample"" rel=""nofollow noreferrer"">https://github.com/DataDog/dd-trace-dotnet/tree/master/samples/AutomaticTraceIdInjection/Log4NetExample</a></p>
<p>While during last 2 weeks, I failed to achieve it:</p>
<p>Try 1: Add log4net.ext.Json(ver. 2.0.8.3) NuGet Package in Visual Studio. Since Sitecore has capsulated log4net(ver. 2.0.9), so I do not need to add log4net NuGet package.</p>
<p>Then I configured config file as follows:</p>
<pre><code>&lt;appender name=&quot;JsonFileAppender&quot; type=&quot;log4net.Appender.RollingFileAppender&quot;&gt;
      &lt;file value=&quot;$(dataFolder)/logs/json_log/json_log.json&quot; /&gt;
      &lt;appendToFile value=&quot;true&quot; /&gt;
      &lt;maxSizeRollBackups value=&quot;10&quot; /&gt;
      &lt;maximumFileSize value=&quot;100MB&quot; /&gt;
      &lt;!--json formatted log4net logging--&gt;
      &lt;layout type='log4net.Layout.SerializedLayout, log4net.Ext.Json'&gt;
          &lt;decorator type=&quot;log4net.Layout.Decorators.StandardTypesDecorator, log4net.Ext.Json&quot; /&gt;
          &lt;!--explicit default members--&gt;
          &lt;default /&gt;
          &lt;!--remove the default preformatted message member--&gt;
          &lt;remove value=&quot;ndc&quot; /&gt;
          &lt;remove value=&quot;message&quot; /&gt;
          &lt;!--add raw message--&gt;
          &lt;member value=&quot;message:messageobject&quot; /&gt;
          &lt;!--add value='properties' to emit Datadog properties --&gt;
          &lt;member value='properties'/&gt;
          &lt;member value='dd.env' /&gt;
          &lt;member value='dd.service' /&gt;
          &lt;member value='dd.version' /&gt;
          &lt;member value='dd.trace_id' /&gt;
          &lt;member value='dd.span_id' /&gt;
      &lt;/layout&gt;
&lt;/appender&gt;
&lt;root&gt;
      &lt;level value='INFO'/&gt;
      &lt;appender-ref ref=&quot;JsonFileAppender&quot;/&gt;
&lt;/root&gt;
</code></pre>
<p>The result is that Json file is created, while no content written into the log file.</p>
<p>Try 2: I downloaded log4net.Ext.Json source code from official gitlab site, and create an empty project and involved all necessary C# source files of the log4net.Ext.Json into my solution. I also renamed the project name to log4netExtJson, which will not make conflict to existing Out-of-the-box log4net.Ext.Json.dll. To make log4net.Core working, I also have to add log4net(ver. 2.0.12) NuGet package. The source code is as follows:</p>
<p><a href=""https://gitlab.com/reeloadead/log4net.Ext.Json#installation"" rel=""nofollow noreferrer"">https://gitlab.com/reeloadead/log4net.Ext.Json#installation</a></p>
<p>And the config file I changed to:</p>
<pre><code>&lt;appender name=&quot;JsonFileAppender&quot; type=&quot;log4net.Appender.RollingFileAppender&quot;&gt;
      &lt;file value=&quot;$(dataFolder)/logs/json_log/json_log.json&quot; /&gt;
      &lt;appendToFile value=&quot;true&quot; /&gt;
      &lt;maxSizeRollBackups value=&quot;10&quot; /&gt;
      &lt;maximumFileSize value=&quot;100MB&quot; /&gt;
      &lt;!--json formatted log4net logging--&gt;
      &lt;layout type='log4net.Layout.SerializedLayout, log4netExtJson'&gt;
          &lt;decorator type=&quot;log4net.Layout.Decorators.StandardTypesDecorator, log4netExtJson&quot; /&gt;
          &lt;!--explicit default members--&gt;
          &lt;default /&gt;
          &lt;!--remove the default preformatted message member--&gt;
          &lt;remove value=&quot;ndc&quot; /&gt;
          &lt;remove value=&quot;message&quot; /&gt;
          &lt;!--add raw message--&gt;
          &lt;member value=&quot;message:messageobject&quot; /&gt;
          &lt;!--add value='properties' to emit Datadog properties --&gt;
          &lt;member value='properties'/&gt;
          &lt;member value='dd.env' /&gt;
          &lt;member value='dd.service' /&gt;
          &lt;member value='dd.version' /&gt;
          &lt;member value='dd.trace_id' /&gt;
          &lt;member value='dd.span_id' /&gt;
      &lt;/layout&gt;
&lt;/appender&gt;
&lt;root&gt;
      &lt;level value='INFO'/&gt;
      &lt;appender-ref ref=&quot;JsonFileAppender&quot;/&gt;
&lt;/root&gt;
</code></pre>
<p>The outcome of this round is that even no json log file is created.</p>
<p>Try 3: I contacted Sitecore Support portal and got response that they did not tested log4net.Ext.Json package, and they need me to try SitecoreLoggingExtensions NuGet package as a workaround. Source codes is as follows:</p>
<p><a href=""https://github.com/sitecoreops/sitecore-logging-extensions"" rel=""nofollow noreferrer"">https://github.com/sitecoreops/sitecore-logging-extensions</a></p>
<p>The config file I wrote is as follows:</p>
<pre><code>&lt;appender name=&quot;JsonFileAppender&quot; type=&quot;log4net.Appender.RollingFileAppender&quot;&gt;
      &lt;file value=&quot;$(dataFolder)/logs/json_log/json_log.json&quot; /&gt;
      &lt;appendToFile value=&quot;true&quot; /&gt;
      &lt;maxSizeRollBackups value=&quot;10&quot; /&gt;
      &lt;maximumFileSize value=&quot;100MB&quot; /&gt;
      &lt;!--json formatted log4net logging--&gt;
      &lt;!--SerializedLayout--&gt;
      &lt;layout type='SitecoreLoggingExtensions.JsonLayout, SitecoreLoggingExtensions'&gt;  
          &lt;!--explicit default members--&gt;
          &lt;default /&gt;
          &lt;!--add value='properties' to emit Datadog properties --&gt;
          &lt;member value='properties'/&gt;
          &lt;!-- Manual changes: start --&gt;
          &lt;member value='dd.env' /&gt;
          &lt;member value='dd.service' /&gt;
          &lt;member value='dd.version' /&gt;
          &lt;member value='dd.trace_id' /&gt;
          &lt;member value='dd.span_id' /&gt;
          &lt;!-- Manual changes: end --&gt;
      &lt;/layout&gt;
&lt;/appender&gt;
&lt;root&gt;
      &lt;!--&lt;level value='INFO'/&gt;--&gt;
      &lt;appender-ref ref=&quot;JsonLogFileAppender&quot;/&gt;
&lt;/root&gt;
</code></pre>
<p>The Json log with content is finally created! But after I made the setting on Azure, there is no connection with Datadog with log file. My guess is that the Json log with the Layout of SitecoreLoggingExtensions is not compatible with Datadog. So this way is not useful for my need.</p>
<p>I wonder if someone has successfully configured log4net.Ext.Json package in Sitecore related project before.</p>",,1,0,,2021-5-19 09:14:40,,2021-12-1 08:57:12,,,,,15969484,,1,0,json|logging|sitecore|datadog|log4net-ext-json,129,8.44236
261596,0,Configuration,68517266,Use deployment label in containers env,"<p>I have a small problem with the labels to put in my pods.
I use an EKS cluster and I followed the instructions to set up the Datadog APM. In the tutorial at <a href=""https://app.datadoghq.com/apm/docs?architecture=container-based"" rel=""nofollow noreferrer"">this address</a>.</p>
<p>It is indicated to put these labels in my deployment:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment

labels:
  tags.datadoghq.com/env: &quot;&lt;environment&gt;&quot;
  tags.datadoghq.com/service: &quot;&lt;service&gt;&quot;
  tags.datadoghq.com/version: &quot;&lt;version&gt;&quot;
</code></pre>
<p>And these environment variables (in the container spec):</p>
<pre><code>env :
        - name : DD_AGENT_HOST
          valeurFrom :
            fieldRef :
              fieldPath : status.hostIP
        - nom : DD_ENV
          valeurFrom :
            fieldRef :
              fieldPath : metadata.labels['tags.datadoghq.com/env']
        - nom : DD_SERVICE
          valeurFrom :
            fieldRef :
              fieldPath : métadonnées.labels['tags.datadoghq.com/service']
        - nom : DD_VERSION
          valeurFrom :
            fieldRef :
              fieldPath : metadata.labels['tags.datadoghq.com/version']
</code></pre>
<p>What I did, however, in my pod these environment variables are empty, by moving these environment variables into labels in <code>spec -&gt; template -&gt; metadata -&gt; labels</code> it works fine, my environment variables are well filled when I check in my pod
But this is not good since in my CI at each update of my app I have to change the <code>tags.datadoghq.com/version label: &quot;&lt;version&gt;&quot;</code> and this only makes sense if it is a deployment label and that's why in my opinion in the doc it says to put these labels at the deployment level, but it doesn't work, do you have any idea how to do it, I really looked a lot but I can't find anything.</p>",,1,6,,2021-7-25 09:28:03,1,2021-7-26 14:29:52,,,,,7991779,,1,0,kubernetes|environment-variables|datadog,72,8.42933
261597,1,Method,69072787,Is there a way to disable datadog's log collection for a development environment?,"<p>I've tried searching on <a href=""https://docs.datadoghq.com/"" rel=""nofollow noreferrer"">datadog's docs</a> but I have not found any information regarding whether it is possible to deactivate log sending on certain envs (development for ex), therefore I'm forced to create a &quot;fake logger&quot; for dev env and only create a datadog logger for the other envs, I'd appreciate to know if it is possible to configure that scenario in order to avoid having to implement 2 different loggers.</p>
<p>I'm using @datadog/browser-logs for typescript, v2.1.1.</p>",69082665,1,0,,2021-9-6 10:09:43,1,2021-9-7 05:20:45,,,,,10391752,,1,1,datadog,36,8.42521
261598,1,Method,59914559,When to use Datadog Distribution and Histogram,<p>I cannot find any article that describes the advantages of using datadog histogram compared to datadog distribution for apps that run on multi instance. Would someone kindly help me on deciding the best choice between those two?</p>,,0,0,,2020-1-26 00:22:01,,2020-1-26 00:22:01,,,,,11229708,,1,2,histogram|distribution|datadog,125,8.38764
261599,3,Monitoring,68359994,Datadog Monitor Setup,"<p>I have one log line e.x: &quot;File Size is 32&quot;</p>
<p>I want to setup a log level monitor which triggers when log would be &quot;File Size is 33&quot; or &quot;File Size is 34&quot; or &quot;File Size is 35&quot; and so on. Basically, I want my monitor to trigger when integer value in log message will be greater than 32 (&gt;32) . Is it possible to setup monitor like that or to apply operations on text inside log message ?</p>
<p>Can someone please help around this ?</p>",,1,4,,2021-7-13 09:40:49,,2021-7-13 13:47:53,2021-7-13 10:13:04,,8928932,,8928932,,1,0,datadog,123,8.35962
261600,1,Method,66416385,What is the best way to represent a chart of distribution of time intervals in Datadog?,"<p>I have a server that processes packets from different devices. Devices can report in different intervals.
I would like to make a chart showing the distribution of intervals by the count of devices (how many devices are reporting within 5 sec/10 sec/60 sec ...)
Intervals for each device can vary.
Now I'm sending metric with <strong>Set</strong> using deviceId with tags that represent interval (5 sec, 10 sec, 30 sec, and more) but I'm not sure that it is correct.
What is the best way to realize it?</p>",,1,0,,2021-3-1 03:49:21,,2021-3-1 22:30:19,,,,,10164463,,1,0,charts|statistics|datadog|statsd,122,8.34544
261601,1,Method,68768669,How is interval used when submitting a rate custom metric to Datadog,"<p>How is the interval parameter used by Datadog when sending custom metrics? What happens if you send overlapping intervals of rate samples to the Datadog custom metrics API? <a href=""https://docs.datadoghq.com/metrics/custom_metrics/#custom-metrics-properties"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/metrics/custom_metrics/#custom-metrics-properties</a>.</p>
<p>Ie. if we sample the per second rate of a thing every 15 minutes by taking into account how many times it happened in the last 30 min. You can provide the interval in to Datadog API (which would be 30 mins in this case), how does Datadog use that? And how does it reconcile it with getting a value for the metric every 15 minutes?</p>",,0,0,,2021-8-13 07:53:49,1,2021-8-15 23:52:21,,,,,4751598,,1,2,datadog,121,8.33114
261602,3,Monitoring,62094698,Datadog Metric Monitor alert threshold to custom time period?,"<p>I want to set a threshold alert for my datadog metric monitor trigger it after 12 hours. </p>

<p><a href=""https://i.stack.imgur.com/WsQe9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WsQe9.png"" alt=""enter image description here""></a></p>

<p>However none of the options are 12 hours. I cant seem to add it in. </p>",,2,0,,2020-5-29 21:16:40,,2021-6-2 20:45:01,,,,,2705849,,1,0,datadog,119,8.30219
261603,1,Method,66134343,DataDog link custom spans and web spans in Azure AppService,"<p>We have a VM with DataDogAgent in azure, on this VM we host <code>ASP.Net core API</code> application. We collect these kind of metrics:</p>
<ul>
<li>web requests metrics collected from environment by <code>DataDogAgent</code></li>
<li>custom spans collected from the <code>C#</code> application code with help of <code>DataDog.Trace</code>.</li>
</ul>
<p>Here is the code of stub API:</p>
<pre><code>    [HttpGet]
    public IEnumerable&lt;WeatherForecast&gt; Get()
    {

        using (var dd1 = Tracer.Instance.StartActive(&quot;test-custom-measure1&quot;))
        using(var dd2 = Tracer.Instance.StartActive(&quot;test-custom-measure2&quot;))
        {
            Task.Delay(100).Wait();
        }
    }
</code></pre>
<p>In data dog dashboard these 2 kind of metrics are stacked together for one request into one trace:
<a href=""https://i.stack.imgur.com/m9wvH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m9wvH.png"" alt=""enter image description here"" /></a></p>
<p>After application migrated to <code>Azure AppService</code>(<code>.NET Datadog APM</code> is used to collect metrics here), spans are not stacked into one trace and appears in dashborad as separate traces.</p>
<p><a href=""https://i.stack.imgur.com/MSBiG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MSBiG.png"" alt=""enter image description here"" /></a></p>
<p>How to merge them into one trace in <code>Azure App Service</code>?</p>",,0,0,,2021-2-10 09:39:14,,2021-2-10 12:27:10,2021-2-10 12:27:10,,3607337,,3607337,,1,1,.net|azure|azure-web-app-service|datadog|azure-appservice,118,8.28753
261604,3,Monitoring,68557811,including monitor thresholds in datadog monitors for terraform 0.13,"<p>I currently am using terraform version 0.13.0 and I have a set of resources of type <code>datadog_monitor</code>.</p>
<p>Within these resources, I have defined <code>monitor_thresholds</code> but when I try to init and plan, i get an error saying <code>An argument named &quot;monitor_thresholds&quot; is not expected here. Did you mean to define a block of type &quot;monitor_thresholds&quot;.</code></p>
<p>I do not understand why monitor_thresholds isn't recognised, since it's given as part of <a href=""https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/monitor"" rel=""nofollow noreferrer"">the documentation</a> - would anyone know of a possible cause?</p>",,0,2,,2021-7-28 09:30:10,,2021-7-28 13:10:34,2021-7-28 13:10:34,,866021,,14917411,,1,0,terraform|datadog|terraform0.12+,117,8.27274
261605,3,Monitoring,66767713,Can we setup datadog alert threshold dynamically based on some template variable or tag,"<p>I want to setup datadog alert threshold different for each tag value passed, how can I do this?
One possibility is I create separate monitor for each tag value and use IN query for a perticular tag value. it will be bit hectic to manage those numbers of monitors.</p>",,0,1,,2021-3-23 16:56:15,,2021-3-23 16:56:15,,,,,5403764,,1,1,datadog,115,8.24279
261606,0,Integration,51707255,DataDog Agent Upgrade on Azure Cloud Service,"<p>I'm running an Azure Cloud Service with a WebRole.</p>

<p>We run the DataDog Agent on each of our server instances, by running a startup task that executes a .cmd file.</p>

<p>Previously we have been using the latest version of DataDog Agent 5, and installing it using this -</p>

<pre><code>start /w cmd
set log=datadog-install.log
set api_key=%1

sc query | findstr DatadogAgent
if ERRORLEVEL 1 (
    echo ""Datadog Agent service not detected"" &gt;&gt; %log%
    echo ""Starting the installation"" &gt;&gt; %log%

    if exist ddagent.msi (
        echo ""Already has the installer"" &gt;&gt; %log%
    ) else (
        echo ""Fetching the Agent Installer"" &gt;&gt; %log%
        powershell -Command ""(New-Object System.Net.WebClient).DownloadFile('https://s3.amazonaws.com/ddagent-windows-stable/ddagent-cli.msi', 'ddagent.msi')""
    )

    echo ""Starting the installer"" &gt;&gt;%log%
    msiexec.exe /qn /i ddagent.msi APIKEY=%api_key% /L+ %log%
) else (
    echo ""Agent already exists, skipping install"" &gt;&gt;%log%
)

echo ""Finished Install"" &gt;&gt;%log%
exit 0
</code></pre>

<p>Now we are trying to upgrade to the latest version of DataDog Agent 6 using this, which is failing to install and register the instance as an available host in DataDogs dashboard - </p>

<pre><code>start /w cmd
set log=datadog-install.log
set api_key=%1

sc query | findstr DatadogAgent
if ERRORLEVEL 1 (
    echo ""Datadog Agent service not detected"" &gt;&gt; %log%
    echo ""Starting the installation"" &gt;&gt; %log%

    if exist ddagent.msi (
        echo ""Already has the installer"" &gt;&gt; %log%
    ) else (
        echo ""Fetching the Agent Installer"" &gt;&gt; %log%
        powershell -Command ""(New-Object System.Net.WebClient).DownloadFile('https://s3.amazonaws.com/ddagent-windows-stable/datadog-agent-6-latest.amd64.msi', 'ddagent.msi')""
    )

    echo ""Starting the installer"" &gt;&gt;%log%
    msiexec.exe /qn /i ddagent.msi APIKEY=%api_key% /L+ %log%
) else (
    echo ""Agent already exists, skipping install"" &gt;&gt;%log%
)

echo ""Finished Install"" &gt;&gt;%log%
exit 0
</code></pre>

<p>The URL is of course different in each case.</p>",51871692,1,0,,2018-8-6 12:05:42,1,2018-8-16 07:24:11,,,,,99900,,1,0,azure|azure-cloud-services|datadog,115,8.24279
261607,0,Configuration,67027425,helm --set with json values for datadog/hazelcast,"<p>I am trying to set up hazelcast metrics pushed to datadog. I followed below documents.</p>
<p><a href=""https://docs.datadoghq.com/agent/kubernetes/prometheus/#configuration"" rel=""nofollow noreferrer"">DD for HazelCast</a></p>
<p>This is what I did:</p>
<ul>
<li>I got default <a href=""https://github.com/hazelcast/charts/blob/master/stable/hazelcast/values.yaml"" rel=""nofollow noreferrer"">values.yaml</a> for hazelcast
-I copied annotations from first link and added to values.yaml</li>
<li>After
<code>helm install my-hazelcast -f values.yaml</code> I see metrics.</li>
</ul>
<p>Now, I have to do same in ci pipeline in my org. So I have to pass these annotation in --set</p>
<pre><code>        ad.datadoghq.com/test-hz.check_names: |
                        [&quot;openmetrics&quot;]
        ad.datadoghq.com/test-hz.init_configs: |
                        [{}]
        ad.datadoghq.com/test-hz.instances: |
            [
              {
                &quot;prometheus_url&quot;: &quot;http://%%host%%:8080/metrics &quot;,
                &quot;namespace&quot;: &quot;my_namespace&quot;,
                &quot;metrics&quot;: [&quot;*&quot;]
              }
            ]        
</code></pre>
<p>You can see it is little bit complicated:</p>
<pre><code>helm upgrade --install test-hz hazelcast/hazelcast                                                      \
-f values.yaml                                                                                      \
--set annotations.&quot;ad\.datadoghq\.com/test-hz\.check_names=[\&quot;openmetrics\&quot;]&quot;     \
--set annotations.&quot;ad\.datadoghq\.com/test-hz\.init_configs=[{}]&quot;                 \
--set metrics.enable=true \
--set metrics.service.port=8080 \
--set image.repository=artifactory.a-us-common.wfk8s.com/docker/hazelcast/hazelcast           \
--set annotations.&quot;ad\.datadoghq\.com/test-hz\.instances=| 
    [ 
        { 
            \&quot;prometheus_url\&quot;: \&quot;http://%%host%%:8080/metrics \&quot;\, \&quot;namespace\&quot;: \&quot;my-namespace\&quot;\, \&quot;metrics\&quot;: [\&quot;*\&quot;] 
        } 
    ]&quot;
</code></pre>
<p>It shows pods running with annotations like this
<a href=""https://i.stack.imgur.com/jD9DV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jD9DV.png"" alt=""enter image description here"" /></a></p>
<p>What I also tried:</p>
<ul>
<li>I added json in one single line</li>
<li>escaping pipe</li>
<li>without pipe</li>
</ul>
<p>I see pods running but no metrics in Datadog. I am 99% sure that json is screwing things up.</p>
<p>Also I tried jq.</p>
<pre><code>helm upgrade --install test-hz hazelcast/hazelcast                                                      \
-f values.yaml                                                                                      \
--set annotations.&quot;ad\.datadoghq\.com/test-hz\.check_names=[\&quot;openmetrics\&quot;]&quot;     \
--set annotations.&quot;ad\.datadoghq\.com/test-hz\.init_configs=[{}]&quot;                 \
--set metrics.enable=true \
--set metrics.service.port=8080 \
--set image.repository=artifactory.a-us-common.wfk8s.com/docker/hazelcast/hazelcast           \
--set annotations.&quot;ad\.datadoghq\.com/test-hz\.instances=|$(jq -ner $myvar | helm_set_escape)&quot;
</code></pre>
<p>where,</p>
<pre><code>myvar='[{                                                                                                                                                                          
   &quot;prometheus_url&quot;: &quot;http://%%host%%:8080/metrics&quot;, &quot;namespace&quot;: &quot;gvp-hazelcast-test&quot;, &quot;metrics&quot;: [&quot;*&quot;] 
 }]'
</code></pre>
<p>and</p>
<pre><code>helm_set_escape(){
  sed -E 's|([}{,])|\\\1|g' /dev/stdin
}
</code></pre>
<p>and I got
<a href=""https://i.stack.imgur.com/Js0EV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Js0EV.png"" alt=""enter image description here"" /></a></p>
<p>Any help is appreciated.</p>",,0,0,,2021-4-9 19:48:24,,2021-4-9 20:37:10,2021-4-9 20:37:10,,4987963,,4987963,,1,0,json|kubernetes|prometheus|kubernetes-helm|datadog,114,8.22762
261608,0,Integration,56238847,Telegraf output plugin to submit metric to datadog agent,"<p>Is anyone aware if there is any telegraf output plugin to submit metric to datadog agent? I can see a <a href=""https://github.com/influxdata/telegraf/tree/master/plugins/outputs/datadog"" rel=""nofollow noreferrer"">datadog output plugin</a> which calls datadog metric api but not anything to submit data to datadog agent. </p>",,0,0,,2019-5-21 12:57:42,,2019-5-21 12:57:42,,,,,785523,,1,1,datadog|telegraf|telegraf-output-plugins,113,8.21231
261609,2,Query,68265416,How to search for negated content in Datadog,<p>I'd like to search for all the messages having certain service and NOT having certain content (message). It's straightforward to search for messages with the matching content. But how to negate them so I can filter out the messages I don't want to see (the ones polluting the context)?</p>,68290157,1,0,,2021-7-6 06:14:46,,2021-7-7 16:53:16,,,,,848025,,1,0,datadog,62,8.16957
261610,2,Query,58266541,Datadog log summarization,"<p>My team and I are trying to add a table to summarize our logs.<br>
Our system logs to datadog (not always but sometimes) as follows:</p>

<pre><code>Large operation start
Smaller operation start (UNIQUE MESSAGE)
Smaller operation end (UNIQUE MESSAGE)
..... More logs with meta data .......
Large operation end
</code></pre>

<p>To group a set of logs (Large operation) and know they are related I added to each a <code>trace_id</code> facet (to each log in between).</p>

<p>Eventually I want to quickly be able to see</p>

<ul>
<li>total operation time</li>
<li>operation time of the smaller action</li>
<li>some meta data collected along the way (somewhere along the way I obtained information I wanted and logged it)</li>
</ul>

<p>What's the best approach?
I don't want to query each time I need the information - I want to create an overview and to apply the system's analytic tool to this summary</p>",,1,0,,2019-10-7 09:11:03,,2019-10-7 13:28:46,,,,,11950262,,1,0,datadog,110,8.16557
261611,1,Method,67631253,How to remove special character in yaml file,"<p>Yaml file details.</p>
<pre class=""lang-yaml prettyprint-override""><code>- server: 1.2.1.4  
  port: 443   
  transport: TCP    
  server_hostname: *.abc.com
</code></pre>
<p>we have one server with *.abc.com but when I used try use with same server it is not allowing.
Error : &quot;did not find expected alphabetic or numeric character&quot;</p>
<p>so I have try to escape * with * or \* but Datadog server it showing _.adc.com</p>
<p>but I want to use in YAML to Datadog  like same *.abc.com</p>
<p>please can anyone help me on this.</p>",,1,0,,2021-5-21 05:28:11,,2021-5-22 00:52:30,2021-5-22 00:51:57,,347964,,12296803,,1,0,yaml|datadog,108,8.13369
261612,1,Method,64611252,is it possible to access a DataDog trace_id from within a Rails application,"<p>We are using DataDog's Distributed tracing in a rails application and would like to write the trace_id (for a controller#action) so that we could access the url later to our Rails logs. How could I do this?</p>
<p>something like?:</p>
<pre><code>trace_id = DataDog.trace_id
</code></pre>",,1,0,,2020-10-30 15:25:29,,2021-3-2 19:08:30,,,,,13053313,,1,0,ruby-on-rails|datadog,108,8.13369
261613,1,Method,65836340,DataDog Agent min_collection_interval global,"<p>I'm using V6 of the Datadog agent on Ubuntu 18.04. I'll like to change the min_collection_interval for all checks from the default 15 seconds to 30 seconds.</p>
<p>It's unclear from the documentation. Is this possible?</p>",,1,0,,2021-1-21 21:58:38,,2021-9-27 11:42:17,,,,,11744097,,1,1,datadog,96,8.12908
261614,3,Monitoring,68285098,Forwarding AWS ECR image scan findings to Datadog and creating alerts,"<p>Was anyone able to get the AWS ECR images scan results into a Datadog? Can't find any documentation on how to do that.</p>
<p>Many thanks in advance!</p>",,1,0,,2021-7-7 11:24:33,,2021-7-13 13:15:28,,,,,10625034,,1,0,amazon-web-services|datadog,107,8.11753
261615,2,Query,65826349,How to escape the whole string in the DataDog Log Search query?,"<p>I query for a particular URL string like <code>https://docs.python.org/3/reference/datamodel.html</code> in my logs. The URL contains lots of special reserved characters.</p>
<p>Escaping each char as stated by <a href=""https://docs.datadoghq.com/logs/search_syntax/#escaping-of-special-characters"" rel=""nofollow noreferrer"">official doc</a> works, but requires too much hustle:</p>
<p><code>https\:\/\/docs.python.org\/3\/reference\/datamodel.html</code></p>
<p>Is there any easier way to escape all special chars in the string?</p>",,0,0,,2021-1-21 11:23:25,1,2021-1-21 11:23:25,,,,,5036360,,1,0,datadog,106,8.10122
261616,0,Configuration,69794487,"ECS TaskDefinition creation fails with ""Invalid request provided: Create TaskDefinition: Unknown volume 'null'.""","<p>I am attempting to use the CDK to deploy an EC2 TaskDefinition for a DataDog agent which uses 3 bind mount volumes. The <a href=""https://docs.datadoghq.com/json/datadog-agent-ecs.json"" rel=""nofollow noreferrer"">DD example task definition</a> looks pretty simple.</p>
<p>The fragment of a Cloudformation template below has been generated by the AWS CDK. Deploying this raises the <code>Unknown volume: 'null'</code> error with all three volumes as is. It deploys successfully if no volumes are set.</p>
<p>I have tried using changesets through the cloudformation UI to add volumes one at a time. Adding the first volume by itself works normally, but adding two or three fails with the <code>Unknown volume: 'null'</code> error, regardless of doing it in stages or in one lump.</p>
<p>Editing the task definition manually through the ECS console does add a second volume successfully.</p>
<p>There doesn't appear to be any documentation on this particular error, and I can't understand how the ECS service is getting a null in the volumes array. I suspect I'm missing something obvious somewhere, so hoping someone has some insight into what's going on.</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;MonitoringTask9D49B4FA&quot;: {
    &quot;Type&quot;: &quot;AWS::ECS::TaskDefinition&quot;,
    &quot;Properties&quot;: {
      &quot;ContainerDefinitions&quot;: [
        {
          &quot;Cpu&quot;: 10,
          &quot;Environment&quot;: [
            {
              &quot;Name&quot;: &quot;DD_SITE&quot;,
              &quot;Value&quot;: &quot;datadoghq.com&quot;
            }
          ],
          &quot;Essential&quot;: true,
          &quot;Image&quot;: &quot;datadog/agent:latest&quot;,
          &quot;LogConfiguration&quot;: {
            &quot;LogDriver&quot;: &quot;awslogs&quot;,
            &quot;Options&quot;: {
              &quot;awslogs-group&quot;: {
                &quot;Ref&quot;: &quot;MonitoringTaskDatadogAgentLogGroupC5828485&quot;
              },
              &quot;awslogs-region&quot;: &quot;eu-west-1&quot;
            }
          },
          &quot;MemoryReservation&quot;: 256,
          &quot;MountPoints&quot;: [
            {}
          ],
          &quot;Name&quot;: &quot;datadog&quot;
        }
      ],
      &quot;ExecutionRoleArn&quot;: {
        &quot;Fn::GetAtt&quot;: [
          &quot;MonitoringTaskExecutionRole3188D770&quot;,
          &quot;Arn&quot;
        ]
      },
      &quot;Family&quot;: &quot;datadog-agent-task&quot;,
      &quot;NetworkMode&quot;: &quot;bridge&quot;,
      &quot;RequiresCompatibilities&quot;: [
        &quot;EC2&quot;
      ],
      &quot;Tags&quot;: [
        {
          &quot;Key&quot;: &quot;Environment&quot;,
          &quot;Value&quot;: &quot;develop&quot;
        },
      ],
      &quot;TaskRoleArn&quot;: {
        &quot;Fn::GetAtt&quot;: [
          &quot;MonitoringTaskTaskRole70FF4D63&quot;,
          &quot;Arn&quot;
        ]
      },
      &quot;Volumes&quot;: [
        {
          &quot;Host&quot;: {
            &quot;SourcePath&quot;: &quot;/var/run/docker.sock&quot;
          },
          &quot;Name&quot;: &quot;docker_sock&quot;
        },
        {
          &quot;Host&quot;: {
            &quot;SourcePath&quot;: &quot;/proc/&quot;
          },
          &quot;Name&quot;: &quot;proc&quot;
        },
        {
          &quot;Host&quot;: {
            &quot;SourcePath&quot;: &quot;/sys/fs/cgroup/&quot;
          },
          &quot;Name&quot;: &quot;cgroup&quot;
        }
      ]
    },
    &quot;Metadata&quot;: {
      &quot;aws:cdk:path&quot;: &quot;Stack/MonitoringTask/Resource&quot;
    }
  }
}
</code></pre>",69794488,1,0,,2021-11-1 08:42:48,,2021-11-1 08:42:48,,,,,57741,,1,1,amazon-cloudformation|amazon-ecs|aws-cdk|datadog,53,8.0971
261617,2,Query,66595606,Terraform Datadog Query Is not working as it contains some datadog methods,"<p>Getting query invalid for below monitor, Please suggest.</p>
<pre><code>resource &quot;datadog_monitor&quot; &quot;Something&quot; {
    name = &quot;[Some job failed] Something failed&quot;
    type = &quot;metric alert&quot;
    message = &quot;Something orc job failed Notify: @slack-Something&quot;

    query = &quot;events('priority:all tags:env:prod,reported_appname:SomethingIndexBuilder excluded_tags:reported_exitcode:0 \&quot;instance.shutdown\&quot;').rollup('count').last('4h') &gt; 2&quot;

    notify_audit = false # Do not notify when monitor is changed
}
</code></pre>",,0,1,,2021-3-12 07:03:03,,2021-3-12 07:03:03,,,,,5403764,,1,0,terraform|datadog,105,8.08476
261618,3,Monitoring,53222715,Monitor Aurora using Datadog doesn't show changes in Query-volume,"<p>I'm using <a href=""https://www.datadoghq.com/blog/monitor-aurora-using-datadog/"" rel=""nofollow noreferrer"">Datadog dashboard to monitor Aurora clusters</a> I have in my account.</p>

<p>The ""<a href=""https://www.datadoghq.com/blog/monitoring-mysql-performance-metrics/#query-throughput"" rel=""nofollow noreferrer"">query-volume</a>"" section is always empty, even if I go the mysql shell and do a couple of selects. </p>

<p>I'd like to make sure it works before I put high load on my db in production.</p>

<p>for now I only see changes in query-volume section in the charts of <code>Select latency and DML latency</code> and in AWS resource metrics in all charts.
whereas DiskIO section is totally empty, ( Connection and Replication is empty as well, but I know that because I don't have a replica ) </p>

<p>Any idea how can I make sure it works? </p>",53935650,1,0,,2018-11-9 09:08:09,,2018-12-26 18:21:58,,,,,2650254,,1,0,mysql|monitoring|metrics|amazon-aurora|datadog,104,8.06813
261619,0,Integration,68995016,Datadog’s Kafka integration DD_KAFKA_CLIENT_PROPAGATION_ENABLED - Magic v1 does not support record headers,"<p><strong>Issue:</strong></p>
<blockquote>
<p>[org.springframework.kafka.KafkaListenerEndpointContainer#3-0-kafka-consumer-1] WARN  o.a.k.c.consumer.internals.Fetcher - Unknown error fetching data for topic-partition</p>
</blockquote>
<p><strong>Environment Setup:</strong></p>
<ul>
<li><em>Kafka Version:</em> kafka_2.11-1.0.0</li>
<li><em>Producer Details:</em><br />
Spring Boot Version: 2.5.0<br />
Kafka-Client: 2.7.1</li>
<li><em>Consumer Details:</em><br />
Spring Boot Version: 1.5.19.RELEASE<br />
Kafka-Client: 0.10.0.1</li>
<li><em>DataDog</em></li>
</ul>
<p><strong>Scenario 1:</strong></p>
<p>We know Kafka older version does not support message header so we are not setting any message header in producer. So if we run application without datadog and when we publish message from producer, consumer is able to consume message.</p>
<p><strong>Conclusion:</strong> With mixed kafka version and without datadog, consumer and producer are working fine.</p>
<p><strong>Scenario 2:</strong></p>
<p>After adding datadog, messages are not delivered to consumer and on consumer side we can see exception: [org.springframework.kafka.KafkaListenerEndpointContainer#3-0-kafka-consumer-1] WARN  o.a.k.c.consumer.internals.Fetcher - Unknown error fetching data for topic-partition</p>
<p>And when checked Kafka server logs, we can see Magic V1 error. java.lang.IllegalArgumentException: Magic v1 does not support record headers</p>
<p><strong>Conclusion:</strong> With datadog clearly which is adding message header, messages are not delivered to consumer.</p>
<p><strong>Solution</strong> (but not working)
After going through datadog documentation I found solution to this problem which is: <a href=""https://docs.datadoghq.com/fr/tracing/compatibility_requirements/java/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/fr/tracing/compatibility_requirements/java/</a> under the section: Network tracing offers the following features:</p>
<p><strong>Datadog Documentation:</strong>
Kafka / Datadog integration works with Kafka version 0.11+, which supports Header API. This API is used to inject and extract the tracing context. If you are using a mixed-version environment, the Kafka broker may pass the most recent version of Kafka by mistake, and the plotter then attempts to inject headers that are not supported by the local producer. Additionally, older consumers are not able to consume the message due to the presence of the headers. To avoid these problems, if you use a mixed-version environment with Kafka versions prior to 0.11, disable the context of the spread with the variable following environment: <code>DD_KAFKA_CLIENT_PROPAGATION_ENABLED=false</code>.</p>
<p>So we added and run application and datadog agent using below command on windows (added flag <code>Ddd.kafka.client.propagation.enabled=false</code>):</p>
<pre><code>java -javaagent:D:\Tools\Datadog\dd-java-agent-0.9.0.jar -Ddd.profiling.enabled=true -XX:FlightRecorderOptions=stackdepth=256 -Ddd.logs.injection=true -**Ddd.kafka.client.propagation.enabled=false** -Ddd.trace.sample.rate=1 -Ddd.service=my-app -Ddd.env=staging -jar -Dspring.profiles.active=local-postgres &lt;Application Jar File&gt; -Ddd.version=1.0
</code></pre>
<p><strong>Expected behavior:</strong> Datadog should not add header.</p>
<p><strong>Actual behavior:</strong> Datadog is still adding header and so we are still getting Magic V1 error.</p>
<p>Kindly request to help those whoever faced similar issue and please guide in case if you resolved it.</p>",,1,0,,2021-8-31 08:04:40,,2021-9-13 21:56:45,2021-9-3 00:38:23,,10794031,,2732531,,1,0,apache-kafka|datadog,103,8.05135
261620,1,Method,66271979,How to send datadog event stream data into datadog logs?,"<p>Actually, we have two datadog accounts: Let me consider it has account A and account B.
when I push the message to data dog event using API I am able to see the events in events stream and I am able to see the same thing in logs also in the account A.
But when I do the same thing in account B I am able to see the data in event stream but not in logs.
can I know what might be the reason ???
Am I missing something to enable ??
if so can someone help me with this?</p>
<p>BELOW CODE IS USED TO PUSH THE EVENT TO DATA EVENTS STREAM.</p>
<pre><code>from datadog import initialize, API

options = {
    'api_key': '&lt;DATADOG_API_KEY&gt;',
    'app_key': '&lt;DATADOG_APPLICATION_KEY&gt;'
}

initialize(**options)

title = &quot;Something big happened!&quot;
text = 'And let me tell you all about it here!'
tags = ['version:1', 'application:web']

api.Event.create(title=title, text=text, tags=tags)

# If you are programmatically adding a comment to this new event
# you might want to insert a pause of .5 - 1 second to allow the
# event to be available.
</code></pre>",,1,0,,2021-2-19 04:53:17,,2021-2-24 17:20:38,,,,,12543947,,1,0,devops|monitoring|datadog,103,8.05135
261621,1,Method,65248919,Is it possible to remap a key/value attribute directly as tags?,"<p>I am using datadog with the agent, parsing logs in a file in json format.</p>
<p>The way I found to tag json attributes is to edit a pipeline configuration and do something like
<a href=""https://i.stack.imgur.com/1GSGH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1GSGH.png"" alt=""enter image description here"" /></a></p>
<p>But I have to do it for each individual tag key, which implies to know the list of tags in advance and maintain it over time. It's not ideal.</p>
<p>I'd like to know if there is a way to log things in this format</p>
<pre><code>{&quot;level&quot;: &quot;DEBUG&quot;, &quot;message&quot;: &quot;hello stack overflow&quot;, &quot;tags&quot;: {&quot;flavour&quot;: &quot;chocolate&quot;, &quot;context&quot;: &quot;icecream&quot;}}
</code></pre>
<p>And have a pipeline configuration taking all key/value pairs in the &quot;tags&quot; value of the log, and tag each one of them.</p>
<p>What I thought about was doing this</p>
<p><a href=""https://i.stack.imgur.com/fmErG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fmErG.png"" alt=""enter image description here"" /></a></p>
<p>But datadog's documentation is not clear about native attributes and their names so I don't know what would be the target attribute name.</p>",,0,1,,2020-12-11 09:40:11,,2020-12-11 09:40:11,,,,,4354822,,1,0,datadog,102,8.0344
261622,0,Configuration,65769361,HAProxy frontend sessions and backend session connection,"<p>In my project haproxy is used. But time to time I'm getting backend session limit exceed in haproxy when check with datadog graphs. At the same time when check the frontend session count it also bit high than the normal time. I need to know following things.</p>
<ol>
<li><p>What is the connection between frontend sessions and user requests come to the frontend? Single user getting one session for all the requests or multiple sessions?</p>
</li>
<li><p>For one frontend session backend going to create one session or multiple sessions?</p>
</li>
<li><p>If the frontend sessions are hight then can we say requests count to the haproxy is high?</p>
</li>
</ol>
<p>If possible can anyone share a tutorial to clearly understand connections between user requests, frontend sessions and backend sessions.</p>
<p>Thank you.</p>",,0,0,,2021-1-18 05:41:34,,2021-1-18 05:41:34,,,,,9320040,,1,0,haproxy|datadog,101,8.01729
261623,0,Integration,62300720,Can we visualize the performance of JMS or EMS by integrating to datadog?,"<p>We are using JMS (tibco EMS) as messaging service can We visualize the performance using Datadog?
If it is not possible do we have any alternate?</p>",,1,0,,2020-6-10 09:55:10,,2020-6-10 14:48:08,2020-6-10 14:48:08,,8381946,,8668364,,1,0,datadog|tibco-ems,100,8
261624,1,Error,65603045,Missing DataDog metrics when using ThreadStats for a Celery task in Django,"<h2>The problem</h2>
<p>I have a scheduled Celery task that queries <code>x</code> number of rows, does some processing and upon success (or error) it increments a specific metric using ThreadStats. For each execution of this task, the metric should be incremented by <code>x</code> at a specific time.</p>
<p>The problem is that some of these increments are not posted to DataDog. Ex. if the total number of rows is 100 and the task processes <code>x=10</code> at a time, some of those task executions fails to increment the metric and it ends up displaying only 60.</p>
<h2>Attempts for resolution</h2>
<p>This is what I tried to do without success:</p>
<ol>
<li>Manually flushing the metrics in the task by setting <code>flush_in_thread=False</code> and calling the <code>flush()</code> method.</li>
<li>Using <code>dogstatsd-collector</code> library to delay the submission</li>
</ol>
<h2></h2>",,0,0,,2021-1-6 20:36:26,,2021-1-6 20:36:26,,,,,6201129,,1,0,python|django|celery|metrics|datadog,99,7.98254
261625,1,Method,53598295,Logging System Metrics from JAR,"<p>Let's say there is an application x which uses an external jar y. </p>

<p>x uses <code>datadog-api</code> to log metrics and y too. </p>

<p>I want to see if it's possible to log system metrics which are specific to only the jar .i.e. how many gc cycles the jar is using ?</p>

<p>Using <code>system.gc</code> we can find out how many cycles the whole app is using on that host, but what about specific jar?</p>

<p>Please clarify if someone knows.</p>",,1,0,,2018-12-3 16:53:31,,2018-12-3 19:47:46,2018-12-3 19:47:46,,7290236,,4599064,,1,0,java|jar|datadog,54,7.92957
261626,1,Error,68910911,Datadog allows me to create facets but it does not show any values for them,"<p>I am using datadog to see my microservice metrics. When I go to APM tab I can see the spans I created and their corresponding tags are reaching the server correctly. The problem is that If I click in a tag &quot;gear&quot; to convert it to a facet, while the operation completes correctly I can not query for this value nor do I seee any value when I add it as a column to my metrics. Example below:</p>
<p>I can click that gear and convert &quot;Headers-Received&quot; to a string value, there is no error at all from DD, but I cannot query or see any value being registered. But I DO can see the values in each trace of a request reaching my server.</p>
<p>What is going on here?</p>
<p><a href=""https://i.stack.imgur.com/pXHPp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pXHPp.png"" alt=""enter image description here"" /></a></p>",,0,0,,2021-8-24 16:27:37,,2021-8-24 16:27:37,,,,,7723882,,1,0,tags|metrics|datadog,96,7.92908
261627,1,Method,62553019,What should I write logs to in production when using serilog?,"<p>I´am trying to setup serilog for for a project, but have some trouble understanding how it should be setup according to best practice for production.</p>
<p>I have followed this <a href=""https://benfoster.io/blog/serilog-best-practices/"" rel=""nofollow noreferrer"">https://benfoster.io/blog/serilog-best-practices/</a> , but I have some questions regarding the section <strong>Configuration</strong>. There he says;</p>
<blockquote>
<p>In .NET writing to Console is a blocking call and can have a
significant performance impact.</p>
</blockquote>
<p>and he links to <a href=""https://weblog.west-wind.com/posts/2018/Dec/31/Dont-let-ASPNET-Core-Default-Console-Logging-Slow-your-App-down"" rel=""nofollow noreferrer"">https://weblog.west-wind.com/posts/2018/Dec/31/Dont-let-ASPNET-Core-Default-Console-Logging-Slow-your-App-down</a> . Which again says that console is bad for perfomance and has this solution to the problem:</p>
<pre><code> if(Environment.GetEnvironmentVariable(&quot;ASPNETCORE_ENVIRONMENT&quot;) == EnvironmentName.Development) 
        {
            config.AddConsole();
        }
</code></pre>
<p>Which only adds console if in development.</p>
<p>My question is: If I´am not logging to console for production what should i do? Should i write to file? RollingFile? What is the best perfomance vice and will it work on fargate?</p>
<p>My service is running on fargate with firelens sending logs to datadog.</p>",,0,0,,2020-6-24 10:33:05,,2020-6-24 10:33:05,,,,,2144391,,1,1,.net|serilog|aws-fargate|datadog,93,7.87393
261628,0,Configuration,60560889,Tracking sparse metrics with micrometer results in zeroes being published,"<p>I'm using micrometer to publish several different metrics to Datadog; among these there is the number of items processed by several different batch jobs which can be pretty sparse.</p>

<p>Some batch jobs have different intervals, some others are triggered by external events so they don't have a fixed interval at all.</p>

<p>However, my micrometer configuration has a <code>step</code> of 30s, which I don't want to change, as suggested in <a href=""https://stackoverflow.com/questions/53177649/micrometer-sending-metrics-zero-spring-boot"">this question</a>, because I have denser metrics that need to be tracked every 30s.</p>

<p>This means that every 30s, unless a batch job has just run, my application is publishing a zero for each batch job metric, polluting them.</p>

<p>I've been trying to handle this on Datadog by using its <a href=""https://docs.datadoghq.com/dashboards/functions/rollup/"" rel=""nofollow noreferrer"">rollup</a> function, but i don't know the rollup interval beforehand because the interval may be variable for jobs triggered by external events.</p>

<p>Another solution i'm considering is to extend the <code>DataDogMeterRegistry</code> to have it clear all metrics right after publishing, so that it will only publish metrics that registered in the latest 30s interval, but I was wondering if there was a clearer way to prevent micrometer from sending zeroes for sparse metrics.</p>",,0,0,,2020-3-6 09:17:30,,2020-3-6 09:17:30,,,,,1454,,1,2,datadog|micrometer|spring-micrometer,93,7.87393
261629,0,Integration,67586872,datadog breaking tomcat on high load req (100req/sec),"<p>Using datadog for tomcatload testing. While running with datadog, server is stopping every 2 mins and server is restarting giving this error. Running without datadog this is working just fine.</p>
<p><em>A fatal error has been detected by the Java Runtime Environment:
SIGSEGV (0xb) at pc=0x0000ffffa2811614, pid=10202, tid=0x0000ffffa1a3b1f0
JRE version: OpenJDK Runtime Environment (8.0_292-b10) (build 1.8.0_292-8u292-b10-0ubuntu1~18.04-b10)
Java VM: OpenJDK 64-Bit Server VM (25.292-b10 mixed mode linux-aarch64 compressed oops)
Problematic frame:
V  [libjvm.so+0x3fb614]</em></p>
<p><a href=""https://i.stack.imgur.com/3Rp1G.png"" rel=""nofollow noreferrer"">Tomcat req pattern</a></p>",,1,0,,2021-5-18 13:08:35,,2021-5-20 11:25:58,,,,,15961658,,1,0,tomcat|load-testing|datadog,52,7.86401
261630,0,Integration,64937350,Can't install DataDog MSI in Dockerfile,"<p>From everything I have read, I should be able to do this:</p>
<pre><code>FROM mcr.microsoft.com/windows/servercore:ltsc2019

SHELL [&quot;powershell&quot;, &quot;-Command&quot;, &quot;$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';&quot;]

ADD https://ddagent-windows-stable.s3.amazonaws.com/datadog-agent-7-latest.amd64.msi C:\datadog-agent-7-latest.amd64.msi
RUN Start-Process msiexec.exe -Wait -ArgumentList '/qn /i C:\datadog-agent-7-latest.amd64.msi APIKEY=abc123 /L*V C:\install.log'
</code></pre>
<p>It looks like it built correctly:</p>
<pre><code>&gt; docker build -t test .
Sending build context to Docker daemon  4.608kB
Step 1/4 : FROM mcr.microsoft.com/windows/servercore:ltsc2019
 ---&gt; 71d3f266f3af
Step 2/4 : SHELL [&quot;powershell&quot;, &quot;-Command&quot;, &quot;$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';&quot;]
 ---&gt; Running in 1d931a586756
Removing intermediate container 1d931a586756
 ---&gt; 277770d39d39
Step 3/4 : ADD https://ddagent-windows-stable.s3.amazonaws.com/datadog-agent-7-latest.amd64.msi C:/datadog-agent-7-latest.amd64.msi
Downloading [==================================================&gt;]  146.9MB/146.9MB
 ---&gt; 93223b61dad3
Step 4/4 : RUN Start-Process msiexec.exe -Wait -ArgumentList '/qn /i C:\datadog-agent-7-latest.amd64.msi APIKEY=&quot;abc123&quot; /L*V C:\install.log'
 ---&gt; Running in 619d7bb20f3b
Removing intermediate container 619d7bb20f3b
 ---&gt; ddc461e4525a
Successfully built ddc461e4525a
Successfully tagged test:latest
</code></pre>
<p>But when I check in the container to see if it installed in <code>C:\Program Files\Datadog</code>, I am not seeing any of the files I am expecting from the installation.</p>
<p>I added the flag to give the install some extra logs (/L*V C:\install.log) but didn't see much in there. I have confirmed the msiexec command works on a Windows host, just not in the Docker build from what I can tell. Is there something simple that I'm missing?</p>",,0,5,,2020-11-20 21:45:42,,2020-11-23 16:48:11,2020-11-23 16:48:11,,926190,,926190,,1,0,windows|docker|windows-server|datadog,91,7.83617
261631,2,Query,68270987,DataDog Directory file matching pattern fails to match any files,"<p>The directory contains more than 20 files.
As explained in the <a href=""https://github.com/DataDog/integrations-core/blob/master/directory/datadog_checks/directory/data/conf.yaml.example"" rel=""nofollow noreferrer"">DataDog directory documentation</a>, a maximum of 20 files can be tracked.</p>
<p>So I tried to configure the data dog directory configuration</p>
<pre><code>init_config:

instances:
  - directory: /mnt/ftp/generic/SalesorderPosition
    pattern: '20([0-9][0-9]_12_+.*).csv'
    filegauges: true
</code></pre>
<p>Files are in the next format <code>yyyy_mm_SalesorderPosition.csv</code>
and I need to show the latest month in all previus years.</p>
<p>When the pattern is enabled in the config, no files are matched. When I remove it files are counted.</p>
<p>What did I miss here?</p>",68271255,1,0,,2021-7-6 12:56:20,,2021-7-6 14:20:59,2021-7-6 14:20:59,,8910547,,6166504,,1,0,regex|datadog|fnmatch,51,7.83028
261632,3,Monitoring,66439995,Is it possible to monitor a datadog metric from the start of the day,"<p>I want to monitor our application's usage of a 3rd party API with datadog. We have a daily quota of x calls we are allowed to perform every day. The number of calls resets every day at midnight.</p>
<p>I have added a datadog metric in our code that increases every time we make a call to that API. Now I want to create a monitor that will alert us whenever we reach 80% of our allowed daily calls. In other words, I want to get the calls we made from the beginning of the day. Is that possible with datadog?</p>",,0,0,,2021-3-2 13:05:36,,2021-3-2 13:05:36,,,,,7515745,,1,0,monitoring|datadog,90,7.81697
261633,1,Method,68217768,Implementing open tracing (datadog) with Scala Futures,"<p>this is my attempt at implementing open tracing (with datadog) but I am running into the issue where if this function is called with high concurrency, the spans on datadog start mixing together (i.e. spans for different operation names coalesce into 1 span). I have a suspicion this has to do with context switching between threads but would anyone know be able to point me in the right direction? Thanks</p>
<p><a href=""https://scastie.scala-lang.org/DZCCyl8WRoOBgeragoDbXQ"" rel=""nofollow noreferrer"">https://scastie.scala-lang.org/DZCCyl8WRoOBgeragoDbXQ</a></p>",,0,1,,2021-7-1 22:56:49,,2021-7-9 15:59:26,,,,,2807904,,1,1,scala|datadog|opentracing,89,7.79756
261634,3,Visualization,67214798,"What's a good practice for tracking status code responses so that future, unknown, codes can be visualized on a time series chart?","<p>Say I have an API client, and I track the response codes I get like this:
&quot;some_api_client.#{response.code}&quot; resulting in any number of metrics like: some_api_client.200, some_api_client.400, some_api_client.123456whatever</p>
<p>How can I build a time series chart that is future proof.  That is, I do not want to have to add all potential codes at creation.  I want to add a wildcard.  The best process I have to today is to guess at which codes show up and periodically go into the chart and look for new metrics to add.</p>
<p>That process results in missed codes.  e.g. one day a new code pops up but we don't see/notice it because we hadn't added the metrics.</p>
<p>The answer to this question would explicitly solve for that scenario -- new, unknown, codes would automatically show up in the graph whenever they deem fit to show themselves.</p>",67244102,1,0,,2021-4-22 14:00:56,,2021-4-24 14:54:10,,,,,115462,,1,0,time-series|datadog,50,7.79588
261635,3,Visualization,68818451,"How do get updated with AWS Status Dashboard, without checking manually?","<p>I am looking to build some certain types of alerts for <a href=""https://status.aws.amazon.com/"" rel=""nofollow noreferrer"">https://status.aws.amazon.com/</a>, to get constantly updated on the status for certain services.</p>",,1,0,,2021-8-17 13:41:35,,2021-8-18 03:40:51,2021-8-18 03:40:51,,174777,,15446876,,1,1,amazon-web-services|amazon-ec2|alert|dashboard|datadog,25,7.79176
261636,1,Error,69024317,Can't find Flink Histogram metric in Datadog,"<p>I integrated Flink job with Datadog. In my Flink job, I added metrics of counter and histogram, but from Datadog side, I could only found the metrics of counter, not histogram.</p>
<p>I'm able to see the histogram metric from Flink side:
<a href=""https://i.stack.imgur.com/bzqLX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bzqLX.png"" alt=""enter image description here"" /></a></p>
<p>but can't find it from Datadog side:
<a href=""https://i.stack.imgur.com/hN6X2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hN6X2.png"" alt=""enter image description here"" /></a></p>
<p>Also I'm able to find the counter metrics I added for the job.
<a href=""https://i.stack.imgur.com/XBDI3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XBDI3.png"" alt=""enter image description here"" /></a></p>
<p>My understanding is</p>
<ol>
<li>I could find histogram metric in the Flink UI, which means there's no issue of my code that collect metrics</li>
<li>I could find counter metric(from the same job) in Datadog, which means there's no issue of my Flink &lt;--&gt; Datadog integration.</li>
</ol>
<p>Combine 1) and 2), I can't figure out how to debug it. Any idea? Thanks!</p>
<p>Here's how I created histogram</p>
<pre><code>@transient private var eventTimeLagHistogram: Histogram = _
            
override def open(config: Configuration): Unit = {
  val dropwizardHistogram: com.codahale.metrics.Histogram  =
                  new com.codahale.metrics.Histogram(new SlidingWindowReservoir(500))
            
  eventTimeLagHistogram = getRuntimeContext()
                 .getMetricGroup.addGroup(&quot;OrderItemUpdateJobTest&quot;)
                  .histogram(&quot;eventTimeLagHistogram&quot;, new DropwizardHistogramWrapper(dropwizardHistogram))
              }
        
        
override def map(t: ObjectNode): OrderItemUpdate = {
              .....
  eventTimeLagHistogram.update(System.currentTimeMillis()- ItemTimestamp)
              .....
    }
</code></pre>",69035778,1,0,,2021-9-2 04:51:12,,2021-9-2 18:59:46,,,,,7876879,,1,0,apache-flink|datadog,49,7.76078
261637,1,Method,68173083,Using dd-trace-go for worker span IDs,"<p>I'm trying to setup dd-trace-go for my worker which listens to an active mq queue and process the message. However, I'm unable to get the span ID if I pass the context to another function. Here is a demo:</p>
<pre><code>package main

import (
    &quot;context&quot;

    &quot;github.com/davecgh/go-spew/spew&quot;
    &quot;gopkg.in/DataDog/dd-trace-go.v1/ddtrace/tracer&quot;
)

func main() {
    tracer.Start(
        tracer.WithServiceName(&quot;my-worker&quot;),
        tracer.WithGlobalTag(&quot;env&quot;, &quot;dev&quot;),
    )
    defer tracer.Stop()

    ctx := context.Background()


    // for loop to listen a message here
    // for every loop, init contextWithCancel
    ctxWithCancel, cancelFunction := context.WithCancel(ctx)

    span, _ := tracer.StartSpanFromContext(ctxWithCancel, &quot;Listen&quot;, tracer.ServiceName(&quot;my-worker&quot;))

    spew.Dump(ctxWithCancel)
    spew.Dump(span.Context().TraceID())

    test(ctxWithCancel)
    cancelFunction()
}

// callback to process the message
func test(c context.Context) {
    span, _ := tracer.SpanFromContext(c)
    spew.Dump(&quot;inside test function&quot;)
    spew.Dump(span.Context().TraceID())
}
</code></pre>
<p>The output is:</p>
<pre><code>(*context.cancelCtx)(0xc00006ce80)(context.Background.WithCancel)
(uint64) 6512722789050192664
(string) (len=20) &quot;inside test function&quot;
(uint64) 0
</code></pre>
<p>Why is the trace ID inside test function is 0 instead of 6512722789050192664 ? Thanks</p>",,0,0,,2021-6-29 05:30:11,,2021-6-29 05:30:11,,,,,11263733,,1,0,trace|datadog,87,7.75808
261638,0,Configuration,68904165,Terraform reusing config blocks,"<p>Terraform noob here. I have a datadog config along the line of (shamelessly copied from terraform website):</p>
<pre><code>resource &quot;datadog_dashboard&quot; &quot;ordered_dashboard&quot; {
  title        = &quot;Ordered Layout Dashboard&quot;
  description  = &quot;Created using the Datadog provider in Terraform&quot;
  layout_type  = &quot;ordered&quot;
  is_read_only = true

  widget { # SIMILAR GRAPH
    alert_graph_definition {
      alert_id  = &quot;895605&quot;
      viz_type  = &quot;timeseries&quot;
      title     = &quot;Widget Title&quot;
      live_span = &quot;1h&quot;
    }
  }

  widget { # SIMILAR GRAPH
    alert_graph_definition {
      alert_id  = &quot;895605&quot;
      viz_type  = &quot;timeseries&quot;
      title     = &quot;Widget Title&quot;
      live_span = &quot;2h&quot;
    }
  }

  widget {
    alert_value_definition {
      alert_id   = &quot;895605&quot;
      precision  = 3
      unit       = &quot;b&quot;
      text_align = &quot;center&quot;
      title      = &quot;Widget Title&quot;
    }
  }

  widget { # SIMILAR GRAPH
    alert_graph_definition {
      alert_id  = &quot;895605&quot;
      viz_type  = &quot;timeseries&quot;
      title     = &quot;Widget Title&quot;
      live_span = &quot;1w&quot;
    }
  }

  widget { # SIMILAR GRAPH
    alert_graph_definition {
      alert_id  = &quot;895605&quot;
      viz_type  = &quot;timeseries&quot;
      title     = &quot;Widget Title&quot;
      live_span = &quot;2w&quot;
    }
  }
}
</code></pre>
<p>As seen, we have 5 graphs: graph 1,2,4 and 5 are very similar, and only differ at 1 attribute, whereas graph 3 is very different. Is there a strategy in TF to reuse all the configs for graph 1,2 4 and 5?</p>
<p>I have attempted at using dynamic blocks, but while dynamic blocks can generate graphs 1 and 2 well, graph 3 is so different that I cannot fit it in the dynamic block.</p>
<p>In practice, the graphs are much more complex, and there are more of them, hence I find the config to be too hard to manage.</p>
<p>This is what I tried:</p>
<pre><code>resource &quot;datadog_dashboard&quot; &quot;ordered_dashboard&quot; {
  title        = &quot;Ordered Layout Dashboard&quot;
  description  = &quot;Created using the Datadog provider in Terraform&quot;
  layout_type  = &quot;ordered&quot;
  is_read_only = true

  dynamic widget {
    for_each = [&quot;1h&quot;, &quot;2h&quot;]
    iterator = live_span_iterator
    content {
      alert_graph_definition {
        alert_id  = &quot;895605&quot;
        viz_type  = &quot;timeseries&quot;
        title     = &quot;Widget Title&quot;
        live_span = live_span_iterator.value
      }
    }
  }

  widget {
    alert_value_definition {
      alert_id   = &quot;895605&quot;
      precision  = 3
      unit       = &quot;b&quot;
      text_align = &quot;center&quot;
      title      = &quot;Widget Title&quot;
    }
  }

  dynamic widget {
    for_each = [&quot;1w&quot;, &quot;2w&quot;]
    iterator = live_span_iterator
    content {
      alert_graph_definition {
        alert_id  = &quot;895605&quot;
        viz_type  = &quot;timeseries&quot;
        title     = &quot;Widget Title&quot;
        live_span = live_span_iterator.value
      }
    }
  }
}
</code></pre>
<p>Are there any other strategies that might help me reduce the code duplication? I hope to retain the order of the graphs if possible.</p>",,0,2,,2021-8-24 08:26:35,,2021-8-24 08:26:35,,,,,3713927,,1,1,terraform|code-duplication|datadog,87,7.75808
261639,1,Method,51728966,How to implement a class function for incrementing counter?,"<p>How would I use the following to create a <em>function</em> or <em>class</em> so that I can import this module to increment a count anytime an event has happen in my app. </p>

<p>I don't want to input those 3 lines everywhere on my app whenever i need to increment a count.</p>

<pre><code>const metrics = request.server.app.metrics;
const metricName = 'user.additem.counter';
metrics.counter(metricName).inc();
</code></pre>

<p>This is what i have so far but it's not working.</p>

<pre><code>const hoek = require('hoek')
 const incrementCounter = (context, metricName) =&gt; {
    const appMetrics = hoek.reach(context, 'request.server.app.metrics');
    if (appMetrics) {
        appMetrics.counter(metricName).inc();
    }
 };

module.exports = { 
    incrementCounter
};
</code></pre>",,1,3,,2018-8-7 14:14:55,,2018-8-7 15:55:06,2018-8-7 15:55:06,,179125,,10054034,,1,0,javascript|node.js|reactjs|datadog,83,7.67631
261640,0,Integration,59614515,Having datadog listen to a socket in C++,"<p>I have a the following function to publish datadog metrics</p>

<pre><code>int PublishMetric(Metric* metric)
{
    printf(""Sending metric to datadog metric: %s value: %.2f\n"", metric-&gt;metricName, metric-&gt;value);
    DogFood::Tags datadogTags;
    for (int idx=0; idx&lt;metric-&gt;numOfTags; idx++)
    {
        datadogTags[metric-&gt;tags[idx].tagName] = metric-&gt;tags[idx].tagValue;
    }
    DogFood::Metric(
            metric-&gt;metricName,
            metric-&gt;value,
            (DogFood::Type)metric-&gt;metricType,
            1,
            datadogTags
    );
}
</code></pre>

<p>I am now trying to change it so it will go through a socket, as I find in this following page:</p>

<p><a href=""https://github.com/garrettsickles/DogFood"" rel=""nofollow noreferrer"">https://github.com/garrettsickles/DogFood</a>  under <strong>UDS - Unix Domain Sockets (Custom)</strong>.</p>

<p>Now the function looks like this:</p>

<pre><code>int PublishMetric(Metric* metric)
{
    printf(""Sending metric to datadog metric: %s value: %.2f\n"", metric-&gt;metricName, metric-&gt;value);
    DogFood::Tags datadogTags;
    for (int idx=0; idx&lt;metric-&gt;numOfTags; idx++)
    {
        datadogTags[metric-&gt;tags[idx].tagName] = metric-&gt;tags[idx].tagValue;
    }
    DogFood::Send(
        DogFood::Metric(
                metric-&gt;metricName,
                metric-&gt;value,
                (DogFood::Type)metric-&gt;metricType,
                1,
                datadogTags
        ),
        DogFood::UDS(""/var/run/datadog/dsd.socket"")
    );
}
</code></pre>

<p>When I try to compile it, I get 'not a member' errors. How can I include them as members?</p>

<pre><code>/TCP_Server/src/DatadogAdapter/DatadogAdapter.cpp: In function 'int PublishMetric(Metric*)':
/TCP_Server/src/DatadogAdapter/DatadogAdapter.cpp:30:14: error: 'Send' is not a member of 'DogFood'
     DogFood::Send(
              ^~~~
/TCP_Server/src/DatadogAdapter/DatadogAdapter.cpp:30:14: note: suggested alternative: 'Set'
     DogFood::Send(
              ^~~~
              Set
/TCP_Server/src/DatadogAdapter/DatadogAdapter.cpp:38:18: error: 'UDS' is not a member of 'DogFood'
         DogFood::UDS(""/var/run/datadog/dsd.socket"")
                  ^~~
make[2]: *** [src/DatadogAdapter/CMakeFiles/DatadogAdapter.dir/DatadogAdapter.cpp.o] Error 1
</code></pre>",,0,4,,2020-1-6 15:12:18,,2020-1-6 15:12:18,,,,,10522758,,1,0,c++|unix-socket|datadog,82,7.65526
261641,1,Method,67423573,Push Metrics from Camel to External Service,"<p>I am trying to use camel to push metrics to datadog/cloudwatch. I explored meter and micrometer component, but there is no full-on example on how to actually push metrics data to some external service. I have explored all available examples, and nothing seems give me a proper big picture. . The metrics I require are the stats for each route which is running. Any resource/example which points me there would be really helpful.</p>",,2,3,,2021-5-6 17:54:15,,2021-5-10 22:56:37,,,,,15855792,,1,0,apache-camel|amazon-cloudwatch|metrics|datadog|micrometer,78,7.56838
261642,3,Monitoring,67785538,How to setup alerts for two metrics having same threshold value in one monitor in datadog?,"<p>I am trying to set the threshold limit to <code>2</code> errors for my two resources <code>abc.pqr</code> and <code>abc.jkl</code>. I want to trigger the alert and notifications when any of them get past threshold value of <code>2</code>. I don't want to alert when aggregated they surpass threshold limit rather individually.</p>
<p>Let's say if <code>abc.pqr</code> has <code>0</code> errors but <code>abc.jkl</code> has <code>3</code> errors then I should get the alert something like <code>abc.jkl has high error rate</code> and nothing about <code>abc.pqr</code>.</p>
<p>When I tried to aggregate by the <code>Add Query</code> option I get the final query like this</p>
<pre><code>sum(last_10m):avg:trace.trace.annotation.errors{env:stage,service:xyz-stage,resource_name:abc.pqr}.as_count() + avg:trace.trace.annotation.errors{env:stage,service:xyz-stage,resource_name:abc.jkl}.as_count() &gt; 2
</code></pre>
<p>When I tried to choose OR and adding more options after <code>from</code> then the final query is like this.</p>
<pre><code>sum(last_10m):avg:trace.trace.annotation.errors{env:stage AND service:xyz-stage AND resource_name:abc.pqr OR env:stage AND service:xyz-stage AND resource_name:abc.jkl}.as_count() &gt; 2
</code></pre>
<p>I don't think any of this is a valid query for what I am trying to achieve. Is this even possible in datadog or do I have to create separate monitors for both the metrics even though they have the same threshold value?</p>",,1,0,,2021-6-1 08:42:26,,2021-6-8 11:35:05,,,,,10807253,,1,0,datadog,77,7.54596
261643,0,Configuration,65817477,Cannot start webserver on ElasticBeanStalk while specifying javaagent in procfile,"<p>I have an elastic bean stalk server with a spring boot server configured to run on it. Procfile content for the working app is as follows:</p>
<p><code>web: java -jar -Dspring.profiles.active=profile_name my-app.jar</code></p>
<p>However, when I specify a javaagent for APM as follows:</p>
<p><code>web: java -javaagent:/home/ec2-user/dd-java-agent.jar -jar -Dspring.profiles.active=profile_name my-app.jar</code></p>
<p>The app log gives the following error:</p>
<pre><code>Error occurred during initialization of VM
agent library failed to init: instrument
</code></pre>
<p>I tried googling and added quotes, brackets around the java agent jar but it gives the same error.
I also modified the permissions on the <code>dd-java-agent.jar</code> file to be globally accessible.</p>
<p>Also, when I run the following command directly from the command line on elastic bean stalk:</p>
<p><code>java -javaagent:/home/ec2-user/dd-java-agent.jar -jar -Dspring.profiles.active=profile_name my-app.jar</code></p>
<p>The server starts up as expected. Any help guys?</p>",,0,1,,2021-1-20 20:51:44,1,2021-1-20 20:51:44,,,,,2025997,,1,0,spring-boot|amazon-elastic-beanstalk|javaagents|datadog|procfile,77,7.54596
261644,1,Method,67442061,Pytest - how to access all collected test reports for reporting?,"<p>I am looking to gather all test reports from a pytest session and send them to DataDog as log messages.</p>
<p>As part of this process I want to be able to do a bulk creation and send of the analytics (vs sending on each test report).</p>
<p>From what I can tell, all reporting hooks run per test - <a href=""https://docs.pytest.org/en/stable/reference.html#reporting-hooks"" rel=""nofollow noreferrer"">https://docs.pytest.org/en/stable/reference.html#reporting-hooks</a></p>
<p>This means if I'm going to do this each test will incur a slight delay as the call to send results to DataDog will happen sequentially and once per test.</p>
<p>Ideally, I'd be able to iterate over and parse the entire test report for each test case in one hook and build a single API call to DataDog (or send multiple, in cases when there are too many results).</p>
<p>Is it possible with a pytest hook to have access to <em>all</em> test results in a single hook?</p>",,0,0,,2021-5-7 21:50:46,,2021-5-7 21:50:46,,,,,1048539,,1,0,python|pytest|datadog,76,7.52325
261645,1,Error,57557508,Datadog client doesn't send trace to server,"<p>I have following function:</p>

<pre><code>def trace(request):
    caller_name = sys._getframe(1).f_code
    span_name = caller_name.co_filename+""/""+caller_name.co_name+"":""+str(caller_name.co_firstlineno)
    if hasattr(request, 'span') and request.span:
        tracer_context = request.span
        span = tracer.start_span(name=span_name, child_of=tracer_context, service=""pso"")
    else:
        span = tracer.start_span(name=span_name, child_of=None, service=""pso"")

    request.span = span
    return request
</code></pre>

<p>I use it as following:</p>

<pre><code>def my_func(request):
    request = trace(request)
    curr_span = request.span
    // some logic
    curr_span.finish()
    print(curr_span.pprint())
</code></pre>

<p>In logs I see spans as following:</p>

<pre><code>2019-08-19 12:44:55+0000 [-]       name my_func:66
2019-08-19 12:44:55+0000 [-]         id 13718618561359452918
2019-08-19 12:44:55+0000 [-]   trace_id 390415684220986929
2019-08-19 12:44:55+0000 [-]  parent_id 1137199070532920484
2019-08-19 12:44:55+0000 [-]    service my_service
2019-08-19 12:44:55+0000 [-]   resource my_func:66
2019-08-19 12:44:55+0000 [-]       type None
2019-08-19 12:44:55+0000 [-]      start 1566218695.56
2019-08-19 12:44:55+0000 [-]        end 1566218695.56
2019-08-19 12:44:55+0000 [-]   duration 0.000800s
2019-08-19 12:44:55+0000 [-]      error 0
2019-08-19 12:44:55+0000 [-]       tags 
2019-08-19 12:44:55+0000 [-]            env:my-env
</code></pre>

<p>Agent works fine but on datadog server I don't see the trace. What is wrong?</p>",,0,0,,2019-8-19 13:14:05,1,2019-8-19 13:20:27,2019-8-19 13:20:27,,3014866,,3014866,,1,1,python|trace|datadog,76,7.52325
261646,1,Method,62294715,How to instrument code using OpenTracing / OpenCensus?,"<p>I have to instrument our java/python/c++ application using one of the apis (opentracing/opencensus). The problem is that the instrumentation requires that i start a span, set tags, set logs and then close the span in each method call. This is too much change for us. Can anyone help in solving this problem ? how do i achieve this without polluting my code with all the instrumentation code ?</p>",,1,0,,2020-6-10 02:08:43,,2020-6-12 16:38:39,,,,,4764684,,1,0,datadog|jaeger|opentracing|distributed-tracing|opencensus,76,7.52325
261647,1,Method,68400137,Send KafkaStreams jmx metrics with java datadog agent,"<p>I'm using the <a href=""https://docs.datadoghq.com/tracing/setup_overview/setup/java/?tab=containers"" rel=""nofollow noreferrer"">dd javaagent</a> method to launch my java application that is a Kafka streams application.  I'd like the kafkastreams metrics defined <a href=""https://docs.confluent.io/platform/current/streams/monitoring.html#accessing-metrics-via-jmx-and-reporters"" rel=""nofollow noreferrer"">here</a> to be shipped to dd, however dd is not getting them.</p>
<p>the <code>dd.jmxfetch.enabled</code> arg is set to true, which it already is by default anyway. I'm unsure if I need to create any other <code>metrics.yaml</code> file or <code>jmx.yaml</code> datadog config for this to work?</p>",,1,0,,2021-7-15 19:59:33,,2021-9-24 22:13:46,2021-7-15 22:37:10,,1698794,,1698794,,1,1,monitoring|apache-kafka-streams|jmx|datadog,67,7.5043
261648,2,Query,66639586,How do I query DataDog for a phrase in the stack trace?,"<p>DataDog is so useless in its querying and its intuitiveness ... I'm looking for a custom exception in the stack trace. I found individual log entries in the last 18 hours that contain my exception class name, but attempting to write a log query that will find me all the occurrences is returning nothing. E.g.:</p>
<p>environment:prod @thrown.extendedStackTrace:UserDoesNotExistException</p>
<p>I'd like to include more words in the query, but even reducing down a single word fails to find anything. I've looked at their documentation, which is zero help.</p>",,1,0,,2021-3-15 14:11:28,,2021-12-2 16:59:27,,,,,187423,,1,1,datadog,67,7.5043
261649,1,Error,69645732,No metrics showing on Datadog,"<p>I am trying to send metrics generated from my Go microservice similar to <a href=""https://stackoverflow.com/questions/51124961/send-data-to-datadog-using-go"">this</a>-</p>
<pre class=""lang-golang prettyprint-override""><code>import (
    &quot;github.com/DataDog/datadog-go/statsd&quot;
    &quot;log&quot;
)

func main() {
    // Create the client
    c, err := statsd.New(&quot;127.0.0.1:8125&quot;)
    if err != nil {
        log.Fatal(err)
    }
    // Prefix every metric with the app name
    c.Namespace = &quot;myapp.&quot;
    // Count two events
    err = c.Count(&quot;my_counter&quot;, 2, nil, 1)
    if err != nil {
        log.Fatal(err)
    }
    // Close the client
    err = c.Close()
    if err != nil {
        log.Fatal(err)
    }
}
</code></pre>
<p>I am using Gitlab to push the image into Azure Container Registry and eventually deploying it into my Kubernetes cluster. I am able to see the logs for this microservice, but not being able to see the custom metrics generated. I have already set the <code>hostPort</code> as mentioned <a href=""https://docs.datadoghq.com/developers/dogstatsd/?tab=kubernetes"" rel=""nofollow noreferrer"">here</a> for the Kubernetes agent setup. Any help in finding the cause of the error will be really helpful.</p>",69793104,1,0,,2021-10-20 12:09:40,,2021-11-1 05:55:07,,,,,11827585,,1,0,go|kubernetes|datadog,74,7.47693
261650,3,Visualization,64539546,Is it possible to create a timeseries graph of an SLO in Datadog?,"<p>Is it possible to graph an SLO as a time-series graph using just native Datadog components?</p>
<p>If so, how?</p>
<p>I can only find a way to show an SLO as a number, I'd like to show how it changes over time in a graph-format.</p>",64542859,1,0,,2020-10-26 14:55:11,,2020-10-26 18:21:26,2020-10-26 15:05:10,,1106539,,1106539,,1,0,graph|time-series|datadog,41,7.45114
261651,1,Method,62074299,Datadog data validation between on-prem and cloud database,"<p>Very new to Datadog and need some help. I have crafted 2 SQL queries (one for on-prem database and one for cloud database) and I would like to run those queries through Datadog and be able display the query results and validate that the daily results fall within an expected variance between the two systems. </p>

<p>I have already set up Datadog on the cloud environment and believe I should use DogStatsD to create a custom metric but I am pretty lost with how I can incorporate my necessary SQL queries in the code to create the metric for eventual display on a dashboard. Any help will be greatly appreciated!!!</p>",,1,0,,2020-5-28 20:50:31,,2020-5-28 21:23:05,,,,,12822559,,1,0,validation|datadog,69,7.3554
261652,0,Configuration,68131379,How to turn off nginx log collections in DataDog?,"<p><strong>datadog.conf</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>    log_level: warn
    log_file: /var/log/datadog/agent.log
    log_to_syslog: no
    
    dd_url: https://app.datadoghq.com
    api_key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
    
    process_config:
    enabled: &quot;true&quot;
    logs_enabled: true
    
    tags:
      - env:stage
      - nginx
      - webserver
      - frontend
      - php-fpm
      - php
    
    use_dogstatsd: true
    dogstatsd_port: 8125
</code></pre>
<p>I needed to enable flag <code>logs_enabled: true</code> in order to collect PHP logs but with this, I also collect Nginx logs (access and error logs)</p>
<p>If <code>logs_enabled</code> is set to false then I do not collect even PHP logs.</p>
<p>How can I exclude Nginx logs from being collected?</p>
<pre class=""lang-yaml prettyprint-override""><code># php.d/conf.yaml

init_config:
    
instances:
    
logs:
  - type: file
    path: &quot;/var/www/{{ env }}/var/log/app-log.json&quot;
    service: php
    source: php
    sourcecategory: sourcecode
</code></pre>",68159826,1,0,,2021-6-25 12:58:26,,2021-6-28 08:14:21,2021-6-25 13:30:08,,16016348,,6166504,,1,0,php|nginx|logging|datadog,67,7.3043
261653,1,Error,68864089,can't get custom metrics for hpa from datadog,"<p>hey guys i’m trying to setup datadog as custom metric for my kubernetes hpa using the official guide:</p>
<p><a href=""https://docs.datadoghq.com/agent/cluster_agent/external_metrics/?tab=helm"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/cluster_agent/external_metrics/?tab=helm</a></p>
<p>running on <strong>EKS 1.18</strong> &amp; Datadog Cluster Agent (<strong>v1.10.0</strong>).
the problem is that i can't get the external metrics's for my HPA:</p>
<pre><code>apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: hibob-hpa
spec:
  minReplicas: 1
  maxReplicas: 5
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: something
  metrics:
  - type: External
    external:
      metricName: **kubernetes_state.container.cpu_limit**
      metricSelector:
        matchLabels:
            pod: **something-54c4bd4db7-pm9q5**
      targetAverageValue: 9
</code></pre>
<p>horizontal-pod-autoscaler  unable to get external metric:</p>
<pre><code>canary/nginx.net.request_per_s/&amp;LabelSelector{MatchLabels:map[string]string{kube_app_name: nginx,},MatchExpressions:[]LabelSelectorRequirement{},}: unable to fetch metrics from external metrics API: the server is currently unable to handle the request (get nginx.net.request_per_s.external.metrics.k8s.io)
</code></pre>
<p>This is the errors i'm getting inside the cluster-agent:</p>
<pre><code>datadog-cluster-agent-585897dc8d-x8l82 cluster-agent 2021-08-20 06:46:14 UTC | CLUSTER | ERROR | (pkg/clusteragent/externalmetrics/metrics_retriever.go:77 in retrieveMetricsValues) | Unable to fetch external metrics: [Error while executing metric query avg:nginx.net.request_per_s{kubea_app_name:ingress-nginx}.rollup(30): API error 403 Forbidden: {&quot;status&quot;:********@datadoghq.com&quot;}, strconv.Atoi: parsing &quot;&quot;: invalid syntax]
</code></pre>
<pre><code># datadog-cluster-agent status
Getting the status from the agent.
2021-08-19 15:28:21 UTC | CLUSTER | WARN | (pkg/util/log/log.go:541 in func1) | Agent configuration relax permissions constraint on the secret backend cmd, Group can read and exec
===============================
Datadog Cluster Agent (v1.10.0)
===============================

  Status date: 2021-08-19 15:28:21.519850 UTC
  Agent start: 2021-08-19 12:11:44.266244 UTC
  Pid: 1
  Go Version: go1.14.12
  Build arch: amd64
  Agent flavor: cluster_agent
  Check Runners: 4
  Log Level: INFO

  Paths
  =====
    Config File: /etc/datadog-agent/datadog-cluster.yaml
    conf.d: /etc/datadog-agent/conf.d

  Clocks
  ======
    System UTC time: 2021-08-19 15:28:21.519850 UTC

  Hostnames
  =========
    ec2-hostname: ip-10-30-162-8.eu-west-1.compute.internal
    hostname: i-00d0458844a597dec
    instance-id: i-00d0458844a597dec
    socket-fqdn: datadog-cluster-agent-585897dc8d-x8l82
    socket-hostname: datadog-cluster-agent-585897dc8d-x8l82
    hostname provider: aws
    unused hostname providers:
      configuration/environment: hostname is empty
      gce: unable to retrieve hostname from GCE: status code 404 trying to GET http://169.254.169.254/computeMetadata/v1/instance/hostname

  Metadata
  ========

Leader Election
===============
  Leader Election Status:  Running
  Leader Name is: datadog-cluster-agent-585897dc8d-x8l82
  Last Acquisition of the lease: Thu, 19 Aug 2021 12:13:14 UTC
  Renewed leadership: Thu, 19 Aug 2021 15:28:07 UTC
  Number of leader transitions: 17 transitions

Custom Metrics Server
=====================
  External metrics provider uses DatadogMetric - Check status directly from Kubernetes with: `kubectl get datadogmetric`


Admission Controller
====================
  Disabled: The admission controller is not enabled on the Cluster Agent


=========
Collector
=========

  Running Checks
  ==============

    kubernetes_apiserver
    --------------------
      Instance ID: kubernetes_apiserver [OK]
      Configuration Source: file:/etc/datadog-agent/conf.d/kubernetes_apiserver.d/conf.yaml.default
      Total Runs: 787
      Metric Samples: Last Run: 0, Total: 0
      Events: Last Run: 0, Total: 660
      Service Checks: Last Run: 3, Total: 2,343
      Average Execution Time : 1.898s
      Last Execution Date : 2021-08-19 15:28:17.000000 UTC
      Last Successful Execution Date : 2021-08-19 15:28:17.000000 UTC

=========
Forwarder
=========

  Transactions
  ============
    Deployments: 350
    Dropped: 0
    DroppedOnInput: 0
    Nodes: 497
    Pods: 3
    ReplicaSets: 576
    Requeued: 0
    Retried: 0
    RetryQueueSize: 0
    Services: 263

  Transaction Successes
  =====================
    Total number: 3442
    Successes By Endpoint:
      check_run_v1: 786
      intake: 181
      orchestrator: 1,689
      series_v1: 786

==========
Endpoints
==========
  https://app.datadoghq.eu - API Key ending with:
      - f295b

=====================
Orchestrator Explorer
=====================
  ClusterID: f7b4f97a-3cf2-11ea-aaa8-0a158f39909c
  ClusterName: production
  ContainerScrubbing: Enabled
  ======================
  Orchestrator Endpoints
  ======================

  ===============
  Forwarder Stats
  ===============
    Pods: 3
    Deployments: 350
    ReplicaSets: 576
    Services: 263
    Nodes: 497

  ===========
  Cache Stats
  ===========
    Elements in the cache: 393
    Pods:
      Last Run: (Hits: 0 Miss: 0) | Total: (Hits: 7 Miss: 5)
    Deployments:
      Last Run: (Hits: 36 Miss: 1) | Total: (Hits: 40846 Miss: 2444)
    ReplicaSets:
      Last Run: (Hits: 297 Miss: 1) | Total: (Hits: 328997 Miss: 19441)
    Services:
      Last Run: (Hits: 44 Miss: 0) | Total: (Hits: 49520 Miss: 2919)
    Nodes:
      Last Run: (Hits: 9 Miss: 0) | Total: (Hits: 10171 Miss: 755)```


and this is what i get from datadogmetric:

</code></pre>
<pre><code>Name:         dcaautogen-2f116f4425658dca91a33dd22a3d943bae5b74
Namespace:    datadog
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  datadoghq.com/v1alpha1
Kind:         DatadogMetric
Metadata:
  Creation Timestamp:  2021-08-19T15:14:14Z
  Generation:          1
  Managed Fields:
    API Version:  datadoghq.com/v1alpha1
    Fields Type:  FieldsV1
    fieldsV1:
      f:spec:
      f:status:
        .:
        f:autoscalerReferences:
        f:conditions:
          .:
          k:{&quot;type&quot;:&quot;Active&quot;}:
            .:
            f:lastTransitionTime:
            f:lastUpdateTime:
            f:status:
            f:type:
          k:{&quot;type&quot;:&quot;Error&quot;}:
            .:
            f:lastTransitionTime:
            f:lastUpdateTime:
            f:message:
            f:reason:
            f:status:
            f:type:
          k:{&quot;type&quot;:&quot;Updated&quot;}:
            .:
            f:lastTransitionTime:
            f:lastUpdateTime:
            f:status:
            f:type:
          k:{&quot;type&quot;:&quot;Valid&quot;}:
            .:
            f:lastTransitionTime:
            f:lastUpdateTime:
            f:status:
            f:type:
        f:currentValue:
    Manager:         datadog-cluster-agent
    Operation:       Update
    Time:            2021-08-19T15:14:44Z
  Resource Version:  164942235
  Self Link:         /apis/datadoghq.com/v1alpha1/namespaces/datadog/datadogmetrics/dcaautogen-2f116f4425658dca91a33dd22a3d943bae5b74
  UID:               6e9919eb-19ca-4131-b079-4a8a9ac577bb
Spec:
  External Metric Name:  nginx.net.request_per_s
  Query:                 avg:nginx.net.request_per_s{kube_app_name:nginx}.rollup(30)
Status:
  Autoscaler References:  canary/hibob-hpa
  Conditions:
    Last Transition Time:  2021-08-19T15:14:14Z
    Last Update Time:      2021-08-19T15:53:14Z
    Status:                True
    Type:                  Active
    Last Transition Time:  2021-08-19T15:14:14Z
    Last Update Time:      2021-08-19T15:53:14Z
    Status:                False
    Type:                  Valid
    Last Transition Time:  2021-08-19T15:14:14Z
    Last Update Time:      2021-08-19T15:53:14Z
    Status:                True
    Type:                  Updated
    Last Transition Time:  2021-08-19T15:14:44Z
    Last Update Time:      2021-08-19T15:53:14Z
    Message:               Global error (all queries) from backend
    Reason:                Unable to fetch data from Datadog
    Status:                True
    Type:                  Error
  Current Value:           0
Events:                    &lt;none&gt;
</code></pre>
<pre><code>
this is my cluster agent deployment: 

</code></pre>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: &quot;18&quot;
    meta.helm.sh/release-name: datadog
    meta.helm.sh/release-namespace: datadog
  creationTimestamp: &quot;2021-02-05T07:36:39Z&quot;
  generation: 18
  labels:
    app.kubernetes.io/instance: datadog
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: datadog
    app.kubernetes.io/version: &quot;7&quot;
    helm.sh/chart: datadog-2.7.0
  name: datadog-cluster-agent
  namespace: datadog
  resourceVersion: &quot;164881216&quot;
  selfLink: /apis/apps/v1/namespaces/datadog/deployments/datadog-cluster-agent
  uid: ec52bb4b-62af-4007-9bab-d5d16c48e02c
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: datadog-cluster-agent
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        ad.datadoghq.com/cluster-agent.check_names: '[&quot;prometheus&quot;]'
        ad.datadoghq.com/cluster-agent.init_configs: '[{}]'
        ad.datadoghq.com/cluster-agent.instances: |
          [{
            &quot;prometheus_url&quot;: &quot;http://%%host%%:5000/metrics&quot;,
            &quot;namespace&quot;: &quot;datadog.cluster_agent&quot;,
            &quot;metrics&quot;: [
              &quot;go_goroutines&quot;, &quot;go_memstats_*&quot;, &quot;process_*&quot;,
              &quot;api_requests&quot;,
              &quot;datadog_requests&quot;, &quot;external_metrics&quot;, &quot;rate_limit_queries_*&quot;,
              &quot;cluster_checks_*&quot;
            ]
          }]
        checksum/api_key: something
        checksum/application_key: something
        checksum/clusteragent_token: something
        checksum/install_info: something
      creationTimestamp: null
      labels:
        app: datadog-cluster-agent
      name: datadog-cluster-agent
    spec:
      containers:
      - env:
        - name: DD_HEALTH_PORT
          value: &quot;5555&quot;
        - name: DD_API_KEY
          valueFrom:
            secretKeyRef:
              key: api-key
              name: datadog
              optional: true
        - name: DD_APP_KEY
          valueFrom:
            secretKeyRef:
              key: app-key
              name: datadog-appkey
        - name: DD_EXTERNAL_METRICS_PROVIDER_ENABLED
          value: &quot;true&quot;
        - name: DD_EXTERNAL_METRICS_PROVIDER_PORT
          value: &quot;8443&quot;
        - name: DD_EXTERNAL_METRICS_PROVIDER_WPA_CONTROLLER
          value: &quot;false&quot;
        - name: DD_EXTERNAL_METRICS_PROVIDER_USE_DATADOGMETRIC_CRD
          value: &quot;true&quot;
        - name: DD_EXTERNAL_METRICS_AGGREGATOR
          value: avg
        - name: DD_CLUSTER_NAME
          value: production
        - name: DD_SITE
          value: datadoghq.eu
        - name: DD_LOG_LEVEL
          value: INFO
        - name: DD_LEADER_ELECTION
          value: &quot;true&quot;
        - name: DD_COLLECT_KUBERNETES_EVENTS
          value: &quot;true&quot;
        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME
          value: datadog-cluster-agent
        - name: DD_CLUSTER_AGENT_AUTH_TOKEN
          valueFrom:
            secretKeyRef:
              key: token
              name: datadog-cluster-agent
        - name: DD_KUBE_RESOURCES_NAMESPACE
          value: datadog
        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED
          value: &quot;true&quot;
        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED
          value: &quot;true&quot;
        - name: DD_COMPLIANCE_CONFIG_ENABLED
          value: &quot;false&quot;
        image: gcr.io/datadoghq/cluster-agent:1.10.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 6
          httpGet:
            path: /live
            port: 5555
            scheme: HTTP
          initialDelaySeconds: 15
          periodSeconds: 15
          successThreshold: 1
          timeoutSeconds: 5
        name: cluster-agent
        ports:
        - containerPort: 5005
          name: agentport
          protocol: TCP
        - containerPort: 8443
          name: metricsapi
          protocol: TCP
        readinessProbe:
          failureThreshold: 6
          httpGet:
            path: /ready
            port: 5555
            scheme: HTTP
          initialDelaySeconds: 15
          periodSeconds: 15
          successThreshold: 1
          timeoutSeconds: 5
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/datadog-agent/install_info
          name: installinfo
          readOnly: true
          subPath: install_info
      dnsConfig:
        options:
        - name: ndots
          value: &quot;3&quot;
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: datadog-cluster-agent
      serviceAccountName: datadog-cluster-agent
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          defaultMode: 420
          name: datadog-installinfo
        name: installinfo
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: &quot;2021-05-13T15:46:33Z&quot;
    lastUpdateTime: &quot;2021-05-13T15:46:33Z&quot;
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &quot;True&quot;
    type: Available
  - lastTransitionTime: &quot;2021-02-05T07:36:39Z&quot;
    lastUpdateTime: &quot;2021-08-19T12:12:06Z&quot;
    message: ReplicaSet &quot;datadog-cluster-agent-585897dc8d&quot; has successfully progressed.
    reason: NewReplicaSetAvailable
    status: &quot;True&quot;
    type: Progressing
  observedGeneration: 18
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
</code></pre>
<pre><code></code></pre>",,0,0,,2021-8-20 14:47:08,0,2021-8-20 14:54:01,2021-8-20 14:54:01,,12711868,,12711868,,1,0,kubernetes|monitoring|autoscaling|datadog|hpa,67,7.3043
261654,3,Monitoring,69741911,How can I monitor mongodb memory with terraform in percentage? The monitoring tool is datadog,"<p>I am trying to create a dashboard that alert when MongoDB memory usage is 90%. I am creating datadog dashboard by using terraform. This is the MondoDB metrics i need to get (mongodb.atlas.mem.resident). The problem is that metric is in MB but I want to do in percentage. Here is my code:
In the code query = .... &gt; 90 should be percentage but it is 9K on datadog dashboard.trying to find a way to change it to percentage instead of KB, MG, or GB.</p>
<pre><code>  resource &quot;datadog_monitor&quot; &quot;mongodb_memory&quot; {
  name    = &quot;MongoDB Max Memory Usage Alerts - ${var.kubernetes_cluster_name}&quot;
  type    = &quot;metric alert&quot;
  message = &quot;Monitor triggered. Notify: @hipchat-channel&quot;

  query = &quot;avg(last_5m):avg:mongodb.atlas.mem.resident{clustername:${var.kubernetes_cluster_name}} &gt; 90&quot;
  

  monitor_thresholds {
    warning           = 80 
    warning_recovery  = 70
    critical          = 90
    critical_recovery = 80
  }

  notify_no_data    = false
  renotify_interval = 60

  notify_audit = false
  timeout_h    = 60
  include_tags = true

  tags = [&quot;Name:mongodb memory&quot;, &quot;type:metric alert&quot;, &quot;cluster_name:${var.kubernetes_cluster_name}&quot;]
}
</code></pre>",,1,4,,2021-10-27 16:00:08,,2021-10-27 16:51:56,,,,,17253669,,1,0,mongodb|terraform|monitoring|mongodb-atlas|datadog,67,7.3043
261655,3,Visualization,57165269,"Is there a way to add a separate graph for each host on a Datadog dashboard, when the hosts frequently change?","<p>I'm trying to make a dashboard to monitor a process which runs on 5 remote machines simultaneously. I want the dashboard to display the metrics for each machine separately - basically, I want to create five separate graphs, one for each machine that runs the process. My problem is that the remote machines are reassigned periodically, so I have no way of knowing the name of the host at any given time.</p>

<p>I've tried creating five separate graphs, with each one filtered by a different host name tag, but the graphs do not seem to pick up the new host when the lease for the process is changed. I also know you can split out one graph for each host using metrics explorer, but I haven't found any way to automatically do that on a dashboard. Does anyone know if this is possible? Leases for the process are assigned through AWS, if that is helpful.</p>

<p>Thanks in advance for any suggestions.</p>",,0,0,,2019-7-23 13:28:20,0,2019-7-23 13:28:20,,,,,11736052,,1,1,monitoring|dashboard|datadog,66,7.27818
261656,3,Visualization,53044670,Is it possible to export Datadog Aurora dashboard as cloud formation?,"<p>I'm using <a href=""https://www.datadoghq.com/blog/monitor-aurora-using-datadog/"" rel=""nofollow noreferrer"">this built-in dashboard</a> for monitoring Aurora and was wondering how can I have as code, as cloud formation stack precisely. </p>

<p>I'm aware of those three repos which do backup and monitoring of changing of the dashboard in the API and then commit back to GitHub, but I only want to export it once.</p>",53458998,1,0,,2018-10-29 11:34:10,,2018-11-24 14:09:27,2018-10-30 14:12:37,,1034105,,2650254,,1,-1,amazon-web-services|amazon-cloudformation|dashboard|datadog,74,7.27693
261657,0,Configuration,68619614,databricks secrets in init scripts,"<p>In order to use Datadog, I use an init script according to <a href=""https://docs.datadoghq.com/integrations/databricks/?tab=driveronly"" rel=""nofollow noreferrer"">databricks datatdog integration</a>.</p>
<p>Hence I fill spark environment variables as on the attached picture. <a href=""https://i.stack.imgur.com/eL5p8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eL5p8.jpg"" alt=""spark env vars"" /></a></p>
<p>It is working when I fill in the datadog key not encrypted but as soon as I fill it with <code>{{secrets/datadog/api_key}}</code>, it is not working.</p>",,0,1,,2021-8-2 09:57:22,,2021-8-2 09:57:22,,,,,7223727,,1,1,databricks|datadog,64,7.22472
261658,3,Monitoring,69646642,AWS Cloudwatch alarms in Datadog,"<p>Looking through Datadog AWS integration documentation I found mention that AWS alarms can be streamed into Datadog. It's stated that you can choose two different methods how to send AWS CloudWatch alarms to the Datadog Event Stream <a href=""https://docs.datadoghq.com/integrations/amazon_web_services/?tab=roledelegation#alarm-collection"" rel=""nofollow noreferrer"">right here</a> in the <em>Alarm collection</em> section.
But there is no further explanation on how to do that or what should be set upped to do that. Moreover, trying to google something like &quot;Datadog aws alarm polling&quot; gives you vague descriptions of some other functionalities but not the AWS CloudWatch alarms.</p>
<p>My question is is it even possible ?</p>
<p>What I tried so far is set upped DataDog Lambda Forwarder that sends CloudWatch logs (metrics and alarms I suppose too?) to DD. I gave permission to that lambda. I created some AWS metric filter and AWS alarm to trigger when specific event occurs. I run some lambda code to throw exception and trigger CloudWatch alarm to change its status.</p>
<p>I clearly see lambda logs in DD, but I can't find anything related to my alarms in DD events. And I suppose it's not a problem with DD-AWS integration because we use it in big organization and it was configured long before I joined the company.
What am I doing wrong ?</p>
<p>Cloudformation script below (I removed some parts, so it's not working as is)</p>
<pre><code>Resources:
  DatadogForwarderLambda:
    Type: AWS::Lambda::Function
    Properties:
      Description: Pushes logs, metrics and traces from AWS to Datadog.
      Role: !GetAtt &quot;DatadogForwarderLambdaRole.Arn&quot;
      Handler: lambda_function.lambda_handler
      Code:
        S3Bucket: config-sandbox
        S3Key: 'aws-dd-forwarder-3.38.0.zip'
      MemorySize: 1024
      Runtime: python3.7
      Timeout: 120
      Tags:
        - Key: &quot;dd_forwarder_version&quot;
          Value:  3.38.0
      Environment:
        Variables:
          DD_ENHANCED_METRICS: &quot;false&quot;
          DD_API_KEY_SECRET_ARN: 
            Ref: DdApiKeySecret
          DD_S3_BUCKET_NAME: config-sandbox
          DD_SITE: datadoghq.com
          DD_: datadoghq.com
          DD_TAGS_CACHE_TTL_SECONDS: 300
          DD_FETCH_LAMBDA_TAGS: true
          DD_USE_TCP: false
          DD_NO_SSL: false
          REDACT_IP: false
          REDACT_EMAIL: false
          DD_USE_PRIVATE_LINK: false
          DD_USE_VPC: false
      ReservedConcurrentExecutions: 100


  DatadogReadonlyPolicy:
    Type: 'AWS::IAM::Policy'
    Properties:
      PolicyName: !Sub &quot;DatadogReadonlyPolicy&quot;
      PolicyDocument:
        Version: &quot;2012-10-17&quot;
        Statement:
          - Effect: Allow
            Action:
              - 'cloudwatch:Get*'
              - 'cloudwatch:List*'
              - 'cloudwatch:DescribeAlarmHistory'
              - 'cloudtrail:LookupEvents'
              - 'ec2:Describe'
              - 's3:GetObject'
              - 's3:PutObject'
              - 's3:DeleteObject'
              - 's3:ListBucket'
              - 'lambda:List*'
              - 'tag:GetResources'
              - 'tag:GetTagKeys'
              - 'tag:GetTagValues'
              - 'support:*'
            Resource: !GetAtt DatadogForwarderLambda.Arn
          - Effect: Allow
            Action:
              - secretsmanager:GetSecretValue
            Resource:
              - Ref: DdApiKeySecret
      Roles: 
        - !Ref DatadogForwarderLambdaRole
       

  DatadogForwarderLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
              AWS:
                - Fn::Sub:
                  - &quot;arn:aws:iam::${AccountId}:role/human-role/some-role-name&quot;
                  - { AccountId: !Ref 'AWS::AccountId' }
            Action:
              - sts:AssumeRole    
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole
      Path: /
      PermissionsBoundary:
        Fn::Join:
              - ''
              - - 'arn:aws:iam::'
                - Ref: AWS::AccountId
                - ':policy/some-organisation-permission-boundary'
      RoleName:               
        Fn::Sub:
        - 'a${AIID}-dd-forwarder-lambda-${StackID}'
        - { StackID: !Select [4, !Split [&quot;-&quot;, !Ref 'AWS::StackId']],
            AIID: !Ref AIID }


  IncomingQueueHasMessagesExceptionAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmDescription: Incoming queue has unprocessed messages, new processing round can't be started
      AlarmName: !Sub &quot;IncomingQueueHasMessagesExceptionAlarm&quot;
      ComparisonOperator: GreaterThanThreshold
      Threshold: 0 # no messages are allowed in queue if new round started
      EvaluationPeriods: 1
      Period: 10  
      Namespace: dev-logs
      MetricName: QueueHasMessagesException
      Statistic: Sum   
      TreatMissingData: missing


  IncomingQueueHasMessagesExceptionMetricFilter: 
    Type: AWS::Logs::MetricFilter
    Properties: 
      LogGroupName: 
        !Sub '/aws/lambda/${SomeLambdaName}'
      FilterPattern: &quot;QueueHasMessagesException&quot;
      MetricTransformations: 
        - 
          MetricNamespace: dev-logs
          MetricName: QueueHasMessagesException
          MetricValue: 1
  
</code></pre>",70037533,1,0,,2021-10-20 13:12:59,,2021-11-19 15:51:51,,,,,9547346,,1,0,amazon-web-services|aws-lambda|amazon-cloudformation|amazon-cloudwatch|datadog,63,7.19736
261659,0,Configuration,51933957,SMBJ no longer working when I modify JMX port settings,"<p>I have an application running under spring boot utilizing SMBJ to mount and read remote files, and it works perfectly.   However I am trying to set up some datadog reporting and trying to use JMX as a datasource for datadog... </p>

<p>TO do this I am staring the springboot jar with the following:</p>

<pre><code> ENTRYPOINT java -Djava.rmi.server.hostname=myhost 
     -Dcom.sun.management.jmxremote 
     -Dcom.sun.management.jmxremote.local.only=false 
     -Dcom.sun.management.jmxremote.port=8089 
     -Dcom.sun.management.jmxremote.rmi.port=8089 
     -Dcom.sun.management.jmxremote.ssl=false 
     -Dcom.sun.management.jmxremote.authenticate=false 
     -jar demo-0.0.1-SNAPSHOT.jar
</code></pre>

<p>And once I do this, SMBJ no longer creates the mount.  If I remove these parameters the code works fine again and SMBJ is able to create/mount to the share, If I have them it simply times out trying to create the share.  I thought maybe it was the RMI hostname change, but removing just this this doesn't seem to fix it.  </p>

<p>Can anyone offer any help on this?  Is SMBJ really dependent on the jmxremote settings?   It certainly seems to be..I have tried removing the overriding of the ports, so they go to their default ports as well, but this didn't fix it either.  </p>

<p>Any help would be appreciated.</p>",,0,2,,2018-8-20 15:23:46,,2018-8-20 15:23:46,,,,,282172,,1,0,rmi|jmx|datadog|smbj,63,7.19736
261660,3,Monitoring,52755069,Is it possible to trigger alarm based on output of bash command,<p>Is it possible to trigger alarm in zabbix or datadog based on output of bash command. I.e. I have a bash command that returns value and I want to trigger an alarm if value rises above some level.</p>,,1,0,,2018-10-11 07:59:41,,2018-10-11 11:16:24,,,,,7380427,,1,1,monitoring|zabbix|datadog,172,7.14211
261661,0,Configuration,61092487,Datadog: How to automate configuring Log Archives,"<p>How can I automate configuring Log Archives on GCP?</p>

<p>I can do it manually by following steps
<a href=""https://docs.datadoghq.com/logs/archives/?tab=googlecloudstorage"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/archives/?tab=googlecloudstorage</a></p>

<p>I guess selenium can help this 
but I looking for a more programmatic way like Terraform or REST API</p>

<p>Thank you.</p>",,1,2,,2020-4-8 02:37:29,,2021-4-6 03:12:47,2020-4-8 21:51:28,,4214037,,13254852,,1,1,google-cloud-platform|terraform|datadog,54,7.12958
261662,0,Configuration,63793906,Terraform: Automate backup of infrastructure,"<p>In my company, we have Datadog dashboards and monitors which we often make changes to and thus we wish to have some version control. After discussions it was decided that Terraform is the way to go with this. I've successfully been able to manually back up our current Datadog infrastructure using Google's <em>Terraformer</em> (to import the infrastructure as resources) and Terraform's CLI commands to import the corresponding infrastructure's states. It is also possible to make changes to the resources through commands such as <code>terraform plan</code> and <code>terraform apply</code>. The manual changes to the resources can then be backed up with Git.</p>
<p>Now to the issue I'm having. We wish to automate the the back-up of our resources, that is, through making changes in the Datadog GUI and to have those changes be reflected in our resource files. That means the other way around of what I've managed to accomplish above. It is rather straight forward to do this manually, where one imports the infrastructure resources that have been changed using the GUI (using terraformer). If &quot;conflicts&quot; arise, one needs to manually remove certain resources and re-import them which I've found difficult to automate.</p>
<p>My question is: is there a straight forward way of automating back-up of changes made &quot;remotely&quot; (e.g. in the Datadog GUI) to resource files? My research for Terraform CLI's for this purpose has lead me to nothing thus far.</p>
<p>Any suggestions are appreciated!</p>
<p>Thanks</p>",,0,2,,2020-9-8 12:25:03,,2020-9-8 12:25:03,,,,,13767817,,1,0,terraform|datadog,60,7.11261
261663,0,Integration,69284684,No data received on Datadog from Postman integration,"<p>I want to create a monitor using Postman metrics. I installed the integration, but it is showing <code>no data received</code>.</p>
<p><a href=""https://i.stack.imgur.com/YWqZv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YWqZv.jpg"" alt=""Postman integration on Datadog"" /></a></p>
<p>I have configured the integration properly and the Postman monitor is also running fine. I am not being able to find the problem. Because of this issue, I am not being able to access the Postman metrics.</p>
<p>P.S. I am using the free version of Postman.</p>
<p><a href=""https://i.stack.imgur.com/MwDx8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MwDx8.jpg"" alt=""Postman monitor"" /></a></p>",69659541,1,0,,2021-9-22 12:55:09,,2021-10-21 09:40:08,,,,,11827585,,1,1,postman|datadog,30,7.10849
261664,3,Monitoring,67004358,How to create datadog 'change alerts' using terraform?,"<p>I am trying to create a change <a href=""https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/monitor"" rel=""nofollow noreferrer"">monitor using terraform</a>. To create a monitor that checks that overtime a count stays at 0 for example every day (the value will go up to one some times and get back to 0).</p>
<p>I found on the UI the capacity to create a <code>change alert</code>.</p>
<p><a href=""https://i.stack.imgur.com/zrntL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zrntL.png"" alt=""enter image description here"" /></a></p>
<p>I cant seem to find a way to define the configuration for this type. Is terraform just supporting only a subset of the monitors? or does the query need to be change in some specific way that I cant find documentation for?.</p>",,1,0,,2021-4-8 12:50:38,,2021-9-6 08:30:11,,,,,3086403,,1,0,terraform|datadog,59,7.08341
261665,2,Query,57071999,Why do I see different values for a gauge in different timeframes?,"<p>I have a gauge in datadog and want to see the 'last' value.</p>

<p>But when I select <code>the last day</code> in the dashboard time-picker I see a different value than when I select <code>the last week</code>. My natural understanding is that 'last' gauge value should be the same regardless of how far back in time I go.</p>

<p>Am I missing some understanding of how gauges or datadog works?</p>

<p><a href=""https://i.stack.imgur.com/woCW2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/woCW2.png"" alt=""enter image description here""></a></p>",,0,0,,2019-7-17 08:49:55,,2019-7-17 08:49:55,,,,,488802,,1,1,metrics|gauge|datadog,58,7.05371
261666,1,Parse,67109633,"datadog ""ignored"" timestamp in metrics","<p>i have some problem to settings my dashboard metrics in datadog, the case is about current connection of my apps, for example when there is user connected my app the value goes add by 1, but when its disconnected it will reduced the value by 1. the problem when im using datadog, they will evaluate based on timestamp, so for example if i want to check per 5 minutes, when first 5 minutes there is 10 users connected it will add by 10 the monitoring show 10, it should not be a problem, but the problem when the next 5 minutes when there is 5 disconnected users, it will reduce the value by 5, and it should be <strong>5</strong> not <strong>-5</strong>. is there any function that i used to somehow ignore the timestamp in datadog ? additional information with last case i mention earlier if the next 5 hours there is no user that connected / disconnected again it should be show as 5 users regardless what time batch series i take. is that possible to do that in datadog ?</p>",,0,0,,2021-4-15 13:42:18,,2021-4-15 13:42:18,,,,,9813109,,1,0,datadog,57,7.0235
261667,0,Integration,67382293,Winston Http Transport not working inside MochaJS custom Reporter,"<p>I'm trying to log errors using Http Transport and it's not working properly inside the custom report EVENT_TEST_FAIL. transport.Console is logging the error when a test fails but it is not passing the data to the log server.</p>
<p>This is my code inside the custom reporter</p>
<pre><code>runner
 .on(EVENT_TEST_FAIL, (test, err) =&gt; {
   ddLogger.log('error', 'HELLO')
 })
</code></pre>
<p>Here's the code for ddLogger:</p>
<pre><code>const httpTransport = new transports.Http({
  host: 'http-intake.logs.datadoghq.com',
  path: `/v1/input/${process.env.DATADOG_API_KEY}?ddsource=nodejs&amp;service=PEPINO`,
  ssl: true,
})

const ddLogger = createLogger({
  level: 'error',
  exitOnError: false,
  format: format.json(),
  transports: [
    httpTransport,
    // new transports.Console
  ],
})
</code></pre>
<p>This outputs {&quot;level&quot;:&quot;error&quot;,&quot;message&quot;:&quot;HELLO&quot;} in Console but data is not passed to log server.</p>",,1,0,,2021-5-4 09:39:59,,2021-11-6 19:37:54,2021-5-10 12:27:55,,1761130,,1761130,,1,1,mocha.js|winston|datadog,50,6.99588
261668,0,Integration,66378338,Datadog logging within Gitlab pipeline,"<p>Hey I am trying to logging within a script and send a log message to Datadog via their api: POST <a href=""https://http-intake.logs.datadoghq.com/v1/input"" rel=""nofollow noreferrer"">https://http-intake.logs.datadoghq.com/v1/input</a>. My issue is that the code works perfectly well when running on the team's machine but once it's in the pipeline, it keeps throwing 400 errors as a response from Datadog. The Key is perfectly valid and the log message works on our machine so I cannot see why I am getting a 400 error. Wondering if anyone else has run into this problem</p>
<p><a href=""https://i.stack.imgur.com/1FrmQ.png"" rel=""nofollow noreferrer"">pic of gitlab-cl.yml</a></p>
<p>Thanks,</p>",,0,2,,2021-2-26 00:13:13,,2021-3-1 23:05:44,2021-3-1 23:05:44,,8797952,,8797952,,1,0,gitlab|gitlab-ci-runner|datadog,56,6.99275
261669,3,Monitoring,68124683,send log messages from datadog to slack channel,<p>Is it possible to send log messages(content in the log message) in Datadog to one of the slack channel? I have tried using the monitors. But there it just provides an alert if log messages exceeds the provided threshold level. I need the exact log message to be sent to the slack channel.</p>,,0,2,,2021-6-25 02:29:05,,2021-6-25 02:29:05,,,,,13194342,,1,2,logging|slack|datadog,56,6.99275
261670,1,Parse,68719662,Grok Parsing a log file with nested JSON,"<p>I'm trying to use a Grok parser (inside of Datadog's Logs service) to extract the json key:values in the following log lines (dont worry, I randomized some of the values):</p>
<pre><code>[INFO]  2021-08-09T23:20:48.282Z    49be9ba-000c-4d90-1011-10101001 Received event: {&quot;PayloadData&quot;: &quot;Yq==&quot;, &quot;WirelessDeviceId&quot;: &quot;abcd1234-abc12-3456-78910-abc9adkasd&quot;, &quot;WirelessMetadata&quot;: {&quot;LoRaWAN&quot;: {&quot;ADR&quot;: true, &quot;Bandwidth&quot;: 125, &quot;ClassB&quot;: false, &quot;CodeRate&quot;: &quot;4/5&quot;, &quot;DataRate&quot;: &quot;3&quot;, &quot;DevAddr&quot;: &quot;019cf43e&quot;, &quot;DevEui&quot;: &quot;844oda0000006a80&quot;, &quot;FCnt&quot;: 2530, &quot;FOptLen&quot;: 0, &quot;FPort&quot;: 10, &quot;Frequency&quot;: &quot;903900000&quot;, &quot;Gateways&quot;: [{&quot;GatewayEui&quot;: &quot;abc123fffp09fd12&quot;, &quot;Rssi&quot;: -102, &quot;Snr&quot;: 8}], &quot;MIC&quot;: &quot;31p41ed2&quot;, &quot;MType&quot;: &quot;UnconfirmedDataUp&quot;, &quot;Major&quot;: &quot;LoRaWANR1&quot;, &quot;Modulation&quot;: &quot;LORA&quot;, &quot;PolarizationInversion&quot;: false, &quot;SpreadingFactor&quot;: 7, &quot;Timestamp&quot;: &quot;2021-08-09T23:20:47Z&quot;}}}
</code></pre>
<p>But I can't quite figure out how to do it properly.  Grok feels very tedious and fragile.  The format of the logs remain the same as above, but obviously the values are constantly changing.</p>
<p>Any recommendations on how to get started down the right path?</p>",,0,0,,2021-8-9 23:49:29,,2021-8-9 23:49:29,,,,,15692365,,1,0,json|parsing|logging|datadog|grok,56,6.99275
261671,1,Method,66875990,How does the serverless datadog forwarder encrypt/encode their logs?,"<p>I am having trouble figuring out how the datadog forward encodes/encrypts its messages from the datadog forwarder. We are utilizing the forwarder on datadog using the following documentation: <a href=""https://docs.datadoghq.com/serverless/forwarder/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/serverless/forwarder/</a> . On that page there, Datadog has an option to send the same event to another lambda that it invokes via the AdditionalTargetLambdaARNs flag. We are doing this and having the other lambda invoke but the event input that we are getting is long string that looks like it is base64 encoded but when I put it into a base64 decoder, I get gibberish back. I was wondering if anyone knew how datadog is compressing/encoding/encrypting their data/logs that they send so that I can read the logs in my lambda and be able to preform actions off of the data being forwarded? I have been searching google and the datadog site for documentation on this but I can't find any.</p>",,1,0,,2021-3-30 17:47:44,,2021-3-31 00:15:15,,,,,1467883,,1,0,aws-lambda|datadog,54,6.92957
261672,3,Monitoring,51363915,use data-dog for testing a web app,"<p>I am using webdriver-io to test a web-app, the test automation framework runs every day and stores the result in json files. Can I use the data from these json files to create a dashboard. Is this possible?</p>",,0,1,,2018-7-16 14:16:00,,2018-7-16 14:21:44,2018-7-16 14:21:44,,5912110,,5912110,,1,1,testing|webdriver-io|datadog,54,6.92957
261673,2,Query,68786967,Ccloudexporter - How to count messages sent to a Kafka topic,"<p>We are using <a href=""https://github.com/Dabz/ccloudexporter"" rel=""nofollow noreferrer"">https://github.com/Dabz/ccloudexporter</a> in order to monitor our Kafka broker. We collect the metrics using Datadog and we are trying to count the number of messages sent to specific topics in certain periods of time.</p>
<p>We've created a timeseries visualization using the ccloud_metric_sent_records metric, but in the overview visualization, the sum column is way larger than the number of sent messages logged by the producer application.
And it's even weirder that the sum is larger when we filter for the last 2 days than when we filter for the last week.</p>
<p>Is it possible to count the number of messages sent to a topic using ccloudexporter and Datadog? If so are we using the wrong metric?</p>",,0,0,,2021-8-14 20:54:16,,2021-8-14 20:54:16,,,,,2751115,,1,0,apache-kafka|confluent-platform|datadog,54,6.92957
261674,1,Parse,58210228,Datadog string encoding when character is not from English alphabet,"<p>I have a service and send Datadog events from it using com.github.arnabk.java-dogstatsd-client. In order to send json string I use <code>JsonObject</code> where I put all properties which I need then convert it to string using <code>toString()</code> method on <code>JsonObject</code> and send string as a message body. Everything works perfect unless I have a character in a string which is not from english alphabet. Example: µ. In this case instead of having correct json <code>{""Smth"":""µ""}</code> in Datadog I'm getting incorrect string without closing curly brace <code>{""Smth"":""µ""</code>. Has anybody experienced the same and knows how to deal with this?</p>",,0,4,,2019-10-2 22:42:36,,2019-10-2 22:42:36,,,,,5634191,,1,0,java|datadog,54,6.92957
261675,3,Monitoring,69089581,How to create ImagePullBackOff alert and its recovery alert for Kubernetes on Datadog?,"<p>I am trying to create an alert due to <code>ImagePullBackOff</code> on Kubernetes cluster using Datadog as following (for details see <a href=""https://www.datadoghq.com/blog/create-manage-kubernetes-alerts-datadog/"" rel=""nofollow noreferrer"">this</a> documentation)</p>
<p><a href=""https://i.stack.imgur.com/ZgQSt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZgQSt.png"" alt=""metric description"" /></a></p>
<p>Although the alert is created properly, I am having a problem with the recovery alert. It never recovers after the error is corrected, since after it is corrected the pod is destroyed and it never goes to zero as shown below. Is there any way to create the recovery alert?</p>
<p><a href=""https://i.stack.imgur.com/Z2pPk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z2pPk.png"" alt=""pods"" /></a></p>",,1,0,,2021-9-7 13:58:30,,2021-9-20 10:11:48,2021-9-7 18:20:50,,781754,,11827585,,1,0,kubernetes|datadog,54,6.92957
261676,0,Integration,68092279,Fixing DataDog agent congestion issues in Amazon EKS cluster,"<p>A few months ago I integrated DataDog into my Kubernetes cluster by using a DaemonSet configuration. Since then I've been getting congestion alerts with the following message:</p>
<blockquote>
<p>Please tune the hot-shots settings
<a href=""https://github.com/brightcove/hot-shots#errors"" rel=""nofollow noreferrer"">https://github.com/brightcove/hot-shots#errors</a></p>
</blockquote>
<p>By attempting to follow the docs with my limited Orchestration/DevOps knowledge, what I could gather is that I need to add the following to my DaemonSet config:</p>
<pre><code>spec
.
.
  securityContext:
        sysctls:
          - name: net.unix.max_dgram_qlen
            value: &quot;1024&quot;
          - name: net.core.wmem_max
            value: &quot;4194304&quot;
</code></pre>
<p>I attempted to add that configuration piece to one of the auto-deployed DataDog pods directly just to try it out but it hangs indefinitely and doesn't save the configuration (Instead of adding to DaemonSet and risking bringing all agents down).</p>
<p>That hot-shots documentation also mentions that the above <code>sysctl</code> configuration requires unsafe sysctls to be enabled in the nodes that contain the pods:</p>
<pre><code>kubelet --allowed-unsafe-sysctls \
  'net.unix.max_dgram_qlen, net.core.wmem_max'
</code></pre>
<p>The cluster I am working with is fully deployed with EKS by using the Dashboard in AWS (Little knowledge on how it is configured). The above seems to be indicated for manually deployed and managed cluster.</p>
<ul>
<li>Why is the configuration I am attempting to apply to a single DataDog agent pod not saving/applying? Is it because it is managed by DaemonSet or is it because it doesn't have the proper <code>unsafe sysctl</code> allowed? Something else?</li>
<li>If I do need to enable the suggested <code>unsafe sysctl</code>on all nodes of my cluster. How do I go about it since the cluster is fully deployed and managed by Amazon EKS?</li>
</ul>",,0,0,,2021-6-23 00:56:34,,2021-6-23 00:56:34,,,,,9643936,,1,0,amazon-web-services|kubernetes|amazon-eks|datadog,53,6.8971
261677,0,Integration,67334771,How to run the datadog agent container in kubernetes?,"<p>Looking to send metrics/logs from Elasticsearch deployments to datadog, so thinking to deploy the <a href=""https://hub.docker.com/r/datadog/agent"" rel=""nofollow noreferrer"">dd agent</a> inside kubernetes to ship es metrics/logs to dd with the <a href=""https://docs.datadoghq.com/integrations/elastic/?tab=host"" rel=""nofollow noreferrer"">ES integration</a>. What should the deployment.yml look like assuming using a deployment set?</p>",,0,0,,2021-4-30 13:28:20,,2021-4-30 13:28:20,,,,,1698794,,1,0,elasticsearch|kubernetes|datadog,53,6.8971
261678,1,Error,67957930,datadog chef wrapped cookbook failing,"<p>I have created a wrapped cookbook from datadog cookbook, and for some reason is failing repository.rb recipe. I must say that if a run kitchen in the original datadog cookbook it works really good. I'm really new in CHEF and Im not been able of solving this. Any help would be awesome. Thanks in advance :)</p>
<pre><code>kitchen converge dd-agent-ubuntu-1804-14
-----&gt; Starting Test Kitchen (v2.11.2)
$$$$$$ Deprecated configuration detected:
require_chef_omnibus
Run 'kitchen doctor' for details.

-----&gt; Converging &lt;dd-agent-ubuntu-1804-14&gt;...
       Preparing files for transfer
       Preparing dna.json
       Resolving cookbook dependencies with Berkshelf 7.2.0...
       Removing non-cookbook files before transfer
       Preparing solo.rb
-----&gt; Chef installation detected (14)
       Transferring files to &lt;dd-agent-ubuntu-1804-14&gt;
       Starting Chef Client, version 14.15.6
       resolving cookbooks for run list: [&quot;mycookbook_datadog::default&quot;]
       Synchronizing Cookbooks:
         - mycookbook_datadog (1.0.1)
         - chef_handler (3.0.3)
         - datadog (4.0.1)
         - yum (6.1.1)
         - apt (7.4.0)
       Installing Cookbook Gems:
       Compiling Cookbooks...
       [2021-06-13T11:39:12+00:00] WARN: Resource chef_handler from the client is overriding the resource from a cookbook. Please upgrade your cookbook or remove the cookbook from your run_list.
       Recipe: datadog::dd-handler
         * chef_gem[chef-handler-datadog] action install (up to date)
         * chef_handler[Chef::Handler::Datadog] action enable (skipped due to only_if)
         Converging 21 resources
       Recipe: datadog::dd-agent
         * ruby_block[datadog-api-key-unset] action run (skipped due to only_if)
       Recipe: apt::default
         * file[/var/lib/apt/periodic/update-success-stamp] action nothing (skipped due to action :nothing)
         * apt_update[periodic] action periodic (up to date)
         * execute[apt-get update] action nothing (skipped due to action :nothing)
         * execute[apt-get autoremove] action nothing (skipped due to action :nothing)
         * execute[apt-get autoclean] action nothing (skipped due to action :nothing)
         * directory[/var/cache/local] action create (up to date)
         * directory[/var/cache/local/preseeding] action create (up to date)
         * template[/etc/apt/apt.conf.d/10dpkg-options] action create (up to date)
         * template[/etc/apt/apt.conf.d/10recommends] action create (up to date)
         * apt_package[apt-transport-https, gnupg, dirmngr] action install (up to date)
       Recipe: datadog::repository
         * apt_package[install-apt-transport-https] action install (up to date)
         * apt_repository[datadog] action add
           
           ================================================================================
           Error executing action `add` on resource 'apt_repository[datadog]'
           ================================================================================
           
           NoMethodError
           -------------
           undefined method `start_with?' for nil:NilClass
           
           Resource Declaration:
           ---------------------
           # In /tmp/kitchen/cache/cookbooks/datadog/recipes/repository.rb
           
            53:   apt_repository 'datadog' do
            54:     keyserver keyserver
            55:     key 'A2923DFF56EDA6E76E55E492D3A80E30382E94DE'
            56:     uri node['datadog']['aptrepo']
            57:     distribution node['datadog']['aptrepo_dist']
            58:     components components
            59:     action :add
            60:     retries retries
            61:   end
            62: 
           
           Compiled Resource:
           ------------------
           # Declared in /tmp/kitchen/cache/cookbooks/datadog/recipes/repository.rb:53:in `from_file'
           
           apt_repository(&quot;datadog&quot;) do
             action [:add]
             default_guard_interpreter :default
             declared_type :apt_repository
             cookbook_name &quot;datadog&quot;
             recipe_name &quot;repository&quot;
             keyserver &quot;hkp://keyserver.ubuntu.com:80&quot;
             key [&quot;XXXXX&quot;]
             uri nil
             distribution &quot;stable&quot;
             components [&quot;7&quot;]
             retries 4
             repo_name &quot;datadog&quot;
           end
           
           System Info:
           ------------
           chef_version=14.15.6
           platform=ubuntu
           platform_version=18.04
           ruby=ruby 2.5.8p224 (2020-03-31 revision 67882) [x86_64-linux]
           program_name=/opt/chef/bin/chef-solo
           executable=/opt/chef/bin/chef-solo
           
       
       Running handlers:
       [2021-06-13T11:39:22+00:00] ERROR: Running exception handlers
       Running handlers complete
       [2021-06-13T11:39:22+00:00] ERROR: Exception handlers complete
       Chef Client failed. 0 resources updated in 11 seconds
       [2021-06-13T11:39:22+00:00] FATAL: Stacktrace dumped to /tmp/kitchen/cache/chef-stacktrace.out
       [2021-06-13T11:39:22+00:00] FATAL: Please provide the contents of the stacktrace.out file if you file a bug report
       [2021-06-13T11:39:22+00:00] FATAL: NoMethodError: apt_repository[datadog] (datadog::repository line 53) had an error: NoMethodError: undefined method `start_with?' for nil:NilClass
&gt;&gt;&gt;&gt;&gt;&gt; ------Exception-------
&gt;&gt;&gt;&gt;&gt;&gt; Class: Kitchen::ActionFailed
&gt;&gt;&gt;&gt;&gt;&gt; Message: 1 actions failed.
&gt;&gt;&gt;&gt;&gt;&gt;     Converge failed on instance &lt;dd-agent-ubuntu-1804-14&gt;.  Please see .kitchen/logs/dd-agent-ubuntu-1804-14.log for more details
&gt;&gt;&gt;&gt;&gt;&gt; ----------------------
&gt;&gt;&gt;&gt;&gt;&gt; Please see .kitchen/logs/kitchen.log for more details
&gt;&gt;&gt;&gt;&gt;&gt; Also try running `kitchen diagnose --all` for configuration
</code></pre>",,0,0,,2021-6-13 11:51:11,,2021-6-13 11:51:11,,,,,9292104,,1,0,chef-infra|cookbook|datadog,52,6.86401
261679,0,Configuration,66833514,"what is the ""user"" in datadog","<p>I am little embarrased, as this is probably very banal, but it's really not making any sense to me.</p>
<p>I have a little golang web-application, that I for now am just localhosting.</p>
<p>I have some logging with logrus going on in the project, and want to use datadog to get a nice and visual dashboard for my application.</p>
<p>So I follow a bunch of the steps on their page, download their dockerimage and run it, and i am able to see some data!</p>
<p>I then follow the instructions further on, under a headline called &quot;docker integration&quot;.
here I am asked to run the following command:</p>
<pre><code> usermod -a -G docker dd-agent
</code></pre>
<p>To which the response &quot;cannot find user &quot;dd-agent&quot;&quot; is given. But what is this &quot;user&quot; value?
Is it the user to which I have signed up to datadog with? something else?
I find it very unspecified?
would very much appreciate a helping hand</p>",,0,2,,2021-3-27 16:19:34,,2021-3-27 16:19:34,,,,,15245005,,1,1,go|datadog,51,6.83028
261680,1,Method,69109810,DataDog instrumentation breaks code fragment when Axios post errors,"<p>To isolate a problem I'm seeing after instrumenting an ExpressJS app with dd-trace I've moved some code out of a utility file into the mainline code.</p>
<p><code>this.client</code> is an Axios instance.</p>
<p>This snippet works regardless of whether or not the POST is successful:</p>
<pre class=""lang-js prettyprint-override""><code>return (async (fn) =&gt; {
  try {
    return Promise.resolve(await fn())
  } catch (e) {
    return Promise.reject(e)
  }

  return Promise.reject(new Error('Unusual Error'))
})(() =&gt; this.client.post(`foo/bars`, data, opts))
</code></pre>
<p>With the following snippet the DD instrumentation code throws an exception and hangs if the Axios POST throws an error. (Yes, it's a single-iteration <code>for</code> loop. Work with me.)</p>
<pre class=""lang-js prettyprint-override""><code>return (async (fn) =&gt; {
  for (let i = 0; i &lt; 1; i++) {
    try {
      return Promise.resolve(await fn())
    } catch (e) {
      return Promise.reject(e)
    }
  }

  return Promise.reject(new Error('Unusual Error'))
})(() =&gt; this.client.post(`foo/bars`, data, opts))
</code></pre>
<p>IRL the code is incrementally trickier, but these fragments reproduce the problem.</p>
<p>Top snippet works w/ instrumentation for both successful and error POSTs, bottom snippet works on success, fails on error.</p>
<p>Not sure how to think about this. Logging doesn't help much (and is hampered w/ an indestructible health ping to boot).</p>
<p>I assume I'm missing something silly. I can provide additional info if needed.</p>
<p>The exception I've seen under some circumstances:</p>
<pre><code>TypeError: Cannot read property 'push' of undefined
  at DatadogSpan._createContext (/app/node_modules/dd-trace/packages/dd-trace/src/opentracing/span.js:76:32)
  at new DatadogSpan (/app/node_modules/dd-trace/packages/dd-trace/src/opentracing/span.js:30:30)
  at DatadogTracer._startSpanInternal (/app/node_modules/dd-trace/packages/dd-trace/src/opentracing/tracer.js:73:18)
  at DatadogTracer._startSpan (/app/node_modules/dd-trace/packages/dd-trace/src/opentracing/tracer.js:62:17)
  at DatadogTracer.Tracer.startSpan (/app/node_modules/opentracing/lib/tracer.js:61:21)
  at Object.wrapMiddleware (/app/node_modules/dd-trace/packages/dd-trace/src/plugins/util/web.js:104:25)
  at callHandle (/app/node_modules/dd-trace/packages/datadog-plugin-router/src/index.js:111:14)
  at wrapCallHandle (/app/node_modules/dd-trace/packages/datadog-plugin-router/src/index.js:41:14)
  at Layer.handle_error (/app/node_modules/express/lib/router/layer.js:71:5)
  at trim_prefix (/app/node_modules/express/lib/router/index.js:315:13)
</code></pre>",,0,7,,2021-9-8 21:22:33,,2021-9-8 23:25:11,2021-9-8 23:25:11,,438992,,438992,,1,0,javascript|node.js|express|datadog,51,6.83028
261681,2,Query,56277311,How to list instances that have tag1:<anyvalue> AND tag2:<anyvalue> AND tag3:<anyvalue>,"<p>I need to identify in my infrastructure which hosts have tag1 and tag2 and tag3.
I'm new to datadog but it seems that when filtering I have to specify a value for a specific tag.
I also need to identify the inverse, list hosts that have tag1 but are missing tag2 OR tag3.</p>

<p>I have setup a dashboard for each of the tags, I seem to be limited by up 2 tags.
e.g filter by -> env:dev<br>
    group by -> tag1 , tag2</p>

<p>I would expect to be able to see what hosts have tag1 AND tag2 AND tag3</p>

<p>And the inverse -> what hosts do not have all 3 tags.</p>",,1,0,,2019-5-23 14:20:18,,2019-5-26 08:01:46,,,,,8353186,,1,0,amazon-web-services|tags|monitoring|terraform|datadog,49,6.76078
261682,0,Integration,67767886,aws lambda datadog integration import issue,"<p>I have a lambda written in python. I have been trying to send some custom metrics from this lambda to datadog. but trying to do so I get the below error</p>
<pre><code>[ERROR] Runtime.ImportModuleError: Unable to import module 'index': cannot import name '_rand' from 'ddtrace.internal' (/var/task/ddtrace/internal/__init__.py)
Traceback (most recent call last):
</code></pre>
<p>my requirement.txt file for the lambda is as below ,</p>
<pre><code>requests==2.25.0
logging-formatter-anticrlf==1.2
pymysql==1.0.2
datadog==0.39.0
ddtrace==0.45.0
datadog-lambda==2.28.0
</code></pre>
<p>from my understanding, this happens due to a dependency mismatch. can someone help me with this ?</p>",,0,1,,2021-5-31 04:23:07,,2021-5-31 04:23:07,,,,,4859626,,1,0,python|aws-lambda|datadog,48,6.72497
261683,2,Query,70046214,"Terraform Datadog: trace_service_definition does not accept block for ""query"" or ""formula"" even tho it is in documentation","<p>Am i doing something wrong?</p>
<pre><code>  widget {
    widget_layout {
      x      = 0
      y      = 47
      width  = 50
      height = 25
    }
    timeseries_definition {
      request {
        formula {
            formula_expression = &quot;query1 * 100&quot;
            alias              = &quot;Total Session Capacity&quot;
        }
        query {
          metric_query {
            data_source = &quot;metrics&quot;
            query       = &quot;sum:.servers.available{$region,$stage,$service-name} by {availability-zone}&quot;
            name        = &quot;query1&quot;
          }
        }
      }
    }
  }
</code></pre>
<p>Documentation links:</p>
<p><a href=""https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/dashboard#nestedblock--widget--group_definition--widget--timeseries_definition--request--query"" rel=""nofollow noreferrer"">https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/dashboard#nestedblock--widget--group_definition--widget--timeseries_definition--request--query</a></p>
<p><a href=""https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/dashboard#nested-schema-for-widgetgroup_definitionwidgettimeseries_definitionrequestformula"" rel=""nofollow noreferrer"">https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/dashboard#nested-schema-for-widgetgroup_definitionwidgettimeseries_definitionrequestformula</a></p>
<pre><code>$terraform --version
Terraform v1.0.11
on darwin_amd64
+ provider registry.terraform.io/datadog/datadog v2.21.0
</code></pre>
<pre><code>$terraform validate
╷
│ Error: Unsupported block type
│ 
│   on weekly_ops_dashboard.tf line 152, in resource &quot;datadog_dashboard&quot; &quot;weekly_ops&quot;:
│  152:         formula {
│ 
│ Blocks of type &quot;formula&quot; are not expected here.
╵
╷
│ Error: Unsupported block type
│ 
│   on weekly_ops_dashboard.tf line 156, in resource &quot;datadog_dashboard&quot; &quot;weekly_ops&quot;:
│  156:         query {
│ 
│ Blocks of type &quot;query&quot; are not expected here.
</code></pre>",70046391,1,0,,2021-11-20 13:09:23,,2021-11-20 13:32:20,,,,,1648015,,1,0,terraform|datadog,25,6.59176
261684,1,Method,67170011,How do I add a Subrouter to muxtrace.Router()?,"<p>I currently have a function in Go that creates a subrouter if a mountpoint is passed when creating a new server. I want to set up tracing using <code>muxtrace</code>.</p>
<pre><code>import {
  muxtrace &quot;gopkg.in/DataDog/dd-trace-go.v1/contrib/gorilla/mux&quot;
  ...
}

...

router := muxtrace.NewRouter()

if mountpoint != &quot;&quot; {
    router = router.PathPrefix(mountpoint).Subrouter()
}
</code></pre>
<p><code>router = router.PathPrefix(mountpoint).Subrouter()</code> returns an error:</p>
<pre><code>cannot use router.PathPrefix(mountpoint).Subrouter() (value of type *mux.Router) as *mux.Router value in assignment
</code></pre>
<p>It doesn't look like there is a Subrouter for the <code>*mux.Router</code> type used in the tracing package. So how would I trace the sub route?</p>",,0,0,,2021-4-19 22:16:07,,2021-4-19 22:16:07,,,,,10709519,,1,0,go|backend|trace|datadog|go-server,44,6.57381
261685,0,Integration,68237977,Datadog - Instrumentation of Python Lambda,"<p>We are trying to enable APM for the lambda function developed in python 3.7 by following these steps - <a href=""https://docs.datadoghq.com/serverless/installation/python/?tab=custom"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/serverless/installation/python/?tab=custom</a></p>
<p>So far we have installed –</p>
<ol>
<li>Datadog-forwarder</li>
<li>Datadog-lambda-library (using the package option)</li>
</ol>
<p>But we are not able to install the datadog lambda extension. As per the documentation, we have to refer the extension as a layer of our Lambda function. But our IAM roles don’t have permission to execute “GetLayerVersion” from other account-id.</p>
<p>is it possible to install the lambda extensions locally (using pip install) and then deploy this as an extension? Please suggest for any other workaround</p>",,0,0,,2021-7-3 16:13:32,,2021-7-3 16:13:32,,,,,9795624,,1,0,datadog,44,6.57381
261686,3,Monitoring,68874923,Real User Monitoring for a C# Desktop App,"<p>I am looking a viable approach to record the usage / performance of various parts of a C# desktop application. I see strong parallels with the web focused <a href=""https://docs.datadoghq.com/real_user_monitoring/"" rel=""nofollow noreferrer"">Real User Monitoring</a> technique and have access to Datadog to process my data.</p>
<p>Is RUM outside the web a viable approach? I believe I will have to implement the equivalent of the JS Datadog SDK if I want to go down this road, is there a way to do this out of the box?</p>
<p>I want to be able to track user usage for certain function calls and capturing exceptions seem sensible.</p>",69320187,1,0,,2021-8-21 16:38:55,,2021-9-24 19:24:46,2021-8-22 17:17:53,,383838,,383838,,1,-1,c#|datadog,49,6.56078
261687,0,Integration,69945055,Docker -v flag translated to AWS ECS Task Definition,"<p>I'm following the datadog guide here: <a href=""https://docs.datadoghq.com/database_monitoring/setup_postgres/aurora/?tab=docker"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/database_monitoring/setup_postgres/aurora/?tab=docker</a></p>
<p>which says to run this docker command:</p>
<pre><code>docker run -e &quot;DD_API_KEY=${DD_API_KEY}&quot; \
  -v /var/run/docker.sock:/var/run/docker.sock:ro \
  -l com.datadoghq.ad.check_names='[&quot;postgres&quot;]' \
  -l com.datadoghq.ad.init_configs='[{}]' \
  -l com.datadoghq.ad.instances='[{
    &quot;dbm&quot;: true,
    &quot;host&quot;: &quot;&lt;AWS_INSTANCE_ENDPOINT&gt;&quot;,
    &quot;port&quot;: 5432,
    &quot;username&quot;: &quot;datadog&quot;,
    &quot;password&quot;: &quot;&lt;UNIQUEPASSWORD&gt;&quot;
  }]' \
  gcr.io/datadoghq/agent:${DD_AGENT_VERSION}
</code></pre>
<p>That's all well and good, the labels are easy to configure; what's not clear to me is how to set the task definition for the volume (ideally in the console)</p>
<p>I'm not sure how to translate <code>-v /var/run/docker.sock:/var/run/docker.sock:ro</code> into these inputs:
<a href=""https://i.stack.imgur.com/hnb7T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hnb7T.png"" alt=""ECS Task"" /></a>
<a href=""https://i.stack.imgur.com/VBoe3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VBoe3.png"" alt=""ECS Container"" /></a></p>
<p>I currently have this in my <code>Dockerfile</code> (but I think that's only part of the solution - and potentially incorrect):</p>
<p><code>VOLUME [&quot;/var/run/docker.sock:/var/run/docker.sock:ro&quot;]</code></p>",,1,0,,2021-11-12 15:11:50,,2021-11-12 18:15:35,,,,,3425324,,1,0,docker|amazon-ecs|aws-fargate|datadog|ecs-taskdefinition,43,6.53387
261688,2,Query,63807817,Can I sum up different logs in a query widget for datadog dashboard?,"<p>I have a bunch of logs that are supposed to be summed up into a value, that I would like to monitor. I tried sum function in the query widget and in table widget but nether seems to work with logs and not metrics. Anyone did something similar? Thanks</p>",,0,0,,2020-9-9 08:34:54,,2020-9-9 08:34:54,,,,,6082943,,1,2,logging|metrics|datadog,43,6.53387
261689,1,Parse,66632769,Do Datadog has the ability to exclude some of the websites from profiling?,"<p>Do Datadog has the ability to exclude some of the websites from profiling?
ie, Suppose client has 10 websites hosted in IIS but he needs only 3 websites to be profiled.Is it possible to do so?
Any help is highly appreciated :)</p>",,0,4,,2021-3-15 05:24:53,,2021-3-15 06:40:06,2021-3-15 06:40:06,,8063431,,8063431,,1,0,profiling|datadog,43,6.53387
261690,2,Query,64683318,Is there a way to search the entire log entry for text,"<p>Lets suppose I have a log entry like so:</p>
<pre><code>{&quot;@timestamp&quot;:&quot;2020-11-04T15:44:19.852Z&quot;,&quot;message&quot;:&quot;Done received event&quot;, &quot;level&quot;:&quot;INFO&quot;,&quot;eventMetadata.id&quot;:&quot;07lbGJhAoql_Em0NF24yT8z&quot;,&quot;event.partition&quot;:&quot;58/64&quot;,&quot;goober.id&quot;:&quot;slba_H1METFVDufIc4CKAhpUqs&quot;}
</code></pre>
<p>I want to search everything for slba_H1METFVDufIc4CKAhpUqs, not just the message.
I don't want the user to have know any kind of facet name like goober.id.</p>
<p>Any ideas or tips?</p>
<p>Thanks, Eric</p>",,0,0,,2020-11-4 15:49:15,,2020-11-4 15:49:15,,,,,9268952,,1,1,json|logging|datadog,42,6.493
261691,0,Configuration,68346624,Need to setup a Customized Kubernetes Logging strategy,"<p>So far in our legacy deployments of webservices to VM clusters, we have effectively been using Log4j2 based multi-file logging on to a persistent Volume where the log files are rolled over each day. We have a need to maintain logs for about 3 months, before they can be purged.</p>
<p>We are migrating to a Kubernetes Infrastructure and have been struggling on what would be the best logging strategy to adapt with Kubernetes Clusters. We don't quite like the strategies involving spitting out all logging to STDOUT/ERROUT and using come centralized tools like Datadog to manage the logs.</p>
<p>Our Design requirements for the Kubernetes Logging Solution are:</p>
<ol>
<li>Using Lo4j2 to multiple files appenders.</li>
<li>We want to maintain the multi-file log appender structure.</li>
<li>We want to preserve the rolling logs in archives for about 3-months</li>
<li>Need a way to have easy access to the logs for searching, filtering  etc.</li>
<li>The Kubectrl setup for viewing logs may be a bit too cumbersome for our needs.</li>
<li>Ideally we would like to use the Datadog dashboard approach BUT using multi-file appenders.</li>
<li>The serious limitation of Datadog we run into is the need for having everything pumped to STDOUT.</li>
</ol>",68347683,1,0,,2021-7-12 11:30:06,,2021-8-24 15:44:16,,,,,1439695,,1,-3,kubernetes|logging|log4j2|kubectl|datadog,33,6.47406
261692,1,Parse,68162086,Implementing custom log levels in logback (not markers) for use in datadog filters,"<p>I am trying to implement a custom log level for some of our logs which are extensively repeated and are not required in the INFO logs. I know that in log4j  exists but I am not sure if it does in logback and I couldn't find any reference to the same.</p>
<p>I've seen a lot of suggestions for other similar questions asking to implement markers instead but that will print out as an INFO log level message itself right?</p>
<p>The reason why I want to implement a custom log level is so that it is easy to filter out in datadog.</p>",,0,1,,2021-6-28 10:54:40,,2021-6-28 10:54:40,,,,,2910554,,1,0,java|log4j|logback|datadog,41,6.45114
261693,3,Visualization,69102390,Datadog wrong displaying of http.server.requests.count metric,"<p>In my project we use spring-boot2 with actuator and datadog metrics - package <code>io.micrometer:micrometer-registry-datadog</code>.</p>
<p>Metrics are sent to datadog by http API.</p>
<p>I've created time series chart in datadog for <code>http.server.requests.count</code> to see how many requests per seconds we have.</p>
<p>I see that depending on chosen time window in datadog I see completely different values on charts.</p>
<p>E.g. for 15 minutes I see around 300req/s</p>
<p><a href=""https://i.stack.imgur.com/OhxF1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OhxF1.png"" alt=""enter image description here"" /></a></p>
<p>while for 1 day it is around 50req/s.</p>
<p><a href=""https://i.stack.imgur.com/7y0up.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7y0up.png"" alt=""enter image description here"" /></a></p>
<p>I've experimented with <code>http.server.requests.count</code> in datadog, to change it to be treated as rate occurrence/sec, but it didn't help.</p>
<p>I've added <code>@Timed</code> annotation on controllers, and customized datadog step to 30s. There rest is default.</p>
<p>Do you have any ideas why it is happening? Where I made a mistake?</p>",,0,0,,2021-9-8 11:41:33,,2021-9-8 11:41:33,,,,,3568385,,1,0,java|spring-boot-actuator|datadog|micrometer|spring-micrometer,41,6.45114
261694,0,Integration,56993743,"Django2 is trying to render Jinja2 templates, even though it's a REST API","<p>We're utilizing the <code>django-rest-framework</code> to create a RESTful API and using generic views or view sets to create the endpoint views. There is no templating happening, all the frontend is in React. </p>

<p>However, upon watching the traces in Datadog, we're seeing that SOMETIMES (not every time), Jinja2 is rendering, causing a 500-800ms latency. Does anyone have any clues to why this might be happening and how to turn it off?</p>",,1,0,,2019-7-11 16:35:48,,2019-7-12 21:03:23,2019-7-11 18:07:30,,8661686,,1224827,,1,0,django-rest-framework|jinja2|datadog|django-2.2|django-rest-viewsets,40,6.40824
261695,0,Configuration,68084887,DataDog directory - listen multiple directories at one host,"<p>I need to know is there a possibility to listen to two directories at the same time on one host.</p>
<p><a href=""https://docs.datadoghq.com/integrations/directory/"" rel=""nofollow noreferrer"">Directory (datadog.com)</a></p>
<p>I used an example document <a href=""https://github.com/DataDog/integrations-core/blob/master/directory/datadog_checks/directory/data/conf.yaml.example"" rel=""nofollow noreferrer""><code>system.disk.directory</code> - Configuration example</a> for initial setup.
One directory is fine, but I need to listen to two directories.</p>
<pre><code>init_config:

instances:
  - directory: /first/directory/path/
    filegauges: true

  # second one is ignored?
  - directory: /second/directory/path/
    filegauges: true
</code></pre>
<p>Is this even possible?</p>",68085908,1,0,,2021-6-22 13:55:30,,2021-6-22 14:55:35,,,,,6166504,,1,0,analytics|metrics|datadog|infrastructure,22,6.36969
261696,1,Method,68738629,Datadog airflow metric,"<p>Is there any way, we can find total number of tasks(irrespective of state) and scheduled time of dag using airflow metric such as:</p>
<pre><code>airflow.dag.task.duration
</code></pre>
<p>I see metrics for running and starving tasks but not for total number of tasks.</p>",,0,0,,2021-8-11 08:37:10,,2021-8-11 08:37:10,,,,,12965658,,1,0,airflow|monitoring|datadog,39,6.36426
261697,3,Visualization,67234999,Build a dashboard from a metric with Datadog,"<p>Is there any way with Datadog to build an automatic dashboard if a metric is sent for the first time?
Let's say I send a metric with a value and a &quot;path&quot; like <code>dashboard_name.some_group.metric_value</code> for the first time. Datadog &quot;checks&quot; if there is already an existent <code>dashboard_name</code>. If not builds it. Then it checks if inside the dashboard there is a widget called <code>some_group</code>. If not builds it. Then check if there is a metric widget (for example using Timeseries widget). If not builds it.</p>",,1,0,,2021-4-23 18:13:58,,2021-4-26 21:22:05,2021-4-26 21:22:05,,244037,,324315,,1,0,datadog,37,6.27281
261698,1,Parse,66753780,Datadog:Category processor- Define logs as errors-Unable to parse the logs as error,"<p>We have the below Category processor pipeline for Lambdas such that when there is a log with (@error.kind:* or @error.message:*) , the logs will be defined as an error.</p>
<p><a href=""https://i.stack.imgur.com/8kJEh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8kJEh.png"" alt=""enter image description here"" /></a></p>
<p>To test above Category process rule, I have create a simple Lambda called python-test as below</p>
<p><a href=""https://i.stack.imgur.com/RUlZH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RUlZH.png"" alt=""enter image description here"" /></a></p>
<p>When I run this lambda with below test event, I don’t see the log messages being converted to error</p>
<pre><code>{
  &quot;log&quot;: &quot;error.kind: Some log to be monitored&quot;
}
</code></pre>
<p>Eg: In Datadog as you can see below,
it still remains as log ie. it is not converted to error (ErrorType –error.kind is not set)
Can you please advise If I am missing something?
<a href=""https://i.stack.imgur.com/5rjR9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5rjR9.png"" alt=""enter image description here"" /></a></p>",,0,1,,2021-3-22 20:59:49,,2021-3-22 20:59:49,,,,,6683100,,1,0,logging|datadog,37,6.27281
261699,0,Configuration,52594973,Unable to run a windows task that makes https request under SYSTEM account,"<p>I have created a windows cmd file that calls three independent bat files. I want to create a windows task that calls this cmd file and runs every 5 minutes. The problem is that this task runs perfectly fine only when I'm logged into the system. <em>But I'm unable to make this task continue to run ""whether I'm logged in or not""</em>. </p>

<p>I even asked my colleague to login to that machine and run this task under his account - it worked. I created a local admin user on that machine, logged in as that user, tried to run this task - it did not work - the script waits forever while post_results.bat. I even tried to schedule a jenkins job that basically does the same thing - it did not work - the jenkins job waits forever while post_results.bat (I killed the jenkins job after waiting for ~20 min).</p>

<p>Here is a summary of what these tasks are doing:</p>

<p><strong>run_all.cmd</strong></p>

<pre><code>call ""run_test.bat""
call ""post_results.bat""
call ""clean.bat""
</code></pre>

<p><strong>run_test.bat</strong> - executes a jmeter script</p>

<pre><code>C:\Users\Administrator\LS2\apache-jmeter-4.0\bin\jmeter -n -t api_strategy_synthetic_tests.jmx -JTestEnv=amer1 -l Result_log.jtl
</code></pre>

<p><strong>post_results.bat</strong> - calls a python script that posts the jmeter test results to datadog</p>

<pre><code>python post_jmeter_results_to_datadog.py Result_log.jtl
</code></pre>

<p><strong>post_jmeter_results_to_datadog.py</strong> - uses the datadog python api to post metrics to datadog</p>

<pre><code>#!/usr/bin/env python3
import sys
import pandas as pd
from datadog import initialize, api

options = {
    'api_key': &lt;API_KEY&gt;,
    'app_key': &lt;APPLICATION_KEY&gt;
}
initialize(**options)

jtl_file = sys.argv[1]
df = pd.read_csv(jtl_file)
for index, row in df.iterrows():
    tag = ""success:"" + str(row['success'])
    api.Metric.send(
        metric=row['label'],
        points=[(row['timeStamp']/1000,row['elapsed'])],
        tags=[tag]
        )
</code></pre>

<p><strong>clean.bat</strong> - deletes the jmeter test result files</p>

<pre><code>rmdir /s /q ""errors""
del ""jmeter.log""
del ""Result_log.jtl""
</code></pre>

<p>All I need is to be able to run this task every 5 minutes. If anyone is able to see what I'm doing wrong and points that out... I'd be really grateful.</p>",,1,2,,2018-10-1 16:09:08,,2018-10-8 13:32:21,,,,,4386440,,1,0,windows|batch-file|jmeter|taskscheduler|datadog,37,6.27281
261700,0,Configuration,67161917,How to update arguments of a role using host vars values in Ansible?,"<p>I am using Ansible <a href=""https://galaxy.ansible.com/Datadog/datadog"" rel=""nofollow noreferrer"">Datadog role</a> and trying to install and configure datadog agents in target servers however, i am stuck at a point where i need to use host variables and update a section of the playbook using these variables. The variable has got multiple values separated by a space. I want to ensure that these values are added in the playbook based on the variable values. Following example will help in understanding the requirement.</p>
<p><strong>Playbook:</strong></p>
<pre><code>- hosts: servers
  roles:
    - { role: datadog.datadog, become: yes }
  vars:
    datadog_api_key: &quot;{{ DD_API_KEY }}&quot;
    datadog_config:
      tags:
        - &quot;AID:&quot;{{ AID }}&quot;
</code></pre>
<p>Here, the tag value AID is using a host variable with the same name i.e., AID and in some cases this host variable can have values like the following:</p>
<p><code>AID: 100 101 102 103</code></p>
<p>Is there a way that the while executing tag section of the playbook is parsed based on the variable values in following format.</p>
<pre><code>  tags:
    - AID: 100
    - AID: 101
    - AID: 102
    - AID: 103
</code></pre>
<p>I believe i cannot use templates for such requirements since the configurations are used under vars in the role. Any suggests would be appreciated.</p>",,1,3,,2021-4-19 12:21:25,,2021-4-20 19:09:15,,,,,1945171,,1,0,ansible|ansible-2.x|datadog,36,6.22521
261701,0,Integration,68454248,How can i integrate Vertex AI logs with Datadog?,"<p>Was reading through this <a href=""https://docs.datadoghq.com/integrations/google_cloud_platform/?tab=datadogussite#log-collection"" rel=""nofollow noreferrer"">documentation</a> trying to figure out how to integrate logs from Vertex AI (A managed service in GCP) into Datadog,
Created the pubsub and the topic, also a sink that pushes the pubsub's logs into Data dog,
should i be able to see the logs in DataDog now? how?</p>",,0,0,,2021-7-20 11:41:15,,2021-7-20 11:41:15,,,,,14876604,,1,0,datadog|google-ai-platform|google-cloud-vertex-ai,36,6.22521
261702,3,Monitoring,68660696,Monitor not sending notification on missing data datadog,"<p>facing some issue with notification when data is missing.
We receive metrics every 1 hour.
We have set a monitor to notify if no metrics received for 70 minutes.
But even if we fail to send the metrics, no notification is sent to us after 70 minutes.
Why might this be happening?</p>
<pre><code>{
    &quot;id&quot;: &quot;&lt;ID&gt;&quot;,
    &quot;name&quot;: &quot;Test Monitor&quot;,
    &quot;type&quot;: &quot;metric alert&quot;,
    &quot;query&quot;: &quot;sum(last_1m):count:jobs.operation_time{*} by {job_type}.as_count() &gt; 20  (this is an impossible condition)&quot;,
    &quot;message&quot;: &quot;{{#is_no_data}}\nNo data received\n{{/is_no_data}}@myname@company.io&quot;,
    &quot;tags&quot;: [
        &quot;tag1&quot;
    ],
    &quot;options&quot;: {
        &quot;notify_audit&quot;: false,
        &quot;locked&quot;: false,
        &quot;timeout_h&quot;: 0,
        &quot;silenced&quot;: {},
        &quot;include_tags&quot;: true,
        &quot;no_data_timeframe&quot;: 70,
        &quot;require_full_window&quot;: true,
        &quot;new_host_delay&quot;: 300,
        &quot;notify_no_data&quot;: true,
        &quot;renotify_interval&quot;: 0,
        &quot;escalation_message&quot;: &quot;&quot;,
        &quot;thresholds&quot;: {
            &quot;critical&quot;: 20
        }
    },
    &quot;priority&quot;: 1,
    &quot;classification&quot;: &quot;metric&quot;
}
</code></pre>",,1,3,,2021-8-5 04:40:08,,2021-8-12 15:21:04,,,,,14770558,,1,0,datadog,36,6.22521
261703,2,Query,69841636,how do I combine strings with jmespath queries to build up a webhook body?,"<p>I am trying to use <code>Cloud Custodian</code> <a href=""https://cloudcustodian.io/docs/actions.html"" rel=""nofollow noreferrer"">webhooks</a> to create tagged events in <code>Datadog</code> using the <a href=""https://docs.datadoghq.com/api/latest/events/"" rel=""nofollow noreferrer"">Datadog API.</a></p>
<p>The following code nearly works, except <code>account_id</code> is not created as a tag in <code>Datadog</code>. If I capture the body sent, it contains <code>&quot;01234&quot;</code> (i.e. a string.)</p>
<pre><code>- type: webhook
        url: https://api.datadoghq.eu/api/v1/events
        method: POST
        headers:
          DD-API-KEY: '`{{ dd_api_key }}`'
        body: |- 
          {
            &quot;title&quot;: `nutkin news`, 
            &quot;text&quot;: `squirrel found in account`, 
            &quot;tags&quot;: [resource.Name, policy.name, account_id]
          }

</code></pre>
<p>If I remove the <code>jmespath</code> queries in tags and just send string literals e.g.</p>
<pre><code>`01234`
</code></pre>
<p>, it will not be appear in <code>Datadog</code> as a tag, but if I send</p>
<pre><code>`aws_account_id:01234`
</code></pre>
<p>it will appear as a tag.</p>
<p>Ideally, for all the tags, I would like a mix of a string and the result of the <code>jmespath</code> query, as it would be more useable for users of <code>Datadog</code> (e.g. something like what is included below.)</p>
<pre><code>&quot;tags&quot;: [`resource_name:`resource.Name, `policy_name:`policy.name, `account_id:`account_id]
</code></pre>
<p>I've spent days on this. I've read all the docs on <code>custodian</code>, <code>json</code> and <code>jmespath</code> and just can't find the right syntax of brackets, quotes and backticks. Maybe it is not even possible to mix string literals and <code>jmespath</code> queries.</p>
<p>Just to reiterate the question, <em>how do I combine string literals with <code>jmespath</code> queries to build up a web hook body in <code>custodian web hooks</code>?</em></p>",,1,5,,2021-11-4 15:22:25,,2021-11-8 14:57:50,2021-11-8 13:31:14,,2963536,,2963536,,1,1,json|webhooks|datadog|jmespath|cloudcustodian,32,6.2206
261704,0,Configuration,69219352,Configure datadog agent to avoid kube-proxy logs,"<p>I am having lots of info logs from <code>kube-proxy</code> in Datadog and for now I would like to avoid that.
The kind of logs I am having looks like the following:
<a href=""https://i.stack.imgur.com/0NoUe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0NoUe.png"" alt=""enter image description here"" /></a></p>
<p>I have <a href=""https://docs.datadoghq.com/logs/guide/how-to-set-up-only-logs/?tab=kubernetes"" rel=""nofollow noreferrer"">disabled payload</a> expecting it would help, but with no luck.</p>
<p>If anyone can point me in the right direction that would be very appreciated.</p>",69227392,1,0,,2021-9-17 07:26:04,,2021-9-17 17:25:16,,,,,844451,,1,0,logging|datadog|kube-proxy,20,6.20412
261705,3,Visualization,68488733,Is there any way we can visualize non-negative derivative in Datadog?,"<p>We are currently using Grafana which takes data from infludb. We have few dashboards on grafana. Now I want similar dashboards in Datadog. Most of our dashboards, queries are asking non_negative_derivative_(1s), which I am unable to find in Datadog. There is a derivative function in Datadog but not the non_negative_derivative_1s. I have currently set that up as per second but that doesn't seem to work as it should as there is a huge mismatch in values. Anyone has experienced this and found a solution? Thanks</p>",,0,0,,2021-7-22 16:49:46,,2021-7-22 16:49:46,,,,,16504530,,1,0,grafana|influxdb|datadog,35,6.17627
261706,0,Integration,65320127,How to integrate and start Datadog monitoring for Windows Apache web servers?,"<p>I need to integrate Datadog monitoring on Apache web servers which are on Windows servers. Is there a link/blog available detailing the same for Windows server specifically ? <a href=""https://www.datadoghq.com/blog/monitor-apache-web-server-datadog/"" rel=""nofollow noreferrer"">I got a blog link from Datadog but it seems not to cover Windows servers.</a> Need it specifically for Windows servers.</p>",,1,0,,2020-12-16 08:58:01,,2020-12-22 04:52:25,,,,,6893383,,1,0,apache|webserver|monitoring|datadog,35,6.17627
261707,1,Parse,69772535,Datadog Grok Parsing - extracting fields from list of JSON,"<p>I want to extract fields from list of json</p>
<pre class=""lang-json prettyprint-override""><code>[
  {
    &quot;ExecMainStartTimestamp&quot;: &quot;Wed 2021-10-27 11:31:36 UTC&quot;,
    &quot;hostname&quot;: &quot;i-XX&quot;,
    &quot;_time&quot;: &quot;2021-10-27 12:20:01&quot;,
    &quot;ASG&quot;: &quot;prod1&quot;,
    &quot;ExecMainPID&quot;: &quot;1447&quot;,
    &quot;Names&quot;: &quot;nginx.service&quot;,
    &quot;LoadState&quot;: &quot;loaded&quot;,
    &quot;ActiveState&quot;: &quot;active&quot;,
    &quot;UnitFileState&quot;: &quot;enabled&quot;
  },
  {
    &quot;ExecMainStartTimestamp&quot;: &quot;Wed 2021-10-27 11:31:46 UTC&quot;,
    &quot;hostname&quot;: &quot;i-XX&quot;,
    &quot;_time&quot;: &quot;2021-10-27 12:20:01&quot;,
    &quot;ASG&quot;: &quot;prod1&quot;,
    &quot;ExecMainPID&quot;: &quot;1669&quot;,
    &quot;Names&quot;: &quot;gunicorn.service&quot;,
    &quot;LoadState&quot;: &quot;loaded&quot;,
    &quot;ActiveState&quot;: &quot;active&quot;,
    &quot;UnitFileState&quot;: &quot;enabled&quot;
  }
]
</code></pre>
<p>Can someone help me to do this in datadog grok pipeline?</p>",,0,1,,2021-10-29 17:08:26,,2021-10-29 19:08:38,2021-10-29 19:08:38,,9561771,,9561771,,1,0,json|parsing|datadog|grok,34,6.12592
261708,3,Monitoring,67805803,Datadog: Using a Facet as Notification destination,"<p>I'm using Datadog Monitors to get notified when some specific messages are logged.</p>
<p>I'd like to send a copy of this notification to a destination that's in a Facet. Can Datadog reference a Facet when defining Monitor Notifications?</p>",,0,1,,2021-6-2 13:11:59,,2021-6-2 15:24:37,2021-6-2 15:24:37,,743433,,743433,,1,0,logging|monitoring|datadog,34,6.12592
261709,2,Query,69492866,Datadog RUM: Count unique actions per session,"<p>I have a custom datadog metrics being logged using <code>DD_RUM.addAction</code> For eg: searching for a city name.
I want to create dashboard where I can see the no of unique cities searched by the user <strong>per session</strong></p>
<p>I tried <code>$service $env $applicationId $os @action.target.&lt;MY_CUSTOM_ACTION_NAME&gt; @type:session</code> with filters <code>Count unique from_city_name (@context.from_name)</code>. It does not return even return the session count but if I change <code>@type:action</code>, it returns the total no of actions.</p>
<p>How can I get the metric <strong>per session</strong>?</p>",,0,0,,2021-10-8 08:34:06,,2021-10-8 08:34:06,,,,,5466933,,1,0,instrumentation|datadog,33,6.07406
261710,3,Monitoring,68919302,DataDog monitor containers stuck in restart-loops,"<p>Trying to monitor a repeatedly crushing container inside an AWS ECS cluster monitored by DataDog.</p>
<p>Created a demo app which crushes after 120 second (ECS tries to run it non-stop).</p>
<p><strong>Cant figure out how to see if its stuck in restart loop like this app</strong></p>
<p>Current DataDog metric :</p>
<p><a href=""https://i.stack.imgur.com/wB0lT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wB0lT.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/vlPKE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vlPKE.png"" alt=""enter image description here"" /></a></p>",68974093,1,3,,2021-8-25 08:21:48,,2021-8-29 14:51:46,,,,,15648070,,1,0,docker|containers|monitoring|amazon-ecs|datadog,33,6.07406
261711,2,Query,69433008,Datadog - use a metric as a filter in a query,"<p>I would like to filter (in a dashboard or a monitor) based on a metric. An artificial example: Let’s say I want to create an alert if the CPU usage of all servers that have more than 4 CPUs is larger than 90%. How would I do that? Until now I only found ways to filter based on tags, but not on dynamic data coming from the servers themselves.</p>
<p>I basically would like to transform the following:</p>
<pre><code>avg:system.cpu.idle{env:staging} by {host}
</code></pre>
<p>to something like that:</p>
<pre><code>avg:system.cpu.idle{system.cpu.num_cores&gt;4} by {host}
</code></pre>",,0,0,,2021-10-4 08:26:56,,2021-10-4 08:26:56,,,,,17069394,,1,0,datadog,30,5.90848
261712,1,Error,68736063,Unable to see custom metrics in datadog sent using statsd for Springboot apps,"<p>I have a spring boot application and I am trying to send custom metrics using statsd and meterregistry to datadog, I can see the metrics using '/actuator/metrics' url but don't see anything in datadog.</p>
<p>pom.xml</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.datadoghq&lt;/groupId&gt;
    &lt;artifactId&gt;java-dogstatsd-client&lt;/artifactId&gt;
    &lt;version&gt;2.10.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>Class:</p>
<pre><code>  private final MeterRegistry meterRegistry;


    public Starter(@Lazy @Autowired MeterRegistry meterRegistry) {
        

        this.meterRegistry = new SimpleMeterRegistry();

        Counter counter = Counter
                .builder(&quot;test_counter&quot;)
                .baseUnit(&quot;beans&quot;) // optional
                .description(&quot;a description of what this counter does&quot;) // optional
                .tags(&quot;region&quot;, &quot;test&quot;) // optional
                .register(meterRegistry);

        Timer.builder(&quot;test.timer&quot;)
                .publishPercentiles(0.95)
                .register(meterRegistry);
</code></pre>
<p>Application.yml</p>
<pre><code>management:
  metrics:
    enable:
      all: true
    export:
      statsd:
        flavor: datadog
        host: localhost
        port: 8125
        protocol: udp
</code></pre>",,0,0,,2021-8-11 04:32:27,,2021-8-11 06:31:27,2021-8-11 06:31:27,,6201287,,6201287,,1,0,spring|metrics|datadog|statsd|spring-micrometer,30,5.90848
261713,0,Integration,68901484,DarkaOnLine / L5-Swagger conflict with Datadog,"<p>I got an wired issue that I cannot run both L5-Swagger and Datadog at the same time in my laravel(v6.2) &amp; PHP (v7.2) project.</p>
<p>I am using a dev.sh file config in my docker file (docker-compose-dev.yml) like this :</p>
<pre><code> command:
   - bin/dev.sh
</code></pre>
<p>In dev.sh if I do <code>-t public</code> (start app from public folder), the Datadog works fine, but not for L5-Swagger.If I do <code>-t public public/index.php</code>, the Datadog stop working but it works for L5-Swagger. The Swagger error is &quot;The requested resource /docs/api-docs.json was not found on this server.&quot; when Swagger UI call <code>http://localhost:8081/docs/api-docs.json</code>and The Swagger UI throw fetch 404 error. My api-docs.json lives in storage/docs/api-docs.json and I tried to change the params for <code>'docs' =&gt; storage_path('api-docs')</code> (in l5-swagger.php), but nothing changes and doesn't work. The only way make Swagger UI works which is change dev.sh code to -t public public/index.php
Any one can give me some clues? Appreciated in advanced!</p>
<pre><code>#!/bin/bash

PORT=8081

echo &quot;&quot;
echo &quot;---------------&quot;
echo &quot;Application listening at $HOSTNAME:$PORT&quot;
echo &quot;---------------&quot;
echo &quot;&quot;

php \
  -S $HOSTNAME:$PORT \
  -t public # only works for Datadog
  # -t public public/index.php # only works for L5-Swagger
</code></pre>",,0,0,,2021-8-24 03:40:40,,2021-8-24 03:40:40,,,,,4985852,,1,0,php|swagger|datadog|laravel-6.2,30,5.90848
261714,1,Method,69477205,How to send the results of a newman report to Datadog?,"<p>I have built a small microservice that is connected to Datadog, i.e. API calls to this service using Postman are shown in Datadog. I have generated the newman report for the service using -</p>
<pre><code>newman run collection.json --reporters cli,json --reporter-json-export output.json
</code></pre>
<p>Now, I want the contents of my newman report <code>output.json</code> to be shown in Datadog. Any help/idea on how to do that would be really appreciated.</p>",,1,0,,2021-10-7 07:41:13,,2021-10-8 07:35:59,,,,,11827585,,1,0,postman|datadog|postman-collection-runner|newman,30,5.90848
261715,0,Configuration,69971605,How to configure proxy from Datadog datadog_synthetics_test Terraform resource,"<p>We are using <a href=""https://docs.datadoghq.com/synthetics/"" rel=""nofollow noreferrer"">Datadog synthetic</a> managed by Terraform and we need to configure properly the http proxy as possible like from the Datadog console in Advanced options :
<a href=""https://i.stack.imgur.com/cDbLK.png"" rel=""nofollow noreferrer"">Advanced options</a></p>
<p>The question is now how to configure our Terraform scripts to use the proxy settings. Nothing is related <a href=""https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/synthetics_test"" rel=""nofollow noreferrer"">in the documentation</a>.</p>
<p>We have done some tests after retreiving the output via the API, but nothing works really.</p>
<pre><code>// KO
  request_definition {
    url    = &quot;https://test.com&quot;
    method = &quot;GET&quot;
  }
  proxy {
    url = &quot;http://myproxy.com&quot;
  }

// KO 
  request_definition {
    url    = &quot;https://test.com&quot;
    method = &quot;GET&quot;
    proxy {
      url  = &quot;http://myproxy.com&quot;
    }
  }

// KO 
  request_definition {
    url    = &quot;https://test.com&quot;
    method = &quot;GET&quot;
    proxy  = &quot;http://myproxy.com&quot;
  }
</code></pre>
<p>API output when we configure synthetic via the dashboard:</p>
<pre><code>    &quot;config&quot;: {
        &quot;request&quot;: {
            &quot;url&quot;: &quot;https://test.com&quot;,
            &quot;headers&quot;: {
                &quot;Content-Type&quot;: &quot;application/json&quot;
            },
            &quot;method&quot;: &quot;GET&quot;,
            &quot;proxy&quot;: {
                &quot;url&quot;: &quot;http://myproxy.com&quot;
            }
        },
        &quot;assertions&quot;: [...]
</code></pre>
<p>Help will be very appreciate</p>",,0,0,,2021-11-15 08:50:47,,2021-11-17 13:50:55,2021-11-17 13:50:55,,17416019,,17416019,,1,0,terraform|datadog,30,5.90848
261716,0,Configuration,66494646,How does one configure Datadog to customize event correlation?,"<p>Am looking for information on configuring Datadog to perform event correlation using custom event attributes (application components output events records in JSON format).</p>
<p>Also, is it possible in Datadog to then configure notifications based on correlated events?</p>
<p>Appreciate any pointers on the above or where I can get the above information.
TIA.</p>
<p>RKH</p>",,0,0,,2021-3-5 14:48:46,,2021-3-5 14:48:46,,,,,2291904,,1,0,events|correlation|datadog,30,5.90848
261717,1,Error,69732972,How to use Micrometer registry.counter,"<p>i have question, i want to send metrics counter to my datadog... i have try use</p>
<pre><code>https://docs.spring.io/spring-metrics/docs/current/public/datadog
</code></pre>
<p>but when i see and try it use this @EnableDatadogMetrics</p>
<p>i cannot run it.. and i see this is error.. <code>Caused by: java.lang.IllegalStateException: Failed to introspect annotated methods on class org.springframework.metrics.boot.RecommendedMeterBinders</code></p>
<p>i think this code example already deprecated.</p>
<p>my question is if i want to store metrics like this</p>
<p>registry.counter(&quot;payment.status&quot;, &quot;http-status&quot;, &quot;200/400/500&quot;, &quot;payment-method&quot;, &quot;Paypal/IDB/others&quot; dst...);</p>
<p>how to do that using micrometer or something ?</p>",,0,1,,2021-10-27 04:58:13,,2021-10-27 04:58:13,,,,,16150140,,1,0,java|spring-boot|datadog,30,5.90848
261718,1,Method,69818484,I don't understand how the Downtime API works and I'm trying to automate scheduling downtime,"<p>The documentation is very unclear when it comes to the DataDog Downtime API.  I've based my example off of the following documentation: <a href=""https://docs.datadoghq.com/api/latest/downtimes/#schedule-a-downtime"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/latest/downtimes/#schedule-a-downtime</a></p>
<p>I have the following environment variables set:</p>
<pre><code>    DD_APP_TOKEN: Your datadog application token.
    DD_API_TOKEN: Your datadog API token.
    DD_SITE: The address of the DD API server.  Usually &quot;datadoghq.com&quot;
</code></pre>
<p>My example code:</p>
<pre><code>import (
    &quot;context&quot;
    &quot;encoding/json&quot;
    &quot;fmt&quot;
    &quot;github.com/DataDog/datadog-api-client-go/api/v1/datadog&quot;
    &quot;github.com/slack-go/slack&quot;
    &quot;os&quot;
    &quot;time&quot;
)

func main() {
    ctx := datadog.NewDefaultContext(context.Background())

    var DataDogFiltersByTag = []string{
        &quot;printing&quot;,
        &quot;OTIS&quot;,
        &quot;env:production&quot;,
    }

    Message := &quot;Downtime Test&quot;
    var ctime int64 = time.Now().Unix()     // Current Time
    var dtime int64 = ctime + (45 * 60)     // 45 minutes

    body := *datadog.NewDowntime() // Downtime | Schedule a downtime request body.
    body.Message = &amp;Message
    body.Start = &amp;ctime
    end := datadog.NullableInt64{}
    end.Set(&amp;dtime)
    body.End = end
    body.MonitorTags = &amp;DataDogFiltersByTag

    configuration := datadog.NewConfiguration()

    apiClient := datadog.NewAPIClient(configuration)
    resp, r, err := apiClient.DowntimesApi.CreateDowntime(ctx, body)
    if err != nil {
        fmt.Fprintf(os.Stderr, &quot;Error when calling `DowntimesApi.CreateDowntime`: %v\n&quot;, err)
        fmt.Fprintf(os.Stderr, &quot;Full HTTP response: %v\n&quot;, r)
    }
    // response from `CreateDowntime`: Downtime
    responseContent, _ := json.MarshalIndent(resp, &quot;&quot;, &quot;  &quot;)
    fmt.Fprintf(os.Stdout, &quot;Response from DowntimesApi.CreateDowntime:\n%s\n&quot;, responseContent)
}
</code></pre>
<p>This results in the following error message:</p>
<pre><code>/private/var/folders/p0/jbhf5p4n3f3c3rc5rc_rc6400000gn/T/GoLand/___6go_build_ClusterUpgradeNotification
Error when calling `DowntimesApi.CreateDowntime`: 400 Bad Request
Full HTTP response: &amp;{400 Bad Request 400 HTTP/1.1 1 1 map[Cache-Control:[no-cache] Connection:[keep-alive] Content-Length:[39] Content-Security-Policy:[frame-ancestors 'self'; report-uri https://api.datadoghq.com/csp-report] Content-Type:[application/json] Date:[Wed, 03 Nov 2021 00:33:09 GMT] Pragma:[no-cache] Strict-Transport-Security:[max-age=15724800;] X-Content-Type-Options:[nosniff] X-Frame-Options:[SAMEORIGIN]] {{&quot;errors&quot;: [&quot;Invalid scope parameter&quot;]}} 39 [] false false map[] 0xc00019a800 0xc0003342c0}
Response from DowntimesApi.CreateDowntime:
{}

Process finished with the exit code 1
</code></pre>",69841659,1,4,,2021-11-3 00:47:22,,2021-11-4 15:24:11,,,,,3056541,,1,0,go|datadog,29,5.84959
261719,3,Monitoring,68403759,Datadog AWS RDS monitoring storage used,"<p>Setting up Datadog alert to monitor AWS RDS Aurora databased by percentage used. If the percentage of storage is above 80% alert. Using the following two metrics</p>
<pre><code>A: aws.rds.free_local_storage
B: aws.rds.volume_bytes_used

a/b 

Alert if greater than 0.8
</code></pre>
<p>Is this possible in Datadog using the two metrics and diving them? or is their another way of achieving this in Datadog ?</p>",,0,0,,2021-7-16 05:14:35,1,2021-7-16 05:14:35,,,,,12759484,,1,0,amazon-web-services|amazon-rds|amazon-aurora|datadog,29,5.84959
261720,2,Query,68629022,Can I apply multiple aggregate functions?,"<p>I have a service which horizontally scales to N nodes (where N could be anywhere &gt; 2).</p>
<p>Each node collects and emits lots of gauge metrics; this causes duplicates as each node reports the same data.</p>
<h3>Example</h3>
<p>Lets say each node emits a metric with tag <code>dim</code></p>
<pre><code>node: 1; dim: foo = 10
node: 2; dim: foo = 10
node: 1; dim: bar = 5
node: 2; dim: bar = 5
</code></pre>
<p>If I <code>max by dim</code> I get the right values <code>foo=10; bar=5</code> — this is great for charting by <code>dim</code>.</p>
<p>But if I <code>sum</code> this to get the total value, I get 30 — which is wrong (should be 15).</p>
<h3>Question</h3>
<p>I could solve this if I could first apply <code>max by dim</code> and then take the <code>sum</code> — but I can't see how to apply multiple aggregation functions in datadog — is this just not possible?</p>
<h3>Alternatives</h3>
<p>If I can't solve this in datadog; I'll have to consider emitting the data differently — options:</p>
<ol>
<li>Force only 1 node to report the values</li>
<li>Emit a separate metric without the dimension; or the same metric where <code>dim=total</code></li>
<li>Don't use gauge; emit events</li>
</ol>",,0,0,,2021-8-3 00:02:34,,2021-8-4 03:11:21,2021-8-4 03:11:21,,4076315,,2728136,,1,0,datadog,29,5.84959
261721,2,Query,68289763,DataDog group spans,"<p>We have a <code>.net</code> application, which runs on Qa-server, Staging-server and Prod-Server. Each server has <code>DataDogAgent</code> which sends metrics to DataDog. For Qa and Staging spans are stacked into stacktraces correctly:</p>
<p><a href=""https://i.stack.imgur.com/KvNXg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KvNXg.png"" alt=""enter image description here"" /></a></p>
<p>But for Production server DataDog doens't group spans, instead of kinda 'stackTrace' i see scattered spans:</p>
<p><a href=""https://i.stack.imgur.com/mRngG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mRngG.png"" alt=""enter image description here"" /></a></p>
<p>How to group together into 'stacktrace' spans in DataDog ?</p>",,0,0,,2021-7-7 16:27:30,,2021-7-8 14:58:48,2021-7-8 14:58:48,,3607337,,3607337,,1,1,c#|.net|metrics|datadog,29,5.84959
261722,3,Monitoring,69569848,Tracing .Net core API in datadog,"<p>I am trying to integrate one of my .Net core API with Datadog for monitoring by utilizing the Datadog .Net Runtime integration.</p>
<p>I have installed the package <code>Datadog.Trace -Version 1.28.8</code> to my .net core application but here I am stuck. What else libraries I will need to add and what configuration I need to implement in API code to trace the performance of API.</p>
<p>If anyone could help me with initial documentation links or the steps to follow, will be of great help.</p>",,0,2,,2021-10-14 11:23:48,,2021-10-14 11:23:48,,,,,15219642,,1,0,performance|.net-core|webapi|datadog,26,5.65989
261723,3,Monitoring,69036627,How to create a Datadog monitor for Kubernetes container restarts strictly due to OOM?,"<p>Currently running a monitor on any kubernetes container restarts:</p>
<pre><code>max(last_10m):monotonic_diff(default_zero(sum:kubernetes.containers.restarts{kube_namespace:production} by {kube_stateful_set})) &gt; 0
</code></pre>
<p>I'd like to see some information about the reason - specifically OOM restarts.</p>",,0,0,,2021-9-2 20:27:56,,2021-9-2 20:27:56,,,,,365333,,1,0,kubernetes|datadog,26,5.65989
261724,3,Monitoring,67622781,How to apply a threshold on an anomaly monitor in Datadog,"<p>I have an anomaly detection monitor that looks at a metric, and it works fine for that purpose. Unfortunately I can't seem to apply any kind of threshold on it, which means that the alert triggers even for low volumes.</p>
<p>To solve this I'm using a composite monitor, which looks at the anomaly monitor and at another monitor of type &quot;Threshold Alert&quot;. This gives me the desired behavior, but unfortunately it has 2 disadvantages:</p>
<ul>
<li>we have to have another monitor (the threshold monitor) which adds more complexity.</li>
<li>the (Slack) alerting on the composite monitor doesn't include the chart from the anomaly monitor, so you'd have to go inside Datadog to actually see that chart for the triggered metric. This is probably the main disadvantage that I see.</li>
</ul>
<p>Does anyone know how I can apply thresholds to anomaly detection monitors, or how to display the chart from anomaly monitor when composite monitor is alerting?</p>",,0,0,,2021-5-20 14:49:05,,2021-5-20 14:49:05,,,,,880230,,1,0,datadog,26,5.65989
261725,3,Visualization,66357361,What graphing library does the Datadog use on its dashboard page for showingthe graphs?,<p>I've been using the Datadog dashboard and I really like how strong and flexible it is. I am wondering what type of API they are using</p>,,0,0,,2021-2-24 19:10:40,,2021-2-24 19:10:40,,,,,1303244,,1,0,graph|visualization|dashboard|datadog,26,5.65989
261726,2,Query,68496554,Datadog. Unclear interaction of 3 aggregation functions in JSON Table widget definition,"<p>can anyone explain how 3 aggregation functions (<code>max</code>, <code>avg</code>, <code>sum</code>) in Datadog Table JSON query interact with each other and affect the final result shown in Datadog Table widget?</p>
<pre><code>{
&quot;q&quot;: &quot;max:some_metric{tag_filters_for_some_metric} by {tag_for_grouping_the_some_metric}.rollup(avg, 60)&quot;,
&quot;aggregator&quot;: &quot;sum&quot;,
...
</code></pre>
<p>}</p>
<p>in my understanding:</p>
<ul>
<li>set of <code>some_metric</code> values is narrowed by <code>tag_filters_for_some_metric</code></li>
<li>then grouped by <code>tag_for_grouping_the_some_metric</code></li>
<li>then max value is calculated for each group (#1)</li>
<li>then for UI smoothing purposes the time interval is split into 60s buckets and <code>avg</code> is applied on #1 values, and a single value for each time bucket is produced</li>
<li>Q: where and how <code>sum</code> &quot;aggregator&quot; (seen in UI editor as &quot;roll-up timeframe by&quot;) is applied?</li>
</ul>
<p>feel free to correct my understanding,
thanks</p>
<p>i've read many places in Datadog docs including:</p>
<ul>
<li><a href=""https://docs.datadoghq.com/metrics/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/metrics/</a></li>
<li><a href=""https://docs.datadoghq.com/dashboards/widgets/table/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/dashboards/widgets/table/</a></li>
<li><a href=""https://docs.datadoghq.com/dashboards/querying/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/dashboards/querying/</a></li>
</ul>",,0,0,,2021-7-23 08:51:18,,2021-7-23 08:51:18,,,,,336184,,1,0,metrics|datadog,26,5.65989
261727,1,Method,68464460,Spring metrics for batch jobs,"<p>I have a batch process that runs a series of tasks (not spring-batch). I want to record metrics (such as rows read, rows written plus many other things) for each task for each batch run. The results might look something like this (in json notation to demonstrate the hierarchial nature of the metrics):</p>
<pre><code>{
  &quot;batchRun&quot;: 1234,
  &quot;tasks&quot;: [
    {
      &quot;id&quot;: &quot;task1&quot;,
      &quot;duration&quot;: 500,
      &quot;inputs&quot;: [
        {
          &quot;file&quot;: &quot;data1.csv&quot;,
          &quot;rowsRead&quot;: 10,
          &quot;duration&quot;: 100
        }
      ],
      &quot;outputs&quot;: [
        {
          &quot;file&quot;: &quot;data-out1.csv&quot;,
          &quot;rowsWritten&quot;: 20,
          &quot;duration&quot;: 200
        }
      ]
    },
    {
      &quot;id&quot;: &quot;task2&quot;,
      &quot;duration&quot;: 5000,
      &quot;inputs&quot;: [
        {
          &quot;file&quot;: &quot;data2.csv&quot;,
          &quot;rowsRead&quot;: 100000,
          &quot;duration&quot;: 1000
        }
      ],
      &quot;outputs&quot;: [
        {
          &quot;file&quot;: &quot;data-out2.csv&quot;,
          &quot;rowsWritten&quot;: 200000,
          &quot;duration&quot;: 2000
        }
      ]
    }
  ]
}
</code></pre>
<p>I'd like to be able to see these metrics as a whole for a batch run, and I'd like to be able to compare 2 job runs against each other.</p>
<p>How would I go about doing this with micrometer and any of the metric consuming applications? (Prometheus, Datadog etc).</p>
<p>Is it possible to have this hierarchial nature? And compare across 2 batch runs?</p>
<p>The examples I've seen all seem to be more related to system metrics like memory or thread usages - so more like a point in time sample, without the this extra hierarchial context.</p>
<p>Thanks.</p>
<p>Update 1:</p>
<p>I know I can add tags to a metric when it is recorded eg <a href=""https://www.baeldung.com/micrometer"" rel=""nofollow noreferrer"">https://www.baeldung.com/micrometer</a> but I don't immediately see how the frontends would make use of them in a sensible way - and they don't seem to support key/value pairs.</p>
<p>So without knowing any better, I'd think it'd look something like:</p>
<pre><code>Counter counter = Counter
  .builder(&quot;instance&quot;)
  .description(&quot;Count read rows&quot;)
  .tags(
     &quot;batchRun=1234&quot;, 
     &quot;tasks[1]&quot;, 
     &quot;taskId=task1&quot;,
     &quot;input=data1.csv&quot;,
     &quot;/tasks/0/inputs/0/file/data1.csv&quot;
  )
  .register(registry);
</code></pre>
<p>I'm hoping someone has done exactly what I am trying to do so will understand the end goal and have some suggestions which will help me understand.</p>",,0,0,,2021-7-21 05:36:39,,2021-7-21 23:20:10,2021-7-21 23:20:10,,20242,,20242,,1,0,prometheus|datadog|spring-micrometer,26,5.65989
261728,1,Method,68256175,"How to force immediate DataDog logging, not after 14 seconds","<p>Some time after DataDog is initialized:</p>
<pre><code>    datadogLogs.init({
      clientToken: 'TOKEN',
      sampleRate: 100,
      forwardErrorsToLogs: false,
    });
</code></pre>
<p>I am logging an event:</p>
<pre><code>  datadogLogs.logger.log(
    'Something went wrong',
    { env: 'mySpecialEnv' },
    StatusType.warn
  );
</code></pre>
<p>When there is a problem that will make the user most likely to quit the app altogether.</p>
<p>However, I have noticed it takes 14 seconds since I call the warning to be logged to the time an actual request is sent to DataDog server.</p>
<p>As a result, some events might be not logged at all, because a user will quit before a warning will be sent and saved.</p>
<p>How to force immediate logging with DataDog?</p>",,0,2,,2021-7-5 12:30:30,,2021-7-5 12:30:30,,,,,38940,,1,0,datadog,25,5.59176
261729,0,Integration,66242314,What Datadog integration shall I use to export k8s metrics from k8s.io/kubernetes/pkg/proxy/metrics?,"<p>I'd like to export a few metrics from <a href=""https://pkg.go.dev/k8s.io/kubernetes/pkg/proxy/metrics"" rel=""nofollow noreferrer"">k8s.io/kubernetes/pkg/proxy/metrics</a>: e.g., <code>sync_proxy_rules_duration_seconds</code> and <code>sync_proxy_rules_last_queued_timestamp_seconds</code>, what datadog integration shall I use for it? There's a section of <a href=""https://docs.datadoghq.com/agent/kubernetes/data_collected/#kubernetes-proxy"" rel=""nofollow noreferrer"">K8s proxy metrics</a> that are collected by default but there're not these 2 metrics I'm interested in on that list. Moreover, I can't <a href=""https://docs.datadoghq.com/integrations/?q=proxy"" rel=""nofollow noreferrer"">find</a> kubernetes proxy integration as well even though there's this <a href=""https://github.com/DataDog/integrations-core/blob/master/kube_proxy/README.md"" rel=""nofollow noreferrer"">kube-proxy repo</a> on GitHub.</p>",,0,0,,2021-2-17 12:51:49,,2021-2-17 12:51:49,,,,,15013270,,1,0,kubernetes|datadog|kube-proxy,25,5.59176
261730,2,Query,69849173,APM resource names (HTTP endpoints) grouped by pod name (instance names),"<p>I can't seem to figure out how to query out this basic table of information from APM trace data - any help will be appreciated.</p>
<p>I want to COUNT all resource names (<code>resource_name</code>) grouped by pod name (<code>pod_name</code>) and write to out to a table.  And I want this represented on a dashboard.  Example:</p>
<pre><code>Resource Name                               Pod Name                     Count
GET /api/tenants/{tenantId}/users           application-pod-1            222
GET /api/tenants/{tenantId}/users           application-pod-2            151
GET /api/tenants/{tenantId}/projects        application-pod-1            34
GET /api/tenants/{tenantId}/projects        application-pod-2            37
</code></pre>
<p>I cant seem to query APM data using dashboard tools:
<a href=""https://i.stack.imgur.com/NoBZ3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NoBZ3.png"" alt=""enter image description here"" /></a></p>
<p>And if I try create a custom metric, I don't get the option of <code>resource_name</code> in the count column (in fact, only duration is an option):</p>
<p><a href=""https://i.stack.imgur.com/BKN9n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BKN9n.png"" alt=""enter image description here"" /></a></p>
<p>Furthermore, I cant do this in Trace Analytics either since the querying capabilities are very limited.</p>",,0,1,,2021-11-5 06:01:50,,2021-11-5 06:01:50,,,,,589558,,1,1,datadog,25,5.59176
261731,3,Monitoring,69965319,Datadog % Mem Usage and RSS Memory,"<p>We have a .net core service running inside a docker container which is used for processing files uploaded by user. Some of the incoming requests are with files as large as 20 MB. The application saves the files to Aurora DB as binary blobs and does other processing steps.
Recently the load on service increased and the datadog shows % Mem Usage as around 99%. CPU utilization is only around 2%. We expected memory leak and consequently container running out of memory. But process seems to be running fine for some time now.
Additionally, the RSS memory is 15% only (650 MB on 4 GB container). I am trying to understand the difference in these two memory metrics, and is the high memory usage result of memory leak. The process is critical and we want to take all necessary steps to ensure stability. We are analyzing the application for Memory optimizations, but couldn't identify anything major so far.</p>",,0,0,,2021-11-14 17:05:50,,2021-11-14 17:05:50,,,,,2159471,,1,0,memory-management|datadog,25,5.59176
261732,3,Monitoring,68324048,Are Datadog monitors precise to the minute despite their visualization on the edit screen?,"<p>According to <a href=""https://docs.datadoghq.com/monitors/faq/what-is-the-do-not-require-a-full-window-of-data-for-evaluation-monitor-parameter/"" rel=""nofollow noreferrer"">this documentation</a></p>
<blockquote>
<p>Datadog monitors are evaluated every minute</p>
</blockquote>
<p>I'm trying to put two simple alerts together in a composite:</p>
<ol>
<li>&quot;Work has arrived in the past hour and needs to be handled&quot;</li>
<li>&quot;Worker has been silent for the past hour&quot;</li>
</ol>
<p>When both of these are true, I expect a notification because the worker is down and needs to be restarted.</p>
<p>If only #2 is true, it's likely the middle of the night and there's no work to do. So the Worker's silence is legitimate. I'm trying to avoid notifications for such periods.</p>
<p>(I actually delay the evaluation for #1 and have the time period for #2 include that extra time and evaluate with zero delay, to allow for the worker to lag the arrival of work items by a little bit)</p>
<p><strong>But</strong> when creating the simple alert for #1, I noticed that the visualization of the values on the monitor edit screen never drop to zero even for a specific 3-hour window where I know (and Datadog's explorer show, <em>using the same query as #1</em>) that <code>0</code> work arrived.</p>
<p>I tried making the time span checked by #1 to be 1 minute, 5 minutes, 1 hour, 3 hours, saving it with each change. It doesn't matter -- the visualization shows data points like y=8 or y=2 that are <em>spread out</em> and joined by a straight line that obviously never touches the axis, as opposed to actual y=0 points that I know are there.</p>
<p>Is my #1 alert going to work? Is this visualization on the monitor edit screen using samples only as an approximation, and will the actual monitor see zeros when it should?</p>
<p>Or will #1 be triggered the entire time even during the dead of night and never serve its purpose of blocking my composite from waking me up?</p>
<p>#1 is set to trigger when the metric is <code>below</code> <code>1</code> for the last <code>n</code> minutes, <code>count * group by everything</code></p>",,0,0,,2021-7-10 01:36:56,,2021-7-10 02:46:27,2021-7-10 02:46:27,,3735178,,3735178,,1,0,datadog,25,5.59176
261733,0,Configuration,68438569,Exclude pod from datadog agnet,"<p>I deployed datadog with helm to k8s cluster to use APM feature only,but here we have some redis  pods that its data is not really important for us and want to exclude them to don't send its data to datadog.
Is there any way to do that?</p>",,0,0,,2021-7-19 09:59:46,,2021-7-19 09:59:46,,,,,2938063,,1,0,kubernetes|deployment|datadog|apm,25,5.59176
261734,2,Query,68281028,how datadog montiors can query for a ratio,"<p>In datadog I wish to create a log monitor where the query gets the ratio of logs fulfilling a certain criteria to logs fulfilling another set of criteria. I already know how to get a count of both kinds of logs (largely through count *),  but does anyone know how to actually get these two values as a ratio, so I can set a threshold to a relevant percentage?</p>",,0,0,,2021-7-7 06:24:47,,2021-7-7 06:24:47,,,,,14917411,,1,0,datadog,24,5.52084
261735,1,Error,69298850,"Unable to find a @SpringBootConfiguration, you need to use @ContextConfiguration or @SpringBootTest(classes=...)","<p>I have this test class. When I am running this I am getting this error</p>
<pre><code>@RunWith(SpringJUnit4ClassRunner.class)
@SpringBootTest
@ActiveProfiles(value = &quot;test&quot;)
@DirtiesContext(classMode = DirtiesContext.ClassMode.AFTER_EACH_TEST_METHOD)
public class HealthGaugeMetricsIT {

    /**
     * MasterHealthMetricsReporter for creating the gauges.
     */
    @Autowired
    private MasterHealthMetricsReporter masterHealthMetricsReporter;

    /**
     * Holds meter registry instance.
     */
    @Autowired
    private MeterRegistry registry;

    /**
     * Test for creating the vault gauge and testing that it's bound
     * correctly.
     */
    @Test
    public void testVaultGauge ()
    {
        masterHealthMetricsReporter.bindTo(registry);
        final Gauge g = registry.get(&quot;encryption.sampleApp.health.vault&quot;).gauge();
        assertEquals(&quot;validating that vault gauge return the correct value&quot;,
            1.0, g.value(), 0.0);
    }
}
</code></pre>
<p>I am unable to run this testcase because of this issue. Verified Package structure, Package structure is same in both src/main/java and src/test/java.</p>",,0,0,,2021-9-23 11:08:44,,2021-9-23 11:08:44,,,,,16038505,,1,0,spring|spring-boot|junit|datadog,24,5.52084
261736,3,Visualization,68548632,How to load Jira content into Datadog and create Datadog dashboards,"<p>I am new to Datadog and I want to create a dashboard with Jira content. The dashboard which I am looking should have Jira Sprint board information also the status of each tickets.</p>
<p>Basically the Jira sprint board and ticket's status should be monitored in Datadog dashboard instead of Jira dashboard.</p>
<p>Thanks!</p>",,0,0,,2021-7-27 16:28:15,,2021-7-27 16:28:15,,,,,15359511,,1,0,jira|datadog,24,5.52084
261737,3,Monitoring,68549793,How can I add a part of log message inside the Datadog notification?,"<p>I have been trying to include log message body inside the notification, but couldn't.
The actual log contains all the attributes in the 'Event Attributes' properly, but I couldn't find a way to include the value of the attributes in the notification body.</p>
<p>For example, the target log contains an event attribute 'thread_name' with a value of '123'.
But when I tried to use that value in the notification body with {{thread_name}} or {{thread_name.name}}, it never shows in the actual notification email.</p>
<p>In the reference docs, I couldn't find how to get the message attribute values, except for the predefined variables.</p>
<p>Is it even possible to include log message values in the notification?</p>",,0,0,,2021-7-27 18:02:53,,2021-7-27 18:02:53,,,,,16539581,,1,0,notifications|datadog,24,5.52084
261738,3,Visualization,67288057,"Can we create Datadog log view with the date column of datadog and also the custom timestamp column created, by parsing from the log?","<p>Can we create Datadog log view with the date column of datadog and also the custom timestamp column created, by parsing from the log? Also can we sort(ascending/descending) the logs view using both the date column of datadog and the custom parsed timestamp log independently?</p>",,0,0,,2021-4-27 17:38:45,,2021-4-27 17:38:45,,,,,5012608,,1,0,sorting|datadog,24,5.52084
261739,3,Visualization,64835003,Communicate from Datadog Dashboard to an application running in AWS EC2,"<p>I need to develop a Datadog dashboard which will monitor metrics, logs of the applications running in AWS EC2. At the same time i have some need to send some messages to Application from Datadog Dashboard. Is it possible to do that? If it is not what are the alternative i can use to achieve this.</p>",,0,2,,2020-11-14 14:36:18,,2020-11-14 14:36:18,,,,,1744489,,1,0,amazon-web-services|amazon-ec2|datadog,23,5.44691
261740,1,Method,67612225,Best metric type in datadog for active SFTP connections,"<p>So my application connects to multiple SFTP servers and performs actions. Now I wanted the metric of how many active connections are present at a certain time for different SFTP servers. I wrote an API which creates a map of all the active SFTP connections and the count of each. I have also written a job which calls the API every hour.</p>
<p>Now, I was trying to send these metrics to datadog but the results on datadog look extremely skewed.</p>
<pre><code>Sample connection map:
{
 &quot;server1.com&quot;:3,
 &quot;server2.com&quot;:5,
.
.
.
}
</code></pre>
<p>My code to send the metrics looks somewhat like this</p>
<pre><code>         for (String conn: connectionMap.keySet()) {
            Map&lt;String, String&gt; tagMap =  new HashMap&lt;&gt;();
            try{
                tagMap.put(&quot;server&quot;, conn);
                metrics.addCountMetric(&quot;number&quot;, connectionMap.get(conn), tagMap);
            } catch (Exception e) {
                ...
            } finally{
                ...
            }
        }
</code></pre>
<p>The number of connections shown in the datadog metrics are extremely high. I have even crossed with the API response. What am I doing wrong here? Am I not setting something in the datadog UI to view it right? For my use case should I use gauge metric, although, since I am sending them only once per hour, I am guessing count should also have worked fine.</p>
<p>Thank you so much internet people.</p>",,0,0,,2021-5-19 23:43:41,,2021-5-19 23:43:41,,,,,7741835,,1,0,datadog,23,5.44691
261741,1,Method,70066149,Can I check GCP traffic in datadog Frontend?,"<p>I'm trying to migrate my FE web (react app) from old cluster to GCP and I want to migrate it slowly starting from 20%.</p>
<p>Can I check the traffic from datadog to know from where the traffic is coming, GCP or old cluster ?</p>
<p>In Datadog right now I just monitor load time, page view, etc basic FE tracking using RUM Application</p>",,0,0,,2021-11-22 12:58:57,,2021-11-22 12:58:57,,,,,9020586,,1,0,google-cloud-platform|frontend|datadog,23,5.44691
261742,1,Error,69115793,Error when calling SyntheticsAPI->create_synthetics_api_test: SSL peer certificate or SSH remote key was not OK,"<p>I am trying to create the monitors in datadog, but getting SSL peer certificate or SSH remote key was not OK</p>
<p>ruby version : 2.7
Platform : windows 10</p>",,0,0,,2021-9-9 09:40:12,,2021-9-9 09:40:12,,,,,808233,,1,0,ruby-on-rails|ruby|datadog,23,5.44691
261743,3,Monitoring,65487235,how do I create monitor in datadog which compare the current with the max value of last month?,"<p>I want to create a monitor in datadog that fires an alert if the current value is larger than the maximum value of last month. something like this:</p>
<pre><code>lastMonthMax = max of period (1-Nov to 30-Nov)
currentValue = (value @ 15-Dec)
if percentage different of (currentValue, lastMonthMax) &gt; 15%, trigger alarm.
</code></pre>
<p>The purpose of this monitor is to check if the monthly increment is larger than 15%.</p>
<p>anyone know if this is possible?</p>",,0,0,,2020-12-29 04:19:37,,2020-12-29 04:19:37,,,,,3691191,,1,0,datadog,23,5.44691
261744,1,Method,69042543,Forward Azure Metrics for very specific resources to DataDog,"<p>This question may sound a little odd, but here it goes: A customer of ours would like to get access to certain metrics of his environment of our product which we host on Azure for the customer. It's a pretty complicated deployment, but in the end it consists of an Application Gateway, some virtual machines and a dedicated Azure SQL database.</p>
<p>The customer now would want to get select metrics from this deployment forward to their own DataDog subscription, e.g. VM CPU metrics, database statistics and those things. DataDog obviously supports all this information (which is good), but as a default would slurp in information from <strong>all</strong> resources within our subscription (which is not OK).</p>
<p>Is there a way to fine-granularly define which data is forwarded to DataDog, e.g. the resources and also which type of metrics to forward for each resource? What are my options here? Is it enough to create a service principal with a limited reading right, or can I configure this somewhere else? I am unfortunately not familiar with DataDog.</p>
<p>The main thing which must be prevented is that the customer due to the metrics forwarding could get access to other metrics in our subscription - we need to control the exact scope of the metrics.</p>",,0,0,,2021-9-3 09:22:29,,2021-9-3 09:22:29,,,,,1692830,,1,0,azure|metrics|forwarding|datadog,23,5.44691
261745,2,Query,69574279,how to query EKS version in datadog?,"<p>I am trying to make a simple dashboard in Datadog to display the current kubernetes versions of EKS clusters in di</p>
<p>fferent environments. How
ever, a</p>
<p>s far as I</p>
<p>can tell this is not someth</p>
<p>ing that is queryable. It appears based on <a href=""https://docs.datadoghq.com/ag"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/ag</a>
ent/kubernetes/data
_collected/ that
this is not a metric t
hat is collec
ted, but I find it hard to believe that there is no way to do</p>
<p>this</p>
<p>.</p>
<p><em><strong>EDITED TO REFORMAT</strong></em></p>",,0,2,,2021-10-14 16:32:47,,2021-10-17 14:04:50,2021-10-17 14:04:50,,16456594,,16456594,,1,0,kubernetes|amazon-eks|datadog,23,5.44691
261746,1,Method,69918774,Datadog RUM: Logging custom metrics,"<p>I have just started to use Datadog RUM for collecting stats and to be able to see the performance of our single-page application. But I would also like to be able to log some metrics and get a corresponding graph in Datadog based some measurements I do client side for the data received from the back-end. So it's not events, but rather regular logging (every few 100 ms). I have looked through the documentation but find no examples on how this can be done with the RUM interface. Can this be done? Or do I need some other Datadog API for this?</p>
<p>Thanks in advance!
/Per</p>",,0,0,,2021-11-10 18:55:45,,2021-11-10 18:55:45,,,,,17156610,,1,0,datadog,22,5.36969
261747,3,Monitoring,69638540,Datadog monitor using terraform,"<p>Currently we have only one monitor that aggregates errors across all the API calls. I want to split and have monitor per API.</p>
<p>I am trying to understand if I can create datadog monitors (alert on error count) for all the end-points in a service without having to implement resource for each end-point. Is there a way datadog can gather this information based on the APIs and construct monitor dynamically?</p>",,1,0,,2021-10-19 23:10:19,,2021-10-24 14:05:01,,,,,1180969,,1,0,terraform|datadog,21,5.28888
261748,1,Parse,69655071,Q: how do you parse out a word that might have a word space word combo?,"<p>Almost have these logs nailed down but having a hard time with a section of the log that provides the fail2ban action. It is usually a single word, but sometimes it is two words with a space between them. I am not sure how to address this issue and greatly appreciate any assistance. I have done a fair amount of research but not found a definitive answer, even trying various solutions.</p>
<p>In these examples below, you will see <strong>Restore Ban</strong>, <strong>Ban</strong>, and <strong>Found</strong>. This is what I am trying to isolate.</p>
<p><strong>Sample logs:</strong></p>
<pre><code>2021-10-20 19:50:39,638 fail2ban.actions        [31705]: NOTICE  [sshd] Restore Ban 68.183.15.177
2021-10-20 16:08:16,315 fail2ban.actions        [6428]: NOTICE  [sshd] Ban 141.98.10.121
2021-10-20 17:21:23,807 fail2ban.filter         [6428]: INFO    [sshd] Found 159.75.130.111 - 2021-10-20 17:21:23
</code></pre>
<p><strong>Current grok:</strong></p>
<pre><code>fail2banRule01 %{date(&quot;yyyy-MM-dd HH:mm:ss,SSS&quot;):Datetime}\s+fail2ban.%{word}\s+\[%{number:PID}\]\:\s+%{word:Level}\s+\[%{word:Jail}\]\s+%{word:ActionType}\s+%{ipv4:ClientIP}(\s+-\s+%{date(&quot;yyyy-MM-dd HH:mm:ss&quot;):ActionDate})?
</code></pre>
<p>NOTE: This will be within Datadog's log processing structure.</p>",,1,0,,2021-10-21 02:19:35,,2021-10-21 14:46:11,,,,,17206703,,1,1,parsing|logging|datadog|grok|fail2ban,18,5.22109
261749,0,Integration,69734949,How to setup MeterRegistry for using DataDog on Micrometer.io,"<p>Hi all i have setting up on my properties like this</p>
<pre><code>management.metrics.export.datadog.apiKey=xxxxx
management.metrics.export.datadog.enabled=true
management.metrics.export.datadog.enabled.step=10s
</code></pre>
<p>and here my code try for registry</p>
<pre><code>   public class DataDogLogService {
        private Process process;
        DatadogConfig config = new DatadogConfig() {
            @Override
            public Duration step() {
                return Duration.ofSeconds(10);
            }
    
            @Override
            public String get(String k) {
                return null; // accept the rest of the defaults
            }
        };
        MeterRegistry registry = new DatadogMeterRegistry(config, Clock.SYSTEM);
        Counter countError = registry.counter(&quot;error&quot;, &quot;requestBuy&quot;);
      public saveError(String id){
     counterError.increment();
}

}
</code></pre>
<p>but i see my data cannot come to my dataDog.. my question is how to setting up MetricRegistry if i want to use dataDog</p>",,0,0,,2021-10-27 08:05:24,,2021-10-27 08:05:24,,,,,16150140,,1,0,java|spring-boot|datadog,20,5.20412
261750,1,Parse,66906870,How does Datadog's statsd handle a `None` value for a tag,"<p>I'm adding logging to my application, and was wondering what the correct way is to add a tag that may or may not have a value.</p>
<p>My code is</p>
<pre><code>statsd.event(title='Bad Request',
             text='Could not parse JSON input',
             alert_type='info',
             tags=[config['always_here'], config['sometimes_here']])
</code></pre>
<p>The <code>config</code> is a dict of configuration variables constructed from another class.</p>
<p>Is it okay to pass objects that might not have a value like the <code>sometimes_here</code> config element? Or do I need to build the tags list separately like</p>
<pre><code>tags_list = [config['always_here']]
if config['sometimes_here']
    tags_list.append(config['sometimes_here'])

statsd.event(title='Bad Request',
             text='Could not parse JSON input',
             alert_type='info',
             tags=tags_list)
</code></pre>
<p>Since there might potentially be even more optional elements in config, I'm hoping that the more compact code will be suitable, but I can't find anything in the Datadog documentation on what it would do about those tags.</p>",,0,0,,2021-4-1 14:57:52,,2021-4-1 14:57:52,,,,,1832474,,1,0,python|datadog|statsd,20,5.20412
261751,1,Error,69907835,Application logs collected in Datadog are replicated,"<p>I have a Java/Spring-Boot container application running on Amazon ECS with EC2 as the underlying service for the container cluster. A datadog-agent container (v7.18.1-jmx) is also running within the cluster to feed the logs/metrics back to datadog servers. The logs are flowing through to the datadog webapp as expected but I see the same log line 3 times in the UI. The following environment variables have been set -</p>
<pre><code>DD_API_KEY=&lt;API-KEY&gt;
DD_APM_ENABLED=true
DD_APM_ENV=dev
DD_APM_NON_LOCAL_TRAFFIC=true
DD_DOGSTATSD_NON_LOCAL_TRAFFIC=true
DD_DOGSTATSD_ORIGIN_DETECTION=true
DD_DOGSTATSD_TAGS=[&quot;env:dev&quot;]
DD_LOG_LEVEL=error
DD_LOGS_CONFIG_COMPRESSION_LEVEL=1
DD_LOGS_CONFIG_USE_COMPRESSION=true
DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL=true
DD_LOGS_CONFIG_USE_HTTP=true
DD_LOGS_ENABLED=true
DD_TAGS=environment:dev
DD_URL=&lt;Datadog-url&gt;
</code></pre>
<p>Following mount points are present in the datadog-agent container definition in ECS -</p>
<pre><code>Container Path            Source Volume     Read only

/var/run/docker.sock      docker_sock       true
/host/proc/               proc              true
/host/sys/fs/cgroup       cgroup            true
/opt/datadog-agent/run    pointdir           
/etc/passwd               passwd            true  
</code></pre>
<p>I tried replicating this by setting up the application and the datadog-agent on my Docker Desktop and there seems to be no issue with that setup. Is it happening because the same log content is getting captured at multiple mount points? Any help would be great!</p>",,1,0,,2021-11-10 03:57:53,,2021-11-21 02:08:18,,,,,4325533,,1,0,docker|amazon-ecs|datadog,20,5.20412
261752,1,Parse,69251982,How to redact geo location data from Datadog RUM?,"<p>I have a web application that I want to hook up with Datadog's Real User Monitoring (RUM) service, but I want to be able to prevent specific information from being collected, such as information about a user's location or device. Their <a href=""https://docs.datadoghq.com/real_user_monitoring/browser/modifying_data_and_context/?tab=npm"" rel=""nofollow noreferrer"">docs</a> do say that you can redact some pieces of information such as referrer URL, but the list of fields that you can redact is rather limited. So my question is how can I manually redact or control other pieces of information that Datadog RUM collects?</p>",,0,0,,2021-9-20 09:23:03,,2021-9-20 09:23:03,,,,,14555313,,1,0,monitoring|datadog|redaction,20,5.20412
261753,1,Parse,69188460,Json object as string inside a json log - Spring Boot,"<p>I am using log4j2 appender to log my application. The appender looks like:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;Configuration monitorInterval=&quot;3&quot;&gt;

    &lt;Appenders&gt;

        &lt;Console name=&quot;STDOUT_JSON&quot; target=&quot;SYSTEM_OUT&quot;&gt;
            &lt;PatternLayout
                    pattern='{&quot;message&quot;: &quot;%level %c{1}:%L - %msg%throwable{separator(\n)}&quot;, &quot;timestamp&quot;: &quot;%d{yyyy-MM-dd HH:mm:ss.SSS}&quot;, &quot;level&quot;: &quot;%level&quot;, &quot;location&quot;: &quot;%l&quot;, &quot;loggerName&quot;: &quot;%C&quot;, &quot;line&quot;: &quot;%L&quot;, &quot;thread&quot;: &quot;%t&quot;, &quot;threadId&quot;: &quot;%tid&quot;}%n'
            /&gt;
        &lt;/Console&gt;

    &lt;/Appenders&gt;

    &lt;Loggers&gt;
        &lt;Root level=&quot;INFO&quot;&gt;
            &lt;AppenderRef ref=&quot;STDOUT_JSON&quot;/&gt;
        &lt;/Root&gt;
    &lt;/Loggers&gt;

&lt;/Configuration&gt;
</code></pre>
<p>So every time I log something it comes out in this specific format which is good. I know there is JSONLayout that I can use - but my requirements don't allow me to do that.</p>
<p>The problem is that I want to log a JSON object as a string inside this log format.</p>
<p>Let's say I have this JSON Object in a variable -</p>
<pre><code>{&quot;key1&quot;:&quot;value1&quot;, &quot;key2&quot;:&quot;value2&quot;}
</code></pre>
<p>When I log this object, the result is:</p>
<pre><code>{&quot;message&quot;: &quot;INFO MyClass:84 - {&quot;key1&quot;:&quot;value1&quot;, &quot;key2&quot;:&quot;value2&quot;}&quot;, &quot;timestamp&quot;: &quot;2021-09-15 06:51:36.449&quot;, &quot;level&quot;: &quot;INFO&quot;, &quot;location&quot;: &quot;com.my.company.MyClass.method(MyClass.java:84)&quot;, &quot;loggerName&quot;: &quot;com.my.company.MyClass&quot;, &quot;line&quot;: &quot;84&quot;, &quot;thread&quot;: &quot;thread-name-1&quot;, &quot;threadId&quot;: &quot;29&quot;}
</code></pre>
<p>What I want is: (What should I do to get this?)</p>
<pre><code>{&quot;message&quot;: &quot;INFO MyClass:84 - {\&quot;key1\&quot;:\&quot;value1\&quot;, \&quot;key2\&quot;:\&quot;value2\&quot;}&quot;, &quot;timestamp&quot;: &quot;2021-09-15 06:51:36.449&quot;, &quot;level&quot;: &quot;INFO&quot;, &quot;location&quot;: &quot;com.my.company.MyClass.method(MyClass.java:84)&quot;, &quot;loggerName&quot;: &quot;com.my.company.MyClass&quot;, &quot;line&quot;: &quot;84&quot;, &quot;thread&quot;: &quot;thread-name-1&quot;, &quot;threadId&quot;: &quot;29&quot;}
</code></pre>
<p>I am exporting the logs to Datadog and if JSON is incorrectly parsed, it does not interpret the message as I want it to.</p>",,0,0,,2021-9-15 07:02:48,,2021-9-15 07:02:48,,,,,13996682,,1,0,json|spring-boot|parsing|escaping|datadog,20,5.20412
261754,3,Monitoring,70133183,Monitor MongoDB Replica Set members States using Datadog Agent,"<p>How to monitor the state of MongoDB Replica Set members using Datadog agent?
And by state I mean whether the node is Primary, Secondary, Arbiter.., and that in order to send an alert when the state changes. It seems that the agent by default doesn't collect the state.
Thank you in advance.</p>",,0,0,,2021-11-27 08:30:59,,2021-11-27 19:14:48,2021-11-27 19:14:48,,17522448,,17522448,,1,1,mongodb|datadog,20,5.20412
261755,0,Integration,69081742,Firehose Adds Redundant Tags in Firehose-Datadog Integration,"<p>We have a setup to forward logs from Cloudwatch to Datadog with Kinesis Firehose. We found that the Kenesis pipeline automatically adds the tags <code>service</code> and <code>source</code> to the logs on Datadog, and populate each with the log-group name.</p>
<p>Let's say I have a user-auth service. Within that service, I'm producing json logs like so</p>
<pre><code>{
  service: 'user-auth',
  message: 'some log',
  level: 30
}
</code></pre>
<p>With the above, Datadog will automatically tag this log as service:user-auth --&gt; this is desired behavior.</p>
<p>However, when Kinesis Firehose comes into the picture, it adds another tag <code>service:user-auth-log-group</code> to the log. Now I have 2 service tags in my log, namely:</p>
<ul>
<li>service:user-auth</li>
<li>service:user-auth-log-group --&gt; this is useless, and odd that log group will be considered a 'service'</li>
</ul>
<p>This adds clutter to the service tags.</p>
<p>Firehose does the same with source tag, in that it adds <code>source:user-auth-log-group</code> automatically.</p>
<p>Is there a way to stop Firehose from adding these tags?</p>",,0,0,,2021-9-7 02:21:58,,2021-9-7 02:21:58,,,,,5204647,,1,0,logging|datadog|firehose,20,5.20412
261756,3,Monitoring,69455584,Implementing Datadog APM on a distributed python application,"<p>I am running a distributed application that consists of 2 separate components.</p>
<p>1 - Sending a message to AWS SQS with some data/configuration for running a particular task.</p>
<p>2 - Reading that message in a python run environment and doing some process.</p>
<p>I have implemented custom instrumentation for these 2 components of my application. Both of these are independent of each other. So, I am getting separate traces for them.</p>
<p>I have a unique id that is common for both the operation. I want to map these 2 traces since it is a single task. How can I achieve that?</p>
<p>I tried tagging the common id to both the traces but I am still getting separate traces for them.</p>",,0,0,,2021-10-5 18:32:38,,2021-10-5 18:32:38,,,,,16493383,,1,0,python|datadog|apm,19,5.11501
261757,3,Visualization,67543113,Creating same Datadog graph for different enviroments,"<p>I created a new graph in my dashboard which works fine.
Now I changed dashboard env from stage to prod, and I see my graph there even though I created it only for stage. Now this is OK because I want to have graphs for both stage and prod. Problem is, when I change the settings in the graph in prod dashboard, they are changing also in stage. Like it is pointing to the same graph. How can I separate those two graphs?</p>",,0,0,,2021-5-15 03:52:19,,2021-5-15 03:52:19,,,,,3077485,,1,0,datadog,19,5.11501
261758,0,Integration,69925179,Does Uvicorn support Stats Server Mechanism?,"<p>I am working on a FastAPI web application and using <a href=""https://www.uvicorn.org/settings/"" rel=""nofollow noreferrer"">Uvicorn</a> ASGI server. Now, want to configure server stats in Uvicorn but have not found a reference regarding.</p>
<p>Ex - As like <a href=""https://uwsgi-docs.readthedocs.io/en/latest/StatsServer.html#the-uwsgi-stats-server"" rel=""nofollow noreferrer"">uWSGI Stats Server</a> provides stats -
<code>uwsgi --socket :3031 --stats :1717 --module welcome</code></p>
<p>So, my question is Does Uvicorn supports the stats server mechanism? or, Is there any other way to achieve this?</p>",,0,0,,2021-11-11 08:26:41,,2021-11-11 08:26:41,,,,,10217732,,1,1,python|fastapi|datadog|uvicorn|asgi,19,5.11501
261759,1,Method,67382553,datadog - different calculation between two cusutom matrices or two logs,"<p>Hello I want to calculate the Latency between two different events which is loged in as Logs by comparing two different Logs's timestamp different. Is there any way we can compare two different logs???</p>
<p>As currently I am only able to manipulate a single event, not with multiple events..</p>
<p>For E.g. I have passed two Logs</p>
<p>Event A : Document Created with timestamp and user info, session info
Event B : Document Published with timestamp and user info, session info</p>
<p>suppose I want to calculate the time for user spend on publishing the document after the Event A called. so for that it will require to Event B's Timestamp - Event A's Timestamp</p>
<p>How can i archive it in Datadog to play between two different logs?</p>",,0,1,,2021-5-4 09:57:16,,2021-5-4 09:57:16,,,,,15831326,,1,0,dashboard|datadog,19,5.11501
261760,3,Visualization,69040001,DataDog Dashboard False Alarm,"<p>I had setup a DataDog Dashboard for missing events (Threshold Alert). Below is the query I had used to verify if there is no event within last 10 minutes then send me an alert. Though there is data, sometimes I am getting an alert and when I go back and verify it on the dashboard everything seems to be correct. Am is missing anything from the below query (defining the metric)? or in the Alert conditions</p>
<p>Query: sum(last_10m):sum:Success{prod}.as_count().rollup(sum, 600) &lt;= 0</p>
<p><a href=""https://i.stack.imgur.com/aVjOX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aVjOX.png"" alt=""Set Alert Conditions"" /></a></p>",,0,0,,2021-9-3 05:39:30,,2021-9-3 05:39:30,,,,,4004863,,1,0,datadog,19,5.11501
261761,1,Method,68279344,How to run synthetic tests for a specific period in datadog?,"<p>Is there a way to pause synthetic tests in data-dog at a particular time and resume them again at another, I want my test to run for a specific period of the day.</p>",,0,0,,2021-7-7 02:38:08,,2021-7-7 02:38:08,,,,,14795686,,1,0,testing|ui-testing|datadog,18,5.02109
261762,1,Method,70016425,Pushing API metrics,"<p>I have a spring-boot application and I want to push metrics like http-response-code, response-time, response-size etc generic metrics to Datadog and for that we have two ways:-</p>
<ol>
<li>I can write the code in a Filter to send such metrics to Datadog from Application.</li>
<li>Using some agent which will send these metrics automatically and decouple this overhead from Application.</li>
</ol>
<p>Now my question is which solution is better? what are the pros and cons of each and what should one choose and why?</p>
<p><strong>NOTE : I am only interested in generic API metrics like http-response-code, response-time, response-size only.</strong></p>",,0,0,,2021-11-18 08:09:40,,2021-11-18 09:00:33,2021-11-18 09:00:33,,1157635,,1157635,,1,0,spring-boot|prometheus|elk|datadog,18,5.02109
261763,3,Monitoring,69679912,Monitoring Multiple ElasticSearch Instances through DataDog,"<p><a href=""https://docs.datadoghq.com/integrations/elastic"" rel=""nofollow noreferrer"">DataDog offers monitoring of ElasticSearch clusters</a>, which are configurable through the elastic.d/conf.yaml file. There is a <code>instances</code> property which identifies the cluster url, among other settings. Is it possible to update the YAML file to include multiple clusters? It does not appear the -url property takes an array, and ideally monitoring would be configurable per cluster.</p>",,1,0,,2021-10-22 16:01:12,,2021-10-26 19:51:25,,,,,15107248,,1,0,elasticsearch|yaml|datadog,18,5.02109
261764,3,Visualization,69370596,plot multiple Datadog widgets based on same counter but different OpenTelemetry attributes,"<p>For example, I have one counter, but 2 attributes:</p>
<pre><code>counter := metric.Must(meter).NewInt64Counter(&quot;some-name&quot;)

attr1 := attribute.String(&quot;same-label-key&quot;, &quot;different-label-value-1&quot;)
counter.Add(ctx, 1, attr1)

attr2 := attribute.String(&quot;same-label-key&quot;, &quot;different-label-value-2&quot;)
counter.Add(ctx, 1, attr2)
</code></pre>
<p>The counter will be +2, in which <code>attr1</code> contributes +1, while <code>attr2</code> contributes another +1.</p>
<p>Is it possible for us to plot 2 different Datadog widgets (one widget for <code>attr1</code>, and another for <code>attr2</code>) based on the same counter?</p>",69377320,1,0,,2021-9-29 04:55:22,,2021-9-29 13:25:18,,,,,16258438,,1,0,datadog|open-telemetry,18,5.02109
261765,0,Configuration,69519870,"Django: How to locate caller of the ""/device/event"" route?","<p>I am using Datadog to understand the frequency of the routes in a Django project.</p>
<p>Datadog reports that the most frequent route is <code>/device/event</code>, and I have no idea what this route is or what is calling it.</p>
<p>How would a person figure out its origin or where it's defined?</p>
<p><a href=""https://i.stack.imgur.com/ir0ep.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ir0ep.png"" alt=""enter image description here"" /></a></p>",,0,1,,2021-10-11 00:16:13,,2021-10-11 00:16:13,,,,,1367204,,1,0,django|api|django-rest-framework|datadog,17,4.9218
261766,1,Error,69998309,Datadog: trace scope return null,"<p>I have this problem:</p>
<p>I wanna add tags for create some dashboards in datadog but when i create the variable span just create a null value.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const tracer = require(""dd-trace"").init();
const tabSize = 2;

const tracerWrapper = (event, context, callback, fn) =&gt; {
  const span = tracer.scope().active();
  if (span) {
    if (event.headers[""region-id""])
      span.setTag(""region_id"", event.headers[""region-id""]);
    if (event.headers[""country-id""])
      span.setTag(""country_id"", event.headers[""country-id""]);
    if (event.headers[""subsidiary-id""])
      span.setTag(""subsidiary_id"", event.headers[""subsidiary-id""]);
    span.setTag(""app_name"", ""mecanik-international"");
    span.setTag(""resolver_name"", fn.name);
  }
  try {
    fn(event, context, callback);
  } catch (e) {
    console.error(`Error Mecanik International Wrapper in ${fn.name}: ${JSON.stringify(e, null, tabSize)}`);
    callback(null, {
      statusCode: 400,
      headers: { ""Content-Type"": ""application/json"" },
      isBase64Encoded: false,
      body: JSON.stringify(e),
    });
  }
};

module.exports.tracerWrapper = tracerWrapper;</code></pre>
</div>
</div>
</p>
<p>Im using a serverless lambda(AWS) with node 12.x
I all ready set the datadog in the serverless.yml</p>
<blockquote>
<p>enableDDTracing: true</p>
</blockquote>",,0,0,,2021-11-17 02:38:28,,2021-11-17 02:38:28,,,,,17433910,,1,0,node.js|serverless|trace|datadog,16,4.81648
261767,1,Method,68852738,DataDog DynamoDB metrics,"<p>According to this document <a href=""https://docs.datadoghq.com/integrations/amazon_dynamodb/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/amazon_dynamodb/</a> we should be seeing</p>
<blockquote>
<p>aws.dynamodb.throttled_requests, aws.dynamodb.throttled_requests,
aws.dynamodb.read_throttle_events, aws.dynamodb.write_throttle_events</p>
</blockquote>
<p>But we don't see it in Datadog Metrics/Monitoring. I read through the doc and don't see any specific configuration that needs to be enabled.</p>",,0,0,,2021-8-19 18:08:03,,2021-8-19 18:08:03,,,,,8072466,,1,0,amazon-dynamodb|datadog,16,4.81648
261768,2,Query,69730541,Can I configure a custom query to Cosmos DB in Datadog,"<p>Is there a way to configure custom query checks to Cosmos DB like we can do for conventional databases like <a href=""https://docs.datadoghq.com/integrations/oracle/?tab=linux#custom-query"" rel=""nofollow noreferrer"">Oracle</a>?</p>
<p>I need to define a custom metric which will collect its values though a query running on database i.e. Azure Cosmos DB.</p>",,0,0,,2021-10-26 22:10:55,,2021-10-26 22:10:55,,,,,971741,,1,0,azure-cosmosdb|datadog,16,4.81648
261769,2,Query,69406581,Is is possible in Datadog to query within a query,"<p>I am new to Datadog, and I need to do a search that may not be possible. I can write this most succinctly in SQL.</p>
<pre><code>SELECT * FROM LOGS WHERE RequestID IN 
     (SELECT RequestID FROM LOGS WHERE FooID = @SomeID) 
</code></pre>
<p>In my scenario, RequestID follows a single API call through the life of that call. However, the API accepts multiple calls on the same Foo in a series. I would like to be able to see everything related to the series of calls. The above query would achieve that for me.</p>",,0,0,,2021-10-1 13:19:53,,2021-10-1 13:19:53,,,,,1481314,,1,0,datadog,15,4.70437
261770,0,Configuration,68283614,DataDog - pass multiple fnmatch paterns to configuration,"<pre><code>init_config:

instances:
  - directory: /mnt/ftp/generic/Salesorder
    pattern: '*_12_*.csv'
    filegauges: true
    dirtagname: history

  - directory: /mnt/ftp/generic/Salesorder
    pattern: '2021_*_*.csv'
    filegauges: true
    dirtagname: this-year
</code></pre>
<p>The directory contains multiple files with the next format YYYY_MM_SalesOrder.csv</p>
<p>How to use multiple patterns in a single instance?</p>",,1,0,,2021-7-7 09:43:06,,2021-8-4 10:12:12,,,,,6166504,,1,0,directory|analytics|datadog|fnmatch,15,4.70437
261771,3,Monitoring,69381310,Metric Monitors in Datadog,"<p>What does this query mean? What does rate mean here?
A rate is a ratio. What ratio is calculating here?</p>
<p>sum:com.mycompany.gco.xyz.search.service{env:lab,service:gco-seva-search-service,method_name:somequery} by {method_name}.as_rate()</p>",,0,0,,2021-9-29 17:55:09,,2021-9-29 17:55:09,,,,,5351746,,1,0,datadog,15,4.70437
261772,0,Configuration,70084016,RUM browser SDK sample rate,"<p>I had a quick question regarding the <code>sampleRate</code> property in the RUM browser SDK initialization parameters. I can see from the <a href=""https://github.com/DataDog/browser-sdk/blob/main/packages/rum/README.md#initialization-parameters"" rel=""nofollow noreferrer"">documentation</a> that the type is <code>number</code>, but I'm having a hard time determining if it supports floating point values. I currently have a sample rate of 1% set up, and I wanted to know if it would be possible to drop it to 0.5%.</p>
<pre><code>datadogRum.init({
  applicationId: &quot;&quot;,
  clientToken: &quot;&quot;,
  site: &quot;datadoghq.com&quot;,
  service: &quot;&quot;,
  sampleRate: 1,
  trackInteractions: true,
})
</code></pre>
<p>Does the SDK support using <code>0.5</code> for a 0.5% sampling rate?</p>",70090243,1,0,,2021-11-23 16:01:20,,2021-11-24 02:58:53,,,,,8446642,,1,0,datadog,15,4.70437
261773,3,Visualization,68390454,Dynamic/composite column in DataDog,"<p>I calculate statistics over log entries in DataDog.<br>
One of my columns is URI with different URIs and some of them are &quot;ke1/ID/key2/ID&quot;, where key1 and key2 are constant values but ID values are different.<br>
When I group by URI column, I have different entries for URIs with different IDs. But for me they mean the same thing.<br></p>
<p>What is a possible way to group by URI column but treat URIs with different IDs as one?<br></p>
<p>I thought of adding dynamic column that gives a single value for URIs with different IDs and group by it, but I don't see how to do that in DataDog.</p>",68393177,1,0,,2021-7-15 08:33:27,,2021-7-15 11:43:52,,,,,1262265,,1,0,datadog,14,4.58451
261774,3,Visualization,68247283,Find dashboards or monitors that use a given custom metric?,"<p>Is it possible to search for dashboards and/or monitors that use a given custom metric?</p>
<p>I've tried using the dashboard search and the monitor search, and it looks like they only search the names of the dashboards and monitors.</p>
<p>My use cases are &quot;I'd like to stop emitting a custom metric, and I want to know whose monitors and dashboards I will be breaking&quot; and &quot;Am I even using this custom metric for anything?&quot;</p>",,0,0,,2021-7-4 17:33:47,,2021-7-4 17:33:47,,,,,1812727,,1,0,datadog,14,4.58451
261775,1,Method,68566000,"DataDog, Serilog .Net - Get trace_id",<p>It`s possible to get trace_id from Datadog in Controller class in Api using .Net Core ?</p>,,0,0,,2021-7-28 18:56:06,,2021-7-28 18:56:06,,,,,14823226,,1,0,.net-core|datadog,14,4.58451
261776,0,Integration,70041873,Elastic Beanstalk ignoring .ebextensions for Datadog config in WAR deployment,"<p>I've looked at everything I can seem to find and nothing has really helped make progress. I'm trying to get Elastic Beanstalk to recognize and process the <code>99datadog.config</code> file in order to set up application profiling. Our app is deployed as a WAR, and the <code>.ebextensions</code> directory is at the top of it. The file structure looks like this:</p>
<pre><code>/usr/share/tomcat8/webapps/
  --api/
    --.ebextensions/
      --99datadog.config
    --META-INF
    --WEB-INF
  --api.war/
  --ROOT/
  --ROOT.war
</code></pre>
<p>The datadog config can be found here: <a href=""https://docs.datadoghq.com/config/99datadog.config"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/config/99datadog.config</a></p>
<p>We've got a different webapp that is structured the same way and the <code>.ebextensions</code> work there, which is even more confusing to me.</p>
<p>I've checked the <code>cfn-init-cmd.log</code> (and all the other logs for that matter) and can't find any mention of it even trying to run anything from that file.</p>
<p>Any ideas?</p>",,0,0,,2021-11-19 23:09:31,1,2021-11-19 23:09:31,,,,,2050069,,1,0,java|amazon-web-services|amazon-elastic-beanstalk|datadog|ebextensions,14,4.58451
261777,2,Query,70073385,How do I search for logs with values in an array in Datadog?,"<pre><code>{
    dd {    
        service cm-api
    },
    level: INFO,
    product_ids: [  
        4105428,
        4105429,
        4105430,
        4105431,
    ],
    time: 2021-11-20T22:45:11.733088+00:00
}
</code></pre>
<p>I want to find all logs that have a certain product_id in them. I want something like (in pseudo-code) <code>@product_ids.contains(4105428)</code>. I have tried <code>@product_ids:5845542</code> but I don't get back logs that have this number in them.</p>",,1,0,,2021-11-22 22:58:11,,2021-11-24 15:17:39,2021-11-22 23:06:00,,10475050,,10475050,,1,0,parsing|logging|datadog,13,4.45577
261778,1,Parse,69566190,Pull value from the log and place in datadog widget,"<p>Log in datadog from my application:</p>
<pre><code>{&quot;type&quot;:&quot;info&quot;,&quot;msg&quot;:&quot;the name added fname:john&quot;,&quot;timestamp&quot;:&quot;2021-10-14T01:11:43.804Z&quot;,&quot;tags&quot;:['fname:john']}
</code></pre>
<p>I can create a widget query  <code>fname == 'john'</code> and place the total the no.of matching logs in the widget.</p>
<p>But I want to create a widget querying the value of <code>fname</code> instead of no.of times exist.  The widget value should be value of fname .eg: 'john' from the recent log.</p>
<p>any help?</p>",,0,0,,2021-10-14 06:46:59,,2021-10-14 06:46:59,,,,,3119467,,1,0,datadog,12,4.31672
261779,2,Query,70160861,Finding the duration of Kubernetes Pods in Datadog?,"<p>I am trying to aggregate the pod duration of the status <strong>Running</strong> over another custom business-logic tag. Then I can calculate how much it cost me to run this service.</p>
<p>I have tried to use <code>docker.uptime</code> but it has not been fruitful as I imagine multiple containers can be run in parallel per node. I saw that Datadog KSM provides <code>pods.age</code> and <code>pods.uptime</code> metrics but they do not appear in Datadog metric explorer.</p>
<p>I do not want to use Prometheus/Grafana to do this because I think this should be possible with Datadog. <a href=""https://stackoverflow.com/questions/57517235/get-average-time-a-pod-stays-in-a-pending-state"">Prometheus Solution</a></p>",,0,0,,2021-11-29 20:21:25,,2021-11-29 20:21:25,,,,,14852359,,1,1,kubernetes|amazon-eks|datadog,11,4.16557
261780,3,Monitoring,67498786,Getting the information of how many times the DataDog's specific alert has been triggered,"<p>We are using the DataDog Tool for monitoring our Client Infrastructure ,our intention is that we are trying to gather information of how many times the specific alert has been triggered and when it was resolved. Can someone help me on this. Also need in help of exporting this in excel,csv or google sheets</p>",,0,0,,2021-5-12 07:08:10,,2021-5-12 12:49:40,2021-5-12 12:49:40,,13519075,,13519075,,1,0,devops|monitoring|datadog,11,4.16557
261781,0,Integration,70222752,Why does adding prometheus instrumentation package increases latency of api?,"<p>We are trying to decrease the latency of our prediction service that is deployed using <code>FastAPI</code>. The predictions are called through the <code>predict</code> endpoint. We looked into the tracing and found one of the bottlenecks is exposing the metrics endpoint so that the metrics are propagated to Prometheus. Why is this happening? The metrics are being exposed at the application-level so not sure if it's possible to have the metrics on a different container, maybe?</p>
<p>We are using the <code>prometheus-fastapi-instrumentator</code> package.</p>
<p><a href=""https://i.stack.imgur.com/SBY3w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SBY3w.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/HNbhh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HNbhh.png"" alt=""enter image description here"" /></a></p>",,0,0,,2021-12-4 03:08:23,,2021-12-4 03:08:23,,,,,5378132,,1,0,prometheus|middleware|metrics|fastapi|datadog,10,4
261782,2,Query,70120674,DataDog Group Search Results and Filter By Time Difference,"<p>I am new to DataDog and exploring the DataDog log search and query functionality,</p>
<p>I created a query that returns below log search results</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Timestamp</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nov 21 23:30:00.173</td>
<td>REQUEST: URI:user/1</td>
</tr>
<tr>
<td>Nov 21 23:30:00.179</td>
<td>REQUEST: URI:user/2</td>
</tr>
<tr>
<td>Nov 21 23:30:00.198</td>
<td>RESPONSE: URI:user/1</td>
</tr>
<tr>
<td>Nov 21 23:30:00.205</td>
<td>RESPONSE: URI:user/2</td>
</tr>
<tr>
<td>Nov 21 23:30:01.198</td>
<td>REQUST: URI:user/3</td>
</tr>
<tr>
<td>Nov 21 23:30:05.205</td>
<td>RESPONSE: URI:user/3</td>
</tr>
</tbody>
</table>
</div>
<p>What I am trying to do is to group the requests by their URI so that the request and its correlated response are together. and then get the request, response rows that have a time difference greater than some value.</p>
<p>So in the above case, it should first group them as below</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Timestamp</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nov 21 23:30:00.173</td>
<td>REQUEST: URI:user/1</td>
</tr>
<tr>
<td>Nov 21 23:30:00.198</td>
<td>RESPONSE: URI:user/1</td>
</tr>
<tr>
<td>Nov 21 23:30:00.179</td>
<td>REQUEST: URI:user/2</td>
</tr>
<tr>
<td>Nov 21 23:30:00.205</td>
<td>RESPONSE: URI:user/2</td>
</tr>
<tr>
<td>Nov 21 23:30:01.198</td>
<td>REQUST: URI:user/3</td>
</tr>
<tr>
<td>Nov 21 23:30:05.205</td>
<td>RESPONSE: URI:user/3</td>
</tr>
</tbody>
</table>
</div>
<p>And then return the pair of request, response rows that match the given time difference, if the time difference in the query is 3s, then,</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Timestamp</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nov 21 23:30:01.198</td>
<td>REQUST: URI:user/3</td>
</tr>
<tr>
<td>Nov 21 23:30:05.205</td>
<td>RESPONSE: URI:user/3</td>
</tr>
</tbody>
</table>
</div>
<p>Or if I can get the correlating request, responses which have the same URI and are greater than a given time difference without grouping, that would also be fine.</p>
<p>I tried playing around with the group by functionality in data dog, but that does not seem to work with my use case,</p>
<p>Any help is highly appreciated. Thanks</p>",,0,0,,2021-11-26 07:10:39,,2021-11-26 07:10:39,,,,,9341540,,1,0,datadog,9,3.81697
261783,2,Query,69978462,Sum the values in a tag in Datadog,"<p>I have a job that runs daily and it processes x amount of records for our database.  At the end of the job I'd like to send summarized data about what the job has done for instance.  I was thinking I could tag the metric with the following</p>
<p>successfully_processed = y
failed = z</p>
<p>The part I am struggling with is, how I would create a dashboard that can parse the tag values (x and y) so I can chart them coherently?</p>",,0,0,,2021-11-15 17:28:45,,2021-11-15 17:28:45,,,,,3289010,,1,0,datadog,8,3.61236
261784,3,Monitoring,69435668,How to monitor Ajax calls in an SPA type website?,"<p>We integrated newrelic to monitor our sites various ajax calls, but found that it doesn't work as newrelic only monitors ajax calls made on the initial page hit, not after route changes.  Whilst they do have SPA SDK and APIs, we cant change the pages code (other than to call a lib onload).</p>
<p>Has anyone come across another tool, such as datadog application insights, which does not have this limitation (i.e. will monitor all ajax requests), so we can report on each ajax call, how long it took, what the response code was etc?</p>",,0,0,,2021-10-4 12:02:04,,2021-10-4 12:02:04,,,,,1072187,,1,0,ajax|monitoring|newrelic|datadog,7,3.38039
