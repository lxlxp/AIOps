,id,post_id,title,body,accepted_answer_id,answer_count,comment_count,community_owned_date,creation_date,favorite_count,last_activity_date,last_edit_date,last_editor_display_name,last_editor_user_id,owner_display_name,owner_user_id,parent_id,post_type_id,score,tags,view_count,hot_score
0,258456,49059485,format splunk query by renaming search elements,"<p>I could use a little help with a splunk query I’m trying to use. </p>

<p>This query works fine for gathering the info I need:</p>

<pre><code>index=prd_aws_billing (source=""/*2017-12.csv"") LinkedAccountId=""1234567810"" OR LinkedAccountId=""123456789"" ProductName=""Amazon Elastic Compute Cloud"" | stats sum(UnBlendedCost) AS Cost by ResourceId,UsageType,user_Name,user_Engagement
</code></pre>

<p>However I’d like to refine that a bit. I’d like to represent user_Engagement as just Engagement and  user_Name as “Resource Name”. </p>

<p>I tried using AS to change the output, like I did to change UnBlendedCost to just “Cost”. But when I do that it kills my query, and nothing is returned. For instance if I do either:</p>

<pre><code>index=prd_aws_billing (source=""/*2017-12.csv"") LinkedAccountId=""123456789"" OR LinkedAccountId=""1234567810"" ProductName=""Amazon Elastic Compute Cloud"" | stats sum(UnBlendedCost) AS Cost by ResourceId AS “Resource Name”,UsageType,user_Name,user_Engagement AS “Engagement”
</code></pre>

<p>Or </p>

<pre><code>index=prd_aws_billing (source=""/*2017-12.csv"") LinkedAccountId=""123456789"" OR LinkedAccountId=""1234567819"" ProductName=""Amazon Elastic Compute Cloud"" ResourceID AS “Resource Name” user_Engagement AS “Engagement” | stats sum(UnBlendedCost) AS Cost by ResourceId AS “Resource Name”,UsageType,user_Name,user_Engagement AS “Engagement”
</code></pre>

<p>The query dies, and no info is returned. How can I reformat the search elements listed after the 'by' clause?</p>",49060727.0,1,0,,2018-3-1 22:09:57,,2018-3-2 00:02:37,,,,,1017466.0,,1,0,splunk,180,10
1,258457,49077872,POST a query to Splunk REST API /search/jobs/ endpoint in Golang,"<p>I would like to send a search/query to Splunk REST API, and return a search id to later consume the results.</p>

<p>I can achieve the desired behavior with the below <code>curl</code>:</p>

<pre><code>#!/bin/bash

user='my_user'
pass='my_pass'

search='search index=short sourcetype=src | head 5'

curl -u $user:$pass -k https://111.22.33.44:8089/services/search/jobs -d search=""$search""
</code></pre>

<p>which returns:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;response&gt;
  &lt;sid&gt;234523452435.6556_234234-3J3J-34J4-2345-123456678E3&lt;/sid&gt;
&lt;/response&gt;
</code></pre>

<p>Here are the relevant Go snippets in which I am trying to achieve the same:</p>

<p>Main:</p>

<pre><code>  //main.go
    sid, err := conn.Query()
    if err != nil {
            fmt.Println(""err creating search: %s"", err)
    } else {
            fmt.Println(""sid:"", sid)
    }
</code></pre>

<p>Query:</p>

<pre><code>    // query.go
    func (conn SplunkConnection) Query() (string, error) {
            data := make(url.Values)
            data.Add(""output_mode"", ""json"")
            data.Add(""search%20index%3Dshort%20sourcetype%3Dsrc%20%7C%20head%205"", ""search"")
            data.Add(""-60m%40m"", ""earliest"")
            data.Add(""-10m%40m"", ""latest"")

        // try httpGet() here
        sid, err := conn.httpPost(fmt.Sprintf(""%s/services/search/jobs"", conn.BaseURL), &amp;data)
        if err != nil {
                return """", err
        }

        return string(sid), err
}
</code></pre>

<p>Helper:</p>

<pre><code>// http.go
func (conn SplunkConnection) httpPost(url string, data *url.Values) (string, error) {
        return conn.httpCall(url, ""POST"", data)
}
</code></pre>

<p>What I expect is a response containing just a JSON blob with my SID. Instead, it returns a huge JSON, which appears to be contain all current jobs at the <code>/services/search/jobs</code> endpoint.</p>

<p>How can I adjust my code to return just the SID? (I intend to poll it for completion and retrieve the results later, but don't need help with this...yet).</p>",49078427.0,1,0,,2018-3-2 21:45:34,,2018-3-2 22:34:41,,,,,4109882.0,,1,0,rest|go|splunk,1605,13
2,258458,49080092,"How do I retain table fields in Splunk after a chart pipe? Tried concatenating the values into 'over' parameter, but get 'No results found'","<p>I've got a search where basically I want to show the number of user types per workspace per customer, but I can only get Customer Name and the user types into the table.</p>

<p>This is what I have that gives me that:</p>

<pre><code> | `workspace_submissions((submissions.workspaceGuid=*), (workspaceGuid=*))` 
 | search userGuid=* 
 | eval userPersona=case(workspaceRole=""ADMIN"", ""Builder"", cellValueChanges&gt;0 OR modelChanges&gt;0, ""Contributor"", cellValueChanges=0 AND modelChanges=0, ""Viewer"") 
 | lookup user-dc5prod userGuid output active 
 | eval US=if(active=1, ""Enabled"", ""Disabled"") 
 | stats count by userPersona workspaceGuid 
 | lookup workspace-dc5prod workspaceGuid output workspaceGuid, currentCustomerGuid as customerGuid 
 | lookup customer-dc5prod customerGuid output type name as customerName sfdcAccountGuid
 | chart values(count) over customerName by userPersona
</code></pre>

<p>And I checked out the response at this splunk forum question : <a href=""https://answers.splunk.com/answers/390709/how-do-i-add-extra-fields-to-a-chart-count-over-fi.html?utm_source=typeahead&amp;utm_medium=newquestion&amp;utm_campaign=no_votes_sort_relev"" rel=""nofollow noreferrer"">https://answers.splunk.com/answers/390709/how-do-i-add-extra-fields-to-a-chart-count-over-fi.html?utm_source=typeahead&amp;utm_medium=newquestion&amp;utm_campaign=no_votes_sort_relev</a> but when I put</p>

<pre><code> | eval customerName=customerName.""#"".workspaceGuid.""#"".sfdcAccountGuid
</code></pre>

<p>before chart, I return no results. I need to table customerName sfdcAccountGuid workspaceGuid Builder Contributor and Viewer</p>",,1,0,,2018-3-3 02:41:18,,2018-3-3 03:50:02,,,,,7507925.0,,1,0,charts|splunk|splunk-query,295,9
3,258459,49091472,Forwarding syslog messages from raspbian to splunk,"<p>how can i configure my raspberry pi having raspbian operating system to send syslogs to splunk?</p>

<p>I have configured rsyslog file with port and ip of splunk but not been able to get syslogs on splunk.</p>",,1,0,,2018-3-4 02:52:04,,2018-3-10 03:43:32,2018-3-4 09:03:16,,4420967.0,,9440219.0,,1,-1,raspbian|splunk,149,8
4,258460,49111651,How to produce rows for non-existing time buckets?,"<p>I have produced a table like this:</p>

<pre><code>+----------+---------+---------+-------+
| _time    | field_1 | field_2 | count |
+----------+---------+---------+-------+
| 08:00:00 | A       | 1       | 2     |
+----------+---------+---------+-------+
| 08:00:00 | B       | 1       | 4     |
+----------+---------+---------+-------+
| 08:00:03 | B       | 3       | 1     |
+----------+---------+---------+-------+
| 08:00:03 | A       | 2       | 3     |
+----------+---------+---------+-------+
</code></pre>

<p>I want to know:
what's average &amp; maximum count, per (field_1+field_2) combination, per second.
The problem is the _time is missing some seconds, so the <code>stats count</code> result only give me the aggregated results on existing time buckets.</p>

<p>How can I expand this table to include every missing time seconds, just by filling count=0, for each (field_1+field_2) combination? As long as I can do this, I can get the result simply by <code>stats avg(count) max(count) by field_1 field_2</code>.</p>",,2,0,,2018-3-5 13:32:58,,2018-3-5 16:17:02,,,,,5184632.0,,1,0,splunk|splunk-query|splunk-formula,1164,13
5,258461,49137334,splunk workflow actions not working,"<p>I am trying to create an incident using splunk POST workflow action. From event when i try to trigger the workflow action a new window is opened and the query string is getting appended to URL but the values are not filled in the fields.</p>

<p>how to resolve this issue?</p>",,0,3,,2018-3-6 18:07:16,,2018-3-6 18:20:47,2018-3-6 18:20:47,,5088859.0,,5088859.0,,1,0,splunk|splunk-query|splunk-formula|splunk-calculation|splunk-sdk,173,8
6,258462,49234052,How to append two queries in splunk?,"<p>I have following two queries:</p>

<p><code>host=""abc*"" sourcetype=""xyz"" Request=""some.jsp""
| stats count as ""TotalCount"" by Request</code></p>

<p>This gives the total count of requests</p>

<p>and </p>

<p><code>host=""abc*"" sourcetype=""xyz"" Request=""some.jsp""
| where TimeTaken &lt; 6000
| stats count as ""ReqLT6Sec"" by Request</code></p>

<p>This gives count of requests which took less than 6 seconds response time.</p>

<p>My requirement is to get these two results by running a single query. I tried appending the queries as below:</p>

<p><code>host=""abc*"" sourcetype=""xyz"" Request=""some.jsp""
| stats count as ""TotalCount"" by Request
| append
[search host=""abc*"" sourcetype=""xyz"" Request=""some.jsp""
| where TimeTaken &lt; 6000
| stats count as ""ReqLT6Sec"" by Request]</code></p>

<p>This would work for simple request as above like single jsp, but if I am using wild card for Request and data is huge, count of ""ReqLT6Sec"" is not matching with result obtained by running individual query runs. Any help is appreciated to get this in simpler way.</p>

<p>Thanks.</p>",49235805.0,1,0,,2018-3-12 11:25:31,,2018-3-12 12:55:17,,,,,5707066.0,,1,1,splunk,5773,16
7,258463,49236713,Splunk Regex field extraction,"<p>I want to extract a certain part of a string, for instance:</p>

<p>Input</p>

<pre><code>\\host1\teams\team1\bla\bla\bla
\\host1\teams\team2\bla\bla
\\host1\teams\team3\bla
\\host1\teams\team4
</code></pre>

<p>Output</p>

<pre><code>team1
team2
team3
team4
</code></pre>

<p>I have a Regular Expression that works on Regex101.com, however, when I take it to Splunk, it returns this error <code>Regex: missing terminating ] for character class</code> and I can't .</p>

<p>The Regex is:</p>

<pre><code>(^\\\\host1\\teams\\)(?P&lt;Team&gt;[^\\]+)
</code></pre>

<p>Example data:</p>

<p><code>""\\host1\teams\team1"",""abc123"",""def678""</code></p>

<p><strong>Update</strong>:</p>

<p><a href=""https://regex101.com/r/c9Lqh6/1"" rel=""nofollow noreferrer"">regex101 example</a></p>",49238285.0,1,0,,2018-3-12 13:42:00,,2018-3-12 14:57:33,,,,,9418600.0,,1,0,regex|pcre|splunk,844,11
8,258464,49251824,syslog NG not starting up when specifying an ip address but works as a catch all and write to file setup,"<p>I am trying to setup a syslog NG server where i could collect all the logs. now ive managed to create the settings where the server will collect all the logs from all the servers and write it to a single file. but i was wondering if its possible to create a separate log file for each ip address. my config file is as below and every time i mention network it fails to start. can you please let me know where im going wrong?</p>

<pre><code>log { source(s_src); filter(f_console); destination(d_console_all);
                                    destination(d_xconsole); };
log { source(s_src); filter(f_crit); destination(d_console); };
log {
  source(s_src);
  };
destination Windest {
  file(""/var/log/test"");
  };
source forwarder {
  network( ip(192.168.1.140));
  };
destination forwarderonedest {
  file(""/var/log/forwarder1"");
  };
log {
  source(forwarder);
  destination(forwarderonedest);
  };

the 
</code></pre>

<p>error i get when i try to restart is
 /etc/init.d/syslog-ng restart
[....] Restarting syslog-ng (via systemctl): syslog-ng.serviceJob for syslog-ng.service failed because the control process exited with error code. See ""systemctl status syslog-ng.service"" and ""journalctl -xe"" for details.
 failed!</p>

<p>what works for me is </p>

<p>};</p>

<pre><code>destination Windest {
  file(""/var/log/test"");
  };
source forwarder {
  tcp();
  udp();
  };
destination forwarderonedest {
  file(""/var/log/forwarder1"");
  };
log {
  source(forwarder);
  destination(forwarderonedest);
  };
</code></pre>

<p>and it works. but all the logs from all the machines get written on to a single file.</p>",,1,0,,2018-3-13 08:52:07,,2018-3-19 15:16:57,,,,,7193162.0,,1,-1,syslog|splunk|rsyslog|syslog-ng,661,11
9,258465,49257971,"How can I find the string ""Moved to Quarantine"" via Splunk using Regex?","<p>How can I find the string &quot;Moved to Quarantine&quot; via Splunk using Regex? (If anything, what would the Regex line be?)</p>
<p>Script would be essentially  &quot;anything Moved to Quarantine anything&quot;</p>
<p>This is the line I'm working with;</p>
<blockquote>
<p>Message=Computer-0104 [Tuesday, March 13, 2018 3:31:39 AM (GMT-06:00)]
(Virus Scan SP1): Result: Moved to Quarantine:
not-a-virus:HEUR:AdWare.Script.Generic</p>
</blockquote>
<p>What i'm working on;
<code>.+?(?=Result:\sMoved to Quarantine:)|:\tMoved\sto\sQuarantine:\s</code></p>
<p><a href=""https://i.stack.imgur.com/LCrEl.png"" rel=""nofollow noreferrer"">Example Pic</a></p>",,1,1,,2018-3-13 13:52:12,,2018-3-13 15:22:48,2020-6-20 09:12:55,,-1.0,,9485809.0,,1,0,regex|splunk,68,7
10,258466,49274539,"Splunk Mint: Failed to archive dSYMs for ""MyApp"" to ""/tmp/splunk-mint-dsyms""","<p>How to fix this error?</p>

<p>Splunk Mint: Archiving ""MyApp"" to ""/tmp/splunk-mint-dsyms/MyApp.zip""
  adding: MyApp</p>

<p>zip error: Interrupted (aborting)
Splunk Mint: Failed to archive dSYMs for ""MyApp"" to ""/tmp/splunk-mint-dsyms""
Command /bin/sh failed with exit code 252</p>

<p>Second error is </p>

<p>Splunk Mint: Archiving ""MyApp"" to ""/tmp/splunk-mint-dsyms/MyApp.zip""
  adding: MyApp (deflated 68%)
Splunk Mint: ERROR ""400"" while uploading ""/tmp/splunk-mint-dsyms/MyApp.zip""</p>",49274597.0,2,7,,2018-3-14 09:54:14,2.0,2019-11-8 00:02:46,,,,,4288553.0,,1,1,ios|objective-c|iphone|crashlytics|splunk,398,11
11,258467,49279349,How to find users who had only error for certain event?,"<p>We have log entries in format like this:
LogLevel=info  username=some1 eventID=update</p>

<p>So in case of error the LogLevel will be LogLevel=error 
LogLevel can also be debug</p>

<p>What I need to do is to find all users that had eventID=update but always had LogLevel=error and present them in table format 
Username | count</p>

<p>If I simply search </p>

<pre><code>eventID=update LogLevel=error | stats count by username
</code></pre>

<p>then I will get all matching entries for users with LogLevel=error but some of these users probably also had entries with LogLevel=info (or debug)
How do I find users that had only LogLevel=error with this eventID in a given time period?</p>",,1,0,,2018-3-14 13:44:38,,2018-3-14 15:43:30,,,,,2232662.0,,1,0,splunk,59,7
12,258468,49288425,Regex for positive lookahead on multiple comma seperated values,"<p>I want to come up with a regex to extract values from DML query statements like insert, update etc for particular columns only.</p>

<p>Sample DML -</p>

<pre><code>insert into abc_emp.employee (emp_name, emp_id, emp_dept)
values ('Scott Tiger', 2246, 'Accounts')

update abc_dept.department set dept_name = 'Sales', dept_head = 'Scott 
Tiger' where dept_id = 10

update abc_cust.dbo.customer set cust_name = 'Adam Jackson'
where cust_id = 1100
</code></pre>

<p>In above queries, columns of interest in my use-case are 
emp_name, emp_id from employee table,
dept_head from department table and cust_name from customer table</p>

<p>The regex I have come up with is-</p>

<pre><code>(?:insert into|update)(?:\s\w+[\.]((dbo\.)?)(employee|department|customer))
(?:\s([\(]|(set\s)))(?=.*?(emp_name|emp_id|dept_head|cust_name))
</code></pre>

<p>This gives the below result -
<a href=""https://regex101.com/r/qV5PJF/3"" rel=""nofollow noreferrer"">given regex</a>
<a href=""https://i.stack.imgur.com/yGsAH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yGsAH.png"" alt=""result""></a></p>

<p>While still incomplete, I was expecting this regex to have 4 matches instead of only 3. It is not matching <strong>emp_id</strong>. I suspect this is due to the | operator in the 6th capturing group -</p>

<pre><code>(emp_name|emp_id|dept_head|cust_name)
</code></pre>

<p>which is returning on the first occurrence of a match for any query. I want to have as many matches(1 or more) as possible from the capturing group not just one.
<strong>Is there any way this can be achieved?</strong></p>

<p>PS: My final requirement is to extract the values being put into these columns and do a substitution on them for masking the data. This regex would be part of a Splunk sed script where the substitution would happen.</p>

<p>Regex flavor - I think Splunk uses PCRE so Perl.</p>

<p>PPS: I'm quite new to regex.</p>

<p>Edit 1: Decided to take update and insert queries separately as suggested by @CAustin</p>

<p>for update, i have only reached till this point
<a href=""https://i.stack.imgur.com/gdILc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gdILc.png"" alt=""enter image description here""></a></p>

<p><a href=""https://regex101.com/r/dt9XTK/41"" rel=""nofollow noreferrer"">this is the regex</a></p>

<p>(?:update\s\w+?.(employee|department|customer)\s+?((set\s+?)))(?=emp_name)|(?=emp_id)|(?=dept_head)|(?=cust_name)|(?=cust_phone)</p>",,0,3,,2018-3-14 22:10:28,,2018-3-17 12:45:19,2018-3-17 12:45:19,,1135198.0,,1135198.0,,1,0,regex|regex-lookarounds|splunk,252,9
13,258469,49309925,Splunk query substract time,"<p>I have a Splunk search query like this:</p>

<pre><code>...earlier query.....
| eval sTime=mvindex(sTime,1), eTime=mvindex(eTime,1), 
TotalTime = strptime(sTime, ""%Y-%m-%dT%H:%M:%S%z"") - strptime(eTime, ""%Y-%m-%dT%H:%M:%S%z"") 
| table sTime eTime TotalTime
</code></pre>

<p>I have start and ebdtime like this in my log:</p>

<p>sTime:2018-03-14T19:18:10.851</p>

<p>eTime:2018-03-14T19:19:20.667</p>

<p>I am getting Totaltime is empty i.e. no values. What is the wrong in this query?
Was following Splunk forum here: <a href=""https://answers.splunk.com/answers/6005/subtract-two-timestamps-in-one-event.html"" rel=""nofollow noreferrer"">Link</a>.</p>",49310732.0,1,0,,2018-3-15 21:59:01,,2018-3-15 23:12:18,,,,,706295.0,,1,0,splunk|splunk-query,148,9
14,258470,49313548,"how can I find all dashboards in splunk, with usage information?","<p>I need to locate data that has become stale in our Splunk instance - so that I can remove it</p>

<p>I need a way to find all the dashboards, and sort them by usage.  From the audit logs I've been able to find all the actively used logs, but as my goal is to remove data, I most need the dashboards not in use</p>

<p>any ideas?</p>",,1,0,,2018-3-16 05:05:44,,2018-3-16 13:55:30,,,,,112321.0,,1,0,splunk|splunk-query,2523,14
15,258471,49323871,Openshift 3.4 Aggregated logging straight to splunk without Elastic Search,"<p>We are running Openshift Container Platform 3.4.1.  It is using the RedHat provided EFK aggregrated logging solution to log to Elastic Search.  We've also enabled to secure forwarder to Splunk.  Splunk is our strategic logging solution so we only really want the logs to go there.</p>

<p>We also experience situations whereby ES has an issue, and the logs no longer get forwarded to Splunk.</p>

<p>I'd consider just building a standalone Fluentd solution (that sends to Splunk), but our users are now familiar with the way that aggregated logging enriches the data, and I don't think a standalone FluentD container would do that.</p>

<p>Does anyone know if it is possible to modify the aggregated logging so that it doesn't go to ES at all, and only uses the secure forwarder to send the data to Splunk?  Or, drop the aggregated logging, and use a standard FluentD installation whilst maintaining the enrichment that aggregated logging provides?</p>

<p>BTW - it's only my second week with Openshift, so apologies if this is a vague or poor question :)
Thanks
M</p>",,0,3,,2018-3-16 15:04:24,,2018-3-16 15:04:24,,,,,345927.0,,1,0,openshift|splunk|openshift-enterprise|efk,365,10
16,258472,49361947,Splunk conditional count,"<p>I have some CSV data about files imported in to Splunk. The data looks like this:</p>

<pre><code>""\\domain\path\to\file\"",""&lt;filename&gt;"",""&lt;fsize&gt;"",""&lt;ext&gt;"",""&lt;Last_access&gt;"",""&lt;last_write&gt;"",""&lt;creation_time&gt;"",""&lt;attributes&gt;"",""&lt;owner&gt;""
</code></pre>

<p>I have converted all the date strings to epoch using:</p>

<pre><code>| eval epoch_LastAccessTime=strptime(LastAccessTime, ""%d/%m/%Y %H:%M:%S"")
...
...
</code></pre>

<p>I want to get:</p>

<ul>
<li>A percentage of files last accessed between 6 months and 3 years ago</li>
<li>A percentage of files last accessed 3 years or more ago.</li>
</ul>

<p>This is the search query that I have tried before getting stuck:</p>

<pre><code>index=""&lt;my_index&gt;"" sourcetype=""&lt;my_sourcetype&gt;"" 
| rex field=DirectoryName ""\\\domain\.org\\\teams\\\(?&lt;Team&gt;[^\\\]*)"" 
offset_field=_extracted_fields_bounds  
| eval epoch_LastAccessTime=strptime(LastAccessTime, ""%d/%m/%Y 
%H:%M:%S"") 
| eval _time=epoch_LastAccessTime
| timechart span=6mon count
</code></pre>

<p>I've tried using commands along the lines of:</p>

<pre><code>| where epoch_LastAccessTime&gt;=three_year_ago_from_now AND 
epoch_LastAccessTime&lt;=six_months_ago_from_now
</code></pre>

<p>However, this excludes everything else (3y+)</p>

<p>I want the result to look something like:</p>

<pre><code>TimeRange  Perc
6m-3y      60%
3y+        40%
</code></pre>",49368493.0,1,0,,2018-3-19 11:31:40,,2018-3-19 17:00:30,,,,,9418600.0,,1,1,splunk|splunk-query,2808,13
17,258473,49362270,SplunkMint uploading dSym,"<p>I am receiving this error every time I try to run the following script (SplunkMint script to upload the dsym file automatically in order to symbolicate):</p>

<pre><code>SCRIPT=`/usr/bin/find ""${SRCROOT}"" -name splunkmint_postbuild_dsym_upload_script.sh | head -n 1`

/bin/bash ""${SCRIPT}"" ""API_KEY"" ""API_TOKEN""
</code></pre>

<p>I put my API_KEY and API_TOKEN but they are hidden for security reasons.</p>

<pre><code>Splunk Mint: Archiving ""appName"" to ""/tmp/splunk-mint-dsyms/appName.zip""
  adding: appName (deflated 72%)
Splunk Mint: ERROR ""400"" while uploading ""/tmp/splunk-mint-dsyms/appName.zip""
Command /bin/sh failed with exit code 252
</code></pre>

<p>I don't know what can cause this issue, I just used other previous versions that I didn't had the issue and now I do.</p>

<p>I am using <strong>Swift 4</strong> and <strong>Xcode 9.2</strong>,
Thanks.</p>",,0,7,,2018-3-19 11:49:54,1.0,2018-3-19 11:49:54,,,,,5288983.0,,1,2,xcode|splunk,217,9
18,258474,49402269,Splunk Query: Find the transactions that are taking more than 5 seconds,"<p>Below is my search properties, Want add dashboard to show the transaction(transID, activity) that are having elapsedTime more than 5000</p>

<p><strong>2018-03-21 04:08:13.159-05:00  INFO - [http-/0.0.0.0:8080-4] |eventStartTime=2018-03-21 04:08:13.092 CDT|STATUS=SUCCESS|activity=POST->/xyz.com|eventElapsedTime=66|status=SUCCESS|transID=30d2e919-bd4d-4bad-a3c8-52cc69d10b43|eventEndTime=2018-03-21 04:08:13.159 CDT</strong> </p>",,1,2,,2018-3-21 09:17:18,,2021-2-23 16:13:57,2021-2-23 16:13:57,,4418.0,,6105986.0,,1,0,splunk|splunk-dashboard,206,10
19,258475,49424315,How can I use regex to replace a character in part of a string,"<p>I have the following string:</p>

<pre><code>  This is part 1: and this is part 2
</code></pre>

<p>The string starts with 2 spaces, has an part before the separator "":"" and a part after the separator.</p>

<p>I want to replace every space before the separator, except the spaces at the beginning of the line, with an underscore and leave the spaces after the separator.</p>

<p>The end result should look like this:</p>

<pre><code>  This_is_part_1: and this is part 2
</code></pre>

<p>The part before the separator is of variable length and has a variable number of spaces.</p>

<p>This regex will be used in Splunk which uses a sed based replacement mechanism.</p>

<p>Is what I want at all possible and if so, how?</p>",,1,1,,2018-3-22 08:58:14,,2018-3-23 08:00:46,,,,,2688929.0,,1,0,regex|sed|replace|splunk,339,11
20,258476,49441826,Splunk: find what a user is searching for?,"<p>I am trying to write a Splunk SPL query that will show me the most popular search terms that a user is looking for in one of my web apps. I have the logs already in Splunk but I am having a hard time extract the search parameter from the event. The event shows the full SQL select statement that looks like the query below:</p>

<pre><code>select result from table where search_term = 'searched for this text'
</code></pre>

<p>How can I have this:</p>

<pre><code>index=my_app search_term | top result
</code></pre>

<p>How do I actually capture the search term?</p>

<p>Thank you</p>",,1,0,,2018-3-23 03:15:53,,2018-3-23 12:05:21,,,,,1467996.0,,1,2,splunk|splunk-query,93,9
21,258477,49518554,Splunk rest curl query fails to execute on the successive attempts with the session key authorization,"<p>First attempt creates the splunk SID, but fails on the successive attempts to create search id.
Same issue occurs while polling/consuming the search after SID creation on its successive attempts. Can anybody point out helping the missing part?</p>

<p><strong>Curl query used :</strong> </p>

<p><strong><em>curl -H 'Authorization:Splunk sessionkeyincluded' <a href=""https://hostname:60659/services/search/jobs"" rel=""nofollow noreferrer"">https://hostname:60659/services/search/jobs</a> -d search=""search sourcetype=\""xx:yy\""  earliest=\""03/19/2018:07:00:00\"" latest=\""03/19/2018:07:15:00\"" | stats count""</em></strong></p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;response&gt;
  &lt;sid&gt;1522168074.7125_E472E7F2-9BEF-4117-A49A-8281B171EF1E&lt;/sid&gt;
&lt;/response&gt;
</code></pre>

<p><strong>successive attempts fails with authentication failure mentioning  ""call not properly authenticated""</strong></p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;response&gt;
  &lt;messages&gt;
    &lt;msg type=""WARN""&gt;call not properly authenticated&lt;/msg&gt;
  &lt;/messages&gt;
&lt;/response&gt;
</code></pre>",,0,0,,2018-3-27 16:59:48,,2018-3-27 16:59:48,,,,,1609653.0,,1,3,curl|splunk|splunk-query|splunk-sdk,519,10
22,258478,49526070,rsyslog.conf file stopped receiving logs,"<p>I am working on a school project to get logs from a printer sent to splunk. This is a project done by a few batches of students. I initially used the original copy of the rsyslog.conf file done by the previous batch student but was unable to retrieve data in Splunk. <em>I had already set the printer to send logs to the IP address and port of the linux server I am using. Configuration in Splunk has also been made to listen to port 2048.</em> I was told by my supervisor that the copy done by the previous student should work.</p>

<p>Original Copy:</p>

<pre><code># rsyslog configuration file

# For more information see /usr/share/doc/rsyslog-*/rsyslog_conf.html
# If you experience problems, see 
http://www.rsyslog.com/doc/troubleshoot.html

#### MODULES ####

# The imjournal module bellow is now used as a message source instead of 
imuxsock.
$ModLoad imuxsock # provides support for local system logging (e.g. via 
logger command)
$ModLoad imjournal # provides access to the systemd journal
#$ModLoad imklog # reads kernel messages (the same are read from journald)
#$ModLoad immark  # provides --MARK-- message capability

# Provides UDP syslog reception
$ModLoad imudp
$UDPServerRun 2048

# Provides TCP syslog reception
#$ModLoad imtcp
#$InputTCPServerRun 514

$template RemoteLogs,""/var/log/syslog/%HOSTNAME%/%FROMHOST-IP%.log""
*.* ?RemoteLogs

#### GLOBAL DIRECTIVES ####

# Where to place auxiliary files
$WorkDirectory /var/lib/rsyslog

# Use default timestamp format
$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat

# File syncing capability is disabled by default. This feature is usually 
not required,
# not useful and an extreme performance hit
#$ActionFileEnableSync on

# Include all config files in /etc/rsyslog.d/
$IncludeConfig /etc/rsyslog.d/*.conf

# Turn off message reception via local log socket;
# local messages are retrieved through imjournal now.
$OmitLocalLogging on

# File to store the position in the journal
$IMJournalStateFile imjournal.state


#### RULES ####

# Log all kernel messages to the console.
# Logging much else clutters up the screen.
#kern.*                                                 /dev/console

# Log anything (except mail) of level info or higher.
# Don't log private authentication messages!
*.info;mail.none;authpriv.none;cron.none                /var/log/messages

# The authpriv file has restricted access.
authpriv.*                                              /var/log/secure

# Log all the mail messages in one place.
mail.*                                                  -/var/log/maillog


# Log cron stuff
cron.*                                                  /var/log/cron

# Everybody gets emergency messages
*.emerg                                                 :omusrmsg:*

# Save news errors of level crit and higher in a special file.
uucp,news.crit                                          /var/log/spooler

# Save boot messages also to boot.log
local7.*                                                /var/log/boot.log


# ### begin forwarding rule ###
# The statement between the begin ... end define a SINGLE forwarding
# rule. They belong together, do NOT split them. If you create multiple
# forwarding rules, duplicate the whole block!
# Remote Logging (we use TCP for reliable delivery)
#
# An on-disk queue is created for this action. If the remote host is
# down, messages are spooled to disk and sent when it is up again.
#$ActionQueueFileName fwdRule1 # unique name prefix for spool files
#$ActionQueueMaxDiskSpace 1g   # 1gb space limit (use as much as possible)
#$ActionQueueSaveOnShutdown on # save messages to disk on shutdown
#$ActionQueueType LinkedList   # run asynchronously
#$ActionResumeRetryCount -1    # infinite retries if host is down
# remote host is: name/ip:port, e.g. 192.168.0.1:514, port optional
#*.* @@remote-host:514
# ### end of the forwarding rule ###
</code></pre>

<p>As I was unable to get the printer to send logs to Splunk with the above configuration, I went to research online and made a few modifications which I managed to get the printer to send logs to Splunk.</p>

<p>Modified Copy:</p>

<pre><code># rsyslog configuration file

# For more information see /usr/share/doc/rsyslog-*/rsyslog_conf.html
# If you experience problems, see                 
http://www.rsyslog.com/doc/troubleshoot.html

#### MODULES ####

# The imjournal module bellow is now used as a message source instead of     
imuxsock.
$ModLoad imuxsock # provides support for local system logging (e.g. via 
logger command)
$ModLoad imjournal # provides access to the systemd journal
#$ModLoad imklog # reads kernel messages (the same are read from journald)
#$ModLoad immark  # provides --MARK-- message capability

# Provides UDP syslog reception
$ModLoad imudp
$UDPServerRun 2048

*$InputUDPServer BindRuleset remote
$UDPServerRun 2048*

# Provides TCP syslog reception
#$ModLoad imtcp
#$InputTCPServerRun 514

$template RemoteLogs,""/var/log/syslog/%HOSTNAME%/%FROMHOST-IP%.log""
*.* ?RemoteLogs


#### GLOBAL DIRECTIVES ####

# Where to place auxiliary files
$WorkDirectory /var/lib/rsyslog

# Use default timestamp format
$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat

# File syncing capability is disabled by default. This feature is usually     
not required,
# not useful and an extreme performance hit
#$ActionFileEnableSync on

# Include all config files in /etc/rsyslog.d/
$IncludeConfig /etc/rsyslog.d/*.conf

# Turn off message reception via local log socket;
# local messages are retrieved through imjournal now.
$OmitLocalLogging on

# File to store the position in the journal
$IMJournalStateFile imjournal.state

**.* action(type=""omfwd"" target=""172.**.***.156"" port=""2048"" protocol=""udp""
action.resumeRetryCount=""100""
queue.type=""linkedList"" queue.size=""10000"")*


#### RULES ####

# Log all kernel messages to the console.
# Logging much else clutters up the screen.
#kern.*                                                 /dev/console

# Log anything (except mail) of level info or higher.
# Don't log private authentication messages!
*.info;mail.none;authpriv.none;cron.none                /var/log/messages

# The authpriv file has restricted access.
authpriv.*                                              /var/log/secure

# Log all the mail messages in one place.
mail.*                                                  -/var/log/maillog


# Log cron stuff
cron.*                                                  /var/log/cron

# Everybody gets emergency messages
*.emerg                                                 :omusrmsg:*

# Save news errors of level crit and higher in a special file.
uucp,news.crit                                          /var/log/spooler

# Save boot messages also to boot.log
local7.*                                                /var/log/boot.log


# ### begin forwarding rule ###
# The statement between the begin ... end define a SINGLE forwarding
# rule. They belong together, do NOT split them. If you create multiple
# forwarding rules, duplicate the whole block!
# Remote Logging (we use TCP for reliable delivery)
#
# An on-disk queue is created for this action. If the remote host is
# down, messages are spooled to disk and sent when it is up again.
#$ActionQueueFileName fwdRule1 # unique name prefix for spool files
#$ActionQueueMaxDiskSpace 1g   # 1gb space limit (use as much as possible)
#$ActionQueueSaveOnShutdown on # save messages to disk on shutdown
#$ActionQueueType LinkedList   # run asynchronously
#$ActionResumeRetryCount -1    # infinite retries if host is down
*remote host is: 172.**.***.43:2048
*.* @@remote-host:514*
# ### end of the forwarding rule ###
</code></pre>

<p>I was <em>able to receive logs from the printer to Splunk</em> after I had made the above modifications. However, after a few days when I tried to generate logs from the printer I <em>no longer receive new logs in Splunk</em>. I <em>did not make any changes</em> to the modified copy of the rsyslog.conf file that I have been using. Hence I don't get why Splunk no longer receives logs from the printer.</p>

<p>I changed the configuration to another port number but still did not receive new logs from the printer, so I changed back to port 2048. I also double checked that I had made the corresponding changes in the printer's EWS and Splunk correctly. I have been researching on this and double checking for the whole day but to no avail.</p>

<p>Note:</p>

<p>-2048 is the port used to retrieve logs from the printer</p>

<p>-172.** . ***.43 is the IP address of the printer (it represents the actual IP address, was told by another user to censor it, i did not put astrids as part of my IP address in the real config file)</p>

<p>-172.** . ***.156 is the IP address of the Linux server I am using (it represents the actual IP address, was told by another user to censor it, i did not put astrids as part of my IP address in the real config file)</p>

<p>-I am using the Linux server as my main workpoint, in the modified copy I added a paragraph with the destination stating my current IP address (linux server), is it the cause why the printer stopped sending logs to my server since I have been directing the logs to ""myself""-config done using the same Linux server as destination host-also Linux server)</p>

<p>-The text with astrids are the ones I added to the original copy</p>

<p>-EWS stands for embedded web service, basically a web portal of the printer.</p>

<p>-How I trigger logs from printer: logging in to the EWS/making failed logins. Details of logins will be sent to Splunk </p>",,1,6,,2018-3-28 04:22:50,,2018-3-30 18:01:51,2018-3-28 05:09:25,,9117889.0,,9117889.0,,1,0,linux|splunk|rsyslog,1355,12
23,258479,49526084,Not able to run localhost:8000 for splunk in a docker container,"<p>I used the below command to start the splunk server using Docker.</p>

<pre><code>docker run -d -e ""SPLUNK_START_ARGS=--accept-license"" -e ""SPLUNK_USER=root"" -p ""8000:8000"" splunk/splunk
</code></pre>

<p>But when I opened the URL localhost:8000, I am getting <strong>Server can't be reached</strong> message</p>

<p>What am I missing here? 
<br>I followed a tutorial from the source :- <br><a href=""https://medium.com/@caysever/docker-splunk-logging-driver-c70dd78ad56a"" rel=""nofollow noreferrer"">https://medium.com/@caysever/docker-splunk-logging-driver-c70dd78ad56a</a></p>",,2,0,,2018-3-28 04:25:41,,2019-12-12 16:10:06,,,,,8905521.0,,1,0,docker|splunk,613,11
24,258480,49560761,Finding transactions where a specific field has changed,"<p>I'm trying to find out-of-order events in Splunk.  This example is analogous to what happens in our system:</p>

<pre><code>...
Time=15:40.09 Id=11 ScenarioId=7 Event=BlockChange Block=A-A
Time=15:40.11 Id=12 ScenarioId=7 Event=BlockChangeConfirmed Block=1-7
Time=15:41.00 Id=13 ScenarioId=2 Event=BlockChange Block=E-6
Time=15:41.01 Id=14 ScenarioId=7 Event=Restart
Time=15:41.05 Id=15 ScenarioId=2 Event=BlockChangeConfirmed Block=B-2
Time=15:41.14 Id=16 ScenarioId=3 Event=BlockChangeConfirmed Block=2-4
Time=15:41.15 Id=17 ScenarioId=2 Event=Restart
Time=15:41.16 Id=18 ScenarioId=3 Event=BlockChange Block=E-1
Time=15:43.24 Id=19 ScenarioId=8 Event=BlockChange Block=C-7
Time=15:43.27 Id=20 ScenarioId=8 Event=BlockChangeConfirmed Block=D-2
Time=15:43.35 Id=21 ScenarioId=8 Event=BlockChange Block=D-2
Time=15:43.40 Id=22 ScenarioId=8 Event=BlockChangeConfirmed Block=4-A
...
</code></pre>

<p>After a BlockChange event occurs for a particular Scenario, it needs to be confirmed before a new Block is assigned.  Occasionally though a BlockChangeConfirmed event will be received before the BlockChange event (see Ids 16 and 18).  I'm trying to write a query that will identify these out-of-order events.</p>

<p>My approach is to only consider Block* events , group them into transactions by ScenarioId starting with BlockChangeConfirmed and ending with BlockChange within a timespan of 10 seconds.  So from a logical perspective, it would transform the above data into this:</p>

<pre><code>ScenarioId=3: Time=15:41.14 Id=16 Event=BlockChangeConfirmed Block=2-4
              Time=15:41.16 Id=18 Event=BlockChange Block=E-1
ScenarioId=8: Time=15:43.27 Id=20 Event=BlockChangeConfirmed Block=D-2
              Time=15:43.35 Id=21 Event=BlockChange Block=D-2
</code></pre>

<p>This correctly identifies Ids 16 and 18, but also incorrectly identifies 20 and 21.  So I need to include the Block field since it always changes between corresponding BlockChange and BlockChangeConfirmed events.</p>

<p>I'm having trouble expressing this in Splunk though.  Depending on the query I use, it either returns <em>every</em> BlockChangeConfirmed event, or it doesn't identify when the Block field changes.</p>

<p>This is the base query that I'm using:</p>

<pre><code>index=""foo"" sourcetype=""bar"" Block AND (Event=BlockChange OR Event=BlockChangeConfirmed)
| streamstats latest(Block) AS last earliest(Block) AS first 
| transaction ScenarioId startswith=""(Event=BlockChangeConfirmed)"" endswith=""(Event=BlockChange)"" maxspan=10s
</code></pre>

<p>I've tried adding and changing the conditions to:</p>

<pre><code>| search first != last

| search eval(first != last)

| where first != last

endswith=eval(first != last)
</code></pre>

<p>without success.</p>

<p>How can I obtain the results I want?</p>",49633308.0,1,0,,2018-3-29 16:17:30,,2018-4-3 15:03:00,,,,,9825.0,,1,0,splunk|splunk-query,446,10
25,258481,49562075,fluentd splunk-http-eventcollector plugin Bad Request 400 when using format json,"<p>I am trying to push the logs to splunk from Kubernetes using fluentd as a daemonset.</p>

<p>When I set <code>format none</code> and push to splunk it works. But I want to add kubernetes_metadata so I updated to <code>format json</code> and added the <code>kubernetes_metadata</code> filter. </p>

<p>Then, I get a 400 Bad Request </p>

<pre><code>{""text"": ""Invalid data format"", ""code"": 6, ""invalid-event-number"": 0 }
</code></pre>

<p>Here is my fluent.conf file</p>

<pre><code># Ignore fluentd log messages
&lt;match fluent.**&gt;
  @type null
&lt;/match&gt;

&lt;source&gt;
  @type tail
  path /var/log/containers/*.log
  pos_file /fluentd/log/docker-containers.log.pos
  tag kubernetes.*
  format json
  read_from_head true
&lt;/source&gt;

&lt;filter kubernetes.**&gt;
  @type kubernetes_metadata
&lt;/filter&gt;

&lt;match kubernetes.**&gt;
  @type splunk-http-eventcollector
  server &lt;host&gt;:8088
  protocol https
  verify false
  token ***
  source fluentd-kube-containers
  sourcetype _json
  host ""#{ENV['HOSTNAME']}""
  buffer_chunk_limit 700k
  batch_size_limit 1000000
  buffer_type file
  buffer_path /fluentd/log/fluentd-buffer
  flush_interval 10s
&lt;/match&gt;
</code></pre>",,1,2,,2018-3-29 17:34:07,,2018-3-30 18:40:29,,,,,4581734.0,,1,0,plugins|kubernetes|splunk|fluentd,1018,12
26,258482,49587528,Splunk - How can I get accumulative vales for a day for a period of time?,"<p>One of the things I'm using Splunk to monitor is electricity usage, one of the fields indexed is the accumulative Kw value for the day, how can I get the last value for the day for a given timespan? So output the total Kw for each day for a month - I've tried using</p>

<pre><code>host=Electricity earliest=-4w@w1 latest=+w@w1 | timechart last(live_day_kw) as Kw
</code></pre>

<p>but for the data I have it seems to be adding each day together so its increasing day on day and not daily values, so for example day1 is 7kw and day2 is 14kw and day3 is 21kw - I'd expect it to be ~7kw a day. Also just checked and the <code>live_day_kw</code> value does reset to zero at midnight</p>",,2,0,,2018-3-31 12:05:19,,2018-4-1 18:54:48,,,,,4584287.0,,1,0,splunk|accumulate,659,11
27,258483,49598672,"msiexec throws ""already exist"" and exiting","<p>I am trying to install Splunk using msiexec. </p>

<p>Below is the command:</p>

<pre><code>msiexec /I splunk.msi /qb INSTALLDIR=C:\splunk
</code></pre>

<p>But it throws this message:</p>

<blockquote>
  <p>This version of Splunk Enterprise has already been install on this computer</p>
</blockquote>

<p>An option would be to use <code>/fa</code> argument which will reinstall the app replacing all the files.
<strong>BUT</strong> I <strong>don't</strong> want to reinstall the app.</p>

<p>I want to install this app without affecting the older one.</p>

<p><strong>Is there any way I can install Splunk even though it is already installed .?</strong></p>

<p>note: my purpose to install an additional Splunk is that I want to run a script on Splunk which should not affect the preinstalled Splunk. So, I will install one additional Splunk, run the script on it and uninstall it. simple !</p>",,1,0,,2018-4-1 13:42:24,,2018-4-2 12:06:37,,,,,9507493.0,,1,1,cmd|window|splunk|windows-installer,51,7
28,258484,49675149,Splunk MINT shell script Issue: Issue occurs when I attempt to archive an application in Xcode 9.3,"<p>I have integrated Splunk MINT SDK with IOS app. Here is the issue details:</p>

<p>I am able to run the app in Simulator, but not in Device. While archiving I got below error. </p>

<pre><code>Splunk Mint: Archiving ""Appointment-Plus"" to ""/tmp/splunk-mint-dsyms/Appointment-Plus.zip""

adding: Appointment-Plus (deflated 68%)

Splunk Mint: ERROR ""400"" while uploading ""/tmp/splunk-mint-dsyms/Appointment-Plus.zip""

Command /bin/sh failed with exit code 252
</code></pre>

<p>Any help?</p>",,1,1,,2018-4-5 14:42:01,,2018-4-9 15:58:32,2018-4-5 14:47:57,,1000551.0,,5371153.0,,1,-1,ios|xcode|splunk|xcode9.3,105,6
29,258485,49680944,Javascript table cell highlighting in 3rd party application,"<p>I'm using Splunk version 7.0.1 and I'm trying to highlight my table cell colors based off of two other fields. Splunk has a sample to do this in Javascript that uses hardcoded values, but I need the values based off of a different field. I have the script working, but it randomly highlights some cells in the wrong color. I can't seem to figure out why.  </p>

<p><strong>The ""Average Response Time"" should be:</strong> </p>

<ul>
<li>Red if the value is greater than or equal to the Threshold</li>
<li>Amber if the value is greater than or equal to the Objective but less than the Threshold</li>
<li>Green if the value is less than the Objective</li>
</ul>

<p>My Table:</p>

<pre><code>Transaction    Count   ""Average Response Time""    Objective   Threshold
A/P - Close Module - FM1    1   7.52    2.00    6.00  **&lt;-Colored Red and correctly**
...
A/P - Diagnosis- Run Search - FM1   2   4.01    100.00  100.00 **&lt;- Colored Amber incorrectly**
</code></pre>

<p>Here is the dashboard source of the table:</p>

<pre><code>&lt;table id=""response_time_highlight""&gt;
        &lt;title&gt;NOTE: An Objective and/or Threshold of 100 indicates that an Objective and/or Threshold have not been identified for this particular transaction type.&lt;/title&gt;
        &lt;search&gt;
          &lt;query&gt;index=arm sourcetype=""arm:transaction"" region=""$region$"" sitename=""$sitename$"" transaction_status IN ( $transaction_status$ ) username IN ( $username$ ) workstation_name IN ( $workstation_name$ ) transaction_name IN ( $transaction_name$ ) 
| stats count avg(response_time) AS response_time BY transaction_name objective threshold 
| eval threshold=round(threshold,2) 
| eval objective=round(objective,2) 
| eval response_time=round(response_time,2) 
| fields transaction_name count response_time objective threshold 
| rename response_time AS ""Average Response Time"" transaction_name AS ""Transaction Type"" objective as Objective threshold as Threshold count as ""Transaction Count""
| sort +transaction_name&lt;/query&gt;
          &lt;earliest&gt;$search_time.earliest$&lt;/earliest&gt;
          &lt;latest&gt;$search_time.latest$&lt;/latest&gt;
          &lt;sampleRatio&gt;1&lt;/sampleRatio&gt;
        &lt;/search&gt;
        &lt;option name=""count""&gt;20&lt;/option&gt;
        &lt;option name=""dataOverlayMode""&gt;none&lt;/option&gt;
        &lt;option name=""drilldown""&gt;none&lt;/option&gt;
        &lt;option name=""percentagesRow""&gt;false&lt;/option&gt;
        &lt;option name=""refresh.display""&gt;progressbar&lt;/option&gt;
        &lt;option name=""rowNumbers""&gt;true&lt;/option&gt;
        &lt;option name=""totalsRow""&gt;false&lt;/option&gt;
        &lt;option name=""wrap""&gt;true&lt;/option&gt;
      &lt;/table&gt;
</code></pre>

<p>Here is the Javascript code (modified version of the dashboard examples one):</p>

<pre><code>    require([
        'underscore',
        'jquery',
        'splunkjs/mvc',
        'splunkjs/mvc/tableview',
        'splunkjs/mvc/simplexml/ready!'
    ], function (_, $, mvc, TableView) {

    var objective_value;
    var threshold_value;

    // Row Coloring Example with custom, client-side range interpretation
    var CustomRangeRenderer = TableView.BaseCellRenderer.extend({
        canRender: function (cell) {
            // Enable this custom cell renderer for response_time field
            return _(['Average Response Time', 'Objective', 'Threshold']).contains(cell.field);
        },
        render: function ($td, cell) {
            // Add a class to the cell based on the returned value
            var value = parseFloat(cell.value);

            if (cell.field === 'Objective') {
                objective_value = value;
            }

            if (cell.field === 'Threshold') {
                threshold_value = value;
            }

            // Apply interpretation for number of historical searches
            if (cell.field === 'Average Response Time') {
                if (value &gt;= threshold_value) {
                    $td.addClass('range-cell').addClass('range-severe');
                }
                else if (value &gt;= objective_value) {
                    $td.addClass('range-cell').addClass('range-elevated');
                }
                else if (value &lt; objective_value) {
                    $td.addClass('range-cell').addClass('range-low');
                }
            }
            // Update the cell content
            $td.text(value.toFixed(2)).addClass('numeric');
        }
    });
    mvc.Components.get('response_time_highlight').getVisualization(function (tableView) {
        // Add custom cell renderer, the table will re-render automatically.
        tableView.addCellRenderer(new CustomRangeRenderer());
    });
});
</code></pre>

<p>EDIT: Added a console.log to the javascript showing the output of each field. Noticed that the first row showing as undefined, and pushing the correct numbers down to the next row. Here is the output:</p>

<pre><code>0 undefined undefined
8.64 100 100
7.52 2 6
0 2 6
7.52 2 6
1.11 10 25
2.28 2 6
2.92 2 6
</code></pre>",,0,0,,2018-4-5 20:25:04,,2019-8-23 04:58:42,2018-4-5 20:35:17,,4739397.0,,4739397.0,,1,2,javascript|highlighting|splunk,633,11
30,258486,49720991,Where to Place AWS ELB for Splunk deployment,"<p>I have questions around AWS ELB for splunk deployment , I am building an enterprise deployment for Splunk on AWS. My question is which type of ELB should I select and in which all subnets I should place an ELB?</p>",,1,4,,2018-4-8 18:00:03,1.0,2018-4-8 19:49:17,,,,,8267604.0,,1,0,amazon-web-services|amazon-elb|splunk,437,10
31,258487,49727498,Merging multiple Splunk results,"<p>I need to write a Splunk query to get the status when given pid, last status should be printed,wrote individual queries to fetch the status but dnt know how to merge the queries.referred few docs but couldn't find a way.</p>

<pre><code>""##payto""|rex field=msg ""personid :(?&lt;pid&gt;[^,]+)"" |rex field=msg "",(?&lt;status&gt;[^,\]]+) 
//if this status is SUCCESS then i need to check for status of next step else i need to print this status

 ""Event :start""|rex field=msg ""personid :(?&lt;pid&gt;[^,]+)""|rex field=msg "" Status :(?&lt;status&gt;[^,]+)""
//if response is 200 then need to go to next step else print this status
</code></pre>",,1,2,,2018-4-9 07:21:11,,2018-6-11 18:55:00,2018-6-11 18:55:00,,1908967.0,,5113758.0,,1,0,splunk|splunk-query,182,9
32,258488,49728111,Splunk: How to filter Source and Destination IP Addresses in firewall log,"<p>Let say I have the following:</p>

<p>Source IP Addresses, <code>10.1.1.1</code> &amp; <code>192.168.1.1</code></p>

<p>Destination IP Address <code>172.16.1.1</code></p>

<p>What is the right syntax to search for firewall log for this combination?</p>

<p>Is this the right syntax? I tried it but did not get the result. I've also tried different combination but didn't work too.</p>

<blockquote>
  <p>index=firewall src_ip=10.1.1.1 or src_ip=192.168.1.1 and
  dest_ip=172.16.1.1</p>
</blockquote>",49732965.0,1,2,,2018-4-9 08:01:03,,2018-4-9 12:26:42,2018-4-9 08:25:48,user9013730,,user9013730,,,1,-1,splunk,13991,16
33,258489,49734587,Pfsense no is populating data to Splunk,"<p>I have a problem, when I am sending the log from the pfsense to the splunk.</p>

<p>Te current configuration is:</p>

<p>Pfsense 192.168.1.128
Splunk 192.168.1.129</p>

<p>(Are 2 different VMs)</p>

<p>So the configuration of Pfsense is send remote loggin to the IP of the spunk in the port 514.</p>

<p>In Splunk I added the service UDP 514:</p>

<pre><code>Input Type
UDP Port
Port Number
514
Source name override
N/A
Restrict to Host
N/A
Source Type
syslog
App Context
search
Host
(IP address of the remote server)
Index
default
</code></pre>

<p>If I do a tcpdump in splunk with:</p>

<pre><code>tcpdump -anni ens33 host 192.168.1.128 and port 514
</code></pre>

<p>I can see the syslog from the pfsense</p>

<pre><code>14:54:30.489541 IP 192.168.1.128.514 &gt; 192.168.1.129.514: SYSLOG local5.info, length: 279
14:54:30.798702 IP 192.168.1.128.514 &gt; 192.168.1.129.514: SYSLOG local5.info, length: 461
14:54:31.578176 IP 192.168.1.128.514 &gt; 192.168.1.129.514: SYSLOG local5.info, length: 290
14:54:32.575437 IP 192.168.1.128.514 &gt; 192.168.1.129.514: SYSLOG local5.info, length: 290
14:54:33.801049 IP 192.168.1.128.514 &gt; 192.168.1.129.514: SYSLOG local5.info, length: 461
14:54:33.803128 IP 192.168.1.128.514 &gt; 192.168.1.129.514: SYSLOG local5.info, length: 290
14:54:34.579268 IP 192.168.1.128.514 &gt; 192.168.1.129.514: SYSLOG local5.info, length: 290
14:54:35.578575 IP 192.168.1.128.514 &gt; 192.168.1.129.514: SYSLOG local5.info, length: 290
14:54:36.465434 IP 192.168.1.128.514 &gt; 192.168.1.129.514: SYSLOG local5.info, length: 294
14:54:36.931271 IP 192.168.1.128.514 &gt; 192.168.1.129.514: SYSLOG ntp.info, length: 69
14:54:36.931530 IP 192.168.1.128.514 &gt; 192.168.1.129.514: SYSLOG ntp.info, length: 69
</code></pre>

<p>But If I check in Screach and reporting, I can see any information in data summary.</p>

<p>Could you give me a little of light in this</p>

<p>Thank you</p>",49831918.0,1,0,,2018-4-9 13:49:17,,2018-4-14 13:24:29,2018-4-9 13:55:43,,9619765.0,,9619765.0,,1,0,splunk|pfsense,291,9
34,258490,49778906,Set up Splunk alert based on average of a field,"<p>I am new to Splunk so pardon me if my question is too naive. I want to set up a Splunk alert if the average of a field is above a threshold. My search is as follows:</p>

<pre><code>sourcetype=""somesourcetype"" search phase | stats avg(f1) as Average 
</code></pre>

<p>If I use </p>

<pre><code>sourcetype=""somesourcetype"" search phase | timechart avg(f1) as Average span=1h
</code></pre>

<p>I can see the table listing the average of field f1. But with <code>stats avg(f1)</code> I do not get anything under statistics panel and I am not sure how to set up an alert if average of f1 is above 100ms. </p>",,1,0,,2018-4-11 15:22:41,,2018-4-12 00:37:39,,,,,3885794.0,,1,0,splunk|splunk-query,703,12
35,258491,49811095,"Splunk search a pattern for log like ""Response Elapsed: 00:00:00.0594215""","<p>There are some information like as following in my log:</p>
<blockquote>
<p>&quot;
.....Response Elapsed: 00:00:00.0194215....</p>
<p>.....Response Elapsed: 00:00:05.0174875....</p>
<p>.....Response Elapsed: 00:00:11.5434871....</p>
<p>.....Response Elapsed: 00:00:01.342283....
&quot;</p>
</blockquote>
<p>I want to search a result for  elapsed time &gt; 5 seconds, in the above information , the result should be like</p>
<blockquote>
<p>Response Elapsed: 00:00:11.5434871</p>
<p>Response Elapsed: 00:00:05.0174875</p>
</blockquote>
<p>I tried &quot;<strong>| Regex &quot;/^Response Elapsed: 00:00:00.0[5-9]/&quot;</strong>&quot;,but it doesn't work.</p>",,1,0,,2018-4-13 07:01:14,,2018-4-13 11:58:15,2020-6-20 09:12:55,,-1.0,,3107803.0,,1,0,splunk|splunk-query,85,7
36,258492,49890179,AWS ALB health check for application server,<p>What can be a simple ALB health check for 2 app servers behind an ALB. These are not web hence I am not installing http on these servers.</p>,,1,1,,2018-4-18 02:22:02,,2018-4-18 02:48:40,,,,,8267604.0,,1,0,amazon-web-services|splunk,512,10
37,258493,49990840,regex:how to specify and end of the capture,"<p>lets say I har raw data thats like this</p>

<pre><code>  ERROR -- : FluentLogger: Can't convert to msgpack: (blah: blah: balh:.... ):
</code></pre>

<p>I want to capture </p>

<pre><code>FluentLogger: Can't convert to msgpack:
</code></pre>

<p>currently I have this </p>

<pre><code>(?&gt;ERROR -- :)(?&lt;msg1&gt;.*):
</code></pre>

<p>but the problem with this is that it would capture </p>

<pre><code>FluentLogger: Can't convert to msgpack: (blah: blah: balh:.... ):
</code></pre>

<p>how would I write the regex for that?</p>",49990932.0,3,1,,2018-4-23 22:32:08,,2018-4-23 23:28:57,,,,,7120793.0,,1,0,regex|splunk,70,8
38,258494,50000199,How to highlight text as desired using regular expressions to be used in Splunk?,"<p>I have the following regex which displays the following output:
<a href=""https://i.stack.imgur.com/oRXxW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oRXxW.png"" alt=""enter image description here""></a>
Desired green text.But when added to Splunk, shows exact same output as previous regex. </p>

<p>What I want is to make it such that the exact highlighted green text gets highlighted in Splunk for field extraction.</p>

<p>Previous regex:
<a href=""https://i.stack.imgur.com/GI1tK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GI1tK.png"" alt=""enter image description here""></a>
The highlighted green text got misaligned. </p>

<p>What I suspect is that I have to make the '&lt; 37 > 1' highlighted in blue too, so that Splunk will extract the green text correctly. As this regex was done by another user when I asked in Splunk, the user did not add '&lt; 37 > 1' in his sample regex which affected the alignment when I added it to Splunk. I've tried different variations to highlight the '&lt; 37 > 1' but to no avail.</p>

<p>Some examples of my variations:</p>

<p>(?:[^\s][^\s][^\s]+\s+){2}(?P[^\s]+(?:\s\w+)?)\s\d+\s+&lt;  </p>

<p>(?:[^\s][^\s]+\s+){2}(?P[^\s]+(?:\s\w+\s)?)\s\d+\s+&lt;</p>

<p>(?:[^\s][^\s]+\s+){2}(?P[^\s]+(?:\s\w+\w)?)\s\d+\s+&lt;</p>

<p>(?:[^\s][^\s]+\s+){2}(?P[^\s]+(?:\s\w+\w\w)?)\s\d+\s+&lt;</p>

<p>\&lt;(?:[^\s][^\s]+\s+){2}(?P[^\s]+(?:\s\w+)?)\s\d+\s+&lt;</p>

<p>(?:[^\s][^\s]+\s+\s){2}(?P[^\s]+(?:\s\w+)?)\s\d+\s+&lt;</p>

<p>Link of the regex:
<a href=""https://regex101.com/r/biHi9a/5"" rel=""nofollow noreferrer"">https://regex101.com/r/biHi9a/5</a></p>",50002733.0,1,0,,2018-4-24 11:10:14,,2018-4-24 13:18:47,,,,,9117889.0,,1,0,splunk,304,9
39,258495,50007417,Splunk: Get a count of all occurrences of a string?,"<p>My log files log a bunch of messages in the same instance, so simply search for a message id followed by a count will not work (I will only count 1 per event when I want to count as many as 50 per event). I want to first narrow down my search to the events which show messages being sent (""enqueued""), and then count all instances of the string ""mid"".</p>

<p>Any ideas? I am very bad with splunk. How to I get all instances of ""mid"" to be a countable field?</p>

<pre><code>index=* service=myservice ""enqueued"" ""mid"" | stats count mid
</code></pre>",,1,0,,2018-4-24 17:12:54,,2021-11-10 17:35:55,,,,,2639322.0,,1,0,splunk|splunk-query,7120,19
40,258496,50057137,Compare two columns and return value from third,"<p>I have three columns. The first two are lists of ip's and the third is a plain number that is associated to the ip in the second column. </p>

<p>I would like to check the following. IF a value from column A appears in column B then return the value from column C. An example can be found in the attached sheet.</p>

<p>I tried using the index formula in excel, but i have a problem. An ip from column B can have multiple different associated values in column C.</p>

<p>The second problem i have is the huge number of rows. I estimate that i should be checking more than 1 million of rows. Using excel will take me a huge ammount of time. Does anyone have any other suggestions? I've heard of Splunk but have no idea on how to use it.</p>

<p>Thanks in advance!</p>",,0,3,,2018-4-27 07:07:54,,2018-4-27 07:07:54,,,,,2453208.0,,1,1,excel|multiple-columns|splunk|cross-correlation,663,11
41,258497,50067375,How to selectively forward the log files to specific indexes in Splunk?,"<p>Is it possible to selectively forward the log files to specific indexes in Splunk. </p>

<p>I want to forward a docker container running 3 services logs to Splunk indexer, the problem is that if I use Docker logging driver, all the data written to STDOUT goes to the same index and data segregation is not possible. Instead of that I've setup forwarder and able to send logs but all are going to the same index, I want to configure splunk forwarder to send specific logs to a specific index. </p>",,1,2,,2018-4-27 17:21:21,,2018-4-28 20:21:49,,,,,7189521.0,,1,0,docker|splunk,572,11
42,258498,50162265,Splunk - LDAP Integration - AuthenticationManagerLDAP Couldn't find matching groups and UserManagerPro - LDAP Login failed / none are mapped to Splunk,"<p>Splunk <strong>7.0.2</strong></p>

<p>Instead of creating Splunk internal users, I'm trying to integrate LDAP in Splunk but getting the following error message (<strong>tail -f splunkd.log</strong>)</p>

<pre><code>05-02-2018 23:17:08.235 +0000 ERROR AuthenticationManagerLDAP - Couldn't find matching groups for user=""1122345"". Search filter=""(cn=CN=Nooka\5C, Chuck,OU=Users,OU=Ov)"" strategy=""Centrify-Based-LDAP""
05-02-2018 23:17:08.235 +0000 ERROR UserManagerPro - LDAP Login failed, could not find a valid user=""1122345"" on any configured servers
05-02-2018 23:17:08.236 +0000 ERROR UiAuth - user=1122345 action=login status=failure reason=user-initiated useragent=""Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.ientip=137.11.121.234
</code></pre>

<p>I referred the documentation but it didn't help much.
<a href=""http://docs.splunk.com/Documentation/Splunk/7.0.3/Security/ConfigureLDAPwithSplunkWeb"" rel=""nofollow noreferrer"">http://docs.splunk.com/Documentation/Splunk/7.0.3/Security/ConfigureLDAPwithSplunkWeb</a> Tried some other posts here or online with various values for LDAP settings (fields etc) but still getting the error </p>

<p><strong>Snapshot of LDAP settings are:</strong>
<br/>
<a href=""https://i.stack.imgur.com/tQqFS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tQqFS.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/qz7y8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qz7y8.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/rvFBL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rvFBL.png"" alt=""enter image description here""></a></p>",50180143.0,1,1,,2018-5-3 19:06:52,,2018-7-25 19:38:57,2018-7-25 19:34:43,,1499296.0,,1499296.0,,1,1,authentication|ldap|monitoring|metrics|splunk,4257,15
43,258499,50163146,Log4j2 TCP-SSL Appender configuration in log4j2-config.xml,"<p>I am planning to configure a Log4j2 Socket Appender with a TCP-SSL Appender.</p>

<p>Here is the configuration I see in Log4j2 website.</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;Configuration status=""warn"" name=""MyApp"" packages=""""&gt;
  &lt;Appenders&gt;
    &lt;Socket name=""socket"" host=""localhost"" port=""9500""&gt;
      &lt;JsonLayout properties=""true""/&gt;
      &lt;SSL&gt;
        &lt;KeyStore location=""log4j2-keystore.jks"" password=""guessme!""/&gt;
        &lt;TrustStore location=""truststore.jks"" password=""guessme!""/&gt;
      &lt;/SSL&gt;
    &lt;/Socket&gt;
  &lt;/Appenders&gt;
  &lt;Loggers&gt;
    &lt;Root level=""error""&gt;
      &lt;AppenderRef ref=""socket""/&gt;
    &lt;/Root&gt;
  &lt;/Loggers&gt;
&lt;/Configuration&gt;
</code></pre>

<p>In this config, what do the <code>Keystore</code> and <code>TrustStore</code> files contain? I don't have these files. </p>

<p>I want to send my logs to Splunk TCP port. </p>

<p>Do I need to create <code>truststore.jks</code> with SSL certs from my Splunk server so that my server trusts Splunk?</p>

<p>What is <code>log4j2-keystore.jks</code>, and where can I download it? Do I need a <code>KeyStore</code> file? What should go in it? Does Splunk need a corresponding public key or trusted certs?</p>",,1,0,,2018-5-3 20:11:39,1.0,2018-10-9 23:06:05,2018-5-3 23:57:11,,65863.0,,9621585.0,,1,1,ssl|log4j2|splunk,885,11
44,258500,50179871,Fluentd sending to Splunk HEC: Want to set sourcetype to the namespace,"<p>Is it possible to programatically set the sourcetype to be the namespace from where the logs were generated?  I am using the fluentd plugin to send data to the Splunk http event collector.  Elsewhere, it was recommended to use ${record['kubernetes']['namespace_name'] to set the index name to be the namespace name.  When I do this for sourcetype, that actual text just shows up in Splunk rather than translating to the specific namespace names.</p>

<pre><code>@include systemd.conf
@include kubernetes.conf

&lt;match kubernetes.var.log.containers.fluentd**&gt;
  type null
&lt;/match&gt;

&lt;match **&gt;
  type splunk-http-eventcollector
  all_items true
  server host:port
  token ****
  index kubernetes
  protocol https
  verify false
  sourcetype ${record['kubernetes']['namespace_name']
  source kubernetes
  buffer_type memory
  buffer_queue_limit 16
  chunk_limit_size 8m
  buffer_chunk_limit 150k
  flush_interval 5s
&lt;/match&gt;
</code></pre>",,0,0,,2018-5-4 16:59:24,,2018-5-4 16:59:24,,,,,9587839.0,,1,1,kubernetes|splunk|fluentd,368,10
45,258501,50316278,"Oracle database via Splunk DB Connect in SQL Explorer - ""Catalog"" is greyed out","<p>This is the first time I'm using Splunk DB Connect for an Oracle database.</p>

<p>I've configured my database successfully and deployed the updated DB Connect app.</p>

<p>But when I try to access my connection in SQL Explorer I can't choose ""Catalog"" and it's greyed out. I can't get any info from the Oracle connection.</p>",,1,0,,2018-5-13 12:47:57,,2018-5-13 20:20:06,2018-5-13 20:20:06,,5212827.0,,9783946.0,,1,0,database|oracle|connect|splunk,116,8
46,258502,50329199,Fluentd events to Splunk,"<p>I need send some events from fluentd to splunk.</p>

<p>I read this: <a href=""https://docs.fluentd.org/v0.12/articles/out_splunk"" rel=""nofollow noreferrer"">https://docs.fluentd.org/v0.12/articles/out_splunk</a>    ,  <a href=""https://www.fluentd.org/dataoutputs"" rel=""nofollow noreferrer"">https://www.fluentd.org/dataoutputs</a>,   so I decided that it is possible. But I don't understand how can I configure it.</p>

<p>I have 
-splunk
-openshift with 'logging' project where I have the EFK (elasticsearch, fluend, kibana) stack </p>

<p>Now I don't understand where I can configure ""out_splunk"" plugin</p>",50385212.0,1,0,,2018-5-14 11:28:33,,2018-5-17 06:58:16,,,,,6920519.0,,1,1,openshift|splunk|fluentd,1515,12
47,258503,50346791,How can I break two Splunk logs in same row?,"<p>I am trying to break two logs which are logged in the same row because of which the next log is not getting published in the result while extracting, while it is showing in the event log.</p>

<pre><code>02:09:50.296 64785434 [http-bio-8085-exec-156] INFO  c.i.p.w.r.s.AdjustmentServiceImpl - publicRequest: ,AccountID=123147309786219,ProductName=""Product Online"",ProductFlavor=""ENHANCED"",EventType=Usage,AdjustmentType=event,AdjustmentInputType=Amount,BillDate=2018-07-09,AdjustmentReason=""Chargeback Event Adjustment"",,SOAP Request:,AccountPOID=""0.0.0.1 /account 324245253535"",EventPOID=""0.0.0.1 /event/activity/usage/payroll_per_employee 311258547968527414"",AdjustmentAmount=1,AdjustmentPercentage=,Currency=GBP,Bill-ItemNo=23627727,1,AdjustmentDate=2018-07-14T04:00-07:00,IS_DEBIT_OR_CREDIT=CREDIT,TaxTreatment=""Included Taxes in this adjustment"",,SOAP Response:HttpStatus=200,,
02:09:50.826 64785964 [http-bio-8085-exec-156] INFO  c.i.p.w.r.s.AdjustmentServiceImpl - publicRequest: ,AccountID=123147309786219,ProductName=""Product Online"",ProductFlavor=""ENHANCED"",EventType=Usage,AdjustmentType=event,AdjustmentInputType=Amount,BillDate=2018-07-09,AdjustmentReason=""Chargeback Event Adjustment"",,SOAP Request:,AccountPOID=""0.0.0.1 /account 324245253535"",EventPOID=""0.0.0.1 /event/activity/usage/payroll_per_employee 7378383763636373"",AdjustmentAmount=1,AdjustmentPercentage=,Currency=GBP,Bill-ItemNo=23627727,1,AdjustmentDate=2018-07-14T04:00-07:00,IS_DEBIT_OR_CREDIT=CREDIT,TaxTreatment=""Included Taxes in this adjustment"",,SOAP Response:HttpStatus=200,,
</code></pre>

<p>if you see the above log, these are two events which are getting set in the same row but I am not able to extract in the table as two events.</p>

<p>Please suggest how can I extract it as two events.</p>",,1,2,,2018-5-15 09:31:40,,2018-5-16 02:48:25,2018-5-15 09:37:31,,4446203.0,,1076837.0,,1,0,splunk|splunk-query,46,6
48,258504,50350363,how do i pass a result from one search into IN clause of another search in splunk?,"<p>i run a query and get list of custId in form of table. how do i pass this result into another search query inside IN clause. </p>

<p>eg:</p>

<p>search 1: index=* ""successful login for""|table custID
 this gives me table with column custID.</p>

<p>Then i have to run</p>

<p>index=* ""mail sent by""|where custID IN (search 1) |table CustID,_time</p>",,2,1,,2018-5-15 12:35:28,1.0,2021-7-19 20:03:10,,,,,9794139.0,,1,2,splunk|splunk-query,6390,19
49,258505,50370008,How do I find first occurence of a particular event for the list of users in splunk,"<p>i have to first occurence of a particular event for the list of users in splunk.</p>

<p>eg: i have list of user say 10 from another query.</p>

<p>i am using below query to find date of first mail sent by customer 12345. How do i find the same for a list of customer that i get from another query? </p>

<p>index=abc appname=xyz ""12345"" ""*\""SENT\""}}""|reverse|table _time|head 1</p>",,1,0,,2018-5-16 11:41:38,,2018-5-16 13:46:09,,,,,9794139.0,,1,0,splunk|splunk-query|splunk-calculation,180,9
50,258506,50428104,Shell Script to read from log file and update to Oracle DB table,"<p>I have a requirement to read a splunk log file  for certain parameters and use that data to update an Oracle 11g DB table once those parameters are found.</p>

<p><em>for e.g.</em> </p>

<p><em>Splunk log file name is:</em> <code>app.log</code></p>

<p><em>input parameters in log file would be:</em> </p>

<pre><code>[timestamp] amount=100,name=xyz,time=19 May 2018 13:45 PM
</code></pre>

<p><em>output from shell script should be:</em> amount should be read in to a variable and 100 should be assigned to that. This value 100 should be stored in a DB table in Oracle.</p>

<p>I may have to use awk script for this. I am not getting an idea on this as I am new to shell scripting.. </p>

<pre><code>tail -f|egrep -wi 'amount' /apps/JBoss/log/app.log 
</code></pre>

<p>This type of commands doesn't seem to be working.</p>",,3,2,,2018-5-19 18:02:25,1.0,2018-5-22 19:21:30,2018-5-22 19:21:30,,2238641.0,,2238641.0,,1,0,oracle|shell|awk|splunk,765,12
51,258507,50459042,Splunk C# Client Issue #1: Custom Auth Token Header Gone Awry,"<p>I wanted to go my own way before using the Splunk C# SDK. I made it past logging in, meaning I can retrieve a valid sessionKey in the response. My issue now is that creating search jobs is returning ""Unauthorized.""</p>

<p>According to the Splunk documentation this should work.
For simplicity:</p>

<pre><code>    string newstring = string.Format(""Splunk {0}"", token);
    client.DefaultRequestHeaders.Add(""Authorization"", newstring);
</code></pre>

<p>But I keep getting this:</p>

<pre><code>    &lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
    &lt;response&gt;
    &lt;messages&gt;
    &lt;msg type=""ERROR""&gt;Unauthorized&lt;/msg&gt;
    &lt;/messages&gt;
    &lt;/response&gt;
</code></pre>

<p>Even though a curl to the host of the same intent is successful.</p>

<pre><code>    &lt;response&gt;
    &lt;sid&gt;1526955996.109&lt;/sid&gt;
    &lt;/response&gt;
</code></pre>",,1,0,,2018-5-22 02:45:22,,2018-5-31 01:41:38,,,,,9825979.0,,1,0,c#|api|splunk,115,8
52,258508,50471867,Bind logs to applications,"<p>How does splunk tie logs to an application?</p>

<p>When using logback TCP appender to send logs to splunk, how does splunk tie the messages to an index or application?</p>",,0,0,,2018-5-22 15:52:34,,2018-5-22 15:52:34,,,,,1102835.0,,1,1,splunk,21,5
53,258509,50480481,"Splunk rex command with curly brackets, round brackets, period and quotation marks","<p>I'm having issues with the rex command on splunk. 
My Query outputs the below.</p>

<blockquote>
  <p>{""(001) NULL.COUNT(1).NUMBER"": ""12345""}</p>
</blockquote>

<p>I am looking to extract just the value 12345, but at the moment, I have below <code>rex</code> command which returns ""<code>{""(001) NULL.COUNT(1).NUMBER"": ""12345""}</code>"".</p>

<pre><code>| rex field=_transfers ""(001) NULL.COUNT(1).NUMBER"": ""(?&lt;value&gt;.*)""
</code></pre>",,1,0,,2018-5-23 05:32:10,,2018-5-23 13:19:42,2018-5-23 05:40:24,,2029022.0,,9832237.0,,1,0,command|splunk|rex,1286,13
54,258510,50484363,How to parse JSON metrics array in Splunk,"<p>I receive JSON from API in the following format:</p>

<pre><code>[
    {
    ""scId"": ""000DD2"",
    ""sensorId"": 2,
    ""metrics"": [
        {
            ""s"": 5414,
            ""dateTime"": ""2018-02-02T13:03:30+01:00""
        },
        {
            ""s"": 5526,
            ""dateTime"": ""2018-02-02T13:04:56+01:00""
        },
        {
            ""s"": 5631,
            ""dateTime"": ""2018-02-02T13:06:22+01:00""
        }
}, .... ]
</code></pre>

<p>Currently trying to display these metrics on the linear chart with dateTime for the X-axis and ""s"" for Y.</p>

<p>I use the following search query:</p>

<pre><code>index=""main"" source=""rest://test3"" | spath input=metrics{}.s| mvexpand metrics{}.s
| mvexpand metrics{}.dateTime | rename metrics{}.s as s 
| rename metrics{}.dateTime as dateTime| table s,dateTime
</code></pre>

<p>And I receive the data in the following format which is not applicable for linear chart. The point is - how to correctly parse the JSON to apply date-time from <code>dateTime</code> field in JSON to <code>_time</code> in Splunk.</p>

<p><a href=""https://i.stack.imgur.com/TosyV.png"" rel=""nofollow noreferrer"">Query results</a></p>",,1,0,,2018-5-23 09:14:06,,2018-5-29 10:23:57,,,,,7094372.0,,1,4,splunk|splunk-query|splunk-calculation,6030,18
55,258511,50512160,transpose one set of characters into another set of characters in Splunk,"<p>I need to replace all the accents in the 5 vowels of the Spanish alphabet using a single replacement regex and 5 capture groups.</p>

<p>In my text I have áéíóúàèìòù and so on. Until now I have this regex:</p>

<pre><code>s/(=?[àáÀÁ])|(=?[èéÈÉ])|(=?[ìíÌÍ])|(=?[òóÒÓ])|(=?[ùúÙÚ])/$1$2$3$4$5/g
</code></pre>

<p>But this regex gives me the same result on each group.</p>

<p>Is there a way of getting a different value on each of the different group? Like so:</p>

<p>group1 ($1)-> A</p>

<p>group2 ($2) -> E</p>

<p>group3 ($3) -> I</p>

<p>group3 ($4) -> O</p>

<p>group3 ($5) -> U</p>

<p>I know how to do this using 5 different regex, but I need to do this in just one. Any thoughts?</p>

<p>Thank you very much!!</p>",,1,0,,2018-5-24 14:42:24,,2018-6-11 18:52:57,2018-6-11 18:52:57,,1908967.0,,2564940.0,,1,-1,regex|substitution|splunk|regular-language|rex,170,8
56,258512,50559852,Specific field values extraction with single value only,"<p>Need to extract customers msisdn (From) who have sent only one SMS (Received) and that too &quot;STOP&quot;. Logs are below -</p>
<blockquote>
<p>5/27/18 11:38:29.598 PM    [2018-27-05 23:38:29.598 UTC] INFO
pool-1-thread-3 [receivedSmsFileLogger]  - Received = &quot;JE S8 TELMA
MALADE&quot;, From = &quot;0765473387&quot;, Valid = &quot;false&quot; host =  Vapp01SN source
= D:\MIP\Logs\SMSC\Cycle1\received_sms.log sourcetype =   MIP_Received_SMS</p>
<p>5/27/18 9:28:30.569 PM     [2018-27-05 21:28:30.569 UTC] INFO pool-1-thread-2 [receivedSmsFileLogger]  - Received =
&quot;''STOP''&quot;, From = &quot;0765757431&quot;, Valid = &quot;false&quot; host =   Vapp01SN
source =  D:\MIP\Logs\SMSC\Cycle1\received_sms.log sourcetype
= MIP_Received_SMS</p>
<p>5/27/18 9:26:25.034 PM     [2018-27-05 21:26:25.034 UTC] INFO pool-1-thread-1 [receivedSmsFileLogger]  - Received =
&quot;1OUI&quot;, From = &quot;0765757431&quot;, Valid = &quot;false&quot; host =   Vapp01SN source
= D:\MIP\Logs\SMSC\Cycle1\received_sms.log sourcetype =   MIP_Received_SMS</p>
<p>5/27/18 9:06:36.889 PM     [2018-27-05 21:06:36.889 UTC] INFO pool-1-thread-3 [receivedSmsFileLogger]  - Received =
&quot;STOP&quot;, From = &quot;0766108902&quot;, Valid = &quot;true&quot; host =    Vapp01SN source
= D:\MIP\Logs\SMSC\Cycle1\received_sms.log sourcetype =   MIP_Received_SMS</p>
</blockquote>",,1,0,,2018-5-28 05:48:34,,2018-5-28 14:13:08,2020-6-20 09:12:55,,-1.0,,5644390.0,,1,0,splunk|splunk-query|splunk-calculation,107,8
57,258513,50583261,Composing a splunk query from the file content using Python,"<p>I am trying to composing a Splunk query by fetching the values from the text file content. Here i dont want to use any Splunk modules/libraries. </p>

<p>This is my simple code - </p>

<pre><code>import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import sys

df = pd.read_excel(""I:\\splunk_dashboards\\FID_list.xlsx"", sheetname='FID_lastweek')
sys.stdout = open(""I:\\splunk_dashboards\\FID.txt"", ""w"")


v = df['FID']
#print(df['FID'])

print(v)
</code></pre>

<p>This is the simple code where it retrieves the particular column values and store it in a text file.</p>

<p>The next step is to form a splunk query with the results stored in the text file.</p>

<p>For example  below is the result from the text file - </p>

<pre><code>0                            CollectionLimitsValidation
1                               PaymentLimitsValidation
2                              AccountDetailsFacadeBean
3                              AccountDetailsFacadeBean
</code></pre>

<p>I do have a splunk query like below in another text file - </p>

<pre><code>index=hfc_new_98764 host=QA FID=$(Value1_from_text_file) OR FID=$(value2_from _text_file) OR.... it goes on upto the final values
</code></pre>

<p>From the above template i need a splunk query like below - </p>

<pre><code>index=hfc_new_98764 host=QA FID=CollectionLimitsValidation OR FID=PaymentLimitsValidation OR FID=.... it goes on upto the final values
</code></pre>

<p>I need help to iterate the values  from the text file and to store in the template file file.</p>",50620101.0,1,0,,2018-5-29 11:29:24,1.0,2018-5-31 08:40:45,2018-5-29 11:42:03,,5687948.0,,5687948.0,,1,1,python|python-3.x|file|splunk,102,9
58,258514,50629477,splunk enterprise: failed to connect to HEC or to port 8088,"<p>i installed splunk enterprise (single server) in roeder to connect it to AWS Kinesis.
i configured the HEC on port 8088, but kinesis failed to connect.
when checked with curl: 
curl -k  ""Authorization: Splunk 5567e908-ec26-4e2d-a569-8c7f5ef7033f""  ""<a href=""https://localhost:8088/"" rel=""nofollow noreferrer"">https://localhost:8088/</a>""
I got:
<strong>404 Not Found</strong><h1>Not Found</h1><p>The requested URL was not found on this server.</p></p>

<p>when checked splunk by: splunk  cmd btool web list --debug|grep -i 808
I got only port 8089 and not 8088 at all.
how can I activate the HEC to listen to port 8088 ??</p>",,1,0,,2018-5-31 17:09:03,,2021-2-23 02:01:32,,,,,6226318.0,,1,1,port|splunk|amazon-kinesis-firehose,517,11
59,258515,50639744,Forwarding logs from kubernetes to splunk,"<p>I'm pretty much new to Kubernetes and don't have hands-on experience on it. </p>

<p>My team is facing issue regarding the log format pushed by <a href=""/questions/tagged/kubernetes"" class=""post-tag"" title=""show questions tagged &#39;kubernetes&#39;"" rel=""tag""><img src=""https://i.stack.imgur.com/aIElQ.png"" height=""16"" width=""18"" alt="""" class=""sponsor-tag-img"">kubernetes</a> to <a href=""/questions/tagged/splunk"" class=""post-tag"" title=""show questions tagged &#39;splunk&#39;"" rel=""tag"">splunk</a>. </p>

<h3>Application is pushing log to stdout in this format</h3>

<pre><code>{""logname"" : ""app-log"", ""level"" : ""INFO""}
</code></pre>

<h3>Splunk eventually get this format (splunkforwarder is used)</h3>

<pre><code>{
  ""log"" : ""{\""logname\"": \""app-log\"", \""level\"": \""INFO \""}"",
  ""stream"" : ""stdout"",
  ""time"" : ""2018-06-01T23:33:26.556356926Z"" 
 }
</code></pre>

<p>This format kind of make things harder in Splunk to query based on properties.</p>

<p>Is there any options in <code>Kubernetes</code> to forward <code>raw logs</code> from app rather than grouping into another json ?</p>

<p>I came across <a href=""https://answers.splunk.com/answers/592640/kubernetesdocker-json-logs.html"" rel=""nofollow noreferrer"">this</a> post in Splunk, but the configuration is done on Splunk side </p>

<p>Please let me know if we have any option from <code>Kubernetes</code> side to send raw logs from application</p>",,3,0,,2018-6-1 09:01:41,1.0,2020-3-22 21:34:01,2019-8-8 14:42:09,,11223488.0,,538776.0,,1,3,kubernetes|splunk,8033,20
60,258516,50719435,Get overall average and average per 5 minutes on a Time Chart,"<p>I have a timechart which currently outputs the average value for every 5 minutes over a period of time for the field ""SERVICE_TIME_TAKEN"" using following query.</p>

<pre><code>service=service1 | timechart span=5m avg(SERVICE_TIME_TAKEN) | fillnull 
</code></pre>

<p>I want to add a second line on this same time chart which shows the overall average value. This would be a single value which draws a straight line on the chart.</p>

<p>If I make a separate query, I am able to get this single value using following query.</p>

<pre><code>service=service1 | chart avg(SERVICE_TIME_TAKEN)
</code></pre>

<p>How can I combine these 2 queries to to show the data on a single time chart?</p>

<p>Tried the following but it only shows the line with the 5 min average.</p>

<pre><code>service=service1 | timechart span=5m avg(SERVICE_TIME_TAKEN) as service_time | eventstats avg(SERVICE_TIME_TAKEN) as overall_service_time  | fillnull 
</code></pre>

<p>This image depicts what I am looking for.
Orange line is the 5 mins average and blue line is the overall average. </p>

<p><a href=""https://i.stack.imgur.com/W1gkh.png"" rel=""nofollow noreferrer"">chart link</a></p>",50726099.0,1,0,,2018-6-6 11:35:21,,2018-6-6 17:20:11,,,,,9401029.0,,1,0,splunk,2378,15
61,258517,50737703,Need Splunk query for finding common elements between two fields when each field is a list,"<p>I have each event as a JSON object below which is indexed by Splunk. How can I have a Splunk query such that I find all such failures which happen to be present in both <code>""failed""</code> and <code>""passed""</code> arrays?</p>

<pre><code>""output"":{
          ""date"" : ""21-09-2017""
          ""failed"": [ ""fail_1"", **""fail_2""** ],
          ""passed"": [ ""pass_1"", ""pass_2"" , **""fail_2""**]
}
</code></pre>

<p>For the above example, the result would be <code>""fail_2""</code>.</p>",,1,0,,2018-6-7 09:26:57,,2018-6-8 21:16:39,2018-6-8 12:56:36,,5463636.0,,4021611.0,,1,0,splunk|splunk-query|splunk-calculation|splunk-formula|splunk-sum,530,10
62,258518,50799048,Does wildcard search slow down Splunk results search?,"<p>I use Splunk Enterprise and try to optimize my query so I write</p>

<p>(1)
    *index = ""main"" AND host = ""<em>prod"" source = ""/sys/logs/myApplication.log"" AND httpStatus = 201</em></p>

<p>(2)
    <em>index = ""main"" AND host = ""com.myorganization.london.prod"" source = ""/sys/logs/myApplication.log"" AND httpStatus = 201</em></p>

<p>We have only one prod instance on which there is myApplication.log so source-host conjunction gives one result but have over
100 prod hosts. Which is better approach (1) or (2).  And why?</p>",50808168.0,1,0,,2018-6-11 13:36:22,,2018-6-12 01:41:22,,,,,1732936.0,,1,0,search|optimization|splunk,161,9
63,258519,50847611,How do I make a stack column chart with two events organized by services?,"<p>Basically I need to make a column chart with the different endpoints along the x axis (services such as log in, etc.) with each endpoint having a unique column stacked with two colors, red for service error events and green for service events.</p>

<p>I can get something with a search string like this:
my search EVENT=""[SERVICES]"" OR EVENT=""[SERVICE_ERROR]"" | chart count by EVENT, ENDPOINT 
though its event on the x axis (but showing both services and service error like I need) with the endpoints being different colored stacked in the chart. 
Yet reversing this causes ONLY SERVICE EVENTS to show up, which is beyond my reasoning since service errors appear in the first search. 
Above is just something I tried. I also tried:
my search | fields ENDPOINT ""[SERVICE]"" ""[SERVICE_ERROR]"" 
according to the splunk documentation on stacking charts here: 
<a href=""https://docs.splunk.com/Documentation/Splunk/7.1.1/Viz/ColumnBarCharts"" rel=""nofollow noreferrer"">https://docs.splunk.com/Documentation/Splunk/7.1.1/Viz/ColumnBarCharts</a> 
(last example at bottom of page).
I wanted to make sure I was thorough with my explanation but in short...
My goal is to have the all the endpoints displayed on the x axis with the count of the different service events and service error events as the actual graph data as one column split into two colors for both events.
Thank you for any help!! </p>",50930954.0,1,0,,2018-6-13 23:19:25,,2018-6-19 14:32:53,,,,user8728708,,,1,0,splunk,477,10
64,258520,50928391,"Can ""eval"" be used to set an event equal to a search string?","<p>I have two specific search strings I use to narrow down my search in the format of:
index=someIndex ""searchtermA"" OR ""searchTermB"" | ....
I want to be able to chart the two values (as their own unique counts in one chart) however field extraction has proven to have too many problems with making field extractions for these two search terms. They are not events or sources or sourcetypes either.</p>

<p>So I've been trying to see if eval could somehow make a sort of token and set it to this search term so it can be used by chart/stats/etc.</p>

<p>Any help would be greatly appreciated!</p>",50929364.0,1,0,,2018-6-19 12:24:30,,2018-6-19 13:10:45,,,,user8728708,,,1,0,splunk|splunk-query,154,8
65,258521,50949709,Splunk: Trying to split multiline event at search time,"<pre><code>2018-06-20T00:04:35.000+00:00 (980) WAL Autocheckpointing, name=C:\Program 
Files\PriceService\data\documents.db
2018-06-20T00:07:16.000+00:00 (980) WAL Autocheckpointing, name=C:\Program 
Files\PriceService\data\store-promotions.db
2018-06-20T00:07:21.000+00:00 (980) WAL Autocheckpointing, name=C:\Program 
Files\PriceService\data\store-promotions.db
2018-06-20T00:07:26.000+00:00 (980) WAL Autocheckpointing, name=C:\Program 
Files\PriceService\data\store-promotions.db
</code></pre>

<p>I have been trying to get my splunk query right in order to split this one event into multiple events but for some reason I cannot get my query right.</p>

<p>I tried to split on newline but the result set comes back unchanged. I understand from reading online I'm supposed to use something on the lines of </p>

<pre><code>myQuery | rex field=_raw ""\[(?P&lt;field1&gt;...).*[\r\n]""
</code></pre>

<p>Apologies by the way. My regex game is not strong.</p>",50951236.0,2,3,,2018-6-20 13:41:42,1.0,2019-3-5 16:04:35,2019-3-5 16:04:35,,1033581.0,,742703.0,,1,2,regex|splunk|splunk-query,7004,23
66,258522,51010525,Unable to integrate Splunk with Jenkins,"<p>I have installed Jenkins on AWS EC2 instance and it is running fine. I have installed Splunk on same EC2 instance.</p>

<pre><code>My Jenkins URL:-
http://10.x.x.x:8080

And my Splunk URL:-
http://10.x.x.x:8000
</code></pre>

<p>I have installed Splunk plugin on Jenkins as well and trying to configure Splunk with Jenkins such that data from Jenkins will act as source to Splunk. I tried a lot but it is not working. On Splunk, I created an '<strong>HTTP Event Collector</strong>' token and trying to configure on Jenkins using this token.</p>

<p>On Jenkins -> Manage Jenkins -> Configure System, under 'Splunk for Jenkins Configuration', </p>

<pre><code>**enable checkbox -&gt; checked, 
indexer hostname -&gt; host IP address of Splunk (which is same as Jenkins)
HTTP Input Port -&gt; 8088
HTTP Input Token -&gt; &lt;&lt;&lt; Token generated at Splunk side &gt;&gt;&gt;
SSL Enabled -&gt; Checked (tried with uncheck as well)
Jenkins Master Hostname -&gt; Host name of Jenkins (same as Splunk)**
</code></pre>

<p>When I click on 'Test connection', nothing happens and fails. Please assist to clear my following doubts :-
Q1) I googled and found HTTP input port for Splunk is 8088 then what is 8000. I can access to Splunk console using 8000. Then it should be 8000 only right instead of 8088?</p>

<p>Q2) Since both Splunk and Jenkins are running on different ports though they have same host name then I believe there should not be any connectivity issue. Please confirm.</p>

<p>Q3) Am I missing anything to confure Splunk with Jenkins finally.</p>",,1,0,,2018-6-24 13:48:04,,2018-12-22 05:41:12,,,,,3016392.0,,1,1,jenkins|amazon-ec2|splunk,1454,13
67,258523,51051724,"ERROR Appenders contains an invalid element or attribute ""Http""","<p>I am using Log4j  Http appender to send data to Splunk using mule cloudhub. During the build it thorws the error:</p>

<blockquote>
  <p>ERROR Appenders contains an invalid element or attribute Http</p>
</blockquote>

<p>and I am not seeing the data in Splunk.</p>

<p>The error happens with Log4j Configuration:</p>

<pre><code>&lt;Http name=""Splunktest"" url=""myurl"" token=""mytoken""  
disableCertificateValidation=""true""&gt;&lt;/Http&gt;
</code></pre>

<p>During the maven build it is throwing the mentioned error. Mule runtime version 3.8.4</p>

<p>Did anyone else face the same error?</p>

<p>Entire Log4j for reference</p>

<p>
</p>

<pre><code>&lt;!--These are some of the loggers you can enable. 
    There are several more you can find in the documentation. 
    Besides this log4j configuration, you can also use Java VM environment variables
    to enable other logs like network (-Djavax.net.debug=ssl or all) and 
    Garbage Collector (-XX:+PrintGC). These will be append to the console, so you will 
    see them in the mule_ee.log file. --&gt;


&lt;Appenders&gt;
    &lt;RollingFile name=""file"" fileName=""${sys:mule.home}${sys:file.separator}logs${sys:file.separator}splunk.log"" 
             filePattern=""${sys:mule.home}${sys:file.separator}logs${sys:file.separator}splunk-%i.log""&gt;
        &lt;PatternLayout pattern=""%d [%t] %-5p %c - %m%n"" /&gt;
        &lt;SizeBasedTriggeringPolicy size=""10 MB"" /&gt;
        &lt;DefaultRolloverStrategy max=""10""/&gt;
    &lt;/RollingFile&gt;

    &lt;Http name=""Splunktest"" url=""myurl"" token=""mytoken"" disableCertificateValidation=""true""&gt;&lt;/Http&gt;

&lt;/Appenders&gt;

&lt;Loggers&gt;


    &lt;!-- Http Logger shows wire traffic on DEBUG --&gt;
    &lt;AsyncLogger name=""org.mule.module.http.internal.HttpMessageLogger"" level=""WARN""/&gt;

    &lt;!-- JDBC Logger shows queries and parameters values on DEBUG --&gt;
    &lt;AsyncLogger name=""com.mulesoft.mule.transport.jdbc"" level=""WARN""/&gt;

    &lt;!-- CXF is used heavily by Mule for web services --&gt;
    &lt;AsyncLogger name=""org.apache.cxf"" level=""WARN""/&gt;

    &lt;!-- Apache Commons tend to make a lot of noise which can clutter the log--&gt;
    &lt;AsyncLogger name=""org.apache"" level=""WARN""/&gt;

    &lt;!-- Reduce startup noise --&gt;
    &lt;AsyncLogger name=""org.springframework.beans.factory"" level=""WARN""/&gt;

    &lt;!-- Mule classes --&gt;
    &lt;AsyncLogger name=""org.mule"" level=""INFO""/&gt;
    &lt;AsyncLogger name=""com.mulesoft"" level=""INFO""/&gt;

    &lt;!-- Reduce DM verbosity --&gt;
    &lt;AsyncLogger name=""org.jetel"" level=""WARN""/&gt;
    &lt;AsyncLogger name=""Tracking"" level=""WARN""/&gt;

    &lt;AsyncRoot level=""INFO""&gt;
        &lt;AppenderRef ref=""file"" /&gt;
    &lt;/AsyncRoot&gt;
     &lt;AsyncLogger name=""splunk.logger"" level=""INFO"" &gt;
      &lt;AppenderRef ref=""Splunktest"" /&gt;

    &lt;/AsyncLogger&gt;
&lt;/Loggers&gt;
</code></pre>

<p></p>",,1,3,,2018-6-26 21:40:52,1.0,2019-6-9 09:28:32,2018-7-5 21:48:25,,7635402.0,,7635402.0,,1,0,log4j|mule|splunk,2064,13
68,258524,51074870,How to upgrade splunk 6.1.4 to 6.5 have license,"<p>I am using splunk 6.1.4 with license, now I want to upgrade splunk 6.5 still holds this license. 
I tried upgrading to 6.5.9 but it does not receive the license.</p>

<p>Can you help me! Please!
Best regards,
Thanks.</p>",,1,0,,2018-6-28 04:45:22,,2018-7-5 07:21:55,,,,,9388869.0,,1,0,upgrade|updates|splunk,259,10
69,258525,51076235,Split splunk log-driver for Docker into multiple indexes,"<p>I have a small java application running inside of docker. The application is a <a href=""https://www.dropwizard.io/1.3.4/docs/"" rel=""nofollow noreferrer"">Dropwizard</a> application, meaning it has java classes that log with log4j, and web endpoints, (JaxRS) that has request logging capability.</p>

<p>Now, I want to forward my container logs to Splunk. The issue is I really want them split into two indexes; one for the application log, and one for the request log. We can call the indexes <code>cool_app</code> and <code>cool_req</code>.</p>

<p>Is there any way of doing this? Starting my dropwizard application causes it to output both the application log and request log to the <code>stdout</code>. (Whilest also logging to two files)</p>

<p>I've tried the docker logdriver but it seems like it's only made for one index.</p>

<p>I might add that it's easy to separate the logs by regEx if that would be an option</p>",,1,1,,2018-6-28 06:40:23,,2018-6-29 17:05:28,2018-6-28 06:46:19,,1776540.0,,1776540.0,,1,1,java|docker|logging|splunk,275,9
70,258526,51091645,How to access values from event in Splunk,"<p>I want to access the values of the events that are coming after splunk search .</p>

<p>Data is coming in below format on the event</p>

<pre><code>18/06/28 14:12:07 250  219561  711914   72864    0  784778 - 18-06-28 14:08:43
</code></pre>

<p>I want to get all the data hourly on the basis this time <code>14:12:07</code></p>

<p>Please suggest how we can do this in Splunk</p>",,1,1,,2018-6-28 21:48:48,,2018-6-29 21:50:29,,,,,2618323.0,,1,0,splunk,33,6
71,258527,51115988,Operation timed out Splunk + Python,"<p>I have install Splunk Python SDK 
and i am trying to connect to splunk cloud 
and give me this error</p>

<p>socket.error: [Errno 60] Operation timed out</p>

<p>this my code </p>

<pre><code>import splunklib.client as client


class SplunkSearch():
    def __init__(self):

        self.service = client.connect(
                    host=Config.DEVELOPMENT_CONF['splunk']['host'],
                    port=Config.DEVELOPMENT_CONF['splunk']['port'],
                    username=Config.DEVELOPMENT_CONF['splunk']['username'],
                    password=Config.DEVELOPMENT_CONF['splunk']['password'],timeout=None)
</code></pre>",,1,1,,2018-6-30 14:55:04,,2018-8-20 07:51:46,,,,,1955098.0,,1,0,python-2.7|splunk,780,12
72,258528,51139242,HA Proxy TCP balance with original hostname,"<p>I have a Splunk search head cluster with 3 nodes, and I am trying to setup an HA Proxy instance to balance user logins to one of these 3 search head nodes.</p>

<ul>
<li>Splunk 7.1</li>
<li>HA Proxy 1.5.18</li>
<li>Centos 7</li>
</ul>

<p>The proxy itself is working and routing to correct SH nodes based on splunk health (running splunk on port 8300)</p>

<p>The problem is that if I go to ""splunk.company.local"" (HA Proxy DNS name), HAproxy redirects the user to one of the 3 nodes, but doesnt keep the ""splunk.company.local"" in the URL, instead the URL changes to ""splunksh01.company.local"" etc,</p>

<p>I cant figure out how to have a single, sticky name, so the user is always under the ""splunk.company.com"" URL and never sees the actual SH node hostnames,</p>

<p>haproxy.cfg</p>

<pre><code># This file managed by Puppet
global
  daemon  
  debug  true
  group  haproxy
  log  10.185.20.168 local0
  pidfile  /var/run/haproxy.pid
  stats  socket /var/lib/haproxy/stats
  user  haproxy

defaults
  log  global
  maxconn  8000
  mode  tcp
  option  redispatch
  retries  3
  timeout  connect 15s
  timeout  client 50s
  timeout  server 50s
  timeout  check 10s
  timeout  server 1m

listen splunk
  bind 0.0.0.0:80 

  mode tcp
  balance source
  server splunksh01.company.local 10.185.20.173:80 check port 8300
  server splunksh02.company.local 10.185.20.174:80 check port 8300
  server splunksh03.company.local 10.185.20.176:80 check port 8300
</code></pre>

<p>Now on each SH Node, Im running Splunk via Apache reverse proxy, </p>

<p>cat /etc/httpd/conf.d/splunk.conf</p>

<pre><code>&lt;VirtualHost *:443&gt;
  ServerName splunk

  ## Logging
  ErrorLog ""/var/log/httpd/splunk-443_error_ssl.log""
  ServerSignature Off
  CustomLog ""/var/log/httpd/splunk-443_access_ssl.log"" combined 

  ## Request header rules
  ## as per http://httpd.apache.org/docs/2.2/mod/mod_headers.html#requestheader
  RequestHeader set X-Forwarded-Proto ""https""

  ## Proxy rules
  ProxyRequests Off
  ProxyPreserveHost Off
  ProxyPass / https://splunksh01.company.local:8300/
  ProxyPassReverse / https://splunksh01.company.local:8300/

  ## SSL directives
  SSLEngine on
  SSLCertificateFile      ""/etc/puppetlabs/puppet/ssl/certs/splunksh01.company.local.pem""
  SSLCertificateKeyFile   ""/etc/puppetlabs/puppet/ssl/private_keys/splunksh01.company.local.pem""

  # SSL Proxy directives
  SSLProxyEngine On
&lt;/VirtualHost&gt;
</code></pre>

<p>How do I retain the same hostname once HAproxy routs the requests to one of the nodes? Thanks.</p>",,0,3,,2018-7-2 15:15:52,,2018-7-2 15:21:06,2018-7-2 15:21:06,,7327476.0,,7327476.0,,1,0,tcp|haproxy|sticky|splunk|balance,920,11
73,258529,51184633,Regular expression splunk query,"<p>I have a line containing </p>

<pre><code>[India,sn_GB] Welcome : { Name:{Customer1},Place:{Mumbai},}
</code></pre>

<p>I want to print the entire line after <code>sn_GB]</code> in splunk, which is  </p>

<pre><code>Welcome : { Name:{Customer1},Place:{Mumbai},}
</code></pre>

<p>I used the below regular expression: </p>

<pre><code>(?&lt;=sn_).*?$
</code></pre>

<p>But it prints, along with <code>GB]</code> like <code>GB] Welcome : { Name:{Customer1},Place:{Mumbai},}</code>.
In the word <code>sn_GB</code>, <code>sn_</code> is constant and the rest two letter will vary, like <code>GB</code>, <code>LB</code>, <code>KB</code>, <code>TB</code> as such.</p>

<p>Please help me in correcting the regular expression. </p>

<p>Thanks</p>",51184692.0,2,1,,2018-7-5 06:29:24,,2018-7-5 07:02:03,2018-7-5 06:42:49,,3832970.0,,7091577.0,,1,-1,regex|splunk-query,256,9
74,258530,51215668,Docker Splunk Logging Driver - Logs to Splunk are sometimes delayed,"<p>I am using the Splunk logging driver to send logs to splunk with the following command line:  <code>docker run -d -p 443:8443 --log-driver=splunk --log-opt splunk-token=REDACTED --log-opt splunk-url=https://myloghost.example.net:8088 --log-opt splunk-sourcetype=idp --log-opt splunk-index=auth_idp --log-opt splunk-insecureskipverify=1 --log-opt splunk-format=raw --log-opt splunk-gzip=true --name shib --restart always --health-cmd 'curl -k -f https://127.0.0.1:8443/idp/status || exit 1' --health-interval=2m --health-timeout=30s</code></p>

<p>The container runs normally, and logs flow into Splunk.  All is good.  This is in a testing environment, so it is not always in use, but the container is left running.  Sometimes, when I start using the service the container provides, nothing is logged to Splunk immediately.  If I wait 10-15 minutes, the logs eventually show up with the correct time stamps, etc.</p>

<p>I've noticed on the docker host that <code>netstat -tpn | grep -e 8088</code> gives me output similar to this:</p>

<pre><code>Active Internet connections (w/o servers)
Proto Recv-Q    Send-Q  Local Address           Foreign Address         State       PID/Program name    
tcp        0    947     xxx.xxx.x.xxx:49010     xxx.xxx.x.xx:8088       ESTABLISHED 12682/dockerd-curre   
</code></pre>

<p>On the Splunk host, the same command shows zeroes in the Recv-Q and Send-Q columns.  The Splunk Distributed Management Console doesn't show any events received during the lag time.  On the Docker host, there is a message in <code>/var/log/messages</code> from Docker that happens at the same time the logs are finally sent to Splunk:</p>

<pre><code>Jul  6 13:14:19 idpdock0-0 dockerd-current: time=""2018-07-06T13:14:19.428396282-04:00"" level=error msg=""Post https://myloghost.example.net:8088/services/collector/event/1.0: read tcp xxx.xxx.x.xxx:49010-&gt;xxx.xxx.x.xx:8088: read: connection timed out""
</code></pre>

<p>It seems to me like the logging driver get stuck trying to do some I/O operation, and when it finally times out, it tries again and the logs are sent.  However, I have no idea what the condition that causes it to get stuck is, nor do I know of any way to adjust the time out period.</p>

<p>I'd like to know why the logs take so long to get to Splunk sometimes, and if there is anything I can do to avoid the delays.</p>",51218223.0,1,0,,2018-7-6 18:02:15,1.0,2018-7-6 22:11:12,,,,,388558.0,,1,0,docker|logging|splunk,589,13
75,258531,51250631,Get TextBox value from Splunk token,"<p>I do not fully understand Splunks token system. I have the most basic of search form. I want to just use the text and on submit send the text to a PHP file(ajax call) and send a call out to my API that will return data to display in 
""display_info"".</p>

<pre><code>&lt;div class=""dashboard-body container-fluid main-section-body"" data-role=""main""&gt;
&lt;div class=""dashboard-header clearfix""&gt;
    &lt;h2&gt;Whois Lookup HTML&lt;/h2&gt;
&lt;/div&gt;
&lt;div class=""fieldset""&gt;
    &lt;div class=""input input-text"" id=""input1""&gt;
        &lt;label&gt;Domain&lt;/label&gt;
    &lt;/div&gt;
    &lt;div class=""form-submit"" id=""search_btn""&gt;
        &lt;button class=""btn btn-primary""&gt;Submit&lt;/button&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div id=""display_info""&gt;&lt;/div&gt;
</code></pre>

<p></p>

<p>So on submit the </p>

<pre><code>submittedTokenModel
</code></pre>

<p>is set with the value from the textbox.And then the submitToken function fires.</p>

<pre><code>  function submitTokens() {
        // Copy the contents of the defaultTokenModel to the submittedTokenModel and urlTokenModel
        FormUtils.submitForm({ replaceState: pageLoading });

//Grab textbox value and run AJAX to PHP function
        console.log(submittedTokenModel); 
//      var tv = submittedTokenModel.get("""");
//      var tv = tokens.get("""");
//     console.log(tv); 
    }
</code></pre>

<p>I do not understand how to access the text value from the submittedTokenModel. The documentation to me doesn't really seem to be helping me on this</p>",,1,0,,2018-7-9 17:00:54,,2018-7-13 19:53:08,,,,,4479573.0,,1,0,javascript|splunk,586,11
76,258532,51263248,Splunk Custom Module,"<p>I am new in Splunk - as well as in Python and start working on Splunk Custom Module and I have taken reference from Splunk Site <a href=""https://docs.splunk.com/Documentation/Splunk/7.1.1/Module/Example6-Include3rd-partylibraries"" rel=""nofollow noreferrer"">Custom Module</a>. When I have created Same file structure using Visual Studio 2017 -> Python3 then its give me an error </p>

<ul>
<li>import controllers.module not found </li>
<li>import splunk not found  </li>
<li>import splunk.search not found  </li>
<li>import splunk.util not found  </li>
<li>import splunk.entity not found    </li>
<li>import json from splunk.appserver.mrsparkle.lib not found</li>
<li>import lib.util as util not found</li>
</ul>

<p>Note: I have already imported <a href=""https://github.com/splunk/splunk-sdk-python"" rel=""nofollow noreferrer"">Splunk SDK</a> using ""pip install splunk-sdk"" Still, I can't find any package in the project.</p>

<p>Please, anyone, guide me how to resolve above custom module package error. </p>

<p>If there is any readymade samples are available then please suggest a link.</p>

<p>Thanks in advance</p>",,2,0,,2018-7-10 10:46:09,,2018-8-2 17:36:37,2018-7-10 10:52:51,,6622587.0,,889510.0,,1,0,python|splunk|splunk-sdk,561,10
77,258533,51311167,DATETIME losing hh:mm when changing CREATE TABLE to SELECT INTO,"<p>I am currently migrating all of my company's reports into Splunk Data Labs input for ingestion. The reports create temp tables using the CREATE TABLE format, which is incompatible with Splunk, however, SELECT INTO format works just fine. </p>

<p>The error that I am getting however when changing to the SELECT INTO format, is the DATETIME variable which should be MM/DD/YYYY hh:mm format loses the hh:mm end, and instead shows MM/DD/YYYY MM/DD/YYYY:</p>

<p>Original SQL:</p>

<pre><code>CREATE TABLE #Stats#(date_slice DATETIME NULL, raw_value REAL NULL)
INSERT INTO #Stats#
    SELECT CONVERT(CHAR(11), data_datetime, 111) + ' ' +
        CASE WHEN DATEPART(MINUTE, data_datetime) &lt; 30 THEN
        RIGHT('0' + LTRIM(str(DATEPART(hour, data_datetime))), 2) + ':00' ELSE
        RIGHT('0' + LTRIM(str(DATEPART(hour, data_datetime))), 2) + ':30' END AS date_hour
        ,SUM(ship_qty) AS moves
    FROM #tmpAllData
    GROUP BY CONVERT(CHAR(11), data_datetime, 111) + ' ' +
        case when DATEPART(minute, data_datetime) &lt; 30 THEN
        RIGHT('0' + LTRIM(str(DATEPART(hour, data_datetime))), 2) + ':00' ELSE
        RIGHT('0' + LTRIM(str(DATEPART(hour, data_datetime))), 2) + ':30' END
    ORDER BY 1
</code></pre>

<p>Modified SQL:</p>

<pre><code>--CREATE TABLE #Stats#(date_slice DATETIME NULL, raw_value REAL NULL)
SELECT CONVERT(CHAR(11), data_datetime, 111) + ' ' +
    CASE WHEN DATEPART(MINUTE, data_datetime) &lt; 30 THEN
    RIGHT('0' + LTRIM(str(DATEPART(hour, data_datetime))), 2) + ':00' ELSE
    RIGHT('0' + LTRIM(str(DATEPART(hour, data_datetime))), 2) + ':30' END date_slice
    ,SUM(ship_qty) raw_value
INTO #Stats#
FROM #tmpAllData
GROUP BY CONVERT(CHAR(11), data_datetime, 111) + ' ' +
    case when DATEPART(minute, data_datetime) &lt; 30 THEN
    RIGHT('0' + LTRIM(str(DATEPART(hour, data_datetime))), 2) + ':00' ELSE
    RIGHT('0' + LTRIM(str(DATEPART(hour, data_datetime))), 2) + ':30' END
ORDER BY 1
</code></pre>

<ul>
<li>Original Output: ""07/12/2018 10:00:00  ""</li>
<li>Modified Output: ""2018/07/12 2018/07/12""</li>
</ul>",51311533.0,2,1,,2018-7-12 17:11:26,,2018-7-12 19:40:49,2018-7-12 17:22:12,,13302.0,,10071915.0,,1,0,sql|sql-server|splunk-query,39,6
78,258534,51325106,Log data from dataPower to splunk,"<p>The question might be looking easy but I am quite struck on this.</p>

<p>I have a requirement whereby I have to store data regarding Timestamp,latency,serviceName etc in a variable and then log that into splunk.</p>

<p>However I am unable to call splunk through datapower xslt.</p>

<p>How can we call splunk through datapower using XSLT</p>

<p>Thanks</p>",,2,0,,2018-7-13 12:28:14,,2018-7-17 17:26:45,,,,,1758521.0,,1,1,splunk|ibm-datapower,1274,12
79,258535,51354061,how to control input variable in dashboard,"<p>I am struggling with below query:
   search..
| table .. AA_ID, BB_ID ..
| where (match(AA_ID, $AAID$) OR isnull($AAID$)) AND (match(BB_ID, $BBID$) OR isnull($BBID$))
.</p>

<p>$AAID$ and $BBID$ are from input variable in a dashboard I made and they are text type.
I want that when user didn't input or set the variables like 'all' or '*' then show all result, but it did not work like I expected.
There were many examples to control query's result by using where or search clause, 
but I haven't seen an example to control for input variable.</p>

<p>Please let me know, if you know anything or releated one.
Thanks in advance!</p>",,0,2,,2018-7-16 03:05:18,,2018-7-16 03:05:18,,,,,7474000.0,,1,0,splunk|splunk-query,99,7
80,258536,51414839,Logging to calculate API response time - best practice?,"<p>I am writing logs for a REST API endpoint to measure the average response time and response time of 50% of requests using splunk.</p>

<p>Something like: ""The request xyz took ??? milliseconds.""</p>

<p>I wanted to know if I should be logging the response time for the requests that returned 4XX or 5XX errors.</p>

<p>Even if I do log them, should I include this in the calculation of response time metric for this API ?</p>",,0,1,,2018-7-19 05:19:15,,2018-7-19 05:19:15,,,,,1774005.0,,1,1,java|rest|api|logging|splunk,669,11
81,258537,51430296,Single log is getting split into two events,"<p>I am not using props.conf. So I guess it is the default behavior.</p>

<p>Below is the single log:</p>

<pre><code>2018-07-19 13:30:40.293 +0000  [http8080] INFO  RequestFilter- {
   ""transaction_id"" : ""aaaaaaaaawwwwwwww"",
   ""http_method"" : ""POST"",
   ""date_time"" : ""2018-07-19 13:30:34.694 +0000"",
   ""requestId"" : ""20180719-dc7bc01d-b02c-43c8-932b-42af542ccefb""
 }
</code></pre>

<p>But it is coming in 2 events</p>

<pre><code>2018-07-19 13:30:40.293 +0000  [http8080] INFO  RequestFilter- {
       ""transaction_id"" : ""aaaaaaaaawwwwwwww"",
       ""http_method"" : ""POST"",
</code></pre>

<p>And</p>

<pre><code>""date_time"" : ""2018-07-19 13:30:34.694 +0000"",
   ""requestId"" : ""20180719-dc7bc01d-b02c-43c8-932b-42af542ccefb""
 }
</code></pre>

<p>It is always breaking from ""date_time""</p>

<p>Any suggestions? how can i fix it?</p>",,1,0,,2018-7-19 19:15:43,,2018-7-20 00:57:54,,,,,6656693.0,,1,0,splunk,68,9
82,258538,51449843,How to remove a certain field from Splunk output,"<p>I am trying to remove a field from Search Result after running a command in Search Head on Splunk.</p>

<p><a href=""https://i.stack.imgur.com/lT8BL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lT8BL.png"" alt=""enter image description here""></a></p>

<p>However as you can see in the following command that I am trying to run I see following error. I am quite new to Splunk and not sure what I need to do. Please guide.</p>

<p><a href=""https://i.stack.imgur.com/GaZcI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GaZcI.png"" alt=""enter image description here""></a></p>",51478063.0,1,0,,2018-7-20 20:22:26,,2018-7-23 11:47:49,2018-7-22 04:25:52,,2308683.0,,2325154.0,,1,1,splunk|splunk-query,2115,16
83,258539,51450897,How to write Regex to extract first few characters from specific word without or without ending delimiters?,"<p>I have the following string and would like to extract the first few characters until the end of the word or until ""Response""</p>

<pre><code>&lt;ns2:GetJobStatus
&lt;ns10:JobIDResponse
&lt;ns2:JobStatusResponse
&lt;ns3:GetJobId
</code></pre>

<p>I would like the regular expression such that I could extract either GetJobStatus and GetJobID from all the above lines. I would like to drop ""Response"" from the result, such that I would get 2 of each in the above example. This is in splunk so I can't use awk or sed or any other unix /linux commands.</p>

<p>Here's what I have done so far</p>

<pre><code>&lt;ns\d+:(?P&lt;ws_name&gt;.+?)(?:Response)
</code></pre>

<p>with the above I am able to extract only where there is ""Response"" </p>",51450967.0,2,3,,2018-7-20 21:59:33,1.0,2018-7-20 23:16:58,2018-7-20 22:06:10,,5360325.0,,5360325.0,,1,0,regex|splunk,767,12
84,258540,51518532,SimpleDateFormat for Oracle Date (TimeStamp with TimeZone),"<p>I am currently trying to parse a Datestring (TimeStamp with TimeZone) coming from an Oracle Database using a <a href=""https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html"" rel=""nofollow noreferrer"">SimpleDateFormat</a>. This is mandatory as the resulting Pattern is supposed to be used in Splunks DBConnect. The String I recieve has the format:</p>

<pre><code>2018-07-24 14:00:02.876611 +0:00
</code></pre>

<p>I am currently trying to use </p>

<pre><code>yyyy-MM-dd HH:mm:ss.S XXX
</code></pre>

<p>as pattern String (minimal Example at the end). As far as I understand, the problem is with the timezone having no leading 0 (e.g. +00:00 would work).
Is there still a way to parse the date using a SimpleDateFormat with a simple Pattern?</p>

<p>Thank you very much in advance.</p>

<pre><code>import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Date;

public class Tester {
    public static void main(String[] args) {
        String input = ""2018-07-24 14:00:02.876611 +0:00"";
        String parser = ""yyyy-MM-dd HH:mm:ss.S XXX"";
        SimpleDateFormat format = new SimpleDateFormat(parser);
        try {
            Date date = format.parse(input);
        } catch (ParseException e) {
            e.printStackTrace();
        }
    }
}
</code></pre>

<h1>Update I:</h1>

<p>In my oppinion this is not a duplicate question due to the limitation of using a SimpleDateFormat Pattern. The <a href=""https://stackoverflow.com/questions/38277202/how-to-parse-timezone-with-single-digit-offset-wed-dec-31-000000-gmt-8-1969"">possible duplicate</a> does neither state wheter or not it could be done using a SimpleDateFormat nor (if possible) how the pattern must be created. It offers the solution to either use another library (not possible) or use preprocessing (not possible).</p>

<h1>Update II:</h1>

<p>It is not possible to preprocess the data, as the access to the database is limited. And the possibility to add further preprocessing in Splunk is also not possible.</p>

<p>Due to the way DBConnect (the application that is going to be using the Pattern) is developed it expects a Pattern that is SimpleDateFormat compliant.</p>

<p>I understand that the answer to this question is, that it is not possible.</p>",,0,9,,2018-7-25 12:09:34,1.0,2018-7-25 12:44:49,2018-7-25 12:44:49,,2642758.0,,2642758.0,,1,0,java|oracle|simpledateformat|splunk,584,11
85,258541,51535866,How to build an ongoing alert that catches sudden spikes for a certain http error code?,"<p>I could really use an ongoing alert that catches a sudden rise (spike) in a certain error code (such as 404 or 502 etc...)
I tried giving this some thought on how to achieve that, and... Well... I could really use your help with the script :-)</p>

<p>From my understanding the search query should ""know"" or, ""sense"" the normal traffic (not sure for how long, maybe for 1hr, 2hrs) and alert when there is a spike in the error code compared to 1-2 hours ago.
I think the error code spike threshold should be more than 5% of total traffic, while occurring for longer than 90 seconds.</p>

<p>Here is a Splunk Query I use today, I appreciate your help tuning it to what I described above:</p>

<pre><code>tag=NginxLogs host=www1 OR host=www2 |stats count by status|eventstats sum(count) as total|eval perc=round((count/total)*100,2)|where status=""404"" AND perc&gt;5
</code></pre>",,1,0,,2018-7-26 09:46:05,,2018-8-8 20:57:19,,,,,7280355.0,,1,1,splunk|splunk-query,335,10
86,258542,51542541,joining 2 different searches in splunk,"<p>Search 1: </p>

<blockquote>
  <p>app=""atlas"" source=""/usr/local/homeaway/atlas-production/logs/*"" index=""aws_prod_applogs"" titan | stats  avg(*responseTime) by date_mday</p>
</blockquote>

<p>Search 2</p>

<blockquote>
  <p>app=""atlas"" source=""/usr/local/homeaway/atlas-production/logs/*"" index=""aws_prod_applogs"" titan statusCode=200 | stats  avg(*responseTime) by date_mday </p>
</blockquote>

<p>How do i join the 2 different search queries?</p>",,1,0,,2018-7-26 15:29:45,,2018-7-27 19:32:21,,,,,10054034.0,,1,0,splunk|splunk-query|splunk-formula,654,11
87,258543,51547924,How to find number of hits for webpage in Splunk?,"<p>I've tried things like:</p>

<ol>
<li>""url"" | stats count </li>
<li>index=""indexname"" ""url"" |stats count</li>
</ol>

<p>Do I need to set up logging in my webpage first to be able to get the number of hits?</p>",,1,3,,2018-7-26 21:43:26,,2018-8-5 09:52:27,,,,,5405028.0,,1,0,count|webpage|splunk,248,9
88,258544,51588087,How to print line numbers for an event in Splunk,"<p>I wanted to print number of lines for an event in Splunk after querying it</p>

<p>Ex: <code>index=* host=* source=*application*</code> this query is giving all the events but I want to print/get number of lines for each and every event. I tried with <code>len()</code> of Splunk query but it didnt work for me</p>",,1,1,,2018-7-30 06:24:55,,2018-8-2 07:11:31,,,,,2931744.0,,1,0,splunk-query,340,10
89,258545,51629850,How to implement a openidm splunk audit failover,"<p>i am using Splunk as the AuditHandler for my openIDM solution and I want to make sure that when the connection to Splunk fails for exp. in a server issue.
Then i want to rewrite the data since to timestamp of the connection loss.</p>

<p>Is there a simple solution for that or do i need to implement a logic?</p>

<p>thanks
Burhan</p>",51635025.0,1,0,,2018-8-1 09:31:55,1.0,2018-8-1 13:49:08,,,,,10164914.0,,1,1,splunk|audit|failover|forgerock|openidm,90,9
90,258546,51630922,SPLUNK subsearch 2 CSV Files join together,"<p>I have 2 Files with order data saved in two different sourcetypes in splunk.
One file contains an orderid, plnum(praefix + orderid (one ordernumer contains 3 plnum)), model (type of the order). The second file contains the same plnum's and Materialnumbers to those plnum's. </p>

<p>I want to search for the top Materials used for one or more Models.</p>

<p>So I searched for how to setup a subsearch:</p>

<pre><code>sourcetype=file1 [search sourcetype=file2 MODEL=""someting""| fields MODEL] |stats values(MATNR) by MODEL
</code></pre>

<p>I dont know why the subsearch dont work.</p>",,1,1,,2018-8-1 10:22:27,,2018-8-1 13:44:33,,,,,9108205.0,,1,0,computer-vision|splunk|spl,208,9
91,258547,51657194,Splunk count 2 different fields with two different group by without displaying them,"<p>I select <code>orderids</code> for a model in a subsearch and than select the most common materials for each orderid, so I get a list of every Material and the time it was a part of an order. I want to display the most common materials in percentage of all orders. So I need this amount how often every material was found and then divide that by total amount of orders.</p>

<pre><code>sourcetype=file1 [subsearch... -&gt;returns Orders] | 
</code></pre>

<p>here I need to select the total amount of orders like:</p>

<pre><code>stats dc(Orders) as totalamount by Orders|
stats dc(Orders) as anz by Material|
eval percentage= anz/totalamount|
sort by percentage desc
</code></pre>

<p>How can I perform the total amount of search?</p>",,2,0,,2018-8-2 15:22:57,,2020-1-14 05:41:39,2018-8-2 16:34:08,,5968255.0,,9108205.0,,1,3,count|statistics|splunk,4034,15
92,258548,51713639,How to find all the events that do not match a pattern in Splunk?,"<p>I am trying to find all the events that do not match a specific string in Splunk. In my case I am trying to build a report for all the events where ResponseCode:401, ResponseCode:404 etc. I short it could be anything but 200.</p>

<p>But not sure how to do so.</p>

<p>Here are some sample events.</p>

<p><strong>Events:</strong></p>

<pre><code>DNS:www.mybonuscenter.com Host:10.94.64.74 RequestMS:2414 EventTime:[06/Aug/2018:14:06:57 -0400] Request:""GET /bizrateapp/app.bundle.dd46e01d637d8dbcc456.js HTTP/1.1"" ResponseCode:200 Size:414360

DNS:www.mybonuscenter.com Host:10.94.64.74 RequestMS:168 EventTime:[06/Aug/2018:14:11:50 -0400] Request:""GET /favicon.ico HTTP/1.1"" ResponseCode:404 Size:209
</code></pre>

<p><strong>Search Head Command using regex:</strong></p>

<p><code>index=""my_cw_index"" | regex (?:[^ResponseCode\:200]*)</code> </p>

<p><strong>Output</strong></p>

<pre><code>Error in 'SearchParser': Missing a search command before '^'. Error at position '39' of search query 'search index=""syn_prod_cw"" | regex (?:[^ResponseCo'.
</code></pre>",,1,0,,2018-8-6 18:28:53,,2018-8-7 06:45:31,,,,,2325154.0,,1,0,splunk|splunk-query,3066,15
93,258549,51748775,Splunk Failed Login Report,"<p>I am relatively new to Splunk and I am trying to create a reportthat will display a hostname and the amount of times that host failed to login within the past five minutes, when they failed 3 or more times. The only way I was able to get the initial search results I want is to look only within the past 5 minutes, as you can see in my query:</p>

<p><code>index=""wineventlog"" EventCode=4625 earliest=-5min | stats count by host,_time | stats count by host | search count &gt; 2</code></p>

<p>This returns the host and the count. The issue is if I use this query in my report, it can run every five minutes, but the hosts that were listed previously get removed as they no longer are included in the search results. </p>

<p>I found ways to generate logs that I can then search for separately (<a href=""http://docs.splunk.com/Documentation/Splunk/6.6.2/Alert/LogEvents"" rel=""nofollow noreferrer"">http://docs.splunk.com/Documentation/Splunk/6.6.2/Alert/LogEvents</a>) but it didn't work the way I expected. </p>

<p>I am looking for an answer to any of these questions that can help me get the intended results:</p>

<ol>
<li>Can my original search be improved to still only get results where the failed logins were within 5 minutes but be able to search over any time period?</li>
<li>Is there a way to send the results from the query I already have to a report, where the results will not be cleared out when the search is run again?</li>
<li>Is there any other option I haven't considered to achieve the desired result?</li>
</ol>",51750996.0,1,0,,2018-8-8 14:09:10,,2018-8-8 15:57:10,2018-8-8 15:27:53,,9947119.0,,9947119.0,,1,0,splunk|splunk-query,1167,12
94,258550,51750168,Group event counts by hour over time,"<p>I currently have a query that aggregates events over the last hour, and alerts my team if events are over a specific threshold. The query was recently accidentally disabled, and it turns out there were times when the alert should have fired but did not. </p>

<p>My goal is apply this alert query logic to the previous month, and determine how many times the alert would have fired, had it been functional. However, I am having a hard time figuring out how best to group these. In pseudo code I basically I would have (running over a 30 day time frame) : </p>

<pre><code>  index=""some_index"" | where count &gt; n | group by hour
</code></pre>

<p>Hopefully this makes sense, if not, I am happy to provide some clarification. </p>

<p>Thanks in advance</p>",51750890.0,1,0,,2018-8-8 15:14:18,3.0,2018-8-8 15:52:36,,,,,5831886.0,,1,3,splunk|splunk-query,9258,28
95,258551,51799979,Splunk - Disabling alerts during maintenance window,"<p>I have a simple cvs file loaded in splunk called StandardMaintenance.csv which looks like this...</p>

<pre><code>UnderMaintenance
NO
</code></pre>

<p>We currently get bombarded with alerts during our maintenance window.  At the start of maintenance, I want to be able to change this to YES to stop the alerts (I have an easy way to do this).  I am looking for something standard to add to all alert queries that check this csv for status (lookup as I understand it) and for the query to return nothing if UnderMaintenance = YES, thus not generate a match to the query.  </p>

<p>It is basically a binary, ON or OFF.  I would appreciate any help you could provide.</p>",,2,0,,2018-8-11 13:05:21,,2020-9-9 01:48:34,,,,,6222635.0,,1,2,splunk,2158,15
96,258552,51824950,AWS ECS Fargate Splunk Logging,"<p>The documentation : <a href=""https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_LogConfiguration.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_LogConfiguration.html</a> 
says that the default config does not support Splunk logging for Fargate. 
Does anyone know of a workaround for this? I am looking to send the logs from multiple fargate containers to a central Splunk instance.</p>",55981711.0,2,0,,2018-8-13 14:42:11,1.0,2019-5-4 10:43:48,,,,,6242398.0,,1,2,logging|splunk|aws-fargate,1452,14
97,258553,51836308,how to run a splunk search for every found orderid,"<p>I want to first select every ORDERID and then perform a search to every ORDERID I found for that day. </p>

<p>So i need a foreach ORDID. I found just the map command, but it wont work the way I did it.</p>

<p>for my search should be smth like that:</p>

<pre><code>`index=* sourcetype=dat ORDID!="""" |dedup ORDID| foreach ORDID| ...search
</code></pre>

<p>And i have to perform it for each ORDID seperatly. so I need to be able to choose the ORDID by smth like $ORDID$.</p>

<p>Thank you guys.</p>",,1,0,,2018-8-14 07:48:40,,2018-8-14 11:29:18,,,,,9108205.0,,1,0,search|foreach|splunk|spl,2294,13
98,258554,51843992,embeding conf files into helm chart,"<p>Im new at helm. Im building a splunk helm chart with numerous conf files. I currently  use something like this in a configmap ..</p>

<pre><code>    apiVersion: v1
kind: ConfigMap
metadata:
  name: splunk-master-configmap
data:
  indexes.conf: |
    # global settings
    # Inheritable by all indexes: no hot/warm bucket can exceed 1 TB.
    # Individual indexes can override this setting.
    homePath.maxDataSizeMB = 1000000
</code></pre>

<p>but I would prefer to have the conf files in a seperate folder e.g. configs/helloworld.conf and have come accross ""tpl"" but am struggling to understand how to implement it. - can anyone advise best practices. On a side note splunk has orders of presidences >> so there may be many indexes.conf files used in various locations. does anyone have any thoughts on how best to implement this?!??!</p>

<p>Cheers.</p>",51847086.0,1,0,,2018-8-14 14:38:03,2.0,2018-8-24 19:38:28,2019-3-18 19:07:12,,-1.0,,2700411.0,,1,4,templates|kubernetes|splunk|configmap|kubernetes-helm,9848,26
99,258555,51856594,Free device logs for analysis,"<p>I am setting up HOME SIEM lab using SPLUNK. I am looking for sources which can provide different logs for various devices but not limited for below ones.</p>

<ul>
<li>Windows Logs </li>
<li>IIS Logs</li>
<li>IDS/IPS Logs</li>
</ul>

<p>Based on the logs i am planning to build search queries for various events and further using the same to build the rules.</p>",,2,0,,2018-8-15 09:55:13,,2018-12-9 03:10:02,,,,,3196630.0,,1,-1,splunk|kibana-4|log-analysis,33,5
100,258556,51875286,Splunk how to combine two queries and get one answer,"<p>I am very new to Splunk and basically been dropped in the deep end!! also very new to language so any help and tips on the below would be great.</p>

<p>The out come i am trying to get is to join the queries and get Username, ID and the amount of logins.</p>

<p>The queries are from diff source, sourcetype and host.</p>

<p>Query 1 is Username and ID and Query 2 is Username and Count of logins. </p>

<p>Query 1:
userName=""<em>"" entityNumber=""</em>"" | eval userName=upper(userName) | dedup userName, entityNumber | rename userName as User | table User, entityNumber </p>

<p>Query 2:
 ""Successfully logged in.""   | rex field=_raw  ""User[\"":]<em>(?[^\""IP]</em>)""| eval User=upper(User) |  Table User | stats count by User </p>

<p>Thanks in advance for your help.
J</p>",,2,1,,2018-8-16 10:51:26,1.0,2019-8-2 12:01:15,2018-9-12 15:29:48,,10233817.0,,10233817.0,,1,2,splunk|splunk-query|splunk-calculation|splunk-formula|splunk-sdk,28862,22
101,258557,51876110,Splunk - Collecting Microsoft Windows DHCP Server Operational Event Logs,"<p>We would like to collect Microsoft Windows DHCP Server Operational Event Logs into Splunk and seem to be having some trouble.</p>
<p>The path to the logs that we're interested in within the windows Event Viewer navigational tree is</p>
<pre><code>&gt; Applications and Services Logs
 &gt; Microsoft
  &gt; Windows
   &gt; DHCP-Server
    -  Microsoft-Windows-DHCP Server Events/Operational
</code></pre>
<p>We believe that the problem is with our input configuration and we've tried a number of different configurations in inputs.conf which seem appropriate for this app including the following but we're yet to receive any events.</p>
<pre><code>[WinEventLog://Microsoft-Windows-DHCP Server Events/Operational]
</code></pre>
<p>and</p>
<pre><code>[WinEventLog://Microsoft-Windows-DHCP-Server/Microsoft-Windows-DHCP Server Events/Operational]
</code></pre>
<p>These configurations have resulted in the following errors:</p>
<blockquote>
<p>message from &quot;&quot;C:\Program Files\SplunkUniversalForwarder\bin\splunk-winevtlog.exe&quot;&quot; splunk-winevtlog - WinEventMon::configure: Failed to find Event Log with channel name='Microsoft-Windows-DHCP Server Events/Operational'</p>
<p>message from &quot;&quot;C:\Program Files\SplunkUniversalForwarder\bin\splunk-winevtlog.exe&quot;&quot; splunk-winevtlog - WinEventMon::configure: Failed to find Event Log with channel name='Microsoft-Windows-DHCP-Server/Microsoft-Windows-DHCP Server Events/Operational'</p>
</blockquote>
<p>Any help on this would be greatly appreciated.</p>
<p>Thanks.</p>",,1,0,,2018-8-16 11:36:32,,2018-11-19 01:08:00,2020-6-20 09:12:55,,-1.0,,10233803.0,,1,0,splunk,1026,12
102,258558,51967993,Extracting 5 fields from logfile containing a string in Splunk,"<p>Below is a sample log file data:</p>

<pre><code>08/22/2018 02:50:06.380 EDT-0400 2 TCP/IP Controller Plugin.Transmitter pool thread &lt;Regular:2&gt;.CybTargetHandlerChannel.call[:695] - Message has been sent: 20180822 02500636+0400 C7STA PLINUX03 ALOPMTA2.N01834/LO.S00001D182340248/MAIN State EXEC SetStart Status(Executing at PLINUX03) Jobno(34523) ChildPid(34527)  User(PLINUX03) Host(localhost)
08/22/2018 02:50:06.382 EDT-0400 5 TCP/IP Controller Plugin.Transmitter pool thread &lt;Regular:2&gt;.CybTargetHandlerChannelLogHelper.logConnectionClose[:133] - Conversation with C7STA closed
08/22/2018 02:51:21.761 EDT-0400 5 TCP/IP Controller Plugin.Transmitter pool thread &lt;Regular:1&gt;.CybTargetHandlerChannel.call[:666] - Attempting to send:    20180822 02512176+0400 C7STA PLINUX03 ALOECPC7.N01745/LO.S00002D182340242/MAIN State COMPLETE Cmpc(0) SetEnd  User(PLINUX03) Host(localhost)
08/22/2018 02:51:21.771 EDT-0400 2 TCP/IP Controller Plugin.Transmitter pool thread &lt;Regular:1&gt;.CybTargetHandlerChannel.call[:695] - Message has been sent: 20180822 02512176+0400 C7STA PLINUX03 ALOECPC7.N01745/LO.S00002D182340242/MAIN State COMPLETE Cmpc(0) SetEnd  User(PLINUX03) Host(localhost)
</code></pre>

<p>I was trying to extract five fields below from the first and fourth line which contains ""Message has been sent"":</p>

<ol>
<li>TimeStamps: 20180822 02500636+0400, 20180822 02512176+0400</li>
<li>JobNames : ALOPMTA2,ALOECPC7</li>
<li>JobNumbers : 01834,1745</li>
<li>Users : User(PLINUX03), User(PLINUX03)</li>
<li>Statuses : MAIN State EXEC SetStart, MAIN State COMPLETE</li>
</ol>

<p>I was able to filter lines containing ""Message has been sent:"" using below expression, but was not sure on extracting 5 fields from this line:</p>

<pre><code>^.*\b(Message has been sent:.)\b.*$
</code></pre>

<p>Can someone help? This is for extraction on Splunk. Thank you!</p>",51969068.0,1,4,,2018-8-22 13:27:05,,2018-8-23 07:23:29,2018-8-22 13:42:43,,1075247.0,,10259814.0,,1,3,regex|splunk|regex-group,773,13
103,258559,51991313,Query to find the unique code in splunk,"<p>can some one suggest a query to send the unique errorcode count.</p>

<p>Example  <a href=""https://i.stack.imgur.com/lDb34.png"" rel=""nofollow noreferrer"">enter image description here</a>  2006 </p>

<p>in between the tags(in place of 2006) different codes are printed
i need to query to pull all the unique error codes </p>",,1,0,,2018-8-23 17:29:58,,2018-8-23 18:41:11,2018-8-23 17:35:53,,10156149.0,,10156149.0,,1,0,splunk|splunk-query|splunk-calculation,417,11
104,258560,51992268,Calling external rest api from Splunk search query,"<p>I am looking for if there is anyway I can call an external REST api (GET call, sample url shown below) from the Splunk search query panel, parse the JSON response and display in a table. </p>

<pre><code>https://jsonplaceholder.typicode.com/posts
</code></pre>

<p>Does Splunk allow this, directly from Search query without using any plugins/add-ons?</p>",,1,0,,2018-8-23 18:37:24,1.0,2021-12-3 02:39:41,,,,,6515567.0,,1,2,splunk|splunk-query,543,11
105,258561,52040311,Is it possible to measure Sharepoint Online page load times with Splunk?,<p>Is it possible to measure Sharepoint Online page load times with Splunk? I know it can be done with Application Insights. But I need to know if it's possible with Splunk</p>,52042854.0,1,0,,2018-8-27 13:18:02,,2018-8-27 15:47:26,,,,,4634206.0,,1,0,performance|performance-testing|sharepoint-online|splunk|pageload,60,8
106,258562,52066447,AWS Batch Logs to splunk,"<p>I am using AWS Batch Service for my job. i want to send the logs generated from AWS Batch directly to Splunk instead of sending that to cloud-watch. How can i configure log-driver in AWS Batch to achieve this?</p>

<p>-ND</p>",,2,0,,2018-8-28 21:29:47,,2018-11-9 21:18:09,,,,,2932707.0,,1,1,amazon-cloudwatch|splunk|aws-batch,642,11
107,258563,52100462,How to embed a timechart visualization from a Splunk query into a web app?,"<p>I have the following Splunk query that produces the following visualization:</p>

<p><a href=""https://i.stack.imgur.com/FsmAo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FsmAo.png"" alt=""enter image description here""></a></p>

<p>I would like to embed this exact visualization into a web app using an iframe. How can I do this?</p>

<p>Currently, I am using the Splunk HTTP REST API and can submit &amp; get the results back from this search using the following endpoint:
<code>/servicesNS/nameofmyapp/nameofmynamespace/search/jobs/1535641234.45678?output_mode=json</code></p>

<p>However, I would like a src URL to the visualization that can be embedded in an iframe. Is there an endpoint that I can query instead that will give the URL to the visualization?</p>",52229325.0,1,1,,2018-8-30 15:18:23,,2018-9-7 20:34:04,,,,,9268003.0,,1,0,iframe|splunk|splunk-query|splunk-sdk,279,10
108,258564,52116369,What does the source tag mean in HttpEventCollectorLogbackAppender?,"<p>I'm using HttpEventCollectorLogbackAppender for writing my java application logs to the splunk server. I've been trying this for very long and still haven't been able to get my logs into splunk. </p>

<p>Can someone please explain what does the <strong>source</strong> tag refers to in the HttpEventLogbackAppender? </p>

<p>Below is the HttpEventLogbackAppender in my logback.xml file:</p>

<pre><code>&lt;appender name=""splunk-httpeventcollector-appender""
          class=""com.splunk.logging.appenders.logback.HttpEventCollectorLogbackAppender""&gt;
    &lt;url&gt;${SPLUNK_HOST_URL}&lt;/url&gt;
    &lt;host&gt;${CFG_DC}_${APP_ENV}_${CONTAINER_ID}&lt;/host&gt;
    &lt;token&gt;${SPLUNK_TOKEN}&lt;/token&gt;
    &lt;source&gt;&lt;/source&gt; // what does this refer to?
     &lt;index&gt;${SPLUNK_INDEX}&lt;/index&gt;

    &lt;disableCertificateValidation&gt;true&lt;/disableCertificateValidation&gt;
    &lt;layout class=""ch.qos.logback.classic.PatternLayout""&gt;
        &lt;Pattern&gt;%d{ISO8601} [%thread] loglevel=%-5level %logger{36} - remotehost=%mdc{req.remoteHost} forwardedfor=%mdc{req.xForwardedFor} requestmethod=%mdc{req.method} requesturi=%mdc{req.requestURI}&lt;/Pattern&gt;
    &lt;/layout&gt;
    &lt;batch_size_count&gt;500&lt;/batch_size_count&gt;
    &lt;send_mode&gt;parallel&lt;/send_mode&gt;

&lt;/appender&gt;
</code></pre>",,2,0,,2018-8-31 13:20:28,,2019-12-20 13:17:42,,,,,1444360.0,,1,0,java|logback|slf4j|splunk,445,10
109,258565,52230037,sqlite3 fetching data based on epoch time,"<p>I am new to sqlite3 and I created an sqlite3 database named result that has _time table that lists IP addresses based on epoch time I queried from Splunk. I am trying to fetch data based on _time (epoch time) table from my sqlite3 database.</p>

<p>Here is a copy of _time and IP addresses listed in sqlite3 database: </p>

<pre><code>1535139799|2002:8672:5ebf::8672:5ebf
1535139799|2002:8672:5ebf::8672:5ebf
1535131073|2002:8672:ba8a::8672:ba8a
1535131058|2002:8672:ba8a::8672:ba8a
1535131058|2002:8672:ba8a::8672:ba8a
1535131413|2002:8672:5ebf::8672:5ebf
1535131413|2002:8672:5ebf::8672:5ebf
1535120613|2002:8672:5ebf::8672:5ebf
1531944594|41.212.170.179
1531944594|38.108.250.243
</code></pre>

<p>The problem is, no matter the commands I've tried, it always outputs ALL of the IP address. I just need the IP addresses from x number of days (30 days ago). I've looked <a href=""https://stackoverflow.com/questions/11771580/deleting-android-sqlite-rows-older-than-x-days"">online</a> and followed many examples but still no success. I've also looked at <a href=""https://www.sqlite.org/lang_datefunc.html"" rel=""nofollow noreferrer"">documentation</a>.
I've tried:</p>

<pre><code>SELECT * FROM result WHERE _time &lt; (DATE('now', '-5 day'));
SELECT * FROM result WHERE _time &gt; (DATE('now', '-5 days')); 
SELECT * FROM result WHERE _time &gt; (SELECT DATE('now', '-7 day')) 
</code></pre>

<p>I've changed the number of 'days', but the commands still fetch all of the listed IP address. I just need to fetch IP addresses from x days ago (10 days ago or 30 days ago). I am assuming that sqlite3 cannot read epoch time in my tables?</p>

<p>I am stuck in this problem for days now. I will continue to look for a solution, I appreciate any feedback!</p>",52239595.0,1,3,,2018-9-7 21:53:42,,2018-9-8 21:37:17,2018-9-7 22:09:18,,10151817.0,,10151817.0,,1,1,python|python-3.x|sqlite|epoch|splunk,68,8
110,258566,52273171,DB Connect task server not connection,"<p>I have installed DB connect app on my Splunk. Am facing Cannot communicate with the task server, please check your settings issue. I found below error in the logs.</p>

<blockquote>
  <p>ERROR ExecProcessor - message from
  ""/home/iapsp02/etc/apps/splunk_app_db_connect/linux_x86_64/bin/server.sh""
  com.splunk.modularinput.Event.writeTo(Event.java:65)\com.splunk.modularinput.EventWriter.writeEvent(EventWriter.java:134)\com.splunk.dbx.server.bootstrap.TaskServerStart.streamEvents(TaskServerStart.java:77)\com.splunk.modularinput.Script.run(Script.java:66)\com.splunk.modularinput.Script.run(Script.java:44)\com.splunk.dbx.server.bootstrap.TaskServerStart.main(TaskServerStart.java:150)\</p>
</blockquote>

<p>and other error.</p>

<blockquote>
  <p>ERROR ExecProcessor - message from
  ""/home/iapsp02/etc/apps/splunk_app_db_connect/linux_x86_64/bin/server.sh""
  03:08:51.576 [main] INFO com.splunk.dbx.utils.TrustManagerUtil -
  action=load_key_manager_succeed</p>
</blockquote>",,0,1,,2018-9-11 09:47:38,,2018-9-11 12:02:59,2018-9-11 12:02:59,,7073340.0,,7364601.0,,1,0,splunk,2332,13
111,258567,52285211,"In Splunk, How to use parsed time to determine duration between events?","<p>I have a log file that records the start and finish times for processing files. The entries contain strings that look like this:</p>

<pre><code>=============== STARTED PROCESSING FILE filename at Thu Jul 19 00:03:55 2018 EDT===============
=============== FINISHED PROCESSING FILE filename at Thu Jul 19 00:04:05 2018 EDT===============
</code></pre>

<p>Initially I came up with a query using _time:</p>

<pre><code>processing.log ""FINISHED PROCESSING FILE"" OR ""STARTED PROCESSING FILE"" | rex field=_raw ""(?&lt;filename&gt;\S*)"" | stats count first(_time) as start last(_time) as finished by filename | eval duration = abs( finished - start)
</code></pre>

<p>That seemed to work just fine until I realized that _time of finished and start could be hours apart even though actual processing to 10 seconds (as in the above example. So now I am trying this query:</p>

<pre><code>processing.log ""FINISHED PROCESSING FILE"" OR ""STARTED PROCESSING FILE"" | rex field=_raw ""(?&lt;filename&gt;\S*) at (?&lt;ptime&gt;.*) EDT"" | eval stime=strptime(ptime,'%a %B %d %Y %H:%M:%S')| stats count first(stime) as start last(stime) as finished by filename | eval duration = abs( finished - start)
</code></pre>

<p>However it is not providing the desired results of showing the processing duration of each filename. What am I doing wrong/how can I fix this?</p>",,1,0,,2018-9-11 22:56:33,,2018-9-12 01:36:53,,,,,1198716.0,,1,0,eval|splunk|strptime,258,9
112,258568,52286485,Parse Deep Security Logs - AWS Lambda 'splunk-logger' node.js,"<p>I am trying to modify a Node.js function called 'splunk-logger'. The problem is that when the SNS Message comes into the function, the events from the Anti-Virus (Trend Micro DeepSecurity) console are grouped together. I already contacted their support and they said this is just the way events are sent and they can't help.</p>

<p>Example: {Message {Event_1} {Event_2} {Event_3}}</p>

<p>Now the JavaScript function works great and the events are forwarded to Splunk. However, since they are grouped together BEFORE they even hit the Lambda function, Splunk sees them as 1 single event instead of 3.</p>

<p>My thought is to take the 'event' variable (since it contains the sns 'message') and parse through that to separate each event (probably using regex or something). Then, I can either create another function to send each event immediately or simply call the ""logger.flushAsync"" function to send them.</p>

<p>Link to splunk-dev explaining the funciton: <a href=""http://dev.splunk.com/view/event-collector/SP-CAAAE6Y#create"" rel=""nofollow noreferrer"">http://dev.splunk.com/view/event-collector/SP-CAAAE6Y#create</a>.</p>

<p>Here is the code from the index.js: </p>

<pre><code>const loggerConfig = {
    url: process.env.SPLUNK_HEC_URL,
    token: process.env.SPLUNK_HEC_TOKEN,
};
const SplunkLogger = require('./lib/mysplunklogger');
const logger = new SplunkLogger(loggerConfig);
exports.handler = (event, context, callback) =&gt; {
    console.log('Received event:', JSON.stringify(event, null, 2));
// Log JSON objects to Splunk
    logger.log(event);
// Send all the events in a single batch to Splunk
    logger.flushAsync((error, response) =&gt; {
        if (error) {
            callback(error);
        } else {
            console.log(`Response from Splunk:\n${response}`);
            callback(null, event.key1); // Echo back the first key value
        }
    });
};
</code></pre>

<p>Here is the code from the mysplunklogger.js file.</p>

<pre><code>'use strict';

const url = require('url');

const Logger = function Logger(config) {
    this.url = config.url;
    this.token = config.token;

    this.addMetadata = true;
    this.setSource = true;

    this.parsedUrl = url.parse(this.url);
    // eslint-disable-next-line import/no-dynamic-require
    this.requester = require(this.parsedUrl.protocol.substring(0, this.parsedUrl.protocol.length - 1));
    // Initialize request options which can be overridden &amp; extended by consumer as needed
    this.requestOptions = {
        hostname: this.parsedUrl.hostname,
        path: this.parsedUrl.path,
        port: this.parsedUrl.port,
        method: 'POST',
        headers: {
            Authorization: `Splunk ${this.token}`,
        },
        rejectUnauthorized: false,
    };

    this.payloads = [];
};

// Simple logging API for Lambda functions
Logger.prototype.log = function log(message, context) {
    this.logWithTime(Date.now(), message, context);
};

Logger.prototype.logWithTime = function logWithTime(time, message, context) {
    const payload = {};

    if (Object.prototype.toString.call(message) === '[object Array]') {
        throw new Error('message argument must be a string or a JSON object.');
    }
    payload.event = message;

    // Add Lambda metadata
    if (typeof context !== 'undefined') {
        if (this.addMetadata) {
            // Enrich event only if it is an object
            if (message === Object(message)) {
                payload.event = JSON.parse(JSON.stringify(message)); // deep copy
                payload.event.awsRequestId = context.awsRequestId;
            }
        }
        if (this.setSource) {
            payload.source = `lambda:${context.functionName}`;
        }
    }

    payload.time = new Date(time).getTime() / 1000;

    this.logEvent(payload);
};

Logger.prototype.logEvent = function logEvent(payload) {
    this.payloads.push(JSON.stringify(payload));
};

Logger.prototype.flushAsync = function flushAsync(callback) {
    callback = callback || (() =&gt; {}); // eslint-disable-line no-param-reassign

    console.log('Sending event(s)');
    const req = this.requester.request(this.requestOptions, (res) =&gt; {
        res.setEncoding('utf8');

        console.log('Response received');
        res.on('data', (data) =&gt; {
            let error = null;
            if (res.statusCode !== 200) {
                error = new Error(`error: statusCode=${res.statusCode}\n\n${data}`);
                console.error(error);
            }
            this.payloads.length = 0;
            callback(error, data);
        });
    });

    req.on('error', (error) =&gt; {
        callback(error);
    });

    req.end(this.payloads.join(''), 'utf8');
};

module.exports = Logger;
</code></pre>",,2,0,,2018-9-12 02:07:17,,2018-12-2 06:42:22,2018-10-25 20:34:46,,2307622.0,,4499362.0,,1,0,javascript|node.js|aws-lambda|splunk|deepsecurity,750,12
113,258569,52339681,SplunkForwarder in Docker container does not take authentication from CLI,"<p>I am trying to run Splunk in docker container and do not want to manually type username and password. So, I am using CLI command in a script: 
/opt/splunkforwarder/bin/splunk --accept-license --answer-yes --no-prompt --auth admin: start</p>

<p>This throws me error stating: 
Command error: The subcommand 'admin:' is not valid for command '-auth'.</p>",52346568.0,1,0,,2018-9-14 21:57:43,,2018-9-15 16:18:44,,,,,3893382.0,,1,0,docker|splunk,172,8
114,258570,52362787,Trigger web-hook when background search job completes on Splunk enterorice,<p>I trying to automate Splunk search and export to CSV using Java SDK. Is it possible to webhook a URL from Splunk when the background job that I have started will be completed? So that I can export the CSV without waiting for the completion of search job.</p>,,1,0,,2018-9-17 07:31:11,,2018-10-12 02:36:22,,,,,4742425.0,,1,0,java|splunk,35,6
115,258571,52373376,Splunk Web only monitoring files from /var/log directory of forwarder,<p>My splunk web only monitoring files from /var/log directory of forwarder and when I am trying to add other files from different directory using add monitor command it is not appearing on Splunk web. Please help! </p>,52403415.0,1,9,,2018-9-17 17:57:14,0.0,2018-9-19 10:10:25,,,,,8604431.0,,1,0,splunk,285,10
116,258572,52385476,Send splunk alert results to SNMP server using python script,<p>I could see Perl script to send Splunk alert results to SNMP server. But i want to write python script which needs to send results to Snmp server.</p>,,1,0,,2018-9-18 11:22:10,,2018-9-25 12:17:13,2018-9-25 10:39:09,,590848.0,,10379972.0,,1,0,python|snmp|splunk,87,7
117,258573,52441129,splunk check if message contains certain string,"<p>In Splunk search query how to check if log message has a text or not?</p>

<p><strong>Log message:</strong></p>

<pre><code>message:     2018-09-21T07:15:28,458+0000 comp=hub-lora-ingestor-0 [vert.x-eventloop-thread-0] INFO  com.nsc.iot.hono.receiver.HonoReceiver - Connected successfully, creating telemetry consumer ...
</code></pre>

<p>and I want to check if message contains <strong>""Connected successfully, creating telemetry consumer ...""</strong> and based on this want to assign 1 or 0 to a variable</p>

<p><strong>Splunk search Query</strong></p>

<pre><code>(index=""05c48b55-c9aa-4743-aa4b-c0ec618691dd"" (""Retry connecting in 1000ms ..."" OR ""Connect or create consumer failed with exception"" OR ""Connected successfully, creating telemetry consumer ..."")) 
| rex field=_raw ^(?:[^ \n]* ){7}(?P&lt;success_status_message&gt;\w+\s+\w+,\s+\w+\s+\w+\s+\w+)""
| timechart count as status | eval status=if(isnull(success_status_message), 0, 1)
</code></pre>

<p>success_status_message is always null</p>",52459385.0,1,2,,2018-9-21 09:55:00,,2018-9-22 17:56:46,2018-9-22 03:09:22,,2742156.0,,2742156.0,,1,2,java|devops|splunk|splunk-query|splunk-calculation,24929,19
118,258574,52506433,Replace for loop with forEach function,"<p>I would like to replace for loop with a <code>forEach</code> function as the loop is not working in Splunk JavaScript.</p>

<p>The reason the for loop is not working is that my JavaScript code is embedded in XML, and when I use the <code>&lt;</code> or <code>&gt;</code> characters in my JavaScript code, I get an error due to them.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>function myFunction() {
  // Declare variables
  var input, filter, ul, li, a, i;
  input = document.getElementById(""mySearch"");
  filter = input.value.toUpperCase();
  ul = document.getElementById(""myMenu"");
  li = ul.getElementsByTagName(""li"");

  // Loop through all list items, and hide those who don't match the search query
  for (i = 0; i &lt; li.length; i++) {
    a = li[i].getElementsByTagName(""a"")[0];
    if (a.innerHTML.toUpperCase().indexOf(filter) &gt; -1) {
      li[i].style.display = """";
    } else {
      li[i].style.display = ""none"";
    }
  }
}</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;input type=""text"" id=""mySearch"" onkeyup=""myFunction()"" placeholder=""Search.."" title=""Type in a category""&gt;

&lt;ul id=""myMenu""&gt;
  &lt;li&gt;&lt;a href=""#""&gt;HTML&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=""#""&gt;CSS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=""#""&gt;JavaScript&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=""#""&gt;PHP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=""#""&gt;Python&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=""#""&gt;jQuery&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=""#""&gt;SQL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=""#""&gt;Bootstrap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=""#""&gt;Node.js&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</code></pre>
</div>
</div>
</p>",,1,14,,2018-9-25 20:45:23,,2018-9-26 00:58:53,2018-9-26 00:58:53,,2234742.0,,7837945.0,,1,0,javascript|splunk|splunk-sdk,286,9
119,258575,52534884,What algorithms Splunk use for password encryption,"<p>Please help me to restore or change my Splunk enterprise password. I forgot Splunk username and password and I don't want to reinstall it cause I am having my data on it. So please help me to crack it. I tried all available ways which I got in google search, But nothing works. </p>",,1,0,,2018-9-27 10:27:25,,2018-9-27 12:57:06,,,,,10423617.0,,1,0,splunk,612,11
120,258576,52584335,"have a join query which i need to optimize using OR and Stats, i am new to splunk and i am confused how to start","<p><code>index=""index1"" sourcetype=sourcetype1 | join commonfield [ search &lt;br&gt;index=""index2"" sourcetype=sourcetype2 ] | sort _time | stats &lt;br&gt;last(index1field1) as state by index2field1, index1field2, index1field3 &lt;br&gt;| where index1field1 != ""UP"" | dedup index2field1 | stats count</code></p>

<p>I want to optimize this query without join using stats and OR, can anyone help me?</p>",,1,2,,2018-10-1 03:48:35,,2018-10-10 03:22:56,2018-10-1 07:15:16,,5841607.0,,10439129.0,,1,0,query-optimization|splunk|splunk-query,307,9
121,258577,52631413,Splunk query to extract string inside XML tag not working with spath command,"<p>I am using the following query:</p>

<pre><code>index=itx ""PAD ="" | dedup BOC | spath output=Channel path=AsRunMessage.Header.Channel  | table BOC, channel
</code></pre>

<p>which results in events with big xml content .. I need to extract the string ""ITX1546"" from inside the  tags.
Also I need to create a table with distinct rows containing unique BOC values.
The Channel field is not being populated.
Here is the XML structure:</p>

<p><a href=""https://i.stack.imgur.com/uGzyi.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uGzyi.gif"" alt=""enter image description here""></a></p>

<p>Any ideas? Thank you</p>",52636631.0,1,0,,2018-10-3 16:09:43,,2018-10-4 23:59:32,,,,,1457340.0,,1,0,xml|splunk,1643,12
122,258578,52638890,Difference in Splunk and JMeter percentile,"<p>I did performance testing for web service and in my service i calculated timeTaken using java and logged in splunk.</p>

<p>I am comparing the splunk and jmeter reports and see the difference in Average,Median,90%Line,95%Line,99%Line,Min and Max</p>

<p>Is this difference expected in splunk and Jmeter report?</p>",52639285.0,1,0,,2018-10-4 04:12:49,,2018-10-4 04:57:04,,,,,2196474.0,,1,1,java|jmeter|performance-testing|splunk,180,11
123,258579,52645827,Merge events by time to create a table for 3D Scatterplot,"<p>I have a list of events, with the following content</p>

<pre><code>event1: _time=123 Tag=""X"" Value=""12.2""
event2: _time=123 Tag=""Y"" Value=""55.2""
event3: _time=123 Tag=""Z"" Value=""3.2""
event4: _time=234 Tag=""X"" Value=""12.4""
event5: _time=234 Tag=""Y"" Value=""55.0""
event6: _time=234 Tag=""Z"" Value=""2.8""
</code></pre>

<p>The values are coordinates (X, Y, Z), that i want to visualize in a 3d scatter plot. Unfortunately i have each coordinate in a single event. </p>

<p>How can i merge those events to create a table afterwards with</p>

<pre><code>(wanted command) | table _time X Y Z
</code></pre>

<p>???</p>",52731968.0,1,0,,2018-10-4 11:42:36,,2018-10-10 02:59:48,,,,,10039517.0,,1,0,splunk|splunk-query,180,10
124,258580,52667762,convert splunk api curl command to python,"<p>I can run a splunk api call in bash and get back a SID which I then use to get back a splunk query. The first part of it is below. However, I am having issues when changing this to a python request using <code>requests</code>. I keep getting an <code>ssl CERTIFICATE_VERIFY_FAILED</code> error.</p>

<p>Bash Command</p>

<pre><code>data=$( curl -k -u username:password https://&lt;splunk_endpoint&gt;/services/search/jobs -d 'search=search earliest=-1m index=_internal')
echo $data
</code></pre>

<p>Bash Output:
      1538748227.228319_D07875A9-FDD6-46E8-BE77-EDF9BD9A73B1 </p>

<p>python requests</p>

<pre><code>import requests

baseurl = 'https://&lt;splunk_endpoint&gt;/services/search/jobs'

headers = {
    ""Content-Type"": ""application/json"",
}

data = {
    'username': 'username',
    'password': 'password',
    ""search"": ""search earliest=-1m index=_internal"",
}

r = requests.get(baseurl, data=json.dumps(data), headers=headers)
print(r.json())
</code></pre>

<p>I'm not exactly sure where to put the username and password. Does that belong in 'data'? in headers? somewhere else? I also don't know if my -d conversino to the data dictionary is correct. I think it is.</p>

<p>Any thoughts</p>",52718572.0,2,2,,2018-10-5 14:24:01,,2020-11-18 10:32:43,,,,,6626531.0,,1,1,python|python-requests|splunk,1467,14
125,258581,52689758,Deploying Splunk Docker image fails to login,"<p>I'm using <code>splunk/splunk</code> docker image with the following commands to build and run, as I learned from the <a href=""https://github.com/splunk/docker-splunk"" rel=""nofollow noreferrer"">repository README</a>:</p>

<pre><code>docker build --network=$DOCKER_NETWORK -t my-splunk .

docker run \
    --name=my-splunk \
    --network=$DOCKER_NETWORK \
    -p 8000:8000 \
    -e SPLUNK_START_ARGS=""--accept-license"" \
    -e SPLUNK_PASSWORD=""1234"" \
    --restart unless-stopped \
    -d my-splunk
</code></pre>

<p>When I open my browser on <code>localhost:8000</code>, I'm getting the Splunk login page as expected, however when typing <code>admin</code> and <code>1234</code>, I'm getting error 401:  </p>

<blockquote>
  <p>No users exist. Please set up a user.</p>
</blockquote>

<p>Taking a look at <code>/opt/splunk/etc/system/local/user-seed.conf</code> from within the container, content looks good:</p>

<pre><code>[user_info]
USERNAME = admin
PASSWORD = 1234
</code></pre>

<p>Full docker logs:</p>

<pre><code>PLAY [localhost] ***************************************************************

TASK [Gathering Facts] *********************************************************
Sunday 07 October 2018  17:39:17 +0300 (0:00:00.091)       0:00:00.091 ********
ok: [localhost]

TASK [include_role : splunk_upgrade] *******************************************
Sunday 07 October 2018  17:39:18 +0300 (0:00:01.821)       0:00:01.913 ********

TASK [include_role : {{ splunk.role }}] ****************************************
Sunday 07 October 2018  17:39:19 +0300 (0:00:00.031)       0:00:01.945 ********

TASK [splunk_common : Install Splunk] ******************************************
Sunday 07 October 2018  17:39:19 +0300 (0:00:00.075)       0:00:02.021 ********
changed: [localhost]

TASK [splunk_common : Install Splunk (Windows)] ********************************
Sunday 07 October 2018  17:39:47 +0300 (0:00:28.257)       0:00:30.278 ********

TASK [splunk_common : Generate user-seed.conf] *********************************
Sunday 07 October 2018  17:39:47 +0300 (0:00:00.041)       0:00:30.320 ********
changed: [localhost] =&gt; (item=USERNAME)
changed: [localhost] =&gt; (item=PASSWORD)

TASK [splunk_common : include_tasks] *******************************************
Sunday 07 October 2018  17:39:47 +0300 (0:00:00.377)       0:00:30.697 ********
included: /opt/ansible/roles/splunk_common/tasks/enable_s2s_port.yml for localhost

TASK [splunk_common : Enable the Splunk-to-Splunk port] ************************
Sunday 07 October 2018  17:39:47 +0300 (0:00:00.062)       0:00:30.759 ********
changed: [localhost]

TASK [splunk_common : include_tasks] *******************************************
Sunday 07 October 2018  17:39:50 +0300 (0:00:02.733)       0:00:33.492 ********
included: /opt/ansible/roles/splunk_common/tasks/start_splunk.yml for localhost

TASK [splunk_common : Start Splunk] ********************************************
Sunday 07 October 2018  17:39:50 +0300 (0:00:00.054)       0:00:33.547 ********
changed: [localhost]

TASK [splunk_common : include_tasks] *******************************************
Sunday 07 October 2018  17:39:57 +0300 (0:00:07.214)       0:00:40.761 ********
included: /opt/ansible/roles/splunk_common/tasks/add_splunk_license.yml for localhost

TASK [splunk_common : Download Splunk license] *********************************
Sunday 07 October 2018  17:39:57 +0300 (0:00:00.056)       0:00:40.818 ********

TASK [splunk_common : Set downloaded license location] *************************
Sunday 07 October 2018  17:39:57 +0300 (0:00:00.037)       0:00:40.855 ********

TASK [splunk_common : Set local license location] ******************************
Sunday 07 October 2018  17:39:57 +0300 (0:00:00.034)       0:00:40.889 ********
ok: [localhost]

TASK [splunk_common : Apply Splunk license] ************************************
Sunday 07 October 2018  17:39:58 +0300 (0:00:00.043)       0:00:40.933 ********
fatal: [localhost]: FAILED! =&gt; {""changed"": true, ""cmd"": [""/opt/splunk/bin/splunk"", ""add"", ""licenses"", ""-auth"", ""admin:1234""], ""delta"": ""0:00:01.050830"", ""end"": ""2018-10-07 17:39:59.189296"", ""msg"": ""non-zero return code"", ""rc"": 4, ""start"": ""2018-10-07 17:39:58.138466"", ""stderr"": """", ""stderr_lines"": [], ""stdout"": ""missing PATH-TO-LICENSE-FILE argument: ./splunk add license [PATH-TO-FILE] "", ""stdout_lines"": [""missing PATH-TO-LICENSE-FILE argument: ./splunk add license [PATH-TO-FILE] ""]}
...ignoring

TASK [splunk_common : Set as license slave] ************************************
Sunday 07 October 2018  17:39:59 +0300 (0:00:01.213)       0:00:42.146 ********

TASK [include_role : splunk_search_head] ***************************************
Sunday 07 October 2018  17:39:59 +0300 (0:00:00.031)       0:00:42.178 ********

PLAY RECAP *********************************************************************
localhost                  : ok=10   changed=5    unreachable=0    failed=0

Sunday 07 October 2018  17:39:59 +0300 (0:00:00.048)       0:00:42.226 ********
===============================================================================
splunk_common : Install Splunk ----------------------------------------- 28.26s
splunk_common : Start Splunk -------------------------------------------- 7.21s
splunk_common : Enable the Splunk-to-Splunk port ------------------------ 2.73s
Gathering Facts --------------------------------------------------------- 1.82s
splunk_common : Apply Splunk license ------------------------------------ 1.21s
splunk_common : Generate user-seed.conf --------------------------------- 0.38s
include_role : {{ splunk.role }} ---------------------------------------- 0.08s
splunk_common : include_tasks ------------------------------------------- 0.06s
splunk_common : include_tasks ------------------------------------------- 0.06s
splunk_common : include_tasks ------------------------------------------- 0.05s
include_role : splunk_search_head --------------------------------------- 0.05s
splunk_common : Set local license location ------------------------------ 0.04s
splunk_common : Install Splunk (Windows) -------------------------------- 0.04s
splunk_common : Download Splunk license --------------------------------- 0.04s
splunk_common : Set downloaded license location ------------------------- 0.03s
include_role : splunk_upgrade ------------------------------------------- 0.03s
splunk_common : Set as license slave ------------------------------------ 0.03s
===============================================================================

Ansible playbook complete, will begin streaming var/log/splunk/splunkd_stderr.log
</code></pre>",52696725.0,1,1,,2018-10-7 15:01:06,1.0,2019-1-28 12:38:10,,,,,1236401.0,,1,1,docker|splunk,1075,13
126,258582,52695831,Table from logs in Splunk,"<p>I have an array of values coming directly from database from table with columns (""Name"", ""Email"",""ProductId"", Quantity). The data looks like this:</p>

<pre><code>{'data':[
    ('Tom', 'tom1234@gmail.com','P1231', 50),
    ('Rob', 'robasq@gmail.com','P6431', 100),
    ('Nick', 'nicasa@gmail.com','P3231', 40),
    ('Li', 'lichan@gmail.com','P1231', 60)]}
</code></pre>

<p>I want to use this array from the logs and display as a table or as a chart in dashboard in Splunk. Does someone know if this is possible in Splunk Enterprise. For eg</p>

<pre><code>Name           Email           ProductId     Quantity
Tom    tom1234@gmail.com        P1231         50
Rob    robasq@gmail.com         P6431        100
Nick   nicasa@gmail.com         P3231         40
Li     lichan@gmail.com         P1231         60
</code></pre>",,1,0,,2018-10-8 05:23:39,,2018-10-9 10:15:39,,,,,1733735.0,,1,1,splunk|splunk-query,157,8
127,258583,52700335,Display Splunk Timechart in Local Time,"<p>I have a very simple <code>timechart</code> query:</p>

<pre><code>index=""myIndex"" ""searchText""
| timechart span=1d
</code></pre>

<p>The Splunk logs are in UTC, but I need the chart to display the data in local time instead.</p>",52706432.0,1,0,,2018-10-8 10:30:51,,2018-10-8 16:22:23,,,,,2450507.0,,1,0,logging|splunk|splunk-query,718,15
128,258584,52743260,Output limit when using API Requests to pull Splunk Data to HDFS,"<p>I'm trying python requests to pull Splunk data into hdfs using an API Call. I don't know if this has anything to do with Splunk data itself or if it is a limitation of API Calls.</p>

<p>I am able to pull small amounts of data, but I tried to pull an hours worth of data and it only returned 100 records. In splunk, the same query returned 100K+ records.</p>

<p>Execute splunk query:</p>

<pre><code>import os
import requests
import sys
import time
import xml.etree.ElementTree as ET

data = {
    'search': search
}

r = requests.post(ENDPOINT, 
                  data=data, 
                  verify=False, 
                  auth=(username, password))`enter code here`

response_xml_as_string = r.text
responseXml = ET.fromstring(response_xml_as_string)
sid= responseXml.find('sid')
</code></pre>

<p>Check to see if it is done. If you get 0, rerun this until you get a 1:</p>

<pre><code>res = requests.get(ENDPOINT + '/{0}' .format(sid), 
               verify=False, 
               auth=(username, password))

root = ET.fromstring(res.text)
for child in root.iter():
    try:
        if child.attrib['name'] == 'isDone':
            is_done = child.text
    except:
        is_done=0
print(is_done)
</code></pre>

<p>Stream splunk data to hdfs:</p>

<pre><code>data = {
  'output_mode': 'csv',
  'count': '5'
}


r = requests.get(ENDPOINT + '/{0}/results' .format(sid), 
                   data=data, 
                   verify=False, 
                   auth=(username, password))

os.system('echo ""{0}"" | hdfs dfs -put - {1}' .format(r.text,hdfs_path))
</code></pre>

<p>I'm not exactly sure what <code>count:5</code> means in my last <code>data</code> dictionary. Can requests only pull in a certain number of records? The dataset is a very narrow  (3 columns) and so I don't think that it is a MB issue. There could be, but that's not what is happening here. I have a much bigger query that I need to run later and so I'd appreciate any insight into size or record limits for API Requests. If I write this to a text file on linux instead of to hdfs I still only get 100 records so I don't think the streaming part is the bottle neck.</p>",,1,0,,2018-10-10 15:04:01,1.0,2018-10-12 01:36:11,,,,,6626531.0,,1,1,python-requests|hdfs|python-3.5|splunk,693,11
129,258585,52759503,Extracting certain fields from Splunk query results,"<p>I want to print the value of a certain field from a set of events that results from running a particular search query. Here's my query:</p>

<pre><code>index=abc ""all events that contain this string"" sourcetype=prd
</code></pre>

<p>Now, this returns certain events that contain a field called <code>traceId</code>. What I want is to extract unique <code>traceId</code>s from the result and print them. Here's the query that I am using currently, but to no avail:</p>

<pre><code>index=abc ""all events that contain this string"" sourcetype=prd | rex field=_raw ""traceId: (?&lt;traceId&gt;.*)""
</code></pre>

<p>This query prints all the fields in the event (events are printed as JSON docs.).</p>

<p>Can someone help me with this? I have never worked with Splunk before, so please go easy if the question looks a bit easy.</p>

<p>Thanks!</p>",,1,2,,2018-10-11 11:57:36,,2018-10-11 22:44:43,,,,,6714426.0,,1,0,splunk,1384,13
130,258586,52763726,Set splunk alert to send alerts for each host,"<p>I am setting up my Splunk search/alert to search for an error in a group of servers. How can i set the alert to mail me when the error happens on multiple hosts but only once for each host? </p>

<p><strong>For example, if the error is continuously happening on two hosts from a group of 6, I need to get two alerts(one for each host) but only once till the next day. (for the once till the next day option I am using the throttling feature)</strong>
Is that possible with splunk? </p>",,2,0,,2018-10-11 15:24:22,,2018-10-11 20:06:48,,,,,2689092.0,,1,0,splunk,1946,13
131,258587,52783511,SQLite3 increment Count by 1 for every duplicate data,"<p>I am new to sqlite3. I imported in SQLite through python an exported CSV file listing IP addresses from Splunk and I plan on increasing the count column on my database every time a similar IP address gets recognized. </p>

<p>What I had in mind was to use <a href=""http://sqlite.awardspace.info/syntax/sqlitepg09.htm"" rel=""nofollow noreferrer"">SQLite CASE statement</a>, <a href=""http://www.sqlitetutorial.net/sqlite-case/"" rel=""nofollow noreferrer"">documentation</a>, <a href=""https://stackoverflow.com/questions/2717590/sqlite-insert-on-duplicate-key-update"">update statement</a>, etc. I tried:</p>

<pre><code>SELECT * CASE WHEN src_ip = src_ip THEN UPDATE table SET Count = Count + 1;
</code></pre>

<p>also tried,</p>

<pre><code>UPDATE table SET Count = Count + 1 WHERE src_ip = src_ip;
</code></pre>

<p>I know I'm wrong I can't quite figure this problem out for days.
Here's what my sqlite3 database looks like in cmd prompt:</p>

<pre><code>sqlite&gt; select * from result;
1537536602|2002:8672:d515::8672:d515|
1537288499|150.135.165.114|
1537536602|2002:8672:d515::8672:d515|
1537288499|150.135.165.114|
sqlite&gt;
sqlite&gt; .schema
CREATE TABLE result (_time STR, 'src_ip' STR, Count INT);
sqlite&gt;
</code></pre>

<p>I will continue to look for solution. I appreciate any feedback!</p>",,1,8,,2018-10-12 16:22:25,,2018-10-12 16:46:03,2018-10-12 16:46:03,,10151817.0,,10151817.0,,1,1,python|sqlite|splunk,504,11
132,258588,52787297,How to extract data from the String in splunk?,"<p>I was given a log from splunk and I want to get a particular data in the middle of the string and use it for the dashboard. For example:</p>

<p>msg=""somestring1 somestring2 500 somestring3 ...""</p>

<p>How do I get the value 500?</p>

<p>Sorry, I am not expert in splunk. Thanks in advance</p>",52881456.0,1,2,,2018-10-12 21:37:10,,2018-10-18 19:47:36,,,,,3067802.0,,1,0,splunk,1795,14
133,258589,52796905,How to initialize Splunk HTTP Event Collector via Docker Compose and use it with splunk logging driver,"<p>I'm trying to set up a local development environment with Docker Compose that bootstraps a Splunk Enterprise server and uses the <a href=""https://docs.docker.com/config/containers/logging/splunk/"" rel=""nofollow noreferrer"">splunk logging driver</a> on an app server.</p>

<p>Versions:</p>

<ul>
<li>Docker Engine: 18.06.1-ce</li>
<li>Compose: 1.22.0</li>
<li>Compose File: 3.7</li>
<li>Splunk Enterprise: 7.2.0</li>
</ul>

<p>My <code>docker-compose.yml</code> file looks like this:</p>

<pre><code>version: ""3.7""

services:
  app:
    build: ./app
    command: bash -c ""npm run start:docker""
    depends_on:
      - splunk
    environment:
      - NODE_ENV=development
      - SERVER_PORT=8080
    logging:
      driver: splunk
      options:
        splunk-format: ""json""
        splunk-insecureskipverify: ""true""
        splunk-source: ""app""
        splunk-token: ""XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX""
        splunk-url: ""http://splunk:8088""
        tag: ""{{.ImageName}}/{{.Name}}/{{.ID}}""
    ports:
      - ""80:8080""
    volumes:
      - ""./app:/usr/src/app""

  splunk:
    environment:
      - SPLUNK_ENABLE_LISTEN=9997
      - SPLUNK_START_ARGS=--accept-license --no-prompt --answer-yes
      - SPLUNK_USERNAME=admin
      - SPLUNK_PASSWORD=password
    hostname: splunk
    image: splunk/splunk:7.2.0
    ports:
      - ""8000:8000""
      - ""8088:8088""
      - ""9997:9997""
    restart: always
</code></pre>

<p>In order for this to work as intended, I need to generate an HTTP Event Collector token and make it available to the app service somehow.</p>

<p>I've seen that you can use the environment variable <code>SPLUNK_CMD</code> to run commands, presumably after the Splunk service is up and running, but when I tried using that to <a href=""http://dev.splunk.com/view/event-collector/SP-CAAAE7D"" rel=""nofollow noreferrer"">generate a token with the CLI</a>, nothing happened. I saw no failure in the logs, and no token under Settings > Data Inputs.</p>

<p>Another issue is that Splunk takes some time to start up, and before it starts listening the app service fails to build because the logging driver cannot connect.</p>

<p>Is it possible to do what I'm trying to do? If so, how?</p>",,1,0,,2018-10-13 20:11:14,,2018-10-14 05:49:59,,,,,630544.0,,1,4,docker|docker-compose|splunk,1739,15
134,258590,52810072,How do I use drilldown with a piechart to get a different value?,"<p>I have a field named income where I have the amount of money for each employee per month. I want to show in a pie chart the income types of people.
(|eval income_type= if(income=&lt;4000, ""middle class"",""high class"") | stats count by income_type</p>

<p>When I click on the ""high class"" I want to show in the drilldown which person earns how much money. So I don't want the income_type, I need the income value itself.</p>

<p>How can I get the income value as a token to the drilldown? Now I just get ""high class"" as a string.</p>

<p>I already tried $row.income$ but it didnt work.</p>",,2,0,,2018-10-15 05:12:02,,2018-10-20 16:47:49,2018-10-15 05:16:46,,10493895.0,,10493895.0,,1,0,token|pie-chart|splunk|drilldown,465,10
135,258591,52842020,How to work with Splunk Mint and Facebook ads in same project,"<p>I've been trying to integrate Facebook ads in my app for Android with the audience-network-sdk:5.0.0 and Facebook sdk crashed with this error:</p>

<pre><code>10-16 13:12:07.128 25301-25725/? D/ProxyCache: Open connection  to http://127.0.0.1:43557/ping
        10-16 13:12:08.318 25301-25586/? E/ProxyCache: Error pinging server [attempt: 2, timeout: 1200]. 
        java.util.concurrent.TimeoutException
            at java.util.concurrent.FutureTask.get(FutureTask.java:176)
            at com.facebook.ads.internal.r.b.f.b(Unknown Source)
            at com.facebook.ads.internal.r.b.f.&lt;init&gt;(Unknown Source)
            at com.facebook.ads.internal.r.b.f.&lt;init&gt;(Unknown Source)
            at com.facebook.ads.internal.f.d$1.a(Unknown Source)
            at com.facebook.ads.internal.f.d$1.call(Unknown Source)
            at java.util.concurrent.FutureTask.run(FutureTask.java:237)
            at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1112)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:587)
            at java.lang.Thread.run(Thread.java:818)
        Shutdown server... Error pinging server [attempts: 3, max timeout: 1200].
    10-16 13:12:08.318 25301-25586/? I/ProxyCache: Shutdown proxy server
    10-16 13:12:08.328 25301-25602/? W/System.err: java.lang.InterruptedException
    10-16 13:12:08.348 25301-25602/? W/System.err:     at java.lang.Thread.sleep(Native Method)
            at java.lang.Thread.sleep(Thread.java:1031)
            at java.lang.Thread.sleep(Thread.java:985)
            at com.splunk.mint.ExceptionHandler.uncaughtException(ExceptionHandler.java:64)
            at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:693)
            at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:690)
    10-16 13:12:08.348 25301-25602/? E/AndroidRuntime: FATAL EXCEPTION: Thread-12294
        PID: 25301
        java.lang.NullPointerException: Attempt to invoke virtual method 'void java.io.FileDescriptor.setInt$(int)' on a null object reference
            at java.net.PlainSocketImpl.accept(PlainSocketImpl.java:93)
            at java.net.ServerSocket.implAccept(ServerSocket.java:216)
            at java.net.ServerSocket.accept(ServerSocket.java:140)
            at com.facebook.ads.internal.r.b.f.e(Unknown Source)
            at com.facebook.ads.internal.r.b.f.a(Unknown Source)
            at com.facebook.ads.internal.r.b.f$e.run(Unknown Source)
            at java.lang.Thread.run(Thread.java:818)
</code></pre>

<p>As you can see the trace just tell you that the ping (and the connection to local host) failed but it doesn't tell you more. I am not solving it because it only happens in physical device and only with my app (the example app from Facebook runs great in the same device and same network).</p>

<p>I don't know why is happening and how to fix it.</p>",,1,1,,2018-10-16 18:40:44,,2018-10-19 06:47:00,2018-10-19 06:46:29,,472495.0,,8296720.0,,1,1,android|proxy|localhost|facebook-audience-network|splunk-sdk,291,11
136,258592,52880407,Splunk dbxquery to call stored procedure with subsearch to populate parameter not working,"<p>I have two working Splunk queries as follows.  </p>

<p>The first one takes in an IP Address and datetime and returns a Mac Address:</p>

<blockquote>
  <p>index=dhcp signature=DHCPACK dest_ip=""192.0.0.0""
  latest=""05/30/2018:00:00:00""| rename dest_mac as mac_address | table
  mac_address, _time  | sort - _time | head 1 | fields mac_address</p>
</blockquote>

<p>The second one calls a stored procedure in the database and passes in the mac address and a datetime and returns a machine id, which is a guid:</p>

<blockquote>
  <p>| dbxquery query=""EXEC [dbo].[get_machines_by_mac_date] @mac_address =
  '11:22:33:44:55:66', @utc_date_time = '05/30/2018'""
  connection=""database1""</p>
</blockquote>

<p>What I need to do is combine these two into one query where the Mac Address found in the first is passed to the second one as the mac_address parameter.  I've been working on this, and I think I'm pretty close, but its just not working right, here is the combined query I have:</p>

<blockquote>
  <p>| dbxquery query=""EXEC [dbo].[get_machines_by_mac_date]
  @mac_address = [index=dhcp signature=DHCPACK
  dest_ip=""192.0.0.0"" latest=""05/30/2018:00:00:00""| rename dest_mac
  as mac_address | table _time, mac_address | sort - _time | head 1 |
  return mac_address], @utc_date_time = '05/30/2018'""
  connection=""database1""</p>
</blockquote>

<p>I've read that the inner query (inside the square brackets) gets executed first, so I'm trying to supply the mac_address parameter in the outer query with the results of the inner query.</p>

<p>I keep getting an error back that the mac_address is too long, that the maximum length in the database is 128.  I'm pretty sure this means that the inner query isn't working and  it is trying to send the entire literal text string within the square brackets to the stored procedure. </p>

<p>Here is the error I get back when I try to run the query:</p>

<blockquote>
  <p>com.microsoft.sqlserver.jdbc.SQLServerException: The identifier that
  starts with 'index=dhcp signature=DHCPACK dest_ip=192.0.0.0
  latest=05/30/2018:00:00:00| rename dest_mac as mac_address | table
  _time, ma' is too long. Maximum length is 128.</p>
</blockquote>

<p>I'm new at using Splunk and I would appreciate any help that can be provided! </p>",52989970.0,1,0,,2018-10-18 18:31:33,,2018-10-25 18:55:41,2018-10-18 20:32:20,,89054.0,,89054.0,,1,0,stored-procedures|splunk-query,420,10
137,258593,52937997,Extracting data using rex in splunk adds slash to the data,"<p>I have a log that looks like this:</p>

<pre><code>msg: time=2017-10-25.15:53:07:827 | msg2=somedata:sometitle[{""key1"":""value1"",""key2"":""value2""}]
</code></pre>

<p>I want to get the value2 and here is my splunk query:</p>

<pre><code>index=""some_index"" | rex ""key2\s*(?&lt;data2&gt;.+)\s*""
</code></pre>

<p>here is the extracted <code>data2: \',\'value2\'\'</code></p>

<p>I couldn't figure out how to exclude the slashes and just get value2. Please help. thanks.</p>",52939417.0,1,0,,2018-10-22 21:27:23,,2018-10-23 00:23:02,,,,,3067802.0,,1,1,splunk|splunk-query,129,10
138,258594,52998335,Splunk Client Module for multiple host,"<p>I am pretty new to Splunk and Python.</p>

<p>I am using Splunklib.client to connect the Splunk API. My code is below:</p>

<pre><code>import splunklib.client as client
import splunklib.results as result
HOST = 'Localhost'
PORT = '8000
USERNAME = ""username""
PASSWORD = ""password""

service = client.connect(
    host=HOST,
    port=PORT,
    username=USERNAME,
    password=PASSWORD)
rr = results.ResultsReader(service.jobs.export(""query"")
</code></pre>

<p>My questions is I have multiple host such as localhost1, localhost2,localhost3 etc, is there a way to get the data through this module on multiple host?</p>

<p>Thanks</p>",,1,0,,2018-10-25 21:25:28,,2018-10-25 21:44:30,2018-10-25 21:36:27,,10547504.0,,10547504.0,,1,0,python|python-3.x|splunk,90,7
139,258595,53050064,Using Python SDK export Data,"<p>I am trying to use Python SDK to export Data from Splunk. </p>

<pre><code>for result in rr:
if isinstance(result, results.Message):
    # Diagnostic messages might be returned in the results
    print '%s: %s' % (result.type, result.message)
elif isinstance(result, dict):
    # Normal events are returned as dicts
    print result
assert rr.is_preview == False
</code></pre>

<p>The output data is like:</p>

<pre><code>OrderedDict([('TIME','1'),('UID','BUSINESS')])
OrderedDict([('TIME','12'),('UID','ACC')])
OrderedDict([('TIME','33'),('UID','TRAVEL')])
</code></pre>

<p>I am not sure how to transfer the data to dataframe. I read the document it said ResultsReader is iterable and returns a dict for result. I tried Pandas_DataFrame(rr) and it does not work shows data argument can't be an iterator. </p>",,1,0,,2018-10-29 16:38:24,1.0,2019-9-9 19:39:27,2018-11-18 21:46:27,,2308683.0,,10547504.0,,1,1,python|python-3.x|pandas|dataframe|splunk,362,10
140,258596,53083852,Regex to Match string after time,"<p>Struggling with extracting Hostnames from log messages , it mucks up when the date changes from 2 digit to single digit , like from 31st Oct to 1 nov , the extraction of the keyword start failing ...here are few logs for which i need to extract the hostnames</p>

<ul>
<li>Nov  1 00:00:21 akdcs20.ftc.abcd-ipsn AKDCS20 fpc0
LBCM-L2,brcm_port_learning_config(),1258:(brcm_port_learning_config:1258) Setting L2 learning unit:0,port_num:44, learn_flg 5</li>
<li><p>Nov  1 01:27:16 spnztpm01.abcd-ipsn 553177: LC/0/0/CPU0:Nov  1
01:27:16.040 : ifmgr[200]: %PKT_INFRA-LINEPROTO-5-UPDOWN : Line
protocol on Interface TenGigE0/0/0/1.172153, changed state to Up</p></li>
<li><p>Oct 31 23:59:56 akdcs19.ftc.abcd-ipsn AKDCS19 ufdd[1679]:
ufd_group_config_if_lookup ifname ae4</p></li>
</ul>

<p>For all three above lines i want to extract </p>

<ul>
<li>akdcs19.ftc.abcd-ipsn </li>
<li>spnztpm01.abcd-ipsn</li>
<li>akdcs20.ftc.abcd-ipsn</li>
</ul>

<p>current regex i am using is 
<code>^(?:[^ \n]* ){4}(?P&lt;devicename1&gt;[^ ]+)</code></p>",53083950.0,2,0,,2018-10-31 12:54:45,,2018-10-31 13:07:52,,,,,6483852.0,,1,1,regex|splunk,64,10
141,258597,53110308,Regex to remove everything after -i- (with -i-),"<p>I was trying to find solution for my problem.</p>

<pre><code> Input: prd-abcd-efgh-i-0dflnk55f5d45df

 Output: prd-abcd-efgh

Tried Splunk Query : index=aws-* (host=prd-abcd-efgh*) | rex field=host ""^(?&lt;host&gt;[^.]+)""| dedup host  | stats count by host,methodPath
</code></pre>

<p>I want to remove everything comes after ""-i-"" using simple regex.I tried with regex ""^(?[^.]+)"" listed here </p>

<p><a href=""https://answers.splunk.com/answers/77101/extracting-selected-hosts-with-regex-regex-hosts-with-exceptions.html"" rel=""nofollow noreferrer"">https://answers.splunk.com/answers/77101/extracting-selected-hosts-with-regex-regex-hosts-with-exceptions.html</a></p>

<p>Please help me to solve it.</p>",53110402.0,3,0,,2018-11-1 22:33:14,,2018-11-1 22:42:51,,,,,7303180.0,,1,1,regex|splunk|splunk-query,1531,15
142,258598,53203991,search for all fields which have some string in field,"<p>How can I get only results for specific fields where field name is like something ?</p>

<p>fx.
get all fields which have ""status"" in their field name.
I tried this but It doesnt work:
sta*
I want also to do later this: 
sta* OR STA* OR Sta*</p>",54250127.0,2,1,,2018-11-8 08:33:15,,2019-1-18 08:30:41,2018-11-8 09:33:32,,10622634.0,,10622634.0,,1,0,splunk|splunk-query,35,6
143,258599,53247181,How can I count ids in splunk logs in one line with regex,"<p>I have log like:
Segment 5bbdf7b8bbdd3c685a2110bf : UserMap is [512205885, 512112460, 512369891, 512316786, 58587803, 506882296]</p>

<p>Segment 5bbdf7b8bbdd3c685a2110bf : UserMap is [514348564, 506722271, 513844106, 513725157]<br>
Segment 5bbdfd69bbdd3c685a21129b : UserMap is [502062935]  </p>

<p>I want the stats where I can see number of ids in userMap with respect to the segment. like:</p>

<p>5bbdf7b8bbdd3c685a2110bf - 6</p>

<p>5bbdf7b8bbdd3c685a2110bf - 4 </p>

<p>5bbdfd69bbdd3c685a21129b - 1</p>",53249787.0,1,0,,2018-11-11 08:54:05,,2018-11-11 14:40:43,,,,,5011093.0,,1,0,splunk|splunk-query,149,8
144,258600,53264898,Splunk sendemail fails [Errno 99] with mailserver smtp.gmail.com,"<p>From <code>/opt/splunk/var/log/splunk/python.log</code>:</p>

<pre><code>2018-11-12 14:29:08,776 +0000 ERROR    sendemail:137 - Sending email. subject=""Splunk Alert: Errors in develop"", results_link=""https://localhost:8000/app/search/@go?sid=rt_scheduler__admin__search__RMD58e26482826eced90_at_1542024571_26.426"", recipients=""[u'my_email@gmail.com']"", server=""localhost""
2018-11-12 14:29:08,776 +0000 ERROR    sendemail:458 - [Errno 99] Cannot assign requested address while sending mail to: my_email@gmail.com
</code></pre>

<p>My <code>/opt/splunk/etc/system/local/alert_actions.conf</code>:</p>

<pre><code> [email]
 auth_password = XXX
 auth_username = my_email@gmail.com
 hostname = localhost
 mailserver = smtp.gmail.com:465
 pdf.header_left = none
 pdf.header_right = none
 use_ssl = 1
</code></pre>

<p>I also tried <code>smtp.gmail.com:587</code> with <code>use_ssl = 0 use_tls = 1</code>, got same error in both cases.</p>

<p>However, when trying to send mail directly from search, it works as expected:</p>

<pre><code>... | sendemail to=""my_email@gmail.com"" format=raw sendresults=1 footer=""Sent from Splunk."" from=""SplunkAlerts"" subject=""Splunk Alert"" message=""The following Splunk Alert has been fired:""
</code></pre>

<p>what configuration am I missing? errno 99 is EADDRNOTAVAIL , not clear what is not available, is it the <code>server=""localhost""</code> in the error log? where should I set it?</p>",,2,0,,2018-11-12 15:04:16,,2018-11-14 11:26:42,2018-11-12 15:30:37,,1236401.0,,1236401.0,,1,0,smtp|splunk|mail-server,1299,13
145,258601,53287922,How to forward application logs to Splunk from docker container?,"<p>We're interested in forwarding the logs from a node.js server running in a Docker container to Splunk.</p>

<p>Some options we've considered include a side-car container running a Splunk forwarder.  The side-car would write to a shared volume that the side-car would observe and send on.</p>

<p>Ideally, we would just use a syslog drain or another mechanism, but I can't seem to find any documentation on how to set that up?</p>",,3,0,,2018-11-13 19:07:20,,2018-11-14 11:54:20,,,,,320619.0,,1,3,docker|logging|syslog|splunk,4053,20
146,258602,53307519,Creating Splunk universal forwarder using Alpine base image,"<p>I am trying to create a Splunk universal forwarder image using the <code>alpine:3.8</code> base image.</p>
<pre><code>FROM alpine:3.8

ENV SPLUNK_PRODUCT universalforwarder
ENV SPLUNK_VERSION 6.3.1
ENV SPLUNK_BUILD f3e41e4b37b2
ENV SPLUNK_FILENAME splunkforwarder-${SPLUNK_VERSION}-${SPLUNK_BUILD}-Linux-x86_64.tgz
ENV SPLUNK_SERVER_HOST testapp:9997
ENV SPLUNK_HOME /opt/splunk
ENV SPLUNK_GROUP splunk
ENV SPLUNK_USER splunk
ENV SPLUNK_BACKUP_DEFAULT_ETC /var/opt/splunk
ENV SPLUNK_INDEX test
ENV FORWARD_HOSTNAME InstanceId

# Here we install GNU libc (aka glibc) and set C.UTF-8 locale as default.
RUN apk --no-cache add ca-certificates wget \
    &amp;&amp; wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \
    &amp;&amp; wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.28-r0/glibc-2.28-r0.apk \
    &amp;&amp; apk add glibc-2.28-r0.apk \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

# add splunk:splunk user
RUN addgroup --system ${SPLUNK_GROUP} \
    &amp;&amp; adduser --system --ingroup ${SPLUNK_GROUP} ${SPLUNK_USER}

# Download official Splunk release, verify checksum and unzip in /opt/splunk
# Also backup etc folder, so it will be later copied to the linked volume
RUN apk add sudo curl\
    &amp;&amp; mkdir -p ${SPLUNK_HOME} \
    &amp;&amp; curl -o /tmp/${SPLUNK_FILENAME} https://download.splunk.com/products/${SPLUNK_PRODUCT}/releases/${SPLUNK_VERSION}/linux/${SPLUNK_FILENAME} \
    &amp;&amp; curl -o /tmp/${SPLUNK_FILENAME}.md5 https://download.splunk.com/products/${SPLUNK_PRODUCT}/releases/${SPLUNK_VERSION}/linux/${SPLUNK_FILENAME}.md5 \
    &amp;&amp; tar xzf /tmp/${SPLUNK_FILENAME} --strip 1 -C ${SPLUNK_HOME} \
    &amp;&amp; rm /tmp/${SPLUNK_FILENAME} \
    &amp;&amp; rm /tmp/${SPLUNK_FILENAME}.md5 \
    &amp;&amp; mkdir -p /var/opt/splunk \
    &amp;&amp; cp -R ${SPLUNK_HOME}/etc ${SPLUNK_BACKUP_DEFAULT_ETC} \
    &amp;&amp; rm -fR ${SPLUNK_HOME}/etc \
    &amp;&amp; chown -R ${SPLUNK_USER}:${SPLUNK_GROUP} ${SPLUNK_HOME} \
    &amp;&amp; chown -R ${SPLUNK_USER}:${SPLUNK_GROUP} ${SPLUNK_BACKUP_DEFAULT_ETC} \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

COPY ./config /tmp/splunk

COPY patch-entrypoint.sh /sbin/entrypoint.sh
RUN chmod +x /sbin/entrypoint.sh

# Ports Splunk Daemon, Network Input, HTTP Event Collector
EXPOSE 8089/tcp 1514 8088/tcp

WORKDIR /opt/splunk

# Configurations folder, var folder for everyting (indexes, logs, kvstore)
VOLUME [ &quot;/opt/splunk/etc&quot;, &quot;/opt/splunk/var&quot; ]

ENTRYPOINT [&quot;/sbin/entrypoint.sh&quot;]
CMD [&quot;start-service&quot;]
</code></pre>
<p>Now I am facing a couple of issues here:</p>
<ol>
<li>When I am running /opt/splunkforwarder/bin/splunk start --accept-license I am getting <strong>/opt/splunkforwarder/bin/splunk: not found</strong>.</li>
</ol>
<p>I am using custom output.conf file. It's in config folder.</p>
<pre><code>[tcpout]
defaultGroup = abc
disabled = false

[tcpout:abc]
server = _OUTPUT_SERVERS_
autoLB = true
compressed = false
useACK = true
sendCookedData = true
</code></pre>
<p><code>entrypoint.sh</code> is the script which I am using to replace the environment variable from output.config and restart the Splunk but again restart is not working. How can I fix this?</p>",53310024.0,1,0,,2018-11-14 19:30:07,,2021-6-16 23:31:15,2021-6-16 23:31:15,,472495.0,,6656693.0,,1,3,splunk,617,16
147,258603,53390039,Splunk forwarder with Kubernetes in side car pattern,"<p>I have created a custom Splunk forwarder image.</p>

<p>Image name: <code>vrathore/splunkuniversalforwarder</code></p>

<p>I have verified that the log is pushing to the server. I am using dummy log present in my host (<code>c/Users/var/log</code>). If I run this Docker command:</p>

<pre><code>docker run --name splunkforwarder -d -v /c/Users/var/log://var/log/messages -p 8089:8089 -p 8088:8088 -e SPLUNK_SERVER_HOST=splunk-prodtest-gsp.test.com:9997 -e
FORWARD_HOSTNAME=kubernetes vrathore/splunkuniversalforwarder
</code></pre>

<p>Now I wanted to use the same image in Kubernetes pod, where 2 container will share their log folder with my Splunk forwarder image.</p>

<pre><code>spec:
  revisionHistoryLimit: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 10%
      maxSurge: 10%
  replicas: 1
  template:
    metadata:
      name: %APP_FULL_NAME%-pod
      labels:
        appname: %APP_FULL_NAME%
        stage: %APP_ENV%
        component: app-kube-pod-object
    spec:
      containers:
      - name: %APP_FULL_NAME%-service
        image: %DOCKER_IMAGE%
        imagePullPolicy: Always
        envFrom:
        - configMapRef:
            name: %APP_CONFIG_MAP%
        command: [""catalina.sh"", ""run""]
        ports:
        - containerPort: 8080
      imagePullSecrets:
      - name: %DOCKER_REPO_REGKEY%
  selector:
    matchLabels:
      appname: %APP_FULL_NAME%
      stage: %APP_ENV%
</code></pre>

<p>Kubernetes is new to me. How can I share the log folder between the containers?</p>",,1,0,,2018-11-20 09:36:55,,2019-7-31 21:30:32,2019-7-31 21:30:32,,472495.0,,7756971.0,,1,0,kubernetes|splunk,734,12
148,258604,53400738,how to remove hostname and timestamp from logs coming from remote syslog server,"<p>I am using rsyslog to send all syslog files and few additional application log files to remote syslog server which has syslog-ng server running and it's sending to Splunk using splunk forwarder. My problem is, when rsyslog sending logs to remote syslog server (syslog-ng), in log events it's adding Timestamp and Hostname to it. How do I tell rsyslog to don't add Timestamp and Hostname to any log events?
based on my findings, there is a template in rsyslog.conf. where we can define format and other things about log events. I tried that but it didn't work. </p>

<p>in my rsyslog.conf I have entry for template as, </p>

<pre><code>$template noTimeStampFormat,""%syslogtag% %msg%\n""
$ActionFileDefaultTemplate noTimeStampFormat
</code></pre>

<p>I restarted syslog service, this change didn't work. </p>

<p>can someone please help me here on how to fix this?</p>

<p>Currently events looks like</p>

<pre><code>&lt;timestamp&gt; &lt;hostname&gt; &lt;tag&gt; sudo: pam_unix(sudo:session): session opened for user root by ubuntu(uid=0)
</code></pre>

<p>Ideal would be,</p>

<pre><code>&lt;tag&gt; sudo: pam_unix(sudo:session): session opened for user root by ubuntu(uid=0)
</code></pre>

<p>Thanks in advance!</p>",,2,1,,2018-11-20 20:05:58,,2018-12-23 12:06:54,,,,,6553195.0,,1,1,linux|syslog|splunk|rsyslog|syslog-ng,4174,12
149,258605,53406198,Do we need to install the Universal forwarder in the host(Log originating Server) for scripted inputs?,"<p>I need to forward some database related logs into splunk indexer using scripted inputs (Shell scripts)</p>

<p>My questions are :</p>

<p>1)Do I need to install the universal forwarder in the host side ?</p>

<p>2)Is there any other way rather than installing UF in host that we can extract the logs into indexer using scripted inputs?</p>

<p>3)In order to accomplish this what are the steps do I need to follow ?</p>",53413187.0,2,0,,2018-11-21 06:12:02,,2018-11-21 13:32:08,,,,,1575943.0,,1,0,splunk,247,9
150,258606,53413721,Splunk Alert with run a script action,"<p>Is there any way to run external script with source IP (source IP of device which sent alert to splunk, host= value in event) address as variable?
There is in splunk documentation few variables but non of them are host.
I need to trigger config download from Solar Winds upon change of config. All syslog messages are sent to splunk. So when alert is triggered it would run script ./update $SOURCE_HOST</p>",,0,0,,2018-11-21 13:59:07,,2018-11-21 14:10:06,2018-11-21 14:10:06,,10686197.0,,10686197.0,,1,1,splunk|splunk-query|splunk-sdk,197,9
151,258607,53415441,Loading custom format data into splunk,"<p>I am new to splunk and need some clarification on the best approach to preprocess. I have a file in the following .csv format </p>

<pre><code>field1, field2, field3,             field4, field5 
dummy    dummy   date(YYYYMMMDD)    dummy   time
</code></pre>

<p>The time does not have the 0 preset, so for example <code>13</code> seconds would be listed as .. <code>'13'</code>, 1 hour 50 minutes and 22 seconds would be <code>15022</code>.</p>

<p>Is it possible to resolve this via the default input loader via regex?. It says that 0's don't matter but the time comes out wrong, I have Y%m%d%H%M%S . </p>

<p>The second approach that I been looking at (if someone can point me to a quick guide people) how can I configure so for every matching *file.csv a python rule is triggered? (I don't want it to run at intervals, whenever data is being index/imported into spunk)</p>

<p>Thank you. </p>",,1,0,,2018-11-21 15:32:16,,2018-11-21 22:09:57,,,,,10686648.0,,1,1,python|regex|splunk,63,7
152,258608,53426160,Shared volume of 1 container into another container in Kubernates,"<p>I am stuck on a scenario where I have to get the log folder of container 1 into 2nd container. I have found a solution in which we will create a emptyDir directory.</p>

<pre><code>spec:
  containers:
  - name: app
    image: app-image
    imagePullPolicy: Always
    ports:
    - containerPort: 8080
    volumeMounts:
    - name: logs
      mountPath: /var/log/app/
  - name: uf
    image: splunk/splunkuniversalforwarder
    ...
    volumeMounts:
    - name: logs
      mountPath: /var/log/app/
  volumes:
  - name: logs
    emptyDir: {}
</code></pre>

<p>But in my situation I want to share /usr/var/log/tomcat/ of 1st container into /var/log/message. This is because splunkUF image will monitor /var/log/app/. so I want to share the log folder of different apps, be it  /var/log/app/tomcat or /var/log/messages but at one same location with splunk container /var/log/app/.</p>

<p>I can run copy command to get the log 1 time but how to get the logs continuously? </p>",,2,5,,2018-11-22 07:51:49,,2018-11-22 18:51:23,,,,,7756971.0,,1,1,kubernetes|splunk,83,10
153,258609,53427535,Splunk inputs configuration cron schedule for powershell stanza,"<p>I have a power shell stanza in my etc/system/local/inputs.conf in my forwarder server which goes as follows.</p>

<pre><code>[powershell://SomeSessions]
disabled = 0
script = net session
schedule = */5 * * * *
sourcetype = Some_Shared_User_Sessions
index=Some_Monitoring
</code></pre>

<p>I was able to get data to enterprise every 5 mins but after few hours it stops sending data from the power shell source.</p>

<p>I am not sure if the problem is with permissions to run power shell scripts or something else. Can anyone help?</p>",,0,3,,2018-11-22 09:21:32,,2018-11-22 09:21:32,,,,,5629719.0,,1,0,powershell|permissions|splunk,354,10
154,258610,53437493,Querying in time intervals throughout history in Splunk,"<p>I have a query that returns me a count.</p>

<p>I want to get all the counts of a daily/weekly/monthly granularity, spanning a year back.</p>

<p>Currently I can get the counts manually from the presets (last 30 days, last 15 days, etc), or the date range (e.g. Between 20180101 - 20180201), but what I really want is a query that says </p>

<p>""get me a weekly count that spans a year back from today"", and it'll return:</p>

<pre><code>2018-11-15 to 2018-11-22 : count = 10
2018-11-08 to 2018-11-15 : count = 3
2018-11-01 to 2018-11-08 : count = 6
...
2017-11-15 to 2017-11-22 : count = 11
</code></pre>",,1,0,,2018-11-22 20:09:11,,2018-11-23 00:06:09,,,,,953776.0,,1,0,sql|splunk|splunk-query,47,8
155,258611,53446260,Splunk subsearch for regex outputs,"<p>I want a single search query for below splunk query.</p>

<p>First search will give me a dynamic field myorderid</p>

<p>index=mylog ""trigger.rule: Id - * : Unexpected System Error"" | rex field=_raw ""Id -""""(?[^:]*)"" | table  myorderid</p>

<p>I want to pass the above myorderid in below search criteria</p>

<p>index=mylog API=Order orderid=myorderid </p>

<p>Can anyone please help me to create a single query using subsearch in splunk.</p>",,1,0,,2018-11-23 11:55:27,,2018-11-23 23:53:58,,,,,10695407.0,,1,0,search|splunk|splunk-query,482,11
156,258612,53511433,How to find unused endpoints on Spring REST application running on Tomcat?,"<p>We are running our Spring applications on Tomcat, and over a period of time we have added multiple REST endpoints to our application. We now want to trim it down and remove all the unused endpoints that our GUIs do not use any more.</p>

<p>We do use Splunk, but it will only give the number of hits on active endpoints from the log aggregator on localhost_access file of Tomcat. We want to find the end points that have 0 hits.</p>

<p>The most straightforward way is to write some kind of python script, that copies data from Tomcat start up, and gets all the end points(or manually feed it). Then put them in a hash map, and then go over local host access files in Tomcat server logs for last few months, incrementing a counter when the corresponding endpoint is met. Then print out all the keys in this hash map with value 0.</p>

<p>Is the above a feasible way to do so, or does there exist an easier method?</p>",53519398.0,2,0,,2018-11-28 02:58:02,,2018-11-28 12:22:51,,,,,2292619.0,,1,2,java|spring|tomcat|splunk,611,12
157,258613,53516670,Splunk - subtract two counts and trigger alert,"<p>I'm trying to find proper Splunk documentation about the following, but it seems pretty difficult.
What I need to do is conceptually simple: I want to find out the number of certain events for two successive days and subtract them (simply subtract the numbers). 
For example, I need to find out the number of successful POST calls (HTTP 200) to a certain website ('somewebsite/myaction'), c1, that happened 2 days ago:</p>

<pre><code>search sourcetype = myproject:prod somewebsite post myaction 200 
earliest=-2d@d latest=-1d@d | stats count as c1
</code></pre>

<p>Also, I do the same to find out the same type of events for yesterday, let's call it c2:</p>

<pre><code>search sourcetype = myproject:prod somewebsite post myaction 200 
earliest=-1d@d latest=-0d@d | stats count as c2
</code></pre>

<p>Now all I need to do is find out <em>c1 - c2</em> and trigger an <strong>event</strong> if this value is above a certain threshold.
I'm trying something like this, but it doesn't show me 't':</p>

<pre><code>| set diff [search sourcetype = myproject:prod somewebsite post 
myaction 200 earliest=-2d@d latest=-1d@d | stats count as c1] [search 
sourcetype = myproject:prod somewebsite post myaction 200 
earliest=-1d@d latest=-0d@d | stats count as c2] | eval t=(c1-c2)
</code></pre>

<p>Thanks,</p>

<p>Greetings,</p>

<p>Sorin</p>

<p>P.S.</p>

<p>I come very close with the following: </p>

<pre><code>sourcetype=myproject:prod somewebsite post checkout 200 earliest=-2d@d latest=-1d@d 
| stats count as C1 | appendcols [search sourcetype = myproject:prod somewebsite 
post checkout 200 earliest=-1d@d latest=-0d@d | stats count as C2] | eval t=(C1 
- C2)
</code></pre>

<p>Now all I need to do is to express in an alert that I want it to be triggered when t is above a threshold (e.g. t > 100). How can I do that ?</p>",,1,0,,2018-11-28 09:56:53,,2018-11-28 19:39:08,2018-11-28 18:10:39,,1877098.0,,1877098.0,,1,0,splunk|splunk-calculation,1031,12
158,258614,53519782,INGEST_EVAL in splunk field is not showing,"<p>I have a csv file that is manually uploaded into splunk. I want to perform an INGEST_EVAL on the file however the changes are not propagating after restart. I am not sure why.  Nothing shows. This is the code - what am i doing wrong? do i need to modify any other configuration on the web side of splunk? (the results are not being dispalyed either in the pre-load section that shows the source type). </p>

<pre><code>transform.props : 
[myeval]
INGEST_EVAL = eval_user=""test""

field.props : 
[myeval]
INDEXED = True

props.config 

[newfieldsourcetest]
TRANSFORMS = myeval
DATETIME_CONFIG = 
INDEXED_EXTRACTIONS = csv
KV_MODE = none
NO_BINARY_CHECK = true
SHOULD_LINEMERGE = false
category = Structured
description = Comma-separated value format. Set header and other settings in ""Delimited Settings""
disabled = false
pulldown_type = true
</code></pre>",,1,0,,2018-11-28 12:43:58,,2019-1-21 17:54:59,,,,,10686648.0,,1,0,splunk|splunk-query,635,11
159,258615,53562235,Powershell: recurse directories for where a file extension exists if Exist write out comman if not run a command,"<p>Specifically, I have a directory with a ton of other random named directories (Not really random but that's not important). In those directories some contain files with a <code>.tsidx</code> extension, some do not.  </p>

<p>The directories which contain the <code>.tsidx</code> extension I want to output to screen that a <code>.tsidx</code> file already exists.  The ones that do NOT I want it to output it doesn't exist then run a command to build the tsidx files against the directory using an executable provided by avendor of a program.  </p>

<p>Here is what is in my code so far:</p>

<pre><code>$index = Read-Host ""Enter the Index Name"" #This is to receive a directory  location from user
$loc = ""F:\thawdb\splunk\$index\thaweddb""
$dir = dir $loc

cd ""d:\splunk\bin""
foreach ($d in $dir) 
{
   if (gci -dir | ? { !(gci $_ -file -recur -filter *tsidx) })
      {
         # writes out the directory contains files and doesn't need rebuilt
         Write-host -foregroundcolor Yellow ""TSIDX Exists""
      }
      else
      {
         # writes out rebuild is necessary and runs the rebuild
         write-host -Foregroundcolor Green ""Running Rebuild command against $loc\$d"" | .\splunk.exe rebuild $loc\$d 
      }
}
</code></pre>",53563792.0,1,7,,2018-11-30 17:28:47,,2018-11-30 20:07:30,2018-11-30 20:07:30,,6801321.0,,6801321.0,,1,1,powershell|if-statement|foreach|splunk|spl,57,7
160,258616,53569570,How to get Tomcat logs in Splunk 7.2.0?,"<p>I am working in machine learning recently. My goal is need to see logs from locally installed Tomcat in Splunk search.</p>

<p>I installed Apache Tomcat at a drive in my local machine. Then opened Splunk instance, I installed Tomcat add ons, followed this instruction (splunk docs), created inputs.conf file and placed it in Splunk_TA_tomcat/local folder. Then restarted Splunk. After I went to search page, I entered this command <code>sourcetype = tomcat:access:log</code>. I got nothing.</p>

<pre><code>1. Create an inputs.conf file in $SPLUNK_HOME/etc/apps/Splunk_TA_tomcat/local.

2. Add the following stanzas. Modify the directory name if necessary to use the actual directory your Tomcat files are stored in.

 [monitor:///Applications/apache-tomcat-8.0.23/logs/catalina.*.log]
 disabled = false
 followTail = false
 index = main
 sourcetype = tomcat:runtime:log

 [monitor:///Applications/apache-tomcat-8.0.23/logs/localhost.*.log]
 disabled = false
 followTail = false
 index = main
 sourcetype = tomcat:runtime:log

 [monitor:///Applications/apache-tomcat-8.0.23/logs/manager.*.log]
 disabled = false
 followTail = false
 index = main
 sourcetype = tomcat:runtime:log

 [monitor:///Applications/apache-tomcat-8.0.23/logs/host-manager.*.log]
 disabled = false
 followTail = false
 index = main
 sourcetype = tomcat:runtime:log

 [monitor:///Applications/apache-tomcat-8.0.23/logs/localhost_access_log.*.txt]
 disabled = false
 followTail = false
 index = main
 sourcetype = tomcat:access:log
</code></pre>

<p><a href=""https://i.stack.imgur.com/ZKL8F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZKL8F.png"" alt=""enter image description here""></a></p>",,0,6,,2018-12-1 09:48:33,,2020-3-20 22:26:20,2020-3-20 22:26:20,,472495.0,,2976424.0,,1,0,java|tomcat|splunk|splunk-sdk,616,11
161,258617,53592337,"splunk query taking long time to return the value, can we eliminate append","<p>i have initially  used inputlook to get the output and query was returning output in fractions of sec, but now i want to use the source as input and run the Splunk query but its taking lot of time to return output.</p>

<p>Please suggest solution to optimise the output time.
I am thinking of removing multiple append</p>

<pre><code>index=csvlookups source=""F:\\SplunkMonitor\\csvlookups\\Core_Network\\lookup_table_sip_pbx_usage.csv"" OR source=""F:\\SplunkMonitor\\csvlookups\\Core_Network\\lookup_table_dpt_capacity.csv"" OR source=""F:\\SplunkMonitor\\csvlookups\\Core_Network\\lookup_table_sip_pbx_forecasts.csv"" 
| eval Date=strftime(strptime(Date,""%m/%d/%Y""),""%Y-%m-%d"") 
| sort Date, CLLI 
| rename CLLI as Office 
| search Office=""CLGRAB21DS1"" 
| stats sum(Usage) as Usage by Office, Date 
| append 
    [ search index=csvlookups source=""F:\\SplunkMonitor\\csvlookups\\Core_Network\\lookup_table_sip_pbx_usage.csv"" OR source=""F:\\SplunkMonitor\\csvlookups\\Core_Network\\lookup_table_dpt_capacity.csv"" OR source=""F:\\SplunkMonitor\\csvlookups\\Core_Network\\lookup_table_sip_pbx_forecasts.csv""
    | eval Date=strftime(strptime(Date,""%m/%d/%Y""),""%Y-%m-%d"") 
    | reverse 
    | search Office=""CLGRAB21DS1"" AND Type=""SIP PBX"" 
    | fields Date NB_RTU 
    | fields - _raw _time ] 
| sort Date 
| fillnull value=""CLGRAB21DS1"" Office 
| filldown Usage 
| filldown NB_RTU 
| fillnull value=0 Usage 
| eval _time = strptime(Date, ""%Y-%m-%d"") 
| eval latest_time = if(""now"" == ""now"", now(), relative_time(now(), ""now"")) 
| where ((_time &gt;= relative_time(now(), ""-3y@h"")) AND (_time &lt;= latest_time)) 
| fields - latest_time Date 
| append 
    [ gentimes start=-1 
    | eval Date=strftime(mvrange(now(),now()+60*60*24*365*3,""1mon""),""%F"") 
    | mvexpand Date 
    | fields Date 
    | append 
        [ search index=csvlookups source=""F:\\SplunkMonitor\\csvlookups\\Core_Network\\lookup_table_sip_pbx_usage.csv"" OR source=""F:\\SplunkMonitor\\csvlookups\\Core_Network\\lookup_table_dpt_capacity.csv"" OR source=""F:\\SplunkMonitor\\csvlookups\\Core_Network\\lookup_table_sip_pbx_forecasts.csv""
        | rename ""Expected Date of Addition"" as edate 
        | eval edate=strftime(strptime(edate,""%m/%d/%Y""),""%Y-%m-%d"") 
        | rename edate as ""Expected Date of Addition"" 
        | table Contact Customer ""Expected Date of Addition"" ""Number of Channels"" Switch 
        | reverse 
        | search Customer = ""Regular Usage"" AND Switch = ""CLGRAB21DS1"" 
        | rename ""Number of Channels"" as val 
        | return $val ] 
    | reverse 
    | filldown search 
    | rename search as Usage 
    | where Date != """" 
    | reverse 
    | append 
        [ search index=csvlookups source=""F:\\SplunkMonitor\\csvlookups\\Core_Network\\lookup_table_sip_pbx_usage.csv"" OR source=""F:\\SplunkMonitor\\csvlookups\\Core_Network\\lookup_table_dpt_capacity.csv"" OR source=""F:\\SplunkMonitor\\csvlookups\\Core_Network\\lookup_table_sip_pbx_forecasts.csv""
        | rename ""Expected Date of Addition"" as edate 
        | eval edate=strftime(strptime(edate,""%m/%d/%Y""),""%Y-%m-%d"") 
        | rename edate as ""Expected Date of Addition"" 
        | table Contact Customer ""Expected Date of Addition"" ""Number of Channels"" Switch 
        | reverse 
        | search Customer != ""Regular Usage"" AND Switch = ""CLGRAB21DS1"" 
        | rename ""Expected Date of Addition"" as Date 
        | eval _time=strptime(Date, ""%Y-%m-%d"") 
        | rename ""Number of Channels"" as Forecast 
        | stats sum(Forecast) as Forecast by Date] 
    | sort Date 
    | rename Switch as Office 
    | eval Forecast1 = if(isnull(Forecast),Usage,Forecast) 
    | fields - Usage Forecast 
    | streamstats sum(Forecast1) as Forecast 
    | fields - Forecast1 
    | eval Date=strptime(Date, ""%Y-%m-%d"") 
    | eval Date=if(Date &lt; now(), now(), Date) ] 
| filldown Usage 
| filldown Office 
| eval Forecast = Forecast + Usage 
| eval Usage = if(Forecast &gt;= 0,NULL,Usage) 
| eval _time=if(isnull(_time), Date, _time) 
| timechart limit=0 span=1w max(Usage) as Usage, max(NB_RTU) as NB_RTU, max(Forecast) as Forecast by Office 
| rename ""NB_RTU: CLGRAB21DS1"" as ""RTU's Purchased"", ""Usage: CLGRAB21DS1"" as ""Usage"", ""Forecast: CLGRAB21DS1"" as ""Forecast"" 
| filldown ""RTU's Purchased"" |sort -Forecast
</code></pre>",,2,0,,2018-12-3 10:56:03,,2018-12-10 01:24:23,,,,,9002161.0,,1,0,query-optimization|splunk|splunk-query,340,10
162,258618,53600159,Why won't Splunk recognize these fields?,"<p>I am new to Splunk, forgive me if I need to provide more info.</p>

<p>I am generating logs that track metrics of a few websites with the end goal of sending me alerts when a value changes.</p>

<p>I am forwarding the logs to a Splunk Indexer. My log is in the following format:</p>

<pre><code>fetchTime: 2018-12-02T18:33:56.621Z
fooVersion: 3.2.1
requestedUrl: https://cats.com/
finalUrl: https://cats.com/
accessibilityScore: 0.70
fetchTime: 2018-12-02T18:34:50.345Z
fooVersion: 3.2.1
requestedUrl: https://example.com/
finalUrl: https://example.com/
accessibilityScore: 0.90
fetchTime: 2018-12-03T18:35:50.750Z
fooVersion: 3.2.1
requestedUrl: https://cats.com/
finalUrl: https://cats.com/
accessibilityScore: 0.72
fetchTime: 2018-12-03T18:36:06.868Z
FooVersion: 3.2.1
requestedUrl: https://example.com/
finalUrl: https://example.com/
accessibilityScore: 0.88
</code></pre>

<p>The events show up in Splunk as I hoped:</p>

<p><a href=""https://i.stack.imgur.com/QmGbq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QmGbq.png"" alt=""Splunk event screenshot""></a></p>

<p>But I expected that Splunk could easily identify them as ""Interesting Fields"".</p>

<p>Is there something wrong with my log format? Should I make a custom Field extractor since it does not identify them? Or is something not configured correctly in Splunk? </p>

<p>Thanks in advance.</p>",53603683.0,1,0,,2018-12-3 18:59:32,,2018-12-3 23:48:25,,,,,4424258.0,,1,1,splunk,94,12
163,258619,53631356,"Splunk: Duplicate Fields, different fields - merge","<p>I have a number of individual records in Splunk all with a common field of X, which i'm trying to combine. </p>

<p>E.g</p>

<pre><code>User-name=JG, srcIP=10.0.0.1
User-name=JG,file=jg.docx
User-name=JG, dstIP=10.1.1.0
User-name=JG,Email=jg@jg.com
User-name=AB, srcIP=10.0.0.2
User-name=AB,file=AB.docx
User-name=AB, dstIP=10.2.2.0
User-name=AB,Email=AB@AB.com
</code></pre>

<p>I want to do the following search: Group all the records which match by the <code>User-name</code> fields, and allow me to manipulate the fields.</p>

<p>E.g</p>

<pre><code>USERNAE, srcIP, file, dstIP, Email
JG, 10.0.0.1, jg.docx, 10.1.1.0, jg@jg.com
AB, 10.0.0.2, AB.docx, 10.2.2.0, AB@AB.com
</code></pre>

<p>Thank you!</p>",,1,0,,2018-12-5 11:33:31,,2018-12-5 18:08:53,,,,,2186566.0,,1,0,splunk,320,12
164,258620,53637551,PCF - Stream logs to multiple servers,"<p>I am running Docker image of my <code>spring-boot</code> application in PCF. It is streaming logs to 2 different servers; I have configured 2 User Defined services with syslog drain to Splunk and the other is to Kibana. Both of the services are bound to the <code>spring-boot</code> app.</p>

<p>Would this cause any performance issue that streaming logs to multiple log servers from the same application?</p>",53653086.0,1,0,,2018-12-5 17:17:56,,2018-12-6 14:00:53,2018-12-5 17:27:47,,1806481.0,,1806481.0,,1,0,kibana|splunk|cloud-foundry,166,8
165,258621,53669208,SplunkUF is not forwarding logs from the Kubernates container,"<p>I deployed Splunk Forwarder in my kubernetes cluster using this blog.</p>

<p><a href=""http://jasonpoon.ca/2017/04/03/kubernetes-logging-with-splunk/"" rel=""nofollow noreferrer"">http://jasonpoon.ca/2017/04/03/kubernetes-logging-with-splunk/</a></p>

<p>I have 4 files at</p>

<p>/opt/splunk/etc/system/local</p>

<p>inputs.conf</p>

<p>server.conf</p>

<p>limits.conf</p>

<p>outputs.conf</p>

<p>my inputs.conf looks like this.</p>

<pre><code>[default]
host = testtest

[monitor:///usr/local/tomcat/logs]
whitelist=test.log|.log_WHITELIST_|test
index= abc
sourcetype=log4j
[splunktcp://9997]
compressed = false
</code></pre>

<p>I am not able to get the log in Splunk UI. when I am deploying it as side car. but If I use same docker image locally but putting dummy logs file. i can see the logs. Then why it is not working with kubernates?</p>

<p>I have checked the splunkd.log as well. </p>

<pre><code>/opt/splunk/var/log/splunk # tail splunkd.log

12-07-2018 10:43:38.793 +0000 INFO  TailingProcessor - Adding watch on path: /opt/splunk/var/spool/splunk.
12-07-2018 10:43:38.793 +0000 INFO  TailingProcessor - Adding watch on path: /usr/local/tomcat/logs.
12-07-2018 10:43:38.795 +0000 INFO  loader - Limiting REST HTTP server to 21845 sockets
12-07-2018 10:43:38.795 +0000 INFO  loader - Limiting REST HTTP server to 657 threads
12-07-2018 10:43:38.798 +0000 INFO  TailReader - Registering metrics callback for: batchreader0
12-07-2018 10:43:38.798 +0000 INFO  TailReader - Starting batchreader0 thread
12-07-2018 10:43:38.798 +0000 INFO  TailReader - Registering metrics callback for: tailreader0
12-07-2018 10:43:38.798 +0000 INFO  TailReader - Starting tailreader0 thread
12-07-2018 10:43:38.850 +0000 INFO  TcpOutputProc - Connected to idx=52.204.198.184:9997 using ACK.
12-07-2018 10:44:08.358 +0000 WARN  AuthenticationManagerSplunk - Seed file is not present. Defaulting to generic username/pass pair.
</code></pre>

<p>Any suggestion how to fix this. Stuck here for days. Do I have to open any port under kubernates? although I can ping splunk server from the splunk container.  </p>",,1,3,,2018-12-7 12:05:40,,2018-12-8 21:42:45,2018-12-7 13:05:13,,7756971.0,,7756971.0,,1,1,splunk,491,10
166,258622,53709047,SPLUNK multi-value chart,"<p>I have 2 values <code>restSevice</code> being which endpoint is called and <code>status</code> being the code returned from that endpoint. I would like todo a timechart so that x is time and y is the count of the status value combined with the restService called. ie if I called my <code>restService</code> <code>receiving</code> and/or <code>location</code> then there would be a line for each <code>status</code> returned so there would be a line for <code>receiving 200</code>, <code>receiving 404</code>, <code>location 200</code>, and <code>location 404</code>.</p>",53711687.0,1,0,,2018-12-10 15:45:36,,2018-12-10 18:36:53,,,,,8798103.0,,1,0,rest|splunk|splunk-query,146,11
167,258623,53713769,how to configure ibm cloud kube to forward logs - best way to forward logs from kube,"<p>i have a kubernetes service on ibm cloud, what vendor/solution is best way to persist my logs so i can view them later? The ClI does not save all of the logs, kibana is too noisy in the UI. </p>

<p>Edit - want to use splunk server. Not sure how to forward logs to splunk server, has anyone tried?</p>",,1,0,,2018-12-10 21:16:46,,2018-12-14 10:22:25,2018-12-12 18:25:25,,1350430.0,,1350430.0,,1,-2,logging|kubernetes|ibm-cloud|splunk,47,6
168,258624,53732965,Splunk: Find the difference between 2 events,"<p>I have a server with 2 APIs: /migrate/start and /migrate/end</p>

<p>For each request, I log the userID (field usrid="""") of the user using my service to be migrated and the api called (field api="""").</p>

<p>Users call /migrate/start, then call /migrate/end. I would like to write a slunk query to list the userIDs that are being migrated, i.e. those that called /migrated/start but have yet to call /migrate/end. How would I write that query?</p>

<p>Thank you </p>",53734404.0,2,0,,2018-12-11 21:59:29,,2018-12-12 00:58:15,,,,,1440457.0,,1,1,splunk|splunk-query,451,13
169,258625,53739099,how to combine/merge multiple generic fields/columns in one field/column with average calculation per generic field/column values in Splunk?,"<p>I have following situation in splunk (see picture below).</p>

<p><a href=""https://i.stack.imgur.com/rcGut.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rcGut.png"" alt=""Actual Situation""></a></p>

<p>I need following pattern in Splunk (see picture below).</p>

<p><a href=""https://i.stack.imgur.com/YT0wa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YT0wa.png"" alt=""Target Situation""></a></p>

<p>I have different generic columns where the last part of the column-name (Suffix) is dynamic and unknown. I need to combine/merge this generic columns to one target-column. Within the target-column I want to calculate the average per generic field. I think the picture explains the situation very well.</p>",53824503.0,1,1,,2018-12-12 08:44:10,,2018-12-17 23:38:34,2018-12-13 22:51:39,,88764.0,,10779796.0,,1,0,splunk,1662,12
170,258626,53814789,I need curl query to check connectivity of my Splunk,"<p>I want to check connectivity of my Splunk with Curl command for that I am looking for curl query which will just refresh my Splunk web and returns 200 response.</p>

<p>Can anyone help me regarding this, It will be really helpful for me.</p>

<p>Thanks,
Anshu</p>",53817165.0,1,0,,2018-12-17 11:58:25,,2018-12-17 14:21:57,2018-12-17 12:05:05,,8604431.0,,8604431.0,,1,1,curl|splunk|splunk-query,90,8
171,258627,53825750,Issue with Scheduler task in Spring Boot,"<p>I am using Scheduler to run automatically with fixeddelay = 120000. 
There are two issues</p>

<p>1). In the scheduler there is call to another service, For each iteration service has to be called once but in this case calling twice. Two Instances calling [APP/PROC/WEB/0] and [APP/PROC/WEB/1]</p>

<pre><code>12/17/18
3:15:18.751 PM  
422 &lt;14&gt;1 2018-12-17T09:45:18.751861+00:00 servicename-serverName 6a5a4df1-f683-4748-986d-aa3e2c289e6d [APP/PROC/WEB/0] - - 2018-12-17 09:45:18 INFO  Path.ClassName.callWorkflow(WorkFlowService.java:53)-|||||Response from Bonita :: {""taskId"":0,""statusCode"":0,""processVersion"":""1.8"",""caseId"":790,""processName"":""ValidationFlow"",""statusMessage"":""Success""}
host =  1.2.3.4 source =    tcp:1234 sourcetype =   rfc5424_syslog
12/17/18
3:15:18.258 PM  
422 &lt;14&gt;1 2018-12-17T09:45:18.258514+00:00 servicename-serverName 6a5a4df1-f683-4748-986d-aa3e2c289e6d [APP/PROC/WEB/1] - - 2018-12-17 09:45:18 INFO  Path.ClassName.callWorkflow(WorkFlowService.java:53)-|||||Response from Bonita :: {""taskId"":0,""statusCode"":0,""processVersion"":""1.8"",""caseId"":789,""processName"":""ValidationFlow"",""statusMessage"":""Success""}

host =  2.3.4.5 source =    tcp:1234 sourcetype =   rfc5424_syslog
</code></pre>

<p>2). Before finishing the first iteration second iteration is starting which is behaving like fixedrate, though i am using fixedDelay.</p>

<p>Splunk Logs :</p>

<pre><code>12/17/18
2:55:14.657 PM  
280 &lt;14&gt;1 2018-12-17T09:25:14.657142+00:00 servicename-serverName 6a5a4df1-f683-4748-986d-aa3e2c289e6d [APP/PROC/WEB/1] - - 2018-12-17 09:25:14 INFO  Path.ClassName.execute(OrderScheduler.java:28)-|||||END OrderScheduler :: execute
host =  1.2.3.4 source =    tcp:9408 sourcetype =   rfc5424_syslog
12/17/18
2:55:14.441 PM  
282 &lt;14&gt;1 2018-12-17T09:25:14.441492+00:00 servicename-serverName 6a5a4df1-f683-4748-986d-aa3e2c289e6d [APP/PROC/WEB/1] - - 2018-12-17 09:25:14 INFO  Path.ClassName.execute(OrderScheduler.java:26)-|||||START OrderScheduler :: execute
host =  1.2.3.4 source =    tcp:9408 sourcetype =   rfc5424_syslog
12/17/18
2:53:15.454 PM  
280 &lt;14&gt;1 2018-12-17T09:23:15.454936+00:00 servicename-serverName 6a5a4df1-f683-4748-986d-aa3e2c289e6d [APP/PROC/WEB/0] - - 2018-12-17 09:23:15 INFO  Path.ClassName.execute(OrderScheduler.java:28)-|||||END OrderScheduler :: execute
host =  1.2.3.4 source =    tcp:9408 sourcetype =   rfc5424_syslog
12/17/18
2:53:14.440 PM  
280 &lt;14&gt;1 2018-12-17T09:23:14.440499+00:00 servicename-serverName 6a5a4df1-f683-4748-986d-aa3e2c289e6d [APP/PROC/WEB/1] - - 2018-12-17 09:23:14 INFO  Path.ClassName.execute(OrderScheduler.java:28)-|||||END OrderScheduler :: execute
host =  1.2.3.4 source =    tcp:9408 sourcetype =   rfc5424_syslog
*12/17/18
2:53:13.504 PM  
282 &lt;14&gt;1 2018-12-17T09:23:13.504977+00:00 servicename-serverName 6a5a4df1-f683-4748-986d-aa3e2c289e6d [APP/PROC/WEB/0] - - 2018-12-17 09:23:13 INFO  Path.ClassName.execute(OrderScheduler.java:26)-|||||START OrderScheduler :: execute
host =  1.2.3.4 source =    tcp:9408 sourcetype =   rfc5424_syslog
12/17/18
2:53:13.045 PM  
282 &lt;14&gt;1 2018-12-17T09:23:13.045063+00:00 servicename-serverName 6a5a4df1-f683-4748-986d-aa3e2c289e6d [APP/PROC/WEB/1] - - 2018-12-17 09:23:13 INFO  Path.ClassName.execute(OrderScheduler.java:26)-|||||START OrderScheduler :: execute
host =  1.2.3.4 source =    tcp:9408 sourcetype =   rfc5424_syslog*
12/17/18
2:51:13.504 PM  
280 &lt;14&gt;1 2018-12-17T09:21:13.504222+00:00 servicename-serverName 6a5a4df1-f683-4748-986d-aa3e2c289e6d [APP/PROC/WEB/0] - - 2018-12-17 09:21:13 INFO  Path.ClassName.execute(OrderScheduler.java:28)-|||||END OrderScheduler :: execute
</code></pre>

<p>Code i am using is below:</p>

<pre><code>@SpringBootApplication
@EnableScheduling
public class Application {
  public static void main(String[] args) {
     SpringApplication.run(Application .class, args);
  }
}
</code></pre>

<hr>

<pre><code>@Service
public class testScheduler() {
@Autowired
private ClassName service;

@Scheduled(fixedDelay = 120000)
public void start() {
   service.methodName();
 }
}
</code></pre>

<p>Issue happening only in server side, but in local system it's working fine.
Can any one help me in this issue please.... Thank you </p>",,0,2,,2018-12-18 02:49:20,,2018-12-18 09:39:18,2018-12-18 09:39:18,,2695504.0,,2695504.0,,1,1,java|spring-boot|scheduler|splunk|spring-boot-actuator,104,8
172,258628,53833256,Splunk alert based on the search result value,"<p>I have splunk logs which will give the ExpiryDate in search result based on the value of the result, need to configure an alert before the 10days of expirydate</p>

<p>Splunk result will be</p>

<pre><code>Expiry Date: 12-28-2019
</code></pre>

<p>Thanks in Advance</p>",,1,0,,2018-12-18 12:42:05,,2018-12-19 02:43:37,,,,,2196474.0,,1,0,splunk|splunk-query,109,9
173,258629,53838867,Splunk query to get max indexed timestamp for a source type,"<p>I need Splunk query to get maximum indexed timestamp or latest indexed timestamp for a source type.</p>

<p>Please help as I am stucked here for quite long.</p>

<p>your help is highly appreciated.</p>

<p>thanks</p>",53843834.0,1,0,,2018-12-18 18:13:24,,2018-12-19 02:39:47,,,,,8604431.0,,1,1,splunk|splunk-query|splunk-sdk,448,12
174,258630,53883490,Splunk match partial result value of field and compare results,"<p>I have 3 fields in my splunk result like message, id and docId. 
Need to group the results by id and doc id which has specific messages</p>

<pre><code>message=""successfully added"" id=1234 docId =1345
message=""removed someUniqueId"" id=1234 docId =1345
</code></pre>

<p>I have to group based on the results by both id's which has the specific message</p>

<pre><code> search query | rex ""message=(?&lt;message[\S\s]*&gt;)"" | where message=""successfully added""
</code></pre>

<p>which is giving result for the first search, when i tried to search for second search query which is not giving result due to the someUniqueId"" </p>

<pre><code> search query | rex ""message=(?&lt;message[\S\s]*&gt;)"" | where match(message, ""removed *"")
</code></pre>

<p>Could you pelase help me to filter the results which has the 2 messages and group by id and docID</p>",,1,0,,2018-12-21 10:51:00,,2018-12-21 15:26:57,,,,,2196474.0,,1,0,splunk|splunk-query,870,12
175,258631,53904723,"Splunk : formatting a csv file during indexing, values are being treated as new columns?","<p>I am trying to create a new field during indexing however the fields become columns instead of values when i try to concat. What am i doing wrong ? I have looked in the docs and seems according .. </p>

<p>Would appreciate some help on this. </p>

<p>e.g. </p>

<p><strong>.csv file</strong> </p>

<pre><code>**Header1**, **Header2**
  Value1   ,121244
</code></pre>

<p><strong>transform.config</strong></p>

<pre><code>[test_transformstanza]
SOURCE_KEY = fields:Header1,Header2
REGEX =^(\w+\s+)(\d+)
FORMAT = 
testresult::$1.$2  
WRITE_META = true
</code></pre>

<p><strong>fields.config</strong></p>

<pre><code>[testresult]
INDEXED = True
</code></pre>

<p>The regex is good, creates two groups from the data, but why is it creating a new field instead of assigning the value to  <code>result</code>?. If i was to do ... <code>testresult::$1</code> or <code>testresult::$2</code> it <strong>works fine</strong>, but when concatenating it creates multiple headers with the value as headername. Is there an easier way to concat fields , e.g. if you have a csv file with header names can you just not refer to the header names? (i know how to do these using calculated fields but want to do it during indexing)</p>

<p>Thanks</p>",,0,2,,2018-12-23 15:12:05,,2018-12-23 15:12:05,,,,,10686648.0,,1,0,splunk|splunk-query|splunk-sdk|splunk-calculation|splunk-formula,70,7
176,258632,53921527,How can we link Splunk with BMC ITAM,<p>Is there any options available to link BMC ITAM with Splunk to create alerts and dashboards for monitoring on incidents?</p>,,1,0,,2018-12-25 10:35:01,,2018-12-26 12:25:30,,,,,10828349.0,,1,0,splunk|splunk-query|bmc,58,7
177,258633,54005145,Splunk Enterprise: large json events not indexed,"<p>I have a sourcetype defined like this (system\local\props.conf):</p>

<pre><code>[my_json]
DATETIME_CONFIG = 
INDEXED_EXTRACTIONS = json
KV_MODE = json
NO_BINARY_CHECK = true
TIMESTAMP_FIELDS = Timestamp
category = Structured
description = json 
disabled = false
pulldown_type = 1
TIME_FORMAT = HH:mm:ss.fff
LINE_BREAKER = ([\r\n]+)
</code></pre>

<p>limits.conf:</p>

<pre><code>[spath]
# Number of characters to read from an XML or JSON event when
# auto extracting.
extraction_cutoff = 5000
extract_all = true
</code></pre>

<p>If I try to index following json (no line breaks, I just formated it here):</p>

<pre><code>{
  ""Timestamp"": ""19:51:27.757"",
  ""Level"": ""INFO"",
  ""EventType"": ""Audit"",
  ""EventId"": ""ApiServiceInvocationResponse"",
  ""ThreadId"": ""19"",
  ""Method"": ""TXXXX1234.Common.WCF.ParameterInspector.AfterCall"",
  ""Context"": {
    ""PhoneNumber"": ""48600000000"",
    ""ApplicationId"": ""7C217CF0CC45E0292623203E56AD87EC"",
    ""ApiType"": ""android"",
    ""ApiVersion"": ""6.0"",
    ""AppVersion"": ""1.0.debug"",
    ""UserId"": 25714,
    ""SessionId"": 1440538,
    ""CorrelationId"": ""98ccaec5-4d23-4c5f-b5da-7ce0e440f2e3""
  },
  ""Payload"": {
    ""Operation"": ""Initialize"",
    ""Response"": {
      ""Message"": null,
      ""CanRun"": true,
      ""PhoneNumber"": null,
      ""DefaultPhoneNumber"": ""48600000000"",
      ""DriverPhoneNumber"": null,
      ""RegisterationPhoneNumber"": null,
      ""Favourites"": null,
      ""SessionId"": ""4DC24EB6E4B0261DD03CDD4F6A7C7DC8"",
      ""IsFriendlyCustomer"": true,
      ""OptionsAvailable"": [],
      ""MaxOrderDate"": null,
      ""FavouriteDriverNumber"": null,
      ""ShareMessage"": null,
      ""PaymentInstruments"": [],
      ""InAppPaymentAvailable"": true,
      ""HasActiveOrders"": false,
      ""UserName"": ""some name"",
      ""UserPhone"": ""48600000000"",
      ""ApplicationId"": """",
      ""KioskInfo"": null,
      ""CallResult"": {
        ""Code"": ""SSREA"",
        ""Message"": null
      }
    },
    ""truncate"": false
  },
  ""Message"": null,
  ""Exception"": null
}
</code></pre>

<p>it gets properly indexed. But the following one does NOT get indexed:</p>

<pre><code>{
  ""Timestamp"": ""16:31:27.074"",
  ""Level"": ""INFO"",
  ""EventType"": ""Audit"",
  ""EventId"": ""ApiServiceInvocationResponse"",
  ""ThreadId"": ""5"",
  ""Method"": ""TXXXX1234.Common.WCF.ParameterInspector.AfterCall"",
  ""Context"": {
    ""PhoneNumber"": ""48600000000"",
    ""ApplicationId"": ""A70BAFD855CE7120A8E331E27D39E645"",
    ""ApiType"": ""MOCK"",
    ""ApiVersion"": ""1.0"",
    ""AppVersion"": null,
    ""UserId"": 11852,
    ""SessionId"": 448107,
    ""CorrelationId"": ""28d9cc6f-c207-4199-9c24-ac6c4b4cfc8e""
  },
  ""Payload"": {
    ""Operation"": ""Initialize"",
    ""Response"": {
      ""Message"": ""message"",
      ""CanRun"": false,
      ""PhoneNumber"": ""48600000000"",
      ""DefaultPhoneNumber"": ""48600000000"",
      ""DriverPhoneNumber"": """",
      ""RegisterationPhoneNumber"": null,
      ""Favourites"": [],
      ""SessionId"": ""0778662D04444C9456694B3FAB44F8C6"",
      ""IsFriendlyCustomer"": true,
      ""OptionsAvailable"": [
        ""PaymentCard"",
        ""Combi"",
        ""SevenSeats"",
        ""Animal"",
        ""AirContition""
      ],
      ""MaxOrderDate"": ""2019-01-30 16:31"",
      ""FavouriteDriverNumber"": null,
      ""ShareMessage"": ""some long share message. http://www.sharing.net.pl/"",
      ""PaymentInstruments"": [],
      ""InAppPaymentAvailable"": false,
      ""HasActiveOrders"": false,
      ""UserName"": ""some name"",
      ""UserPhone"": ""48600000000"",
      ""ApplicationId"": """",
      ""KioskInfo"": null,
      ""CallResult"": {
        ""Code"": ""SSREA"",
        ""Message"": ""Zwr klucz sesji dla zarejestrowanego uzytkownika""
      }
    },
    ""truncate"": false
  },
  ""Message"": null,
  ""Exception"": null
}
</code></pre>

<p>UPDATE:
This is what I have found in logs:
01-02-2019 20:40:31.780 +0100 ERROR JsonLineBreaker - JSON StreamId:9928927958268928125 had parsing error:Unexpected character while parsing backslash escape: 'x' - data_source=""C:\Logs\Txxx.log"", data_host=""WIN-BP2MBISNI04"", data_sourcetype=""my_json""</p>",,1,2,,2019-1-2 11:02:06,,2019-1-2 19:57:16,2019-1-2 19:57:16,,1116499.0,,1116499.0,,1,0,json|splunk,1299,12
178,258634,54036530,DBX Join two database together and filter out the result,"<p>I can write a search like this:</p>

<pre><code>| dbquery ""DB1"" ""SELECT A.* AOS.* FROM Assets A JOIN AssetOSs AOS ON A.AssetOSID = AOS.AssetOSID"" | append [ dbquery ""DB2"" ""SELECT DB1A.IPAddressStr DB2M.User FROM DB1.Assets DB1A JOIN DB2.Machines DB2M ON DB1A.IPAddressStr = DB2M.IP"" 
</code></pre>

<p>The above query combines the result of DB1 with DB2 </p>

<p>But how would I do a dbquery to exclude the result of DB2 from DB1 result </p>

<p>For Example :</p>

<p>DB1 Result : A,B,C,D
DB2 Result :  C,D</p>

<p>Since C and D are present DB2 result  i want to exclde them in DB1 Result and get only A, B in the final result </p>

<p>Any help would be great  </p>",,1,0,,2019-1-4 09:52:06,,2019-1-4 13:32:29,,,,,4763216.0,,1,2,join|splunk|dbx,52,7
179,258635,54040119,Splunk lookuptable,"<p>I have a <code>csv</code> with different kind of IoCs in it like email addresses, IPs, etc. I want to run a search on any of my indexes which would return each record that has any match with my list.
This is what I want to achieve:</p>

<pre><code>index=* ""item1"" OR ""item2"" OR ""item3""
</code></pre>

<p>Since I have a thousand items on my list this won't work. So, I uploaded my csv as a <code>lookuptable</code> and tried the following:</p>

<pre><code>index=* [| inputlookup  test.csv]
</code></pre>

<p>This returns nothing, but if I search for each item ""manually"" then I get results.
What am I missing?</p>",,1,1,,2019-1-4 13:42:18,,2019-1-4 16:24:03,,,,,1806838.0,,1,0,splunk|splunk-query,44,7
180,258636,54065678,How to configure PagerDuty alerts in Splunk Cloud?,"<p>I've run into a few different issues with the PagerDuty integration in Splunk Cloud.</p>

<p>The documentation on PagerDuty's site is either outdated, not applicable to Splunk Cloud or else there's something wrong with the way my Splunk Cloud account is configured (could be a permissions issue): <a href=""https://www.pagerduty.com/docs/guides/splunk-integration-guide/"" rel=""nofollow noreferrer"">https://www.pagerduty.com/docs/guides/splunk-integration-guide/</a>. I don't see an Alert Actions page in Splunk Cloud, I have a Searches, Reports and Alerts page though.</p>

<p>I've configured PD alerts in Splunk using the alert_logevent app but it's not clear if I should instead be using some other app. These alerts do fire when there are search hits but I'm seeing another issue (below). The alert_webhook app type seems like it might be appropriate but I was unable to get it to work correctly. I cannot create an alert type using the pagerduty_incident app. . . although I can set it as a Trigger Action (I guess this is how it's supposed to work, I don't find the UI to intuitive here).</p>

<p>When my alerts fire and create incidents in PagerDuty, I do not see a way to set the PagerDuty incident severity.</p>

<p>Also, the PD incidents include a link back to Splunk, which I believe should open the query with the search hits which generated the alert. However, the link brings me to a page with a Page Not Found! error. It contains a link to ""more information about my request"" which brings up a Splunk query with no hits. This query looks like ""index=_internal, host=SOME_HOST_ON_SPLUNK_CLOUD, source=*web_service.log, log_level=ERROR, requestid=A_REQUEST_ID"". It it not clear to me if this is a config issue, bug in Splunk Cloud or possibly even a permissions issue for my account.</p>

<p>Any help is appreciated.</p>",,1,0,,2019-1-6 20:32:30,0.0,2019-2-26 01:05:39,,,,,1354026.0,,1,1,splunk|pagerduty,462,11
181,258637,54107035,Splunk - Get Prefefined Outputs Based on the event count and event data,"<p>I have a query as below. The result is always predefined as -</p>

<ol>
<li>If the query result has 3 events and if the 3rd event has event=""delivered"" as value then the whole transaction needs to be returned as ""COMPLETE"". </li>
<li>If the 3rd event is present and event!=""delivered"" then the status becomes ""PENDING""</li>
<li>If the 3rd event is not present at all, then the transaction is marked as ERROR</li>
</ol>

<p>My Query -</p>

<pre><code>index=myindex OR index=myindex2 uuid=98as786-ffe6-4de1-929y-080e99bc2e6r (status=""202"") OR (TransactionStatus=""PUBLISHED"") | append [search index=myindex2 (logMessage=""Producer created new event"") event=""delivered"" OR event=""processed"" serviceName=""abc"" [search index=myindex uuid=98as786-ffe6-4de1-929y-080e99bc2e6r AND status=""SUCCESS"" AND serviceName=""abc"" | top limit=1 headerId | fields + headerId | rename headerId as message_id]]
</code></pre>

<p>Result events -</p>

<p>Event1 - 202 Accepted</p>

<p>Event 2 - Adapter Success</p>

<p>Event 3 - delivered or error or processed</p>

<p>My high level dashboard should look like below -</p>

<p>Complete - 6378638</p>

<p>Pending - 2173</p>

<p>Error - 6356</p>

<p>The unique ID will be the UUID on which the count to be performed. 
What can be the possible way we can do this - eval ? Lookup ? not sure as I am new to splunk.
Please let me know if more information is needed if I am missing something.</p>",54111335.0,1,0,,2019-1-9 09:38:35,,2019-1-9 13:31:26,,,,,7611518.0,,1,0,splunk|splunk-query,257,10
182,258638,54110465,splunk query not showing all records when converted in table format,"<pre><code>sourcetype=my-job ""Get Connection Details"" | spath input=Message | search FileName=* | rename event.Values.Connections{}.ClientName as ThirdParty
</code></pre>

<p>This query returns some N number of records, but as soon as I apply below filter </p>

<pre><code>| dedup FileName| table  FileName, ThirdParty | fillnull value=N/A | sort  ThirdParty desc
</code></pre>

<p>Query shows only N-M records.</p>

<p>Hence it is not showing all the ThirdParty in result</p>",55673943.0,1,0,,2019-1-9 12:43:48,,2019-4-14 12:27:17,2019-4-14 12:27:17,,8926905.0,,8268647.0,,1,0,splunk-query,79,7
183,258639,54129766,How to debug a Splunk application with pdb?,"<p>How can I spawn a pdb-like debugger in a Splunk application (meaning: an application made for and ran by Splunk) ?</p>

<p>I have no control over the python process itself, so simply putting <code>import pdb; pdb.set_trace()</code> in the code will just result in the web app crashing.</p>

<p>I guess the ideal solution would be to  </p>

<ul>
<li>either run the python part of Splunk manually, so I have control over it (I tried <a href=""https://www.splunk.com/blog/2014/02/03/how-to-debug-django-applications-with-pdb-pycharm-and-visual-studio.html"" rel=""nofollow noreferrer"">this</a>, but it didn't work correctly; mongodb daemon wasn't starting, among other things)</li>
<li>use the good old <code>import pdb; pdb.set_trace()</code> breakpoint but attach to the process somehow, so I'm able to manipulate the debugger (I tried <a href=""https://wiki.python.org/moin/DebuggingWithGdb"" rel=""nofollow noreferrer"">gdb</a>, but nothing worked as expected -- perhaps I didn't use it correctly)</li>
</ul>",54130196.0,1,1,,2019-1-10 13:28:42,,2019-1-10 13:52:25,,,,,1030960.0,,1,0,python|splunk|pdb,71,8
184,258640,54129798,Splunk - Remove events between 1st login and last logout while user has any session open where logins and logouts can happen multiple times in a row?,"<p>The questions sounds a bit confusing, so to break it down, I'm trying to find the time difference between logins and logouts. The catch is two-fold. It's possible that the time range doesn't catch the first login that's logged out during the time range, and vice versa it's possible that the time range doesn't catch the last logout that's logged in during the time range. This can end up looking like below in a resulting table.</p>

<pre><code>| Action | Action Number |
|--------|---------------|
| Login  | 1             |- 1am
| Login  | 2             |- 1:01 am
| Logout | 1             |- 1:02 am
| Logout | 2             |- 1:03 am
| Logout | 3             |- 1:04 am
| Logout | 4             |- 1:05 am
| Login  | 3             |- 1:10 am
| Logout | 5             |- 1:11 am
| Login  | 4             |- 1:15 am
| Login  | 5             |- 1:16 am
| Logout | 6             |- 1:17 am
| Login  | 6             |- 1:18 am
| Logout | 7             |- 1:20 am
| Logout | 8             |- 1:22 am
</code></pre>

<p>Where action number is what number that login/logout is during the time frame. For example, the first login will have an action number of 1, as will the first logout, and so on. </p>

<p>I've written the logic to get that in place, but what I need help with is ""removing"" the events in between the first login and last logout for each break in activity (periods when the user had no sessions logged in). </p>

<p>This would mean that the first login (for the time range) for this user would be Login - 1, and they had a logged in session until Logout 4. This means I would want to remove Login 2 and Logout 1 and 2. Then I can calculate the time difference between the two remaining events to find the total time they were logged in to any session in that period. </p>

<p>To summarize, the following is the result that I'm wanting to generate from the above table, but I can't find a good way to do this.</p>

<pre><code>| Action | Action Number | Flag for Deletion |
|--------|---------------|-------------------|
| Login  | 1             | False             |
| Login  | 2             | True              |
| Logout | 1             | True              |
| Logout | 2             | True              |
| Logout | 3             | True              |
| Logout | 4             | False             |
| Login  | 3             | False             |
| Logout | 5             | False             |
| Login  | 4             | False             |
| Login  | 5             | True              |
| Logout | 6             | True              |
| Login  | 6             | True              |
| Logout | 7             | True              |
| Logout | 8             | False             |
</code></pre>",54407381.0,1,2,,2019-1-10 13:30:08,,2019-1-28 17:34:06,2019-1-11 16:15:47,,7507925.0,,7507925.0,,1,0,splunk|timedelta|splunk-query,335,11
185,258641,54144150,"How can I log from my python application to splunk, if I use celery as my task scheduler?","<p>I have a python script running on a server, that should get executed once a day by the celery scheduler. I want to send my logs directly from the script to splunk. I am trying to use this <a href=""https://github.com/zach-taylor/splunk_handler"" rel=""nofollow noreferrer"">splunk_handler</a> library. If I run the splunk_handler without celery locally, it seems to work. But if I run it together with celery there seem to be no logs that reach the splunk_handler. Console-Log:</p>

<blockquote>
  <p>[SplunkHandler DEBUG] Timer thread executed but no payload was available to send</p>
</blockquote>

<p>How do I set up the loggers correctly, so that all the logs go to the splunk_handler?</p>

<p>Apparently, celery sets up its own loggers and overwrites the root-logger from python. I tried several things, including connecting the setup_logging signal from celery to prevent it to overwrite the loggers or setting up the logger in this signal.</p>

<pre><code>import logging
import os

from splunk_handler import SplunkHandler
</code></pre>

<p>This is how I set up the logger at the beginning of the file</p>

<pre><code>logger = logging.getLogger(__name__)
splunk_handler = SplunkHandler(
host=os.getenv('SPLUNK_HTTP_COLLECTOR_URL'),
port=os.getenv('SPLUNK_HTTP_COLLECTOR_PORT'),
token=os.getenv('SPLUNK_TOKEN'),
index=os.getenv('SPLUNK_INDEX'),
debug=True)

splunk_handler.setFormatter(logging.BASIC_FORMAT)
splunk_handler.setLevel(os.getenv('LOGGING_LEVEL', 'DEBUG'))
logger.addHandler(splunk_handler)
</code></pre>

<p>Celery initialisation (not sure, if <code>worker_hijack_root_logger</code> needs to be set to <code>False</code>...)</p>

<pre><code>app = Celery('name_of_the_application', broker=CELERY_BROKER_URL)
app.conf.timezone = 'Europe/Berlin'
app.conf.update({
    'worker_hijack_root_logger': False,
})
</code></pre>

<p>Here I connect to the setup_logging signal from celery</p>

<pre><code>@setup_logging.connect()
def config_loggers(*args, **kwags):
    pass
    # logger = logging.getLogger(__name__)
    # splunk_handler = SplunkHandler(
    #     host=os.getenv('SPLUNK_HTTP_COLLECTOR_URL'),
    #     port=os.getenv('SPLUNK_HTTP_COLLECTOR_PORT'),
    #     token=os.getenv('SPLUNK_TOKEN'),
    #     index=os.getenv('SPLUNK_INDEX'),
    #     debug=True)
    #
    # splunk_handler.setFormatter(logging.BASIC_FORMAT)
    # splunk_handler.setLevel(os.getenv('LOGGING_LEVEL', 'DEBUG'))
    # logger.addHandler(splunk_handler)
</code></pre>

<p>Log statement</p>

<pre><code>logger.info(""ARBITRARY LOG MESSAGE"")
</code></pre>

<p>When activating debug on splunk handler (set to <code>True</code>), the splunk handler logs out that there is no payload available as already posted above. Does anybody have an idea what's wrong with my code?    </p>",54252612.0,1,0,,2019-1-11 09:59:10,2.0,2019-1-18 11:39:38,,,,,10180115.0,,1,4,python|logging|celery|splunk,4141,16
186,258642,54171039,Learning the basics to query in Splunk,"<p>I am just starting to learn how to work on Splunk. Where can I find the tutorials to learn from the basics for real-time log analysis and generating reports. I tried looking up on google and youtube but, It is somewhat complex for me. Is there a dummy guide where I can start reading or what?</p>",54172529.0,1,0,,2019-1-13 16:49:53,,2019-1-13 19:40:35,,,,,10908498.0,,1,0,real-time|splunk|documentation-generation|log-analysis,778,11
187,258643,54210164,How to add custom field to events in Splunk?,"<p>I want to add custom fields to specific index and have them log accordingly.</p>

<p>Currently there are only a few default fields such as ""host"", ""index"", ""sourcetype"", etc...</p>

<p>Not sure if this is the best place to add additional data or not.</p>

<p>How can I add more fields?</p>

<p>I am currently using the Splunk SDK to submit events.</p>

<p><a href=""https://i.stack.imgur.com/tmBXG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tmBXG.png"" alt=""enter image description here""></a></p>",54217692.0,2,0,,2019-1-16 03:51:26,,2019-1-16 15:56:09,2019-1-16 15:56:09,,6716361.0,,6716361.0,,1,0,events|logging|splunk,790,12
188,258644,54225740,how to run command line in linux from another user,"<p>I need to execute a bash script from a python program (python2.7) in red hat, while this bash script has to run from another user account as splunk user.</p>

<p>In the Linux, I will switch to splunk user <code>su - splunk</code> first, then type the command <code>./mybashFile</code> under as a splunk user</p>

<p>Here is what I tried: </p>

<pre><code>import  subprocess
cmd1=subprocess.Popen([""su"",""-"",""splunk""],shell=True,stdout=subprocess.PIPE)
cmd2=subprocess.Popen([""./path/myBashFile.sh""],shell=True,stdin=cmd1.stdout,stdout=subprocess.PIPE)
cmd2.stdout
</code></pre>

<p>I still cannot run this bash file from as a splunk user. This command cannot pass to another account.</p>",,1,1,,2019-1-16 21:33:32,,2019-1-16 21:38:14,,,,,3885771.0,,1,1,python|linux|subprocess|splunk,108,12
189,258645,54234568,How to consolidate macro results,"<p>I have two macros a and b, both output a table.</p>

<p>How can I create a query to get a table with consolidated results of both macros?</p>",,1,3,,2019-1-17 11:06:14,,2019-1-24 14:35:05,2019-1-17 11:06:37,,77764.0,,6951304.0,,1,0,splunk,43,6
190,258646,54246227,How to filter logs based on severity in fluentd and send it to 2 different logging systems,"<p>i need help to configure Fluentd to filter logs based on severity.</p>

<p>we have 2 different monitoring systems Elasticsearch and Splunk, when we enabled log level DEBUG in our application it's generating tons of logs everyday, so we want to filter logs based severity and push it to 2 different logging systems.</p>

<p>when logs has severity: INFO and ERROR then forward container logs to Splunk and except those DEBUG, TRACE, WARN and other logs should go to elastocsearch, please help me how can we do filter it.</p>

<p>Here is the log generated format:</p>

<p><em>event.log:{""@severity"":""DEBUG"",""@timestamp"":""2019-01-18T00:15:34.416Z"",""@traceId"":</em></p>

<p><em>event.log:{""@severity"":""INFO"",""@timestamp"":""2019-01-18T00:15:34.397Z"",""@traceId"":</em></p>

<p><em>event.log:{""@severity"":""WARN"",""@timestamp"":""2019-01-18T00:15:34.920Z"",""@traceId"":</em></p>

<p>please find below fluentd config.</p>

<p>i have added exclude method inside filter and also installed grep plugin added grep method, its not working.</p>

<p>added filter for testing:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;exclude&gt;
       @type grep
       key severity 
       pattern DEBUG
      &lt;/exclude&gt;</code></pre>
</div>
</div>
</p>

<p>also added:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;filter kubernetes.**&gt;
@type grep
exclude1 severity (DEBUG|NOTICE|WARN)
&lt;/filter&gt;</code></pre>
</div>
</div>
</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>kind: ConfigMap
apiVersion: v1
metadata:
  name: fluentd-config
  namespace: logging
  labels:
    k8s-app: fluentd
data:
  fluentd-standalone.conf: |
    &lt;match fluent.**&gt;
      @type null
    &lt;/match&gt;
    # include other configs
    @include systemd.conf
    @include kubernetes.conf
  fluentd.conf: |
    @include systemd.conf
    @include kubernetes.conf
  fluentd.conf: |
    # Use the config specified by the FLUENTD_CONFIG environment variable, or
    # default to fluentd-standalone.conf
    @include ""#{ENV['FLUENTD_CONFIG'] || 'fluentd-standalone.conf'}""
  kubernetes.conf: |
    &lt;source&gt;
      @type tail
      @log_level debug
      path /var/log/containers/*.log
      pos_file /var/log/kubernetes.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag kubernetes.*
      format json
    &lt;/source&gt;
    &lt;filter kubernetes.**&gt;
      @type kubernetes_metadata
      verify_ssl false
      &lt;exclude&gt;
       @type grep
       key severity 
       pattern DEBUG
      &lt;/exclude&gt;
    &lt;/filter&gt;
    &lt;filter kubernetes.**&gt;
      @type record_transformer
      enable_ruby
      &lt;record&gt;
        event ${record}
      &lt;/record&gt;
      renew_record
      auto_typecast
    &lt;/filter&gt;
    &lt;filter kubernetes.**&gt;
    @type grep
    exclude1 severity (DEBUG|NOTICE|WARN)
    &lt;/filter&gt;
  kubernetes.conf: |
    &lt;source&gt;
      @type tail
      @log_level debug
      path /var/log/containers/*.log
      pos_file /var/log/kubernetes.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag kubernetes.*
      format json
    &lt;/source&gt;
    &lt;filter kubernetes.**&gt;
      @type kubernetes_metadata
      verify_ssl false
    &lt;/filter&gt;
    &lt;filter kubernetes.**&gt;
      @type record_transformer
      enable_ruby
      &lt;record&gt;
        event ${record}
      &lt;/record&gt;
      renew_record
      auto_typecast
    &lt;/filter&gt;
    # The `all_items` paramater isn't documented, but it is necessary in order for
    # us to be able to send k8s events to splunk in a useful manner
    &lt;match kubernetes.**&gt;
      @type copy
      &lt;store&gt;
        @type splunk-http-eventcollector
        all_items true
        server localhost:8088
        protocol https
        verify false
      &lt;/store&gt;
      &lt;store&gt;
        @type elasticsearch
        host localhost
        port 9200
        scheme http
        ssl_version TLSv1_2
        ssl_verify false
        &lt;/buffer&gt;
      &lt;/store&gt;
    &lt;/match&gt;</code></pre>
</div>
</div>
</p>",54246913.0,1,0,,2019-1-18 00:31:03,3.0,2019-1-18 02:16:22,2019-1-18 00:38:18,,8228726.0,,8228726.0,,1,1,elasticsearch|kubernetes|splunk|fluentd,5939,15
191,258647,54261829,Splunk JAVA SDK - Fetch FiredAlerts (i.e Alerts already triggered),"<p>I am trying to fetch the fired alerts using the splunk java sdk. But i am not able to get any reference. Here is the code snippet i have and it is not fetching me any results.</p>

<pre><code>FiredAlertGroupCollection firedAlertGroups = service.getFiredAlertGroups();
         System.out.println(""Fired Alert : "" + firedAlertGroups.size());
         for (FiredAlertGroup entity : firedAlertGroups.values()) {
             EntityCollection&lt;FiredAlert&gt; alerts = entity.getAlerts();
             for (FiredAlert alert : alerts.values()) {
                 System.out.println(""alerts &gt;&gt;&gt;&gt; "" +alert.getName() +"": "" + alert.getSavedSearchName()+"": "" + alert.getTitle());
             }
         }
</code></pre>

<p>Is there a way, i can filter the alerts fired in the last 24 hours etc. </p>

<p>Please help me on the same.</p>",,0,0,,2019-1-18 21:44:49,,2019-1-18 21:44:49,,,,,5021684.0,,1,1,java|sdk|splunk,39,6
192,258648,54308034,"How can I get ""comma separated"" data from .csv file into Json Format on Splunk Web?","<p>I have one ""Bal_123.csv"" file and when I am searching its data on splunk web by providing query "" sourcetype=""Bal_123.csv"" "" I am getting latest indexed raw data in comma separated format. But for further operation I need that data in .Json format</p>

<p>Is there any way we can get that data in .Json format itself. I know I can export the data in Json format but I am using Rest call to get data from splunk and I need that Json data on splunk itself.</p>

<p>can anyone help me regarding this?</p>",,1,2,,2019-1-22 12:10:18,,2019-1-22 15:41:57,,,,,8604431.0,,1,0,splunk|splunk-query|splunk-sdk,270,9
193,258649,54326561,How to use rex command with REST api of splunk curl as client,"<p>i am trying to Extract new field from raw data by regular expression(rex command).My regular expression is working fine in splunk web search bar and getting results. But not not working with REST api curl as client.</p>

<p>i want to extract a field from a csv data set train.csv and want to give it name as ""numbers""</p>

<pre><code>curl -u admin:password -k https://localhost:8089/services/search/jobs -d search=""search source=train.csv|rex field=_raw '^(?:\[^\""\\n\]*\""){2},\\w+,\\d+,\\d+,\\d+,\\d+,\\d+\\.\\d+,(?P&lt;numbers&gt;\[^,\]+)'| top numbers""
</code></pre>

<p>by executing this command i got sid</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;

&lt;response&gt;
 &lt;sid&gt;1548238904.70&lt;/sid&gt;
</code></pre>

<p>by after asking for result i am getting error</p>

<pre><code>curl -u admin:password -k https://localhost:8089/services/search/jobs/1548238904.70/results

Error in 'rex' command: The regex ''^(?:\[^\n\]*){2}' does not extract anything. It should specify at least one named group. Format: (?&amp;lt;name&amp;gt;...).&lt;/msg&gt;
</code></pre>

<p>what is named group ,why its working well in splunk search bar</p>

<p>i want result with ""number"" as column or new field</p>",54446192.0,1,0,,2019-1-23 11:46:59,,2019-1-30 17:25:28,2019-1-25 04:47:49,,504685.0,,9799747.0,,1,0,rest|api|splunk,690,11
194,258650,54362425,run subquery for each row of csv file passing the field in search string,"<p>I want to run a splunk query for all the values in the csv file and replace the value with the field in the csv file. I've imported the file into splunk as input loookup table and able to view the fields using inputlookup query but I want to run that with all the sub queries where I'm fetching maximum count per hour, per day, per week and per month basis</p>

<p>input file is ids.csv which has around 800 rows and its just one column, liek below:</p>

<pre><code>1234,
2345
2346
4567
...
</code></pre>

<p>query that im using:</p>

<pre><code>| inputlookup ids.csv | fields ids as id |  [search index=""abc"" id ""search string here"" |bin _time span=""1hour"" | stats count as maxHour by _time | sort - count | head 1]  |appendcols[search
 index=""abc"" id ""search string here"" |bin _time span=""1day"" | stats count as maxDay by _time | sort - count |head 1 ]|appendcols[search
 index=""abc"" id ""search string here"" |bin _time span=""1week"" | stats count as maxWeek by _time | sort - count | head 1 ]|appendcols[search
 index=""abc"" id ""search string here"" |bin _time span=""1month"" | stats count as maxMonth by _time | sort - count | head 1]
</code></pre>

<p>Im not getting the expected results for this, Im expecting a tabular format where i get the count for each time range with the specific id by passing id field in the search subquery.</p>

<p>How can I solve this?</p>

<p>Thanks</p>",,1,0,,2019-1-25 09:33:45,,2019-1-27 19:29:32,,,,,2340345.0,,1,1,splunk|splunk-query,816,12
195,258651,54401802,Unable to get Splunk query SID,"<p>Below is my code snippet:</p>

<pre><code>search='index=""someindex"" earliest=27/01/2019:0:0:0 latest=27/01/2018:23:59:00'

data = {'search': search, 'max_count':'10000000'}
response = requests.post('https://something:8089/services/search/jobs',
                         auth=('usr', 'pwd'), data=data, verify=False)
#print(response)                         
root = ET.fromstring(response.text)
#print(root)
for tag in root:
    job_id = tag.text
    print(job_id)
print(job_id)
</code></pre>

<p>I'm getting 400 in response and an error in printing <code>job_id</code>.</p>",,1,0,,2019-1-28 12:14:03,,2019-2-26 23:04:46,2019-1-28 14:04:21,,7851470.0,,10147018.0,,1,0,python|splunk|splunk-query|splunk-sdk,154,8
196,258652,54424914,Javascript RequireJS with Async / Await,"<p>Hey I am writing a small lib Splunk to use the ES6 feature Async/Await to communicate with the backend. But when I set the Arrow Function in the define statement <code>define([], async () =&gt; { await smth });</code> it seems that it is not invoked anymore.</p>

<p>Here my code so far:</p>

<p><strong>controller.js</strong></p>

<pre><code>require([
  '../app/splunk-async/splunk-async',
], async (SplunkAsync) =&gt; {
  const SA = new SplunkAsync();

  const data = await SA.get('/services/authentication/users');
  console.log(data)
});
</code></pre>

<p><strong>splunk-async.js</strong></p>

<pre><code>//# sourceURL=splunk-async.js

define(['splunkjs/mvc'], (mvc) =&gt; 
  class asyncSplunk {
    constructor () {
      this._service = mvc.createService();
    }

    /**
     * 
     * @param {String} uri 
     */
    get (uri) {
      return new Promise((resolve, reject) =&gt; {
        const service = this._service;

        service.get(uri)          
          .done(res =&gt; resolve(res))
          .fail((data, status, err) =&gt; reject(data, status, err));
      });
    }
  }
);
</code></pre>

<p>Does anyone has a solution? Thanks :)</p>

<h2>Edit 1:</h2>

<p>Thank you for your responses. I've tried all your suggestions. But none of them seem to work. When I wrap everything in try/catch nothing is ""catched"". The function Promise.promisify() does not exit. It seems that I have to tell the 
define function, that its body is async which I tried: async () => {}.</p>

<p>Any more suggestions?</p>",,0,4,,2019-1-29 15:56:31,,2019-1-30 15:02:57,2019-1-30 15:02:57,,9193813.0,,9193813.0,,1,1,javascript|async-await|requirejs|splunk|ecmascript-2016,1765,12
197,258653,54458551,Splunk search query with where clause not working,"<p>I am using Splunk java SDK to search pattern from Splunk server. I am using pattern </p>

<pre><code>search index=* env=* (GET OR POST OR PUT OR DELETE) | where isNum(httpStatusCode)
</code></pre>

<p>when I am using this query with Java SDK, Splunk is not sending any event. But when I am querying from Splunk web app it is showing the response. My Splunk log contains two type of event. one with httpStatusCode string and another with httpStatusCode number.Both type of events are below:</p>

<p>Type 1</p>

<pre><code>[31/Jan/2019:10:27:49.970 +0000] 10.255.0.93 ""GET URL HTTP/1.1"" 200 46 10 host ""34*, 10.*, 54.*""
HTTP status code =200
</code></pre>

<p>Type 2 </p>

<pre><code>[31/Jan/2019:10:27:49.961 +0000] http-nio-8080-exec-58 INFO RequestID=36de3bde-277a-4f60-82c9-2802debe0593 RequestPath=http:* RequestMethod=GET TimeTaken=3 ms
httpStatusCode =    RequestApplication=someString
</code></pre>

<p>How can I segregate the events which have the status of type number through Splunk rest API?</p>

<p>Thanks in Advance.</p>",,1,0,,2019-1-31 10:33:00,,2019-3-28 06:18:13,2019-2-1 04:35:57,,4410922.0,,8612686.0,,1,1,splunk|splunk-query|splunk-sdk,720,11
198,258654,54487534,Python equivalent of wrapping POST payload in single quotes,"<p>This is more of a python question than Splunk but would be helpful if anyone had done this... specifically <a href=""https://docs.splunk.com/Documentation/Splunk/7.2.3/Data/Sendmetricstoametricsindex"" rel=""nofollow noreferrer"">here</a>, there's a discussion of sending multiple metrics in a single POST to the server. The example they provide is using curl and wrapping the entire payload in single quotes ('), e.g.</p>

<pre><code>curl -k http://&lt;IP address or host name or load balancer name&gt;:8088/services/collector  \
-H ""Authorization: Splunk 98a1e071-bc35-410b-8642-78ce7d829083""                         
\
-d '{""time"": 1505501013.000,""source"":""disk"",""host"":""host_99"",""fields"": 
{""region"":""us-west-1"",""datacenter"":""us-west- 1a"",""rack"":""63"",""os"":""Ubuntu16.10"",""arch"":""x64"",""team"":""LON"",""service"":""6"",""service_version"":""0"",""service_environment"":""test"",""path"":""/dev/sda1"",""fstype"":""ext3"",""_value"":999311222774,""metric_name"":""total""}}
{""time"": 1505511013.000,""source"":""disk"",""host"":""host_99"",""fields"": 
{""region"":""us-west-1"",""datacenter"":""us-west-1a"",""rack"":""63"",""os"":""Ubuntu16.10"",""arch"":""x64"",""team"":""LON"",""service"":""6"",""service_version"":""0"",""service_environment"":""test"",""path"":""/dev/sda1"",""fstype"":""ext3"",""_value"":1099511627776,""metric_name"":""total""}}'
</code></pre>

<p>My question is how to do the same thing in python – i.e. you can't wrap multiple JSON objects in single quotes like in the curl command - that just makes the entire payload a string. Is there some other wrapper that can be used for this purpose?</p>

<p>So, this works:</p>

<pre><code>payload = {""time"": 1505501013.000,""source"":""disk"",""host"":""host_99"",""fields"": 
{""region"":""us-west-1"",""datacenter"":""us-west- 1a"",""rack"":""63"",""os"":""Ubuntu16.10"",""arch"":""x64"",""team"":""LON"",""service"":""6"",""service_version"":""0"",""service_environment"":""test"",""path"":""/dev/sda1"",""fstype"":""ext3"",""_value"":999311222774,""metric_name"":""total""}}
</code></pre>

<p>But this does not:</p>

<pre><code>payload = {""time"": 1505501013.000,""source"":""disk"",""host"":""host_99"",""fields"": 
{""region"":""us-west-1"",""datacenter"":""us-west- 1a"",""rack"":""63"",""os"":""Ubuntu16.10"",""arch"":""x64"",""team"":""LON"",""service"":""6"",""service_version"":""0"",""service_environment"":""test"",""path"":""/dev/sda1"",""fstype"":""ext3"",""_value"":999311222774,""metric_name"":""total""}}
 {""time"": 1505511013.000,""source"":""disk"",""host"":""host_99"",""fields"": 
{""region"":""us-west-1"",""datacenter"":""us-west-1a"",""rack"":""63"",""os"":""Ubuntu16.10"",""arch"":""x64"",""team"":""LON"",""service"":""6"",""service_version"":""0"",""service_environment"":""test"",""path"":""/dev/sda1"",""fstype"":""ext3"",""_value"":1099511627776,""metric_name"":""total""}}
</code></pre>

<p>FYI, then the POST looks like:</p>

<pre><code> resp = requests.post(splunkurl,json=payload,headers=headers)
</code></pre>",54487602.0,1,3,,2019-2-1 21:43:13,,2019-2-2 08:29:36,2019-2-2 08:29:36,,2161778.0,,3712321.0,,1,2,python|json|splunk,172,14
199,258655,54524581,Filtering out parts of file path after a certain point,"<p>File paths are searching into a deeper level than intended. All i need is the /aaa but my search is populating all the values for /aaa/bbbb/ccc/ddd/. is there a way to remove everything after /aaa/?</p>

<pre><code>my search: index=web_pn_iis sourcetype=iis  cs_uri_stem=""*"" sc_status=4* | where not cs_uri_stem=""/*"" 

returned values:/aaa/bbbb/ccc/ddd/
want: /aaa
</code></pre>",54527553.0,1,0,,2019-2-4 21:30:32,,2019-2-5 04:02:48,,,,user10997029,,,1,0,splunk,41,6
200,258656,54561038,Splunk query to filter results in IIS log to identify CRYPT_Protocol values less than 400,<p>I am trying to find a regex expression to help filter splunk results from ingested IIS logs such that when the CRYPT_PROTOCOL response is less than 400 it is displayed. </p>,,1,3,,2019-2-6 19:18:43,,2019-2-7 19:38:18,,,,,11024866.0,,1,-1,regex|iis-8|splunk-query,354,9
201,258657,54562634,Add calculated threshold line on splunk timechart,"<p>I have a simple chart which shows the bottom 5 servers by number of request per minute. I'm looking to add a calculated threshold overlay line that is the average number of requests across all servers minus one standard deviation. I have been searching for hours but I have not been able to find anything. </p>

<p>Current Search Query:</p>

<blockquote>
  <p>sourcetype=x source=y host=""server*"" ENTERING | timechart useother=f
  span=1m count by host WHERE count in bottom5</p>
</blockquote>

<p>I essentially want something like the below (which doesn't work of course):</p>

<blockquote>
  <p>sourcetype=x source=y host=""server*"" ENTERING | timechart useother=f
  span=1m count by host WHERE count in bottom5 | eval
  threshold=(avg(countByHost) - stdev(countByHost))</p>
</blockquote>",,1,0,,2019-2-6 21:12:50,,2019-2-15 20:32:05,,,,,11025260.0,,1,0,splunk|splunk-query,626,11
202,258658,54565821,How to convert from curl to python,"<p>How to convert this to python requests?</p>

<pre><code>curl --noproxy '*' -H ""Content-Type: application/json; charset=utf-8"" \
     -k --user ""admin:password""  \
     -d""search=| savedsearch mysearch"" \
     -X POST 'https://splunk:8089/services/search/jobs' 

def no_ssl_verification(session=requests.Session):
    old_request = session.request
    session.request = partialmethod(old_request, verify=False)

    with warnings.catch_warnings():
        warnings.simplefilter('ignore', InsecureRequestWarning)
        yield

    session.request = old_request


url='https://splunk:8089/services/search/jobs'
jsonDict = {""search"": ""search=| savedsearch mysearch""}
print(jsonDict)
proxy = {'http': None, 'https': None}
with no_ssl_verification():
    r = requests.post(url, data=jsonDict, headers={'username':'admin', 
       'password':'password'}, json=jsonDict, verify=False, proxies=proxy)
    print (r.text)
</code></pre>

<p>=========</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;response&gt;
  &lt;messages&gt;
    &lt;msg type=""ERROR""&gt;Unauthorized&lt;/msg&gt;
  &lt;/messages&gt;
&lt;/response&gt;
</code></pre>",,0,0,,2019-2-7 03:22:09,,2019-2-7 03:51:13,2019-2-7 03:51:13,,4492932.0,,10438913.0,,1,1,python-3.x|splunk-sdk,77,7
203,258659,54567818,Extract Values from a field,"<p>I need to extract the whole value from a field</p>

<p>I have tried different Regex patterns and it did not work and was wondering if there was a simple way to do this.</p>

<p>Here's an example Splunk Event </p>

<pre><code>HelloSample=My tool is too picky and has a hard time
</code></pre>

<p>Here's my splunk query</p>

<pre><code>fields  HelloSample
</code></pre>

<p>This returns</p>

<pre><code>My
</code></pre>

<p>I want it to return the whole string like below</p>

<pre><code>My tool is too picky and has a hard time
</code></pre>",,2,2,,2019-2-7 06:53:49,,2020-11-4 12:58:40,,,,,11026796.0,,1,-1,regex|splunk|splunk-query,123,7
204,258660,54579135,Is it possible to integrate Log4net and Splunk without a Forwarder?,"<p>I'm trying to send Umbraco logs into Splunk, and I'm investigating a way to do this without using a forwarder. We are hosting Umbraco in Azure.</p>

<p>I'm looking into Log4net <a href=""https://logging.apache.org/log4net/release/config-examples.html#RemotingAppender"" rel=""nofollow noreferrer"">RemoteAppender</a>, but it doesn't seem to be a way to authenticate against Splunk.</p>

<p>Is there an other option?</p>

<p>I found this options, but all of them seem to need installing some library or writing code, and I'm looking for an option that only needs changing the config file ""log4net.config"":</p>

<ul>
<li><a href=""http://dev.splunk.com/view/splunk-loglib-dotnet/SP-CAAAEX4"" rel=""nofollow noreferrer"">http://dev.splunk.com/view/splunk-loglib-dotnet/SP-CAAAEX4</a></li>
<li><a href=""https://github.com/AlanBarber/log4net.Appender.Splunk"" rel=""nofollow noreferrer"">https://github.com/AlanBarber/log4net.Appender.Splunk</a></li>
</ul>

<p>And this answer suggest using a forwarder: <a href=""https://answers.splunk.com/answers/94364/plugin-support-for-log4net.html"" rel=""nofollow noreferrer"">https://answers.splunk.com/answers/94364/plugin-support-for-log4net.html</a></p>

<p>Thanks in advance for your help.</p>",,1,0,,2019-2-7 17:36:20,1.0,2021-4-8 22:21:28,,,,,3866548.0,,1,2,umbraco|log4net|splunk,727,11
205,258661,54667315,Will AWS S3 event forwarding to SQS will end up being a different message than S3 to SNS to SQS?,"<p>I am using a Splunk Technical Add-on that will be pulling messages from an SQS queue. Although the TA suggests using S3 forwarding to an SNS and it subscribed to an SQS, there is also the possibility of S3 to forward directly to SQS.</p>

<p>Would SNS make any change on what S3 send to it? Or would it be a fully transparent transport method to SQS?</p>",54674169.0,1,1,,2019-2-13 10:00:18,,2019-2-13 22:10:13,,,,,9541980.0,,1,0,amazon-web-services|amazon-s3|amazon-sqs|amazon-sns|splunk,668,13
206,258662,54683088,Does Splunk SDK service require Disconnect or Close,"<p>I am trying to do a Splunk Seach from Splunk java SDK. Here is the working code. My question is do I need to close <strong>service</strong> after each search. If yes, how to close it? Else is there a maximum number of jobs that I can create in each service?</p>

<pre><code>ServiceArgs serviceArgs = new ServiceArgs();
serviceArgs.setUsername(splunkUserName);
serviceArgs.setHost(splunkHostname);
serviceArgs.setPort(Integer.parseInt(splunkPort));
serviceArgs.setPassword(splunkPassword));
HttpService.setSslSecurityProtocol(SSLSecurityProtocol.TLSv1_2);
Service service = Service.connect(serviceArgs);
JobArgs jobArgs = new JobArgs();
jobArgs.setExecutionMode(JobArgs.ExecutionMode.NORMAL);
jobArgs.setEarliestTime(startDate);
jobArgs.setLatestTime(endData);
jobArgs.setMaximumCount(maxResultCount);
Job job = service.getJobs().create(query,jobArgs);
</code></pre>",54687170.0,1,0,,2019-2-14 04:08:58,,2019-2-14 09:34:32,,,,,6099958.0,,1,0,java|splunk|splunk-sdk,119,8
207,258663,54691234,How to make an Alert in Splunk?,"<p>I want to make an Alert in Splunk when the sum is greater than 1000. 
So if the sum is greater than 1000, I want to send an email to xyz.</p>

<p>I can't select the right opportunity in the Splunk Alert menu. </p>",,1,0,,2019-2-14 13:10:50,1.0,2019-2-14 18:29:11,,,,,10779796.0,,1,1,splunk,117,11
208,258664,54691507,Splunk HEC: Start sending events in JSON format with pre existing raw events,"<p>We are using Splunk Enterprise v 6.6.3. All our indexed events are raw events (logs) and we are planning to use Splunk HEC and send the events in JSON format.</p>

<p>My question: does sending the new events in JSON format affect everything that we have today in Splunk?</p>",54696881.0,1,0,,2019-2-14 13:24:48,,2019-2-14 18:27:48,,,,,7693001.0,,1,0,json|splunk|raw-data,115,9
209,258665,54730007,"Splunk - ""Not Found"" from cli when connecting to remote server","<p>Trying to run a remote command/search via the cli, but getting ""Not Found""</p>

<p><code>allowRemoteLogin=always</code> is already set in <code>server.conf</code></p>

<p>The management port is set to 8081.</p>

<pre><code>/opt/splunk/bin/splunk show splunkd-port -auth 'admin:XXXXXXXX' -uri 'https://XXXXXXXXX:8081'
Not Found
</code></pre>

<p>When run remotely (on the server with Splunk running) it's fine:</p>

<pre><code>/opt/splunk/bin/splunk show splunkd-port -auth 'admin:XXXXXXXXXXXX'
Splunkd port: 8089
</code></pre>

<p>I am running Splunk behind Nginx, but with the URI command line option I am using the IP, not the fully qualified domain name.</p>

<p>Any ideas?</p>",54733696.0,1,0,,2019-2-17 04:06:12,,2019-2-17 13:21:55,,,,,282088.0,,1,0,command-line-interface|splunk,328,11
210,258666,54784603,Stats Count Splunk Query,"<p>I wonder whether someone can help me please.</p>

<p>I'd made the following post about Splunk query I'm trying to write:</p>

<p><a href=""https://answers.splunk.com/answers/724223/in-a-table-powered-by-a-stats-count-search-can-you.html"" rel=""nofollow noreferrer"">https://answers.splunk.com/answers/724223/in-a-table-powered-by-a-stats-count-search-can-you.html</a></p>

<p>I received some great help, but despite working on this for a few days now concentrating on using eval if statements, I still have the same issue with the ""Successful"" and ""Unsuccessful"" columns showing blank results. So I thought I'd cast the net a little wider and ask please whether someone maybe able to look at this and offer some guidance on how I may get around the problem. </p>

<p>Many thanks and kind regards</p>

<p>Chris</p>",,2,0,,2019-2-20 10:54:06,,2019-2-28 07:15:47,,,,,794000.0,,1,3,splunk|splunk-query,3364,15
211,258667,54811476,Exporting logs from GCP to Splunk,"<p>I'm having trouble understand this pricing diagram from GCP: <a href=""https://cloud.google.com/logging/docs/export/#sink-terms"" rel=""nofollow noreferrer"">reference link</a></p>

<p>I'm trying to export logs directly from the logging API to a Cloud Pub/Sub, but all the documentation I see for achieving this is creating a sink from Stackdriver logging: <a href=""https://cloud.google.com/solutions/exporting-stackdriver-logging-for-splunk#set_iam_policy_permissions_for_the_cloud_pubsub_topic"" rel=""nofollow noreferrer"">reference link</a></p>

<p>I don't want logs to be ingested into Stackdriver before being exported to a Cloud Pub/Sub as that would incur costs. Am I looking at the diagram correctly or do they not charge for this?</p>

<p>Any help is appreciated.</p>",54816706.0,2,0,,2019-2-21 16:09:54,0.0,2019-2-21 21:45:39,2019-2-21 16:31:30,,10514497.0,,10514497.0,,1,1,logging|google-cloud-platform|google-compute-engine|monitoring|splunk,820,14
212,258668,54880117,Search for unique IP's in splunk,"<p>I've the splunk data something like:</p>

<blockquote>
  <p>{""@timestamp"":""2019-02-26T05:12:30.090+00:00"",""@version"":""1"",""message"":""\n================>\nRequest
  Details:\n[requestId:abc118f2-qqff-10bb-a900-33cc9b88e333]\n[requestMethod
  = GET]\n[requestUrl = <a href=""http://test.api.tmp.com/rawQuantities]"" rel=""nofollow noreferrer"">http://test.api.tmp.com/rawQuantities]</a>\n[requestHeaders =
  {testing-id=Root=abc-123-xyz, x-forwarded-proto=https,
  host=test.api.tmp.com, x-forwarded-port=443,
  content-type=application/json, x-forwarded-for=xx.xx.xx.xx,
  accept-encoding=gzip,deflate, accept=application/json,
  user-agent=Apache-HttpClient/4.5.2
  (Java/1.8.0_181)}]\n[requestBodySize: 0]\n&lt;================>\n ... ...
  }</p>
</blockquote>

<p>There is IP as: <code>x-forwarded-for=xx.xx.xx.xx</code>
I just want to filter out all the unique IP's.</p>

<p>I've tried some combinations like:</p>

<pre><code>index=api_dev sourcetype=""test-api"" message=""*"" | spath output=field path=_raw.requestDetails.x-forwarded-for

index=api_dev sourcetype=""test-api"" message=x-forwarded-for*
</code></pre>",54922376.0,1,0,,2019-2-26 07:11:17,,2019-2-28 09:47:16,,,,,8958729.0,,1,0,splunk-query|rex,365,10
213,258669,54896417,Splunk Query to find greater than,"<p>I have a splunk log
LOG: <code>""TOTAL NUMBER OF RECORDS IS:0""</code></p>

<p>I need to Query it in a way that it find a log message if the number of records turn out to be more than 0</p>

<p>I have tried the following</p>

<pre><code> sourcetype=mylogs | rex ""\d+:\d+:\d+\s(?&lt;TOTAL NUMBER OF RECORDS IS:&gt;\d+)$"" | where TOTAL NUMBER OF RECORDS IS:&gt;=25
</code></pre>

<p>It gives a terminator Error</p>",54897312.0,2,0,,2019-2-27 00:54:36,,2019-2-27 20:45:34,,,,,11015749.0,,1,0,splunk|splunk-query,9190,15
214,258670,54947194,Spring + Thymeleaf Engine Error Handling and Logging,"<p>Our company is in the process of switching our template engine from Velocity to Thymeleaf. We use Splunk for our logging, and with Velocity we were able to implement <code>org.apache.velocity.runtime.log.LogChute</code> to handle custom logging (format for and log to our splunk log), but I have been unable to find any similar class for Thymeleaf.</p>

<p>I've tried a couple of approaches so far. First, I tried to extend the actual Thymeleaf engine and add a try/catch wrapper around the process method, but that method is final, unfortunately. I saw a suggestion to add a filter to catch the thymeleaf errors, but something must be swallowing the error, because it never reaches that catch block.</p>

<p>The only option I can think of at this point is to simply pull <code>org.thymeleaf.TemplateEngine</code> logs into our splunk log, but then it won't be properly formatted for ingestion and I can't add any custom fields.</p>

<p>Does anybody have any ideas?</p>

<p>EDIT:</p>

<p>Wow, so I just retried the Filter approach and it worked, but the caught exception was <code>org.springframework.web.util.NestedServletException</code>, so JRebel must not have reloaded my changes to catch <code>Exception</code> instead of <code>TemplateEngineException</code> when I tried it last.</p>

<p>That said, if anybody has a better approach, I'd love to hear it.
I'm new to the whole question posting thing; should I post an answer?</p>",54984899.0,1,1,,2019-3-1 15:00:26,,2019-3-5 22:17:21,2019-3-2 04:29:46,,643500.0,,10952458.0,,1,1,spring|spring-boot|thymeleaf|splunk|custom-error-handling,738,11
215,258671,54947547,how to make a dashboard and query in splunk,"<p>I am new in splunk and only have a basic knowledge in querying. I need to create a dashboard that will count the total number of policy for each server. I have an example data, it shows the different host and policy.</p>

<p>Example data:</p>

<p><a href=""https://i.stack.imgur.com/s9pcK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s9pcK.jpg"" alt=""enter image description here""></a></p>

<p>I want to generate a dashboard like this:
<a href=""https://i.stack.imgur.com/FstpQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FstpQ.jpg"" alt=""enter image description here""></a></p>

<p>My code is like this:</p>

<pre><code>eval search if(""$Host$""=""AAA"") | stats count(Policy) as ""AAA"" by Policy |
eval search if(""$Host$""=""BBB"") | stats count(Policy) as ""BBB"" by Policy |
eval search if(""$Host$""=""CCC"") | stats count(Policy) as ""CCC"" by Policy |
</code></pre>",,1,0,,2019-3-1 15:21:47,,2019-3-1 16:16:06,,,,,3823876.0,,1,0,splunk|splunk-query|splunk-formula,244,11
216,258672,54956920,Splunk Log Displaying as Hex When Using Logback SSLSocketAppender,"<p>I am trying to use Splunk to collect logs for my app. I set up TCP data input on port 6514 (with SSL enabled on this port). From my Java application, I am able to connect to the port and send logs. However, when I check these logs on Splunk web, it displays as Hex format.</p>

<p><strong>LOGBACK CONFIGURATION</strong></p>

<pre><code>&lt;configuration debug=""true""&gt;

 &lt;appender name=""console"" class=""ch.qos.logback.core.ConsoleAppender""&gt;
    &lt;encoder&gt;
        &lt;pattern&gt;%date{ISO8601} [%thread] [%cyan(%C.%M\(\))] [%highlight(%level)] : %msg - %ex{short} %n&lt;/pattern&gt;
    &lt;/encoder&gt;
&lt;/appender&gt;

&lt;appender name=""sslsocket"" class=""ch.qos.logback.classic.net.SSLSocketAppender""&gt;
    &lt;remoteHost&gt;127.0.0.1&lt;/remoteHost&gt;
    &lt;port&gt;6514&lt;/port&gt;
    &lt;queueSize&gt;20&lt;/queueSize&gt;
    &lt;reconnectionDelay&gt;20&lt;/reconnectionDelay&gt;
    &lt;ssl&gt;
        &lt;trustStore&gt;
            &lt;location&gt;file:///path/to/truststore.jks&lt;/location&gt;
            &lt;password&gt;truststorepassword&lt;/password&gt;
        &lt;/trustStore&gt;
    &lt;/ssl&gt;
&lt;/appender&gt;

&lt;logger name=""splunk.secure.logger"" additivity=""false"" level=""INFO""&gt;
    &lt;appender-ref ref=""sslsocket""/&gt;
&lt;/logger&gt;

&lt;root level=""DEBUG""&gt;
    &lt;appender-ref ref=""console"" /&gt;
&lt;/root&gt;
&lt;/configuration&gt;
</code></pre>

<p><strong>USAGE</strong></p>

<p>public class Starter {</p>

<pre><code>private final static org.slf4j.Logger logger = LoggerFactory.getLogger(""splunk.secure.logger"");


public static void main(String[] args) {
    logger.info(""Testing SSL Socket Appender Log"");
}

}
</code></pre>

<p><strong>LOG BACK DEBUG OUTPUT TO CONSOLE</strong></p>

<pre><code>11:00:04,701 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - 
About to instantiate appender of type 
[ch.qos.logback.classic.net.SSLSocketAppender]
11:00:04,720 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - 
Naming appender as [sslsocket]
11:00:04,763 |-INFO in 
ch.qos.logback.core.joran.action.NestedComplexPropertyIA - Assuming default type 
[ch.qos.logback.core.net.ssl.SSLConfiguration] for [ssl] property
11:00:04,776 |-INFO in ch.qos.logback.core.joran.action.NestedComplexPropertyIA - Assuming default type [ch.qos.logback.core.net.ssl.KeyStoreFactoryBean] for 
[trustStore] property
11:00:06,035 |-INFO in ch.qos.logback.classic.net.SSLSocketAppender[sslsocket] - SSL protocol 'SSL' provider 'SunJSSE version 1.8'
11:00:06,045 |-INFO in ch.qos.logback.classic.net.SSLSocketAppender[sslsocket] - trust store of type 'JKS' provider 'SUN version 1.8': file:///path/to/truststore.jks
11:00:06,046 |-INFO in ch.qos.logback.classic.net.SSLSocketAppender[sslsocket] - trust manager algorithm 'PKIX' provider 'SunJSSE version 1.8'
11:00:06,063 |-INFO in ch.qos.logback.classic.net.SSLSocketAppender[sslsocket] - secure random algorithm 'SHA1PRNG' provider 'SUN version 1.8'
11:00:06,556 |-INFO in ch.qos.logback.classic.joran.action.LoggerAction - Setting level of logger [splunk.secure.logger] to INFO
11:00:06,557 |-INFO in ch.qos.logback.classic.joran.action.LoggerAction - Setting additivity of logger [splunk.secure.logger] to false
11:00:06,564 |-INFO in ch.qos.logback.core.joran.action.AppenderRefAction - 
Attaching appender named [sslsocket] to Logger[splunk.secure.logger]
</code></pre>

<p><strong>WHAT IS RECEIVED BY SPLUNK WEB</strong></p>

<pre><code>Time    Event
</code></pre>

<p>3/2/19
9:48:45.000 AM<br>
\xAC\xED\x00
host =  127.0.0.1 source =  tcp:6514 sourcetype =   logback</p>

<p><strong>In Summary</strong></p>

<p>From the above, it appears to me that this is not a connection issue, as Splunk is listening to the port 6514 and is able to capture input BUT the captured input is displayed as HEX and not normally.</p>

<p>When I use a normal com.splunk.logging.TcpAppender, my logs are displayed correctly on splunk.</p>

<ol>
<li>Is there any other configuration I may have missed out</li>
<li>Is it possible to enable SSL while using the com.splunk.logging.TcpAppender</li>
<li>Is there a dedicated Splunk SSL appender that can be used instead of ch.qos.logback.classic.net.SSLSocketAppender </li>
<li>Any other suggestion is welcome.</li>
</ol>",54964176.0,1,11,,2019-3-2 09:10:04,,2019-3-2 23:41:36,2019-3-2 10:16:47,,3151251.0,,3151251.0,,1,1,java|ssl|logback|splunk,421,10
217,258673,54977225,Splunk license - what if licensed amount of data is not used?,"<p>I have a question on Splunk licensing. If I buy a license for e.g. 10 GB/day and if I could not ingest 10 GB per day, is that a license violation?</p>",54983229.0,1,0,,2019-3-4 05:37:57,,2019-3-4 12:24:42,2019-3-4 10:30:55,,1627240.0,,1627240.0,,1,0,splunk|splunk-query,56,7
218,258674,54994846,"Our splunk instance reports a DNS query type of ""ALL"" which doesn't exist?","<p>My work splunk instance reports a DNS query type of ""ALL."" As far as I know there is an ""ANY"" which is actually a ""*"" and the pulls cached records for that host, and there is an AXFR (zone transfer) that pulls all records for a domain...</p>

<p>So I've tried to check the RFC's for BIND 9 DNS and etc but the word ""all"" is so common I'm struggling to root cause this. So:</p>

<p>Question 1:</p>

<blockquote>
  <p>Is there a DNS ""ALL"" request in any version of DNS?</p>
</blockquote>

<p>Sub question if the answer to the above is no:</p>

<blockquote>
  <p>Why does splunk present this and what does it actually mean?</p>
</blockquote>",,1,0,,2019-3-5 03:07:48,,2019-3-5 12:13:50,2019-3-5 03:15:10,,213136.0,,7475410.0,,1,0,splunk|splunk-query,114,8
219,258675,55045957,How to pass 'time' query to splunk enterprises using Splunk-Python SDK?,"<blockquote>
  <p>I am trying to pass query from Python(eclipse IDE) to extract data from
  specific dashboard on SPLUNK enterprises. I am able to get data
  printed on my console by passing the required queries however I am not
  able to extract data for specific time interval(like if I want data
  for 1 hour, 1 day, 1 week or 1 month)</p>
</blockquote>

<p>I have tried commands like 'earliest', 'latest' along with my query but every time it throws an error stating    <em>""raise HTTPError(response) splunklib.binding.HTTPError: HTTP 400 Bad Request -- Search Factory: Unknown search command 'earliest'""</em></p>

<p><strong>Here is my code</strong></p>

<pre><code>import splunklib.client as client
import splunklib.results as results


HOST = ""my hostname""
PORT = 8089
USERNAME = ""my username""
PASSWORD = ""my password""
service = client.connect(
host=HOST,
port=PORT, 
username=USERNAME,
password=PASSWORD)
rr = results.ResultsReader(service.jobs.export(""search index=ccmjimmie | stats count(eval(resCode!=00200)) AS errored | chart sum(errored)|earliest=-1d""))

for result in rr:
    if isinstance(result, results.Message):
    # Diagnostic messages might be returned in the results
        print(result.type, result.message)
    elif isinstance(result, dict):
    # Normal events are returned as dicts
        print (result)
assert rr.is_preview == False
</code></pre>

<p><strong>Output I am getting without using time query</strong></p>

<pre><code>OrderedDict([('sum(errored)', '1566')])
OrderedDict([('sum(errored)', '4404')])
OrderedDict([('sum(errored)', '6655')])
OrderedDict([('sum(errored)', '8992')])
etc...
</code></pre>

<p>This output is same as expected but not bounded by time. I want the same output but for Given Time Interval. And time interval should be passed from the search query ""serch.jobs.export()"" in the above Python code</p>

<p>Please let me know how do I pass 'time' query along with my required query.</p>

<p>Any help is most appreciated! Thanks in advance!</p>",,2,0,,2019-3-7 14:17:21,,2021-3-22 13:50:57,,,,,11165683.0,,1,2,python|python-requests|splunk|splunk-query,1398,14
220,258676,55079668,Dashboard table: get count of subquery,"<p>I've been struggling with this for a few hours.</p>

<p>I am trying to build a simple table that will have the following:</p>

<ol>
<li>Result of a search containing an id and a date</li>
<li>Count of a subquery using the id of the search in 1</li>
</ol>

<p>I have two working queries that achieve what I want individually, but when I try to use a join I lose the data of the query for #2.</p>

<p>Query #1: index= source="""" """" AND parentId=1574 | fields childId, date
-> returns 333, 
Query #2: index= source="""" """" AND childId=333 | chart count as childCount | fields childCount
-> returns the count, 4 for example</p>

<p>When I try to join the two using something like this, I lose the count:</p>

<pre><code>index=&lt;redacted&gt; source=""&lt;redacted"" ""&lt;some query text&gt;"" AND 
    parentId=1574 
| fields childId, date
| join type=left childId [ 
    search index=&lt;redacted&gt; source=""&lt;redacted&gt;"" ""&lt;some query text&gt;"" AND 
    childId=333 | chart count as childCount | fields childCount
]
| table artifactId, childCount, date
</code></pre>

<p>I've also tried an outer join, append, etc. to no avail. The count could be 0 as well.</p>

<p>Any help would be appreciated,</p>

<p>Thanks!</p>",55080261.0,1,0,,2019-3-9 16:50:45,,2019-3-9 17:50:28,,,,,6775271.0,,1,1,splunk|splunk-query,115,9
221,258677,55110723,Regarding splunk query,<p>I have some audio files in S3 bucket. I want to write a splunk query to find the top 500 audio files and categorize them by date and store them in a folder. Also I want to do it every night. What should be my query?</p>,,1,0,,2019-3-11 21:31:30,,2019-3-11 23:23:26,,,,,5423438.0,,1,0,splunk,34,6
222,258678,55128615,Splunk with ECS,"<p>I am having a problem with configuring Splunk to send logs on ECS Cluster.
From the event tab in service, this error was there 
Problem Statement: unable to place a task because no container instance met all of its requirements. The closest matching container-instance xxxx is missing an attribute required by your task.
after doing a deep drive I found have to update /etc/ecs/ecs.config. and entry echo ECS_AVAILABLE_LOGGING_DRIVERS='[""splunk"",""awslogs""]'.
But this couldn't help?
still getting the same error.
Can anyone please help?</p>",,1,1,,2019-3-12 18:40:50,,2021-5-12 19:27:22,,,,,10125451.0,,1,1,amazon-ecs|splunk,1429,12
223,258679,55148300,Parsing Cisco System Logs with Regex,"<p>I have three types of logs I would like to parse:</p>

<p><strong>Message #1:</strong> Username entry (random@somewhere.org) with length (253) created for mobile aa:aa:aa:aa:aa:aa</p>

<p><strong>Expected Matches:</strong></p>

<ul>
<li>random@somewhere.org</li>
<li>aa:aa:aa:aa:aa:aa</li>
</ul>

<p><strong>Message #2:</strong> Username entry (hello) is deleted for mobile aa:aa:aa:aa:aa:aa</p>

<p><strong>Expected Matches:</strong></p>

<ul>
<li>hello</li>
<li>aa:aa:aa:aa:aa:aa</li>
</ul>

<p><strong>Message #3</strong> Guest user logged in with user account (randomnonexistentuser) MAC address aa:aa:aa:aa:aa:aa, IP address 127.0.0.1.</p>

<p><strong>Expected Matches:</strong></p>

<ul>
<li>randomnonexistentuser</li>
<li>aa:aa:aa:aa:aa:aa</li>
<li>127.0.0.1</li>
</ul>

<p>So far, I have this regex to identify the MAC addresses (labeled as mobile in two of the three examples): <a href=""https://regex101.com/r/qFE95M/14"" rel=""nofollow noreferrer"">https://regex101.com/r/qFE95M/14</a></p>

<p>I also have this regex...</p>

<pre><code>(?P&lt;IP_address&gt;[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3})
</code></pre>

<p>...to find the IP address. However, I'm not sure how to make just that named group optional and tie it together with the MAC address piece. </p>

<p>Finally, I have this regex...</p>

<pre><code>^[^\(\n]*\((?P&lt;user_account&gt;[^\)]+)\)
</code></pre>

<p>...to find the usernames (could be email addresses, single words, etc.). But again, I'm unsure how to tie it in with the other two named groups.</p>

<p>How do I reconcile these three matches together, so it finds them in the three messages above?</p>",55148489.0,1,2,,2019-3-13 17:49:20,,2019-3-13 18:42:44,,,,,7668467.0,,1,0,regex|splunk,221,10
224,258680,55173777,Splunk extracted field in dashboard,"<p>I am sending some data to splunk which looks like:</p>

<pre><code>""Start|timestamp:1552607877702|type:counter|metricName:cache|count:34488378|End""
</code></pre>

<p>And then extracting the fields using a regex:</p>

<pre><code>search ""attrs.name""=""service"" | regex (Start)(.*)(End) | extract pairdelim=""\""{|}"" kvdelim="":""
</code></pre>

<p>After extraction, I can see the fields (type, metricName, count) under ""INTERESTING FIELDS"". How do I go about using these fields in a dashboard?</p>

<p>Thanks</p>",55175116.0,2,0,,2019-3-15 00:06:42,1.0,2019-3-15 11:36:51,,,,,135740.0,,1,3,splunk|splunk-query,759,14
225,258681,55246816,create a single regex that creates group matches for dest_ip and dest_port,"<p>I'm currently working on regex/Splunk challenges for my job. The challenge is to create a single regex that creates group matches for dest_ip and dest_port from this sample log file.</p>

<pre><code>&gt; Jan 15 15:16:11 10.0.5.9 Jan 15 15:16:11 ff:ff:00:01 nfc_id=20067 exp_ip=10.0.5.25 input_snmp=2 output_snmp=7 protocol=17 src_ip=43.152.96.179 src_host=""unknown"" src_port=1049 dest_ip=40.169.38.123 dest_host=unknown dest_port=137 tcp_flag=...... packets_in=131 bytes_in=22078 src_tos=0 dest_tos=0 src_asn=65535 dest_asn=65535 flow_count=1 percent_of_total=10.626 flow_smpl_id=2 t_int=30015  =24520
</code></pre>

<p>This is the regex that I created, but it seems not to be right. The regex should be extracting the dest_ip and dest_port field</p>

<pre><code>^""(?P&lt;dest_ip&gt;.+?)"",""(?P&lt;dest_port&gt;.+?)""
</code></pre>

<p>Could someone point me in the right direction or send over some documents that could give me examples.</p>",55246895.0,1,0,,2019-3-19 17:28:10,,2019-3-19 17:46:44,,,,,7442401.0,,1,1,regex|splunk,50,6
226,258682,55249562,What regex expression is best for extracting windows file name and file path?,"<p>Create a single regex to extract the two parts of 'New Process Name' as file_path and file_name. 
Note that in this example:</p>

<pre><code>Process Information:
New Process ID:     0x8609
New Process Name:   C:\Windows\System32\example_c.exe
New Process Name:   D:\Intel\Logs\User\Tom Warner\logs.txt
</code></pre>

<ol>
<li>there are two directories</li>
<li>the file is in the C drive and none of the directory names or file name contains a space</li>
</ol>

<p>However, other log samples could have any arbitrary number of file paths within any letter drive. And in Windows, directory and file names are allowed to contain spaces and can be encapsulated in quotes.  Ensure your regex could capture any of these cases.</p>

<p>This is the expression I've come up with. I'm able to match the file_path, but I'm not having any luck matching the file_name. What is the expression should be used to match both file_name and file_path</p>

<pre><code>New Process Name:\t+(?&lt;file_path&gt;\w:*[\\\S|*\S]?.*$).*?(?&lt;file_name&gt;[\w-]+?(?=\.))
</code></pre>",,1,6,,2019-3-19 20:36:07,,2019-3-21 21:42:30,,,,,11228684.0,,1,0,regex|regex-group|splunk,276,9
227,258683,55256079,Gantt Chart Zoom in function is possible in Splunk?,"<p>Hello I have a question</p>

<p>I made a gantt chart using custom viz in splunk like this here.
<a href=""https://splunkbase.splunk.com/app/1741/"" rel=""nofollow noreferrer"">https://splunkbase.splunk.com/app/1741/</a></p>

<p>It's app is very nice so many data visualization I think.</p>

<p>But How can I using zoom in/out function in this apps.</p>

<p>Need modified apps add to zoom in/out function?</p>

<p>How can I do this?</p>

<p>Thanks.</p>",,1,0,,2019-3-20 08:06:31,,2019-3-20 10:42:44,,,,,6936440.0,,1,-1,javascript|css|splunk|gantt-chart|timeserieschart,96,8
228,258684,55262981,Splunk rex query to filter message,"<p>I have a splunk log in the below format: </p>

<pre><code>{""Apple"":
    {""message"":""abcdefgh.ijkl"",""code"":""200""}
} 
</code></pre>

<p>I want to filter the message ""abcdefgh.ijkl"" and code separately.</p>",,1,5,,2019-3-20 14:19:20,,2019-3-20 17:46:39,2019-3-20 14:45:52,,1150918.0,,11232481.0,,1,0,splunk|rex,182,9
229,258685,55274267,How to push mule(Java based) logs to Prometheus storage?,"<p>I have a mule application which mostly does HTTP requests, which is logging as plain text. I want to push these logs as metrics to Prometheus. Since this is a legacy application it would take a substantial amount of time to change code and push metrics directly into Prometheus storage.</p>

<p>Idea is to show Prometheus metrics in Grafana Dashboard.</p>

<p>Is there any intermediate tool that converts plain text to metrics?</p>

<p>Anything that helps with this requirement.</p>

<p>FYI- We have Nagios and Splunk which is doing this task as of now, we are looking to move our solution to Prometheus and Grafana</p>",55277006.0,1,1,,2019-3-21 05:27:32,,2019-3-21 09:12:14,2019-3-21 07:18:04,,4929865.0,,4929865.0,,1,0,grafana|prometheus|nagios|splunk|telegraf,673,13
230,258686,55275491,"Splunk: Splunk-python SDK: How to include pandas, numpy to create custom command","<p>I am making a custom command for splunk, say getInfluentialCommand. So I make a .py file in bin directory. in this .py file, I need to include pandas, numpy. How to do it? Does splunk python environment allow me to include other python module? Do I just need to install as <code>pip install pandas</code> or <code>pip install numpy</code>? </p>",55485757.0,2,0,,2019-3-21 07:24:00,,2019-4-3 01:31:20,,,,,84592.0,,1,1,splunk|splunk-sdk,1037,13
231,258687,55289545,Regex to parse blue coat log file,"<p>I have this log file that I'm currently trying to parse.</p>

<pre><code>Jan 12 2019, 14:51:23, 117, 10.0.0.1, neil.armstrong, standard-users, -, TCP_Connect, ""sports betting"", -, 201, accept, GET, text, https, www.best-site.com, 443, /pages/home.php, ?user=narmstrong&amp;team=wizards, -, ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari/537.36"", 192.168.1.1, 1400, 1463, -, -, -
Jan 12 2019, 14:52:14, 86, 10.0.0.1, neil.armstrong, standard-users, -, TCP_Connect, ""sports betting"", -, 200, accept, POST, text, https, www.upload.best-site.com, 443, /, -, -, ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/ Safari/537"", 192.168.1.1, 230056, 600, -, -, -
Jan 12 2019, 14:52:54, 118, 10.0.0.1, neil.armstrong, standard-users, -, TCP_Connect, ""sports betting"", -, 200, accept, GET, text/javascript, http, google.fr, 80, /search, ?q=wizards, -, ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/ Safari/537"", 192.168.1.1, 1717, 17930, -, -, -
</code></pre>

<p>this is the regex that I'm currently using <a href=""https://regex101.com/r/Asbpkx/3"" rel=""nofollow noreferrer"">https://regex101.com/r/Asbpkx/3</a> it parses the log file fine until it reaches <code>""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/ Safari/537""</code> then it splits at <code>(KHTML, =like Gecko)</code>
How can I complete the regex so that this does not happen?</p>",55301140.0,3,0,,2019-3-21 21:31:05,,2019-3-22 13:54:18,,,,,11228684.0,,1,0,regex|parsing|splunk,143,8
232,258688,55295647,Regular expression to match specific word in Splunk,"<p>I have this type of log file:</p>

<pre><code>182.236.164.11 - - [04/Mar/2019:18:20:56] ""GET /cart.do?action=addtocart&amp;itemId=EST-15&amp;productId=BS-AG-G09&amp;JSESSIONID=SD6SL8FF10ADFF53101 HTTP 1.1"" 200 2252 ""http://www.buttercupgames.com/oldlink?itemId=EST-15"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.46 Safari/536.5"" 506
</code></pre>

<p>I am trying to create a regular expression to only match the word Intel, regardless of the relative position of the string.</p>

<p>I have come up with this regular expression: </p>

<pre><code>^[^;\n]*;\s+
</code></pre>

<p>But it doesn't always work as it will match other strings as well.</p>

<p>I want to match the string Intel only so as to create a field in Splunk.</p>

<p>Any inputs are welcome.</p>",,1,2,,2019-3-22 08:33:11,,2019-3-22 11:12:20,2019-3-22 08:38:31,,7230328.0,,7230328.0,,1,-1,regex|splunk,1084,11
233,258689,55328823,REQ: Assistance with Splunk - Rex Query,"<p>I'm having some issues with a rex query where a single digit date renders an incorrect result, but a double digit date provides the correct result.</p>

<p>These are the log entries I'm querying:</p>

<pre><code>Mar  7 14:24:29 10.52.176.215 Mar  7 12:24:29 963568 - Melbourne details-cable-issue - vdvfvfv

Mar 20 09:52:55 10.52.176.215 Mar 20 07:52:55 963569 - Brisbane cable-issue
</code></pre>

<p>And this is the query:</p>

<pre><code>^(?:[^ \n]* ){7}(?P&lt;extension&gt;[^ ]+)[^\-\n]*\-\s+(?P&lt;location&gt;\w+)
</code></pre>

<p>For the Mar 7 entry, my query is giving me group extension ""7"" whilst my Mar 20 entry is giving me group extension ""963569"" which is correct.</p>

<p>Can someone shed some light on my query to acknowledge a single and double digit date? #7 vs 20</p>

<p>Thanks all :)</p>",55328870.0,1,0,,2019-3-24 21:38:05,,2019-3-24 21:46:25,2019-3-24 21:46:25,,3832970.0,,7949451.0,,1,2,regex|splunk|rex,45,7
234,258690,55360392,Splunk: Calculate TopN hosts but add to that TopN based on a key=value pair,"<p>Is there a way to get a Top Hosts count and add to each hosts count using a value from a k/v pair in the event itself?</p>

<p>Example:</p>

<pre><code>&lt;158&gt;Mar 26 15:01:36 m500 admd SSO: write 35 bytes on fd(11) OK repeatCount=300 source =    tcp:514 sourcetype =    generic_single_line
</code></pre>

<p>So this would come up as <code>300</code> in the count of events for that host.</p>

<p>I'm new to Splunk so not very familiar with the query language. I tried </p>

<pre><code>| metasearch index=* | eval Date=strftime(_time,""%Y-%m-%d"") | chart count over host by Date
</code></pre>

<p>But I don't know how to add the count from that k/v</p>",55368089.0,2,0,,2019-3-26 15:05:42,,2019-3-27 00:27:33,,,,,1106924.0,,1,0,splunk|splunk-query,37,6
235,258691,55368685,how to (still) use a wildcard in the middle of a string?,"<p>I am trying to run the following query</p>

<pre><code>index=one /thispath/file*.pdf
</code></pre>

<p>I know that using wildcards in the middle of a string is not recommended, but  I have too many different files: <code>file001.pdf</code>, <code>fileabc.pdf</code> and others.</p>

<p>What can I do? I am more worried about bogus results than processing speed.</p>

<p>Thanks!</p>",,1,0,,2019-3-27 01:59:16,,2019-3-27 03:18:29,,,,,1609428.0,,1,0,splunk,130,9
236,258692,55370514,Filebeat to splunk,"<p>Is there a way to use filebeat to forward logs to splunk? Has anyone tried that?</p>

<p>We use filebeat to forward logs to ELK stack and want the same forwarder to be able to forward logs to splunk</p>",,1,0,,2019-3-27 05:42:53,,2019-3-27 06:15:23,,,,,4192838.0,,1,1,splunk|filebeat,2793,14
237,258693,55388553,Splunk API not working with cURL using port 8000,"<p>I'm trying to login on my local Splunk server using curl bash command but without success, so far I have tried the following:</p>

<pre><code>curl -u admin:changeme -k http://192.168.1.103:8000/en-US/account/login
curl -u admin:changeme -k http://192.168.1.103:8000/en-US/account/login -d""username=admin&amp;password=changeme""
</code></pre>

<p>Any help would be very appreciated,</p>

<p>Thanks.</p>",55388758.0,1,0,,2019-3-28 00:42:11,,2019-3-28 02:14:12,2019-3-28 02:13:27,,3544399.0,,10588814.0,,1,0,bash|curl|login|splunk,407,12
238,258694,55389244,"Why is the time displayed as ""undefined"" in the Splunk dashboard?","<p>I have a problem with dashboards, I have configured my dashboard using a custom visualization app, but the problem is seen as <code>undefined</code> in time.</p>

<p>If you specify a date range <em>in ""Input Time""</em>, it will look like that.</p>

<p>It is also the time chart, is there a way to solve it?</p>",,1,1,,2019-3-28 02:19:57,,2019-4-4 09:21:38,2019-4-4 09:21:38,,10415695.0,,11269512.0,,1,0,javascript|splunk,235,9
239,258695,55391014,C# SDK - ASP.NET Core 2.1 - The SSL connection could not be established,"<p>C# SDK - ASP.NET Core 2.1 - The SSL connection could not be established</p>

<p>I tried </p>

<pre><code>ServicePointManager.ServerCertificateValidationCallback = delegate { return true; };
</code></pre>

<p>It is running in .NET Framework but not in ASP.NET Core.</p>

<p>I also tried </p>

<pre><code>ServicePointManager.SecurityProtocol = SecurityProtocolType.Ssl3 | SecurityProtocolType.Tls12 | SecurityProtocolType.Tls11 | SecurityProtocolType.Tls;
</code></pre>

<p>But I didn't get success in ASP.NET Core</p>

<p>I want to generate self SSL in asp.net core</p>",,2,0,,2019-3-28 05:57:59,,2021-7-12 18:01:14,2019-3-28 06:21:00,,2309376.0,,9708851.0,,1,1,ssl|asp.net-core|splunk,16584,20
240,258696,55411233,Using Splunk rex command to extract a field between 2 words,"<p>Please assist extracting\creating a new field between 2 fixed words, one of which begins with <code>!</code></p>

<p>Example:</p>

<pre><code>!CASH OUT          $50.00!                        !TOTAL AUD    $61.80! 
</code></pre>

<p><code>!CASH OUT</code> and <code>!TOTAL</code> are fixed but the value amount in between (<code>$22.00!</code>) changes. I would like to create a field so I can filter the events by the cash out amount ect. I would only want the dollar amount to be the field without the <code>!</code> at the end.</p>

<p>I've tried the below search but it creates a cashout field with all data after <code>!CASH OUT</code>  and doesn't cut the field before <code>!TOTAL</code></p>

<pre><code>""CASH OUT"" ""!TOTAL"" | rex ""CASH OUT (?.*)!TOTAL""

search | ""CASH OUT"" ""!TOTAL"" | rex ""CASH OUT (?.*)!TOTAL""
</code></pre>

<p><code>field = $50.00</code></p>",,1,0,,2019-3-29 05:51:36,,2019-3-29 08:50:16,2019-3-29 08:38:18,,7404943.0,,11162885.0,,1,0,splunk|splunk-query|splunk-formula,1330,13
241,258697,55412772,Unable to collect event into Splunk index,"<p>I am trying to create a summary index in Splunk 6.6.7, but unable to get data ingested using <code>collect</code> command.</p>

<p>I have manually enabled it in <code>savedsearches.conf</code> file.</p>

<p>After creating this, I have restarted my Splunk and tried to run the below query using <code>collect</code> command to ingest data.</p>

<p>The data is not getting ingested anytime.</p>

<pre><code>[xxxx_capacity_threshold]
action.summary_index = true
action.summary_index._name = xxxxx
action.email.useNSSubject = 1
alert.track = 0
search = index=""$param$-xxx"" sourcetype=""xxx"" | table maxPercentage percentage
</code></pre>

<pre><code>| makeresults | eval _raw = ""{\""maxPercentage\"":\""70\"", \""percentage\"":\""90\""}"" | table _raw | collect index=""xxxxxx-xx"" file=""new_settings_$timestamp$.stash"" sourcetype=""xxxxxx"" addtime=true testmode=false
</code></pre>

<p>The expectation is that the data needs to get inserted in the index once the collect command is executed</p>

<p>Requesting to help me identify a solution to my problem.</p>

<p>Also, I am presently using an index say ""1234-index"" where I have different source types to cater to my needs. However, I have one particular source type for which I need to collect the summary data.
So is it necessary to create an entirely new index for the summary or can I just use the ""1234-index"" index and mark it to enable capturing summary data as well in a separate source type?</p>

<p>Thanks
Shahid</p>",55484848.0,1,0,,2019-3-29 07:52:17,,2019-4-2 23:23:46,2019-3-31 19:48:28,,5950470.0,,10043801.0,,1,0,splunk,474,10
242,258698,55420448,Splunk: Trying to join two searches so I can create delimters and format as a New Table,"<p>I am still very new to Splunk, but have learned enough to create reports using the ""Extract Fields"" option and using regex / delimeters to allow me to create custom extractions for Tables that I can create reports out of. </p>

<p>Right now I am trying to create a report for our Sales team to inform them when a user is basically using a process they should not be through our system, but am having problems on reporting on them properly. Ive tried using a search using an OR statement to try and join the searches that I am getting, but I noticed that the fields I am extracting duplicate information and the tables don't get joined properly.</p>

<p>Below is an example of two different searches that I am joining so I can get the following outcome after creating extracted fields</p>

<p>_Date/ Time and _Raw are already given and I know how to use extractions and ""AS"" statements to rename my extracted fields if need be. I will be performing a ""-_Raw"" as the reports and alerts that I am configuring will not be needed by the end party. </p>

<p>| _Date/Time | Corp_acct_nbr | LoginId | Product Ordered | _Raw</p>

<p>Below are my two searches that I need to ""Join""</p>

<p>Search 1:</p>

<p>2019-03-29 08:07:14,833 [http-bio-8081-exec-21] INFO  {requestId=4e639bde-5234-11e9-9dc1-0050568edf8a, sessionId=dbe4a47e-522f-11e9-9dc1-0050568edf8a, loginId=bconnell, ipAddress=192.168.103.205} c.n.u.r.w.f.RequestContextFilter.doFilter:67 - Request /AccountService/billing/account/<em>3410061</em>/orderPayment completed in 12.880164272 seconds</p>

<p>Search 2: </p>

<p>2019-03-29 08:07:14,937 [http-bio-8081-exec-22] INFO  {requestId=4e639bde-5234-11e9-9dc1-0050568edf8a, sessionId=dbe4a47e-522f-11e9-9dc1-0050568edf8a, <em>loginId=bconnell</em>, ipAddress=192.168.103.205} c.p.RechargeEvent.createInvoice:1263 - ssnID ssnType&lt;2> <em>Added hmItem to order.getHM: Analytics User $5</em></p>

<p>both these have a host of ace02b-s01.nextiva.zone</p>

<p>My desired results would be </p>

<p>| _Date/Time | Corp_acct_nbr | LoginId | Product Ordered | 
    08:07:14      3410061         bconnell  Analytics User $5</p>

<p>I've italicized the fields as well above. I believe I can join these on the request ID as the transaction will always have a unique one. Any assistance is greatly appreciated </p>",,1,0,,2019-3-29 15:12:04,,2019-4-2 08:15:32,,,,,3489481.0,,1,0,splunk,1045,12
243,258699,55462307,Handling special characters in Splunk Filters,"<p><strong>UPDATED</strong>
New to splunk. I am setting up a new alert and filtering the results which i already know. Wanted to simplify the search by combining the filters around ""error"". Any suggestions on how to merge them?</p>

<pre><code>index=cdapp (""ERROR"" OR ""WARNING"") AND (host=prod-delivery* OR host=prod-microsvc*) | search NOT ""https-jsse-nio-8443-exec"" NOT GetResourceBundleTag NOT AH01630 NOT AH01276 NOT ""*multimedia/images*"" NOT ""*/error/*"" NOT ""*-error-*"" NOT ""*_error"" NOT ""*error/404.jsp"" NOT ""/error.php"" NOT ""/error_page.htm""  NOT ""error=webacc"" NOT ""*MessageType=Error*"" NOT ""*/warning*""  NOT ""(no description)"" | table _time,host,source,_raw
</code></pre>",,0,7,,2019-4-1 19:32:48,,2019-4-2 13:36:28,2019-4-2 13:36:28,,825203.0,,825203.0,,1,0,string|special-characters|splunk|splunk-query,682,11
244,258700,55473976,Splunk Dropdown based on a field from string result,"<p>I have a splunk search that returns String with the format A;B;C
I want to create a dropdown in a splunk dashboard based on the first field (A).</p>

<p>So my problem here, is first, how to extract the first string before semicolon.
And Second, how to use that field dynamically in the dropdown</p>",,0,2,,2019-4-2 11:42:42,,2019-4-2 11:45:34,2019-4-2 11:45:34,,11237823.0,,11237823.0,,1,0,splunk,151,8
245,258701,55486907,Calling multiple REST API’s using Splunk,"<p>Is it possible to call multiple api’s using Splunk to get data? To be more specific I’m trying to call two api’s, one for login to get token and other call will use the token from previous call to get data. If so can you please brief how to do this</p>",,1,0,,2019-4-3 04:04:53,,2019-4-4 04:06:16,2019-4-3 04:11:50,,10066553.0,,10066553.0,,1,-1,splunk,67,7
246,258702,55491439,"How to search in splunk : documentID = NM_APEXIT,{FFAAAA8-59EB-457F-A831-B532CACF20C1},{6080D6666-666-C00B-999C-3C42BBBBBB}","<p>How to search in splunk : <code>documentID = NM_APEXIT,{FFAAAA8-50EB-457F-A831-B532CACF20C1},{6080D6666-666-C00B-999C-3C42BBBBBB}</code></p>",,1,0,,2019-4-3 09:15:53,,2019-4-3 14:38:57,2019-4-3 13:09:42,,704848.0,,7131253.0,,1,0,splunk,33,6
247,258703,55491849,how to monitor the GitHub Appliance instance using splunk,<p>We have GitHub Enterprise appliance and we need to forward the GitHub logs to independent store where splunk can monitor.. How can we achieve this</p>,55507653.0,1,0,,2019-4-3 09:36:12,,2021-7-4 10:19:35,2021-7-4 10:19:35,,366904.0,,9455819.0,,1,1,github|splunk|splunk-query,356,11
248,258704,55494106,how to extract a sentence from multiple sentence using regular expression in splunk,"<p>I'm trying to extract a Applicant sentence from multiple sentences using regular expression in splunk</p>

<p>APPLICANT: BP Exploration (Alaska) Inc., Post Office Box 196612, Anchorage, Alaska 99519
AGENT: Jennifer Collins, BP Exploration (Alaska) Inc.
LOCATION: The project site is located within Section 26, T. 11 N., R. 15 E., Umiat Meridian; USGS Quad Map Beechey Point B-3; Latitude 70.276939º N., Longitude -148.27539º W.; near Prudhoe Bay, Alaska.</p>",,1,4,,2019-4-3 11:30:51,,2019-4-3 12:15:23,,,,,11272185.0,,1,-1,regex|splunk,67,7
249,258705,55597912,How do I cleanly format a string in Python with multiple {} that also need to be ignored?,"<p>Setting up search queries in Python for Splunk access. However, during my runs I ran into a <code>IndexError: tuple index out of range</code>. I found in my code multiple {} and trying to use .format to apply variables earliest and latest was causing said error.</p>

<p>I've tried switching to .format_map but it causes more error unless I did something wrong. (Which could be)</p>

<p>I am not able to change the string that is in the code. So I am trying to find ways to append time variables to the string for use.</p>

<p>Here are the variable calls:</p>

<pre><code>import _datetime_param
_earliest = _datetime_param.earliest()
_latest = _datetime_param.latest()
</code></pre>

<p>Formatting Works: </p>

<pre><code>def eop():
    search_query = (
        'index=""eop"" sourcetype=""eop:trace"" Status=""Quarantined"" earliest={} latest={} | '
        'convert timeformat=""%Y-%m-%d %I:%M:%S"" ctime(_time) as Date | '
        'table Date src_ip src_user subject recipient | sort Date'
    ).format(_earliest, _latest)
    search_query.strip()
    if not (search_query.startswith('search') or search_query.startswith('|')):
        search_query = 'search ' + search_query
    return search_query
</code></pre>

<p>Formatting Runs Into Error:</p>

<pre><code>def okta():
    search_query = (
        'index=""okta"" user!=""CityADSync"" action=""failure"" earliest={} latest={} | '
        'convert timeformat=""%Y-%m-%d %I:%M:%S"" ctime(_time) AS Date | dedup actors{}.ipAddress | '
        'rename actors{}.ipAddress AS IPAddress | rename actors{}.id AS Device | rename targets{}.id AS TargetID | '
        'rename targets{}.displayName AS TargetName| rename targets{}.login AS TargetLogin | '
        'table Date Device IPAddress TargetID TargetName TargetLogin | sort Date'
    ).format(_earliest, _latest)
    search_query.strip()
    if not (search_query.startswith('search') or search_query.startswith('|')):
        search_query = 'search ' + search_query
    return search_query
</code></pre>

<p>In the <code>def okata():</code> I can't change the <code>search_query</code> as its set to run that way in Splunk. So I am looking for a way to cleanly handle both ways while not running into error. Put bluntly is there a way in Python to ignore {} you don't want to use? If it is <code>.format_map</code>, is there a clean way to do so that I am missing?</p>

<p>Thanks in advance!</p>",,1,2,,2019-4-9 17:07:08,,2019-4-9 17:26:27,2019-4-9 17:26:27,,10741174.0,,10741174.0,,1,0,python|format|splunk,63,7
250,258706,55601798,How to send data from splunk's Heavy Forwarder to kafka?,"<p>I'm looking for the best way of sending data from Splunk to Kafka.
The only way I found so far, is to set up a ""middle"" server, which receives data from the HF and acts like a producer for the Kafka.
The middle server Requires maintenance, so I'm looking for a good solution without any other component except Splunk and Kafka.</p>",,0,1,,2019-4-9 21:45:43,2.0,2019-4-9 21:45:43,,,,,11337061.0,,1,4,apache-kafka|splunk,782,11
251,258707,55628779,Splunk doesnt return all the results - using rest API -,"<p>I'm retrieving data from Splunk using rest API via production port 8980, on the GUI I can see 770 events when I retrieve data I got less then a 100.</p>

<p>here is my code in Java to retrieve data:</p>

<pre><code>public JSONObject Post_request() throws IOException, ParseException {
        String Query = ""search "" + OS_Vuln_Query;
        Job job = session.make_Request().getJobs().create(Query);
        while (!job.isDone()) {
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
        JobResultsArgs resultsArgs = new JobResultsArgs();
        resultsArgs.setOutputMode(JobResultsArgs.OutputMode.JSON);
        InputStream results = job.getResults(resultsArgs);

        BufferedReader br = new BufferedReader(new InputStreamReader(results));
        StringBuilder sb = new StringBuilder();
        String line;
        while ((line = br.readLine()) != null)
        {
            sb.append(line);
        }
        JSONParser parser = new JSONParser();
        JSONObject json = (JSONObject) parser.parse(sb.toString());
        String vulns_as_string = json.get(""results"").toString();
        JSONArray vulns_to_json = (JSONArray) parser.parse(vulns_as_string);
        if (vulns_to_json.size()&gt;0)
        {
            System.out.print(""Splunk return results"");
            for (int v = 0; v &lt; vulns_to_json.size(); v++)
            {
                String vuln_as_string = vulns_to_json.get(v).toString();
                Vulnerability vulnerability = new Gson().fromJson(vuln_as_string, Vulnerability.class);
                data_Parsed = true;
                vulnerability.ports_to_List();
                list_of_OS_Vulnerability.add(vulnerability);
            }
            return json;
        }
        System.out.print(""Splunk return empty results"");
        return  null;
    }
</code></pre>

<p>I make request to Splunk from different class - it return service which I used to pass queries to splunk</p>",55630190.0,1,0,,2019-4-11 09:10:43,,2019-4-11 10:21:23,,,,,11317359.0,,1,0,java|json|splunk|splunk-query,808,14
252,258708,55671550,How to build a chronological event list in Splunk,"<p>I have an index that lists (among other things) a device, event date, and level (1-4). Devices change levels at random intervals. I need to build a search that shows how long a particular device has been at a certain level, but I can't do a simple count; if a device is at level 1 for three days, goes to level 2 for five days, then back to level 1 for two days, a count will show five days which is obviously incorrect. How can I generate a 'Consecutive days at current level' field?</p>

<p>I need a query that reports Device, Date, Level, and Days at Current Level. Thanks!</p>",,1,1,,2019-4-14 02:44:48,,2019-4-18 13:19:45,,,,,7973392.0,,1,0,events|splunk|chronological,671,11
253,258709,55695497,Using Named Group Capture With rex In a Splunk Dashboard Query?,"<p>While trying to use <code>rex</code> as part of a splunk search I have a regular expression that works fine:</p>

<pre><code>eventtype=my_type | rex field=_raw "".*\[(?&lt;foo&gt;.*?)\].*"" | table _time, foo
</code></pre>

<p>But when I try to save the search into a dashboard table I get the following error:</p>

<blockquote>
  <p>Error parsing XML on line 29: Premature end of data in tag form line 1</p>
</blockquote>

<p>I know my query is fine because when I click the ""Run Search"" button while adding it to the dashboard table I get a valid result.  But when I click the save button I get the above error.</p>

<p>I suspect the named group capture within the <code>regular expression</code> is throwing off the <code>XML</code> parser.  </p>

<p><strong>How do I use a rex regular expression with name capture as part of a dashboard query?</strong></p>

<p>Thank you in advance for your consideration and response.</p>",55695607.0,1,0,,2019-4-15 18:45:23,,2019-4-15 18:52:46,,,,,1876739.0,,1,0,splunk,662,12
254,258710,55708596,How to regex a phrase up to a parenthesis?,"<p>I'm trying to regex a group called reason, i have got very close but can't quite figure out the last part. I want to regex everything between <code>Reason:</code> up to and not including the first bracket in <code>(winRc=999)</code></p>

<p>The string that is being extracted is below.</p>

<p>Reason: The user name or password is incorrect. (winRc=999)</p>

<p>I wish to have an expression that shows:</p>

<p>A Full Match of ""Reason: The user name or password is incorrect.""
A Group 'Reason' Match of ""The user name or password is incorrect.""</p>",55708831.0,1,3,,2019-4-16 12:51:08,,2019-4-16 14:51:27,2019-4-16 14:50:08,,1143387.0,,11198407.0,,1,-1,regex|splunk,35,6
255,258711,55720690,Splunk query using append,"<p>I have a query that calculates Batch logs from different time slots and shows the output using append command.But in the first time slot i'm getting a batch log which is not there in 2nd timeslot of the same query. In the output of the query after appending i'm not getting the logs that are appearing in only timeslot.</p>

<p>Query using</p>

<p>index=main sourcetype=xml ""MSR*"" earliest=-30d latest=-15d 
in the above query i'm getting MSR1451 batch in the output.</p>

<p>index=main sourcetype=xml ""MSR*"" earliest=-14d latest=now()
in the above query, we are not getting that MSR1451 batch.</p>

<p>index=main sourcetype=xml ""MSR*"" earliest=-30d latest=-15d |fields jobName | eval marker=""Before 15 days"" | append [search index=main sourcetype=xml ""MSR*"" earliest=-30d latest=-15d |fields jobName | eval marker=""After 15 days""] | stats count (eval(marker=""Before 15 days"")) AS Before 15 days, count (eval(marker=""After 15 days"")) AS After 15 days by JobName</p>

<p>In the above query i'm getting only the common jobs that are appearing in both the time slots. I need the jobs that are appearing in only one time slot also should be listed.</p>",55722926.0,1,0,,2019-4-17 05:47:51,,2019-4-27 02:31:36,,,,,11232481.0,,1,0,append|splunk|splunk-query,1024,13
256,258712,55728023,How to subtract a string from value field,"<p>I Need to know to subtract a string from the begining of a value until a specific character in Spl. For example, if I have a field who contains emails or another data:</p>

<p>MAIL FROM: YYYY@XXXXX.com BODY=7BIT</p>

<p>How to get just the email address YYYY@XXXXX.com</p>

<p>Thanks for the help.</p>",,1,0,,2019-4-17 12:54:35,,2019-4-17 21:08:48,2019-4-17 13:04:54,,2630441.0,,2630441.0,,1,-2,splunk|splunk-query,66,7
257,258713,55746115,On click is not working with two div nested in same row,"<p>I have two HTML <code>div</code> nested in one <code>div class=""row""</code>; the first one contains three <code>&lt;button&gt;</code>, the second one four <code>&lt;p&gt;</code>. Before adding the second <code>div</code>, the buttons included in the first <code>div</code> correctly worked if clicked; after having added the second <code>div</code> the button function <code>on.click</code> stopped working - button ID's were not changed, nor the js. If I remove the second <code>div id=""stats""</code>, buttons turn up and running. </p>

<p>I guess there is a type conflict within the <code>row div</code> but so far couldn't find it.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>$('#btn-button-a').on(""click"", function() {
  console.log(""click on button"")
  var url = '....';

  window.open(url, '_blank');
});

$('#btn-button-b').on(""click"", function() {
  console.log(""click on button"")
  var url = '....';

  window.open(url, '_blank');
});

$('#btn-button-c').on(""click"", function() {
  console.log(""click on button"")
  var url = '....';

  window.open(url, '_blank');
});</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""&gt;&lt;/script&gt;
&lt;html&gt;
&lt;div class=""row"" style=""margin-left:0px !important;""&gt;
  &lt;div style=""float:left; width: 184px; height: 152px; background: #bee0cc; border-radius: 8px; margin-left: 12px; margin-right: 12px;""&gt;
    &lt;b&gt;
        &lt;label style=""font-size: 18px; color: white; margin-top: 4px; text-align: center;""&gt;Title&lt;/label&gt;
         &lt;button id=""btn-button-a"" class=""btn btn-primary-custom"" style=""width: 170px; height: 30px; font-size: 13px; padding-left: 0px !important; margin:6px; text-align: center""&gt;
          &lt;span&gt;Button a&lt;/span&gt; &lt;/button&gt;
         &lt;button id=""btn-button-b"" class=""btn btn-primary-custom"" style=""width: 170px; height: 30px; font-size: 13px; padding-left: 0px !important; margin:6px ; text-align: center""&gt;
          &lt;span&gt;Button b&lt;/span&gt; &lt;/button&gt;
         &lt;button id=""btn-button-c"" class=""btn btn-primary-custom"" style=""width: 170px; height: 30px; font-size: 13px; padding-left: 0px !important; margin:6px ; text-align: center""&gt;
          &lt;span&gt;Button c &lt;/span&gt; &lt;/button&gt;
        &lt;/b&gt;
  &lt;/div&gt;
  &lt;div id=""stats"" style=""float:left; width: 400px; height: 152px; background: #bee0cc; border-radius: 8px; margin-left: 12px; margin-right: 12px;""&gt;
    &lt;label style=""font-size: 18px; color: white; margin-top: 4px; text-align: center;""&gt;report&lt;/label&gt;
    &lt;p style=""font-size: 13px; color: black; margin: 15px; text-align: left;""&gt;Frozen users: &lt;b&gt;$result_a$&lt;/b&gt;
    &lt;/p&gt;
    &lt;p style=""font-size: 13px; color: black; margin: 15px; text-align: left;""&gt;Frozen users %: &lt;b&gt;$result_b$&lt;/b&gt;
    &lt;/p&gt;
    &lt;p class=""status-inline"" style=""font-size: 13px; color: black; margin: 15px; text-align: left;""&gt;Status &lt;b&gt;$result_c$&lt;/b&gt;
      &lt;!--&lt;span&gt; &lt;/span&gt; --&gt;
    &lt;/p&gt;
    &lt;p style=""font-size: 13px; color: black; margin: 15px; text-align: left;""&gt;t_number: &lt;b&gt;-&lt;/b&gt;
    &lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;/html&gt;</code></pre>
</div>
</div>
</p>

<p>By removing from the second div <code>$result_a$</code>, <code>$result_b$</code>, <code>$result_c$</code>, which are tokens set by a Splunk search, the button work again. </p>",,0,13,,2019-4-18 12:31:39,,2019-4-23 11:21:03,2019-4-23 11:21:03,,10607772.0,,8338046.0,,1,0,javascript|jquery|html|splunk,130,8
258,258714,55752456,How do I get Data to 'commit' from Python to SQL Server?,"<p>I have a localhost SQL Server running and am able to connect to it successfully. However, I am running into the problem of data not transfering over from temp csv files. Using Python import pyodbc for Server connection.</p>

<p>I've tried with Python Import pymssql but have had worse results so I've stuck with pyodbc. I've also tried closing the cursor each time or just at the end but not to any luck.</p>

<p>Here is a piece of code that I am using. Towards the bottom are two different csv styles. One is a temp in which is used to fill the SQL Server table. The other is for my personal use to make sure I am actually gathering information at the moment but, in the long term will be removed so only the temp csv is used.</p>

<pre><code>@_retry(max_retry=1, timeout=1)
def blocked_outbound_utm_scada():
    # OTHER CODE EXISTS HERE!!!
    # GET Search Results and add to a temp CSV File then send to MS SQL Server
    service_search_results_str = '/services/search/jobs/%s/results?output_mode=csv&amp;count=0' % sid
    search_results = (_service.request(_host + service_search_results_str, 'GET',
                                       headers={'Authorization': 'Splunk %s' % session_key},
                                       body={})[1]).decode('utf-8')
    with tempfile.NamedTemporaryFile(mode='w+t', suffix='.csv', delete=False) as temp_csv:
        temp_csv.writelines(search_results)
        temp_csv.close()
        try:
            cursor.execute(""BULK INSERT Blocked_Outbound_UTM_Scada FROM '%s' WITH (""
                           ""FIELDTERMINATOR='\t', ROWTERMINATOR='\n', FirstRow = 2);"" % temp_csv.name)
            conn.commit()
        except pyodbc.ProgrammingError:
            cursor.execute(""CREATE TABLE Blocked_Outbound_UTM_Scada (""
                           ""Date_Time varchar(25),""
                           ""Src_IP varchar(225),""
                           ""Desktop_IP varchar(225));"")
            conn.commit()
        finally:
            cursor.execute(""BULK INSERT Blocked_Outbound_UTM_Scada FROM '%s' WITH (""
                           ""FIELDTERMINATOR='\t', ROWTERMINATOR='\n', FirstRow = 2);"" % temp_csv.name)
            conn.commit()
        os.remove(temp_csv.name)
    with open(_global_path + '/blocked_outbound_utm_scada.csv', 'a', newline='') as w:
        w.write(search_results)
    w.close()
</code></pre>

<p>I'm just trying to get the information into SQL Server but the code seems to be ignoring <code>cursor.commit()</code>. Any help is appreciated in figuring out what is wrong.</p>

<p>Thanks in Advance!</p>",,2,4,,2019-4-18 19:15:59,,2021-7-10 11:37:48,,,,,10741174.0,,1,1,python|sql-server|splunk|temporary-files,1099,13
259,258715,55753324,How to filter data of one splunk search from another splunk search,"<p>I want to filter results of one splunk search from result of second second splunk search</p>

<p>I have two splunk query</p>

<pre><code>index=pool status=OK Detail=Outgoing | table order

A11 A12 A13

index=pool status=OK Detail=Incmoing| table order

A11 A12
</code></pre>

<p>I want to filter the results of second search from the search of first search so that i can get result like</p>

<p>A13</p>",,1,0,,2019-4-18 20:29:09,,2019-4-19 01:55:02,2019-4-18 21:52:15,,9681368.0,,10695407.0,,1,0,splunk|splunk-query,451,10
260,258716,55766556,"Splunk regex line breaker, combining multiple ERROR [stderr] lines into one event","<p>I am working on displaying logs from docker json-file in splunk. For the most part I have it working, except for when a stack trace is printed. I know <code>e.printStackTrace()</code> isn't best practice, but our services have a few here and there that I want to support in Splunk.</p>

<p>I am configuring the props.conf file, and I have the following <code>LINE_BREAKER</code> regex in props.conf.
<code>LINE_BREAKER=([\n\r]+)\s*{""log"":""[0-9]+.*[0-9]\s+</code></p>

<p>This will match up to the end of a timestamp of this form: <code>{""log"":""2019-04-18 15:18:18,796 ERROR [stderr] ...</code></p>

<p>The problem is that the printed stack trace is coming in multiple lines like this:</p>

<pre><code>{""log"":""2019-04-18 15:18:18,796 ERROR [stderr] (default-threads - 3) java.util.NoSuchElementException: No value present\n"",""stream"":""stdout"",""time"":""2019-04-18T19:18:18.800485539Z""}
{""log"":""2019-04-18 15:18:18,804 ERROR [stderr] (default-threads - 3) at java.util.Optional.get(Optional.java:135)\n"",""stream"":""stdout"",""time"":""2019-04-18T19:18:18.806510971Z""}
{""log"":""2019-04-18 15:18:19,259 ERROR [stderr] (default-threads - 3) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n"",""stream"":""stdout"",""time"":""2019-04-18T19:18:19.264689098Z""}
{""log"":""2019-04-18 15:18:19,259 ERROR [stderr] (default-threads - 3) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n"",""stream"":""stdout"",""time"":""2019-04-18T19:18:19.281810119Z""}
{""log"":""2019-04-18 15:18:19,274 ERROR [stderr] (default-threads - 3) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n"",""stream"":""stdout"",""time"":""2019-04-18T19:18:19.28185714Z""}
{""log"":""2019-04-18 15:18:19,275 ERROR [stderr] (default-threads - 3) at java.lang.reflect.Method.invoke(Method.java:498)\n"",""stream"":""stdout"",""time"":""2019-04-18T19:18:19.281867696Z""}
{""log"":""2019-04-18 15:18:19,275 ERROR [stderr] (default-threads - 3) at org.jboss.weld.interceptor.proxy.TerminalAroundInvokeInvocationContext.proceedInternal(TerminalAroundInvokeInvocationContext.java:49)\n"",""stream"":""stdout"",""time"":""2019-04-18T19:18:19.281875844Z""}
{""log"":""2019-04-18 15:18:19,498 ERROR [org.apache.activemq.ActiveMQSession] (default-threads - 3) error dispatching message: : javax.ejb.EJBTransactionRolledbackException\n"",""stream"":""stdout"",""time"":""2019-04-18T19:18:19.523359025Z""}
</code></pre>

<p>... and so on. until the end of the stack trace. Each line ends up being an event based on my regex for <code>LINE_BREAKER</code>, which is fine for <code>INFO</code> messages and single line <code>ERROR</code> messages from our logger, but not for the stack trace as above.</p>

<p>I want to combine this stack trace of the ERROR [stderr] together into one event. So my line break should match until the next timestamp message that is not [stderr].</p>

<p>from my example, it should match from the first line with <code>ERROR [stderr]</code> until: <code>{""log"":""2019-04-18 15:18:19,498</code> (last log line above).</p>

<p>I have tried adding negative lookahead but now it will only match 1 time. <code>([\n\r]+)\s*{""log"":""[0-9]+.*[0-9]\s+[\s\S]+\[(?!stderr])</code> matches until the last line, but if there are lines under that, none of them match anymore.</p>",55880380.0,1,2,,2019-4-19 18:58:52,,2019-4-27 12:04:42,2019-4-19 20:17:13,,1738539.0,,1738539.0,,1,1,java|regex|docker|logging|splunk,1069,13
261,258717,55780940,Join two Splunk queries without predefined fields,"<p>I am trying to join 2 splunk queries. However in this case the common string   between the 2 queries is not a predefined splunk field and is logged in a   different manner. I have created the regex which individually identifies the   string but when I try to combine using join, I do not get the result. </p>

<p>I have logs like this - </p>

<p>Logline 1 - </p>

<pre><code>21-04-2019 11:01:02.001 server1 app1    1023456789 1205265352567565 1234567Z-1234-1234-1234-123456789123    Application Completed 
</code></pre>

<p>Logline 2 - </p>

<pre><code>21-04-2019 11:00:00.000 journey_ends server1    app1 1035625855585989 .....(lots of text) commonID:1234567Z-1234-1234-1234-123456789123 .....(lots of text) status(value) OK
</code></pre>

<p>the second Logline can be NOTOK as well </p>

<p>Logline 2 - </p>

<pre><code>21-04-2019 11:00:00.000 journey_ends server1    app1 1035625855585989 .....(lots of text) commonID:1234567Z-1234-1234-1234-123456789123 .....(lots of text) status(value) NOTOK 
</code></pre>

<p>I have tried multiple things but the best that I can come up with is - </p>

<pre><code>index=test ""journey_ends"" | rex ""status(value) (?&lt;StatusType&gt;[A-Z][A-Z]*)"" | rex ""commonID\:(?&lt;commonID&gt;[^\t]{37})"" | table StatusType, commonID | join type=inner commonID [ search index=test ""Application Completed"" | rex ""^(?:[^\t\n]*\t){7}(?P&lt;commonID&gt;[^\t]+)"" | table _time, commonID] | chart count over StatusType by commonID 
</code></pre>

<p>However the above query does not provide me the stats. In verbose mode, I can just see the events of query 1. Please note that the above 2 queries run correctly individually. </p>

<p>However currently I have to initially run the query to fetch the commonIDs from ""Application Completed"" logline and then in another query give the list of commonIDs found in the result first query as input and find the status value for each commonId from logline 2. </p>

<p>Expected Result (in a table):</p>

<pre><code>StatusType commonID OK  1234567Z-1234-1234-1234-123456789123 NOTOK  1234567Z-1234-1234-1234-985625623541 
</code></pre>",55782550.0,1,0,,2019-4-21 07:50:54,1.0,2021-8-10 20:28:22,2019-4-21 09:33:42,,11390098.0,,11390098.0,,1,1,splunk|splunk-query,1457,13
262,258718,55781064,How to configure Splunk forwarder to collect data from stdout/stderr instead of from files?,"<p>I have a <strong>Splunk</strong> forwarder configured to read my application server log files and send them to our indexer. That works just fine.</p>

<p>I'd like to re-configure my forwarder to read my logs directly from the standard output to reduce I/O overhead, can anyone help with that please?</p>",,2,2,,2019-4-21 08:12:34,,2019-4-23 11:06:47,2019-4-21 18:20:48,,704848.0,,11053255.0,,1,0,splunk,1201,12
263,258719,55807451,How to calculate the response time of all the internal services being called in a controller class using AOP,"<p>My Rest Api is developed in Spring Boot and for logging we are using Splunk and Spring AOP .My question is how to calculate the response time of all the internal services being  called. For now, I am able to get the end to end response time.
We need this for creating dashboard in Splunk to track the performance of Api.</p>",,1,1,,2019-4-23 08:48:40,,2019-4-23 11:01:41,2019-4-23 09:04:24,,2664350.0,,11398600.0,,1,0,java|spring-boot|spring-aop|splunk,660,11
264,258720,55835663,Find SonicWall Admin login's using splunk,<p>I would like to use splunk to report on when the Admnin credentials are used to login to the Sonicwall. Please help</p>,55843988.0,1,0,,2019-4-24 17:35:59,,2019-4-25 07:27:47,,,,,10891681.0,,1,0,splunk|sonicwall,47,6
265,258721,55849351,How can I minimize my Splunk search function if all of the host names begin with the same letters?,"<p>I have a number of hosts (servers) and I want to search through all of them except for 4 different ones.</p>

<p>Here is what I have working at the moment to exclude the 4 servers:</p>

<pre><code>(host!=""ajl2dal8"" OR host!=""ajl2dal9"" OR host!=""ajl2atl8"" OR host!=""ajl2atl9"")
</code></pre>

<p>While this works fine, its fairly sizable and will only get longer if I need to exclude more. Since they all begin with ajl2 and have either atl or dal and a number, is there any way I could get something like this to work:</p>

<pre><code>(host!=""ajl2[atl|dal][1|2|3|4]"")
</code></pre>",55854082.0,1,0,,2019-4-25 12:30:38,,2019-4-25 16:44:08,,,,,2440230.0,,1,0,splunk,33,7
266,258722,55874657,OpenStack VM creating Using Alerts from Splunk,"<p>As per my understanding, in AWS, we can combine AWS CloudWatch and AWS Elastic Beanstalk for the automation of VM creation. For example, We can configure CloudWatch to trigger an alert for certain condition and depending on that we can create/alter a VM.  Is there a way to do the same with OpenStack using Terraform scripts? </p>

<p>Currently, we are creating and managing OpenStack VM's using terraform and ansible scripts. We have Splunk for dashboard and alerts. Is there a way to execute terraform scripts for VM's as we get an alert from Splunk? Please correct me if my understanding is wrong.</p>",,1,0,,2019-4-26 20:39:35,,2019-5-5 16:11:36,2019-5-5 16:11:36,,5446749.0,,7092963.0,,1,0,ansible|terraform|openstack|splunk,51,6
267,258723,55883350,how to pass a parameter to my regex and loop on an array to find the word corresponding to my parameter,"<p>I want to add span class to highlight each word on a table corresponding to my parameter value.</p>

<p>Now, I can add a span class only when I tape the word on a regex parameter like this : patt=/40/igm;
I use this solution just to test but i want to use my parameter ""token""
(only declared but not yet used)</p>

<pre><code>render: function($td, cell) {
    let token = tokens.get('search_tok'); //search word
    patt=/40/igm; //test with regex parameter
    let value=cell.value; //cells values
    while (matcher = patt.exec(cell.value)) {
        console.log(matcher.index + ' ' + patt.lastIndex);
        value = value.slice(0, matcher.index) + '&lt;span id=""highlight_search""&gt;'+cell.value.slice(matcher.index, patt.lastIndex)+'&lt;/span&gt;'+cell.value.slice(patt.lastIndex);
        }}
</code></pre>

<p>Currently :
- this only add the span class to the word 40 but I want to add it to the word searched (token)
- also when i try with the number 4 only (so with only one character) this not work very well...</p>",,0,2,,2019-4-27 17:55:45,,2019-4-27 17:55:45,,,,,11420189.0,,1,0,javascript|css|splunk|spl,38,6
268,258724,55906038,Use Sed to replace numbers in URL within Splunk,"<p>How can I extract alpha-numeric values within a URL? I have the following query that isn't replacing the right values. </p>

<p>Example Input Data: </p>

<pre><code>/example/endpoint/here/34456dwf45
/endpoint/fddk449372
/434236/example/endpoint
</code></pre>

<p>Expected Output:</p>

<pre><code>/example/endpoint/here/my_var
/endpoint/my_var
/my_var/example/endpoint
</code></pre>

<p>Current Query: </p>

<pre><code>* | rex mode=sed field=request_url ""s/(.*\\/)[^\/]+(\/.*)/\1my_var\2/"" 
  | stats values(request_url)
</code></pre>

<p>How can I use sed to replace any alpha-numeric values within two / characters to be a string inside a URL?</p>",55906500.0,1,0,,2019-4-29 15:01:53,,2019-4-29 15:55:48,2019-4-29 15:34:07,,3832970.0,,9493938.0,,1,1,regex|sed|splunk|splunk-query,1364,13
269,258725,55930867,Splunk Universal Forwarder as sidecar in kubernetes,"<p>I am setting up a splunk universal forwarder as a sidecar with my application through a deployment spec. The splunk universal forwarder is setup as a different docker image where I copy custom inputs.conf and outputs.conf through docker COPY (shown below).</p>

<p>Effectively when I deploy my application, the sidecar is starting. In the current state, the indexer configuration is in the output.conf and which is taking effect.</p>

<p><strong>*The issue comes here: I want to change the indexer server host and port dynamically based on the environment. *</strong></p>

<p>Here is my dockerfile content of splunk universal forwarder.</p>

<pre><code>FROM splunk/universalforwarder:latest

COPY configs/*.conf /opt/splunkforwarder/etc/system/local/
</code></pre>

<p>Built the docker images with name <strong><em>splunk-universal-forwarder:demo</em></strong>
The configs folder have both files inputs.conf and outputs.conf.</p>

<p>The content of outputs.conf is</p>

<pre><code>[tcpout]
defaultGroup = default-lb-group

[tcpout:default-lb-group]
server = ${SPLUNK_BASE_HOST}

[tcpout-server://host1:9997]
</code></pre>

<p>I want to pass the SPLUNK_BASE_HOST environment variable through the sidecar deployment like below.</p>

<pre><code>  - name: universalforwarder
        image: splunk-universal-forwarder:demo
        imagePullPolicy: Always
        env:
          - name: SPLUNK_START_ARGS
            value: ""--accept-license --answer-yes""
          - name: SPLUNK_BASE_HOST
            value: 123.456.789.000:9997
          - name: SPLUNK_USER
            valueFrom:
              secretKeyRef:
                name: credentials
                key: splunk.username
          - name: SPLUNK_PASSWORD
            valueFrom:
              secretKeyRef:
                name: credentials
                key: splunk.password
        volumeMounts:
        - name: container-logs
          mountPath: /var/log/splunk-fwd-myapp
</code></pre>

<p>I have a separate deployment.yaml per environment (dev, stage, uat, qa, prod) and I should be able to pass different indexer host and port  <strong><em>SPLUNK_BASE_HOST</em></strong> based on these environments. If I hardcode the indexer host and port in outputs.conf, it will take the same value across all environments but I don't want that to happen. </p>

<p>The environment variable ${SPLUNK_BASE_HOST} in the outputs.conf is not referring to the value supplied in deployment yaml file.</p>",,1,0,,2019-5-1 02:15:15,,2019-5-1 03:28:32,,,,,3033105.0,,1,0,kubernetes|splunk,1311,12
270,258726,55938833,How do I install the Splunk Mint Gradle plugin?,"<p>According to the Splunk MINT documentation (<a href=""https://docs.splunk.com/Documentation/MintAndroidSDK/5.2.x/DevGuide/Requirementsandinstallation"" rel=""nofollow noreferrer"">https://docs.splunk.com/Documentation/MintAndroidSDK/5.2.x/DevGuide/Requirementsandinstallation</a>) you must manually download the Gradle plugin artifact and include it in your project.</p>

<p>I successfully downloaded and deployed this artifact to a local instance of Artifactory, since it's already archived in a maven repo format.</p>

<p>To test the integration, I simply created a brand new project and added/applied the mint plugin. From here, Gradle sync fails with the following error:</p>

<pre><code>Caused by: java.lang.NoSuchFieldError: javacTask
 at com.splunk.mint.gradle.android.plugin.utils.VariantUtilsKt.getJavaTask(VariantUtils.kt:13)
 at com.splunk.mint.gradle.android.plugin.api.AspectJTransform.setupVariant(AspectJTransform.kt:81)
 at com.splunk.mint.gradle.android.plugin.api.AspectJTransform$prepareProject$1.execute(AspectJTransform.kt:57)
 at com.splunk.mint.gradle.android.plugin.api.AspectJTransform$prepareProject$1.execute(AspectJTransform.kt:43)
 at org.gradle.configuration.internal.DefaultListenerBuildOperationDecorator$BuildOperationEmittingAction$1$1.run(DefaultListenerBuildOperationDecorator.java:150)
 at org.gradle.configuration.internal.DefaultUserCodeApplicationContext.reapply(DefaultUserCodeApplicationContext.java:58)
 at org.gradle.configuration.internal.DefaultListenerBuildOperationDecorator$BuildOperationEmittingAction$1.run(DefaultListenerBuildOperationDecorator.java:147)
 at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:402)
 at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:394)
 at org.gradle.internal.operations.DefaultBuildOperationExecutor$1.execute(DefaultBuildOperationExecutor.java:165)
 at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:250)
 at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:158)
 at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:92)
 at org.gradle.configuration.internal.DefaultListenerBuildOperationDecorator$BuildOperationEmittingAction.execute(DefaultListenerBuildOperationDecorator.java:144)
 at org.gradle.internal.event.BroadcastDispatch$ActionInvocationHandler.dispatch(BroadcastDispatch.java:91)
 at org.gradle.internal.event.BroadcastDispatch$ActionInvocationHandler.dispatch(BroadcastDispatch.java:80)
 at org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:42)
 at org.gradle.internal.event.BroadcastDispatch$SingletonDispatch.dispatch(BroadcastDispatch.java:230)
 at org.gradle.internal.event.BroadcastDispatch$SingletonDispatch.dispatch(BroadcastDispatch.java:149)
 at org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:58)
 ... 126 more
</code></pre>

<h2>Environment:</h2>

<ul>
<li>Android Studio 3.4</li>
<li>Gradle 5.2</li>
<li>Android Gradle Plugin 3.4.0</li>
<li>Splunk Mint Plugin 5.2.5</li>
</ul>

<p>I suspect the Splunk team needs to update this plugin to support newer versions of Gradle. Unfortunately I'm unable to downgrade to a previous 4.x version since that is below the minumum supported version for Android Studio.</p>",,3,0,,2019-5-1 15:39:46,,2020-4-9 08:36:53,,,,,496564.0,,1,2,android|gradle|splunk,916,18
271,258727,55944455,Splunk: Need help using transaction to get the pcode when this error occurs,"<p>I am trying to use the following search method to pull all events to include the actual pcode value here, in my example is 650404 where it says </p>

<blockquote>
  <p>2019-05-01 05:17:04,351 [http-bio-8081-exec-19] INFO 
  {hostName=vmjsbatchprod01-s01}
  EZTaxServiceImpl.processChargeTransaction:206 - pcode: 650404</p>
</blockquote>

<p>when the following error occurs</p>

<blockquote>
  <p>EZTaxServiceImpl.processChargeTransaction:253 - Error
  billsoft.eztax.EZTaxException: PCode not found.</p>
</blockquote>

<p>I've tried using the following search, but it fails to pull any results.</p>

<p>""EZTaxServiceImpl.processChargeTransaction:206"" | transaction endswith=""EZTaxServiceImpl.processChargeTransaction:253""</p>

<p>I wanted to see if I could successfully search for these events so I can use the event builder afterwards to pull my results into a report that goes to our billing team. They both have an output from the host vmjsbatchprod01-s01 and when I view the search results of one, I can easily scroll down and find the other output as well so I know for a fact I should be able to ""join"" these statements to get what I need.</p>

<p>This is the output I get when searching for just </p>

<blockquote>
  <p>""EZTaxServiceImpl.processChargeTransaction:253""
  Which outputs the error code only when bill tax fails to validate the pcode properly on a couple lines above it </p>
  
  <p>2019-05-01 05:17:04,348 [http-bio-8081-exec-19] TRACE
  {hostName=vmjsbatchprod01-s01} --------
  EZTaxServiceImpl.processChargeTransaction:201 Enter --------
  2019-05-01 05:17:04,349 [http-bio-8081-exec-19] INFO 
  {hostName=vmjsbatchprod01-s01}
  EZTaxServiceImpl.processChargeTransaction:202 - Transaction Type: 59
  2019-05-01 05:17:04,349 [http-bio-8081-exec-19] INFO 
  {hostName=vmjsbatchprod01-s01}
  EZTaxServiceImpl.processChargeTransaction:203 - Service Type: 21
  2019-05-01 05:17:04,350 [http-bio-8081-exec-19] INFO 
  {hostName=vmjsbatchprod01-s01}
  EZTaxServiceImpl.processChargeTransaction:204 - Amount: 0.0 2019-05-01
  05:17:04,350 [http-bio-8081-exec-19] INFO 
  {hostName=vmjsbatchprod01-s01}
  EZTaxServiceImpl.processChargeTransaction:205 - Number of Lines: 3
  2019-05-01 05:17:04,351 [http-bio-8081-exec-19] INFO 
  {hostName=vmjsbatchprod01-s01}
  EZTaxServiceImpl.processChargeTransaction:206 - pcode: 650404
  2019-05-01 05:17:04,351 [http-bio-8081-exec-19] INFO 
  {hostName=vmjsbatchprod01-s01}
  EZTaxServiceImpl.processChargeTransaction:207 - event id: null
  2019-05-01 05:17:04,352 [http-bio-8081-exec-19] ERROR
  {hostName=vmjsbatchprod01-s01}
  EZTaxServiceImpl.processChargeTransaction:253 - Error
  billsoft.eztax.EZTaxException: PCode not found.   at
  billsoft.eztax.Util.throwEZTaxException(Util.java:313)
  ~[eztax-9.20.1905-1.jar:?]    at
  billsoft.eztax.Transaction.calculateTaxes(Native Method)
  ~[eztax-9.20.1905-1.jar:?]    at
  billsoft.eztax.Transaction.calculateTaxes(Transaction.java:1231)
  ~[eztax-9.20.1905-1.jar:?]    at
  com.nextiva.billsoft.service.EZTaxServiceImpl.processChargeTransaction(EZTaxServiceImpl.java:236)
  [nextiva-eztax-service-1.0.70.jar:?]  at
  com.nextiva.billsoft.controller.EZTaxController.processChargeTransaction(EZTaxController.java:128)
  [EZTaxController.class:?]     at
  sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source) ~[?:?]
    at
  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  ~[?:1.8.0_171]    at java.lang.reflect.Method.invoke(Method.java:498)
  ~[?:1.8.0_171]    at
  org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)
  [spring-web-4.2.1.RELEASE.jar:4.2.1.RELEASE]  at
  org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)
  [spring-web-4.2.1.RELEASE.jar:4.2.1.RELEASE]  at
  org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:111)
  [spring-webmvc-4.2.1.RELEASE.jar:4.2.1.RELEASE]   at
  org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:806)
  [spring-webmvc-4.2.1.RELEASE.jar:4.2.1.RELEASE]   at
  org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:729)
  [spring-webmvc-4.2.1.RELEASE.jar:4.2.1.RELEASE]   at
  org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
  [spring-webmvc-4.2.1.RELEASE.jar:4.2.1.RELEASE]   at
  org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959)
  [spring-webmvc-4.2.1.RELEASE.jar:4.2.1.RELEASE]   at
  org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893)
  [spring-webmvc-4.2.1.RELEASE.jar:4.2.1.RELEASE]   at
  org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970)
  [spring-webmvc-4.2.1.RELEASE.jar:4.2.1.RELEASE]   at
  org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872)
  [spring-webmvc-4.2.1.RELEASE.jar:4.2.1.RELEASE]   at
  javax.servlet.http.HttpServlet.service(HttpServlet.java:650)
  [servlet-api.jar:?]   at
  org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846)
  [spring-webmvc-4.2.1.RELEASE.jar:4.2.1.RELEASE]   at
  javax.servlet.http.HttpServlet.service(HttpServlet.java:731)
  [servlet-api.jar:?]   at
  org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
  [catalina.jar:7.0.88]     at
  org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
  [catalina.jar:7.0.88]     at
  org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
  [tomcat7-websocket.jar:7.0.88]    at
  org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
  [catalina.jar:7.0.88]     at
  org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
  [catalina.jar:7.0.88]     at
  com.nextiva.utilities.rest.web.filter.RequestContextFilter.doFilter(RequestContextFilter.java:60)
  [nextivaUtilities-6.0.9.jar:?]    at
  org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
  [catalina.jar:7.0.88]     at
  org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
  [catalina.jar:7.0.88]     at
  org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:219)
  [catalina.jar:7.0.88]     at
  org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:110)
  [catalina.jar:7.0.88]     at
  org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:498)
  [catalina.jar:7.0.88]     at
  org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169)
  [catalina.jar:7.0.88]     at
  org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
  [catalina.jar:7.0.88]     at
  org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:683)
  [catalina.jar:7.0.88]     at
  org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:1025)
  [catalina.jar:7.0.88]     at
  org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
  [catalina.jar:7.0.88]     at
  org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:445)
  [catalina.jar:7.0.88]     at
  org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1139)
  [tomcat-coyote.jar:7.0.88]    at
  org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637)
  [tomcat-coyote.jar:7.0.88]    at
  org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:318)
  [tomcat-coyote.jar:7.0.88]    at
  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  [?:1.8.0_171]     at
  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  [?:1.8.0_171]     at
  org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
  [tomcat-coyote.jar:7.0.88]    at java.lang.Thread.run(Thread.java:748)
  [?:1.8.0_171]</p>
</blockquote>",,1,0,,2019-5-2 01:04:41,0.0,2019-5-10 15:53:21,2019-5-2 01:13:35,,3489481.0,,3489481.0,,1,0,splunk,44,6
272,258728,55972378,How to create and call sybase stored procedure,"<p>i have sybase databse with following stored procedure ,</p>

<p>i am also running this query from splunk db connect app</p>

<p>I have tried CALL stored_procedure and
EXEC stored_procedure</p>

<pre><code>create procedure sp__cpu_busy_thread_test
as
begin
select @@servername,@@version
end
return(0)
</code></pre>",,1,9,,2019-5-3 14:56:52,,2019-5-11 20:24:43,2019-5-3 15:00:40,,11428944.0,,11428944.0,,1,0,sql|stored-procedures|sybase|sap-ase|splunk,1809,14
273,258729,56005740,Radar chart Display legend,"<p>I cannot find options (radarOptions) to display a legend in my chart using chart.js in Splunk environment.</p>

<p>This doesn't work for me:</p>

<pre><code>options: {
    legend: {
        display: true,
        labels: {
            fontColor: 'rgb(255, 99, 132)'
        }
    }
}
</code></pre>

<p>My code is below:</p>

<pre class=""lang-js prettyprint-override""><code>    var radarChartData = {
      labels: [""Python"", ""DevOps"", ""BI""],
      datasets: [{
        label: 'Dataset 1',
        fill: true,
        fillColor: ""rgba(151,187,205,0.5)"",
        strokeColor: ""rgba(151,187,205,1)"",
        pointColor: ""rgba(151,187,205,1)"",
        data: testData[0]
      }, {
        label: 'Dataset 2',
        fill: true,
        fillColor: ""rgba(101,197,205,0.5)"",
        strokeColor: ""rgba(151,187,205,1)"",
        pointColor: ""rgba(151,187,205,1)"",
        data: testData[1]
      }]
    }

    return radarChartData;
  },

  // Override this method to put the Splunk data into the view
  updateView: function(viz, radarData) {
    // Set radar chart options
    var radarOptions = {
      scaleOverride: true,
      scaleSteps: 10,
      scaleStepWidth: 1,
      scaleStartValue: 0,
      scaleShowLabels: true,
      datasetFill: true,
      scaleShowLabelBackdrop: true,
    };


    // Create the radar chart
    var myRadar = new Chart(document.getElementById(mychartid).getContext(""2d"")).Radar(radarData, radarOptions); /*radarData,radarOption*/
  }

});
</code></pre>

<p>Please help. How is called the option for displaying legends. Neither <code>legend: true</code> nor <code>legendDisplay:true</code> work for me.</p>",,1,0,,2019-5-6 12:57:32,,2019-5-13 07:37:21,2019-5-6 14:30:42,,1011722.0,,10430493.0,,1,1,javascript|charts|chart.js|splunk|splunk-sdk,231,9
274,258730,56008318,How to successfully add Eventgen App to Splunk,"<p>Having difficulty adding EventGen app to Splunk Enterprise instance per 
<code>Splunk 7 Essentials</code>. </p>

<p><a href=""https://learning.oreilly.com/library/view/splunk-7-essentials/9781788839112/"" rel=""nofollow noreferrer"">https://learning.oreilly.com/library/view/splunk-7-essentials/9781788839112/</a> - 10-day Free Trial to access book</p>

<p>Followed everything from the book, i.e</p>

<ol>
<li><p>Install 60-trial of Splunk Enterprise on <code>C:/</code> (as opposed to <code>C:/Program Files</code>)</p></li>
<li><p>Went to <code>https://github.com/PacktPublishing/Splunk-7-Essentials-Third-Edition</code> and clicked the green button, ""Clone or Download""</p></li>
<li><p>Extracted contents to C:/</p></li>
<li><p>Renamed extracted file to <code>SA-Eventgen</code></p></li>
<li><p>Opened cmd prompt in Administrator mode and ran following command</p></li>
</ol>

<p><code>xcopy SA-Eventgen C:SplunketcappsSA-Eventgen /O /X /E /H /K</code></p>

<p>However, <strong>there is a deviation</strong> in the next step. </p>

<p>The book says when you run</p>

<p><code>dir C:SplunketcappsSA-Eventgen</code> from the command prompt, you should get </p>

<p><a href=""https://i.stack.imgur.com/WrYyN.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WrYyN.jpg"" alt=""enter image description here""></a></p>

<p>But when I run</p>

<p><code>dir C:SplunketcappsSA-Eventgen</code> from the command prompt, I get</p>

<p><a href=""https://i.stack.imgur.com/1H6Go.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1H6Go.jpg"" alt=""enter image description here""></a></p>

<p>And when I restart Splunk instance and log in again, there is no indication that EventGen app has been installed.</p>",,1,0,,2019-5-6 15:32:57,,2019-5-6 16:40:18,,,,,10997704.0,,1,0,splunk,141,8
275,258731,56026003,Android Splunk SDK 5.2.5 gradle is failing to sync,"<p>I am trying to add Splunk sdk and gradle is failing to sync with <code>Cause: javacTask</code> I will add the stacktrace.</p>

<p>I have <code>mint-android-repo-5.2.5.zip</code> folder under app folder (so it is app > mint-plugin-repo-5.2.5)</p>

<p>I am using Android studio 3.4, and below is my gradle file.</p>

<pre><code>apply plugin: 'com.android.application'

apply plugin: 'com.splunk.mint.gradle.android.plugin'

apply plugin: 'kotlin-android'

apply plugin: 'kotlin-android-extensions'

apply plugin: ""kotlin-kapt""

buildscript {
    repositories {
        maven {
            url uri('mint-plugin-repo-5.2.5')
        }
        mavenCentral()
    }
    dependencies {
        classpath 'com.splunk:mint-gradle-android-plugin:5.2.5'
    }
}

repositories {
    maven {
        url uri('mint-plugin-repo-5.2.5')
    }
}

apply from: 'jacoco.gradle'

</code></pre>

<p>stack trace</p>

<pre><code>Caused by: java.lang.NoSuchFieldError: javacTask
    at com.splunk.mint.gradle.android.plugin.utils.VariantUtilsKt.getJavaTask(VariantUtils.kt:13)
    at com.splunk.mint.gradle.android.plugin.api.AspectJTransform.setupVariant(AspectJTransform.kt:81)
    at com.splunk.mint.gradle.android.plugin.api.AspectJTransform$prepareProject$1.execute(AspectJTransform.kt:57)
    at com.splunk.mint.gradle.android.plugin.api.AspectJTransform$prepareProject$1.execute(AspectJTransform.kt:43)
    at org.gradle.configuration.internal.DefaultListenerBuildOperationDecorator$BuildOperationEmittingAction$1$1.run(DefaultListenerBuildOperationDecorator.java:150)
    at org.gradle.configuration.internal.DefaultUserCodeApplicationContext.reapply(DefaultUserCodeApplicationContext.java:58)
    at org.gradle.configuration.internal.DefaultListenerBuildOperationDecorator$BuildOperationEmittingAction$1.run(DefaultListenerBuildOperationDecorator.java:147)
    at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:402)
    at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:394)
    at org.gradle.internal.operations.DefaultBuildOperationExecutor$1.execute(DefaultBuildOperationExecutor.java:165)
    at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:250)
    at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:158)
    at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:92)
    at org.gradle.configuration.internal.DefaultListenerBuildOperationDecorator$BuildOperationEmittingAction.execute(DefaultListenerBuildOperationDecorator.java:144)
    at org.gradle.internal.event.BroadcastDispatch$ActionInvocationHandler.dispatch(BroadcastDispatch.java:91)
    at org.gradle.internal.event.BroadcastDispatch$ActionInvocationHandler.dispatch(BroadcastDispatch.java:80)
    at org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:42)
    at org.gradle.internal.event.BroadcastDispatch$SingletonDispatch.dispatch(BroadcastDispatch.java:230)
    at org.gradle.internal.event.BroadcastDispatch$SingletonDispatch.dispatch(BroadcastDispatch.java:149)
    at org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:58)
    ... 126 more

</code></pre>",,0,2,,2019-5-7 15:31:36,,2019-5-7 15:31:36,,,,,1908005.0,,1,1,android|gradle|splunk|splunk-sdk,172,8
276,258732,56047836,Splunk only select matching JSON data,"<p>I load JSON reports into Splunk and those reports have many arrays. When I search:</p>

<pre><code>source=test| search ""behavior.system.processes.process{}.fileactivities.fileCreated.call{}.path""=""C:\\Windows*""
</code></pre>

<p>I often like to show the matching data. I use table to do so:</p>

<pre><code>source=test| search ""behavior.system.processes.process{}.fileactivities.fileCreated.call{}.path""=""C:\\Windows*"" | table ""behavior.system.processes.process{}.fileactivities.fileCreated.call{}.path""
</code></pre>

<p>However, the issue is that this shows me all fileCreated of the matching event and not only the one starting with C:\Windows.</p>

<p>How do I filter that?</p>",56058397.0,1,0,,2019-5-8 19:32:49,,2019-5-9 11:28:39,,,,,3625671.0,,1,0,splunk|splunk-query,1103,12
277,258733,56111665,How can I implement the calculation of the duration between Value 1 and 0 within one field in splunk?,"<p>How can I calculate the duration between the value 1 and 0 of one common field? The duration should be summed up with the following calculations of the durations, too. 
In the picture you can see the initial situation (first two columns), you get out of the base search! In the right two columns, the time-calculation should be calculated.</p>

<p>Code:
| base search
--> then you get the result, which you can see in the first two columns at the picture.</p>

<p><a href=""https://i.stack.imgur.com/oHwBR.png"" rel=""nofollow noreferrer"">Description of the problem!</a></p>",,1,0,,2019-5-13 11:47:29,,2019-5-14 10:12:03,,,,,10779796.0,,1,1,duration|splunk|splunk-query,108,8
278,258734,56115480,Regex working on regex101 and not on Splunk,"<p>I would like to capture the word unknown and anything after abcd\, abcd.com\ and unknown</p>

<pre><code>unknown                     
abcd\svc-backup
abcd\swt034         
abcd\svc-app-login  
abcd.com\chi572 
abcd\daj144 
abcd\smi556
abcd\mki317
abcd\aiw014
abcd\joh488
abcd\ymc965 
abcd\jet041
abcd\rjo220 
abcd\mst790
abcd.com\sre590
</code></pre>

<p>It captures fine with the regex </p>

<p><a href=""https://regex101.com/r/c9vdia/2/"" rel=""nofollow noreferrer"">https://regex101.com/r/c9vdia/2/</a></p>

<p>But when I use this in the Splunk search its just throwing my domain</p>

<pre><code>index=""paloalto""| table user | rex field=user ""(?P&lt;user_name&gt;((?:abcd\([A-Za-z0-9-]+|\w+)))"" 
</code></pre>

<p>I am only getting the domain name (abcd) but users without domain looks good.</p>",,0,5,,2019-5-13 15:27:11,,2019-5-13 15:30:22,2019-5-13 15:30:22,,1020526.0,,7548343.0,,1,0,regex|pcre|splunk,83,7
279,258735,56157863,Splunk query based on the results of another query,"<p>A Splunk question...</p>

<p>I've found a few Google hits that I thought were going to help with this.  I'm trying to perform a search for all ""rows"" that are returned by an outer search/query.</p>

<p>I am by no means a Splunk expert, not even a power user!</p>

<p>The outer query performs an LDAP search against Active Directory and returns a list of people with a particular group membership (e.g.: all Domain Admins or Account Operators, Etc.)</p>

<p>I then want to perform a search for each of the returned user names against Windows Event Logs … and return the results as one data set.</p>

<p>I've got the LDAP search nailed.  I've got the Windows Event Log search nailed.  I just need to stitch them together.</p>

<p>If I were coding this in a script, I'd either:</p>

<p>i) Enumerate relevant group members into an array.  Run the event log query for users that exist in the array, e.g.: using semantics such as isin() or contains(); or
ii) Enumerate the group members and perform a foreach() type loop. </p>

<p>So, how the #?!@ do I do this in Splunk.  I've tried using the ""search"" command and ""foreach"" command, but have had no joy.  I even toyed with building a lookup and tried isin(), but could not get this to work.</p>

<p>Example LDAP search:</p>

<pre><code>| ldapsearch domain=""contoso.com"" search=""(&amp;(objectclass=user)(objectCategory=person)(memberOf=CN=Domain Admins,OU=MyContainer,DC=contoso,DC=com))"" attrs=""sAMAccountName"" basedn=""DC=contoso,DC=com"" | eval ldapSearchUserName=""contoso\\""+lower(sAMAccountName)
</code></pre>

<p>Example Event log search:</p>

<pre><code>index=""wineventlog"" source=""WinEventLog:Security"" sourcetype=""WinEventLog:Security"" ""LogName=Security"" ""EventCode=4624"" earliest=-1d | rex field=Message "".*Logon Type:\s+(?&lt;LogonType&gt;\d+)"" | eval UserName=mvindex(Security_ID, 1) | table UserName
</code></pre>

<p>Any thoughts, hints or guidance?</p>

<p>Many thanks</p>

<p>S</p>",56159179.0,1,0,,2019-5-15 21:28:45,,2019-5-16 00:13:13,,,,,468457.0,,1,0,splunk|splunk-query,6991,16
280,258736,56168826,Unable to get results after executing saved search from rest API,"<p>I am trying to get the results of a splunk saved search(report) via REST API But getting error as "" Error in 'savedsearch' command: Unable to find saved search named 'test'.""</p>

<p>curl -s -k -u 'usr:pwd' ""<a href=""https://host:8089/servicesNS/admin/search/search/jobs/export"" rel=""nofollow noreferrer"">https://host:8089/servicesNS/admin/search/search/jobs/export</a>"" -d search="" savedsearch test""</p>

<p>Saved search is owned by me.Can anybody please advise what i am missing here? </p>",,1,0,,2019-5-16 12:41:24,,2019-5-17 06:26:46,,,,,10147018.0,,1,0,splunk,894,11
281,258737,56190876,splunk-connect-kuberentes to splunk light - no data error,"<p>I have configured splunk-connect-kubernetes on my cluster and I have told it to send data to splunk-light where I have configured HEC.</p>

<p>HEC is setup correctly because I can POST information directly. However, in splunk-connect-kubernetes I am seeing this in the logs:</p>

<pre><code>2019-05-17 17:10:47 +0000 [error]: #0 Failed POST to http://splunk.default.svc.licence.local:8088/services/collector, response: {""text"":""No data"",""code"":5}
</code></pre>",56239618.0,1,0,,2019-5-17 17:14:19,,2019-5-21 13:40:31,,,,,3171021.0,,1,0,kubernetes|splunk,196,9
282,258738,56194235,How to send a python requests post for an equivalent curl command?,"<p>Hi</p>

<p>Following curl command works and am trying to post the same (with a different JSON data )using <code>requests.post</code> and running into below error shown,any guidance on what is wrong?</p>

<pre><code>curl -vk ""https://splunk-hec.company.com:8088/services/collector"" -H ""Authorization: {token id }"" -d '{""sourcetype"": ""source"",""index"":""indexname"", ""event"": {""a"": ""value1"", ""b"": [""value1_1"", ""value1_2""]}}'
</code></pre>

<p>PYTHON CODE:-</p>

<pre><code>_raw = {

    ""Total_radar_count"":""999"",
    ""Analyze"":{
        ""Screen"":{""count"":110,""radar_link"":""change://problem/50411162&amp;42639456&amp;44776863&amp;43703933""},
        ""Investigate"":{""count"":065,""radar_link"":""change://problem/50411162&amp;42639456&amp;44776863&amp;43703933""},
        ""Review"":{""count"":106,""radar_link"":""change://problem/50411162&amp;42639456&amp;44776863&amp;43703933""}
    },
    ""timestamp"": int(time.time())  # Can also use datetime.datetime.now().isoformat()
}
url = 'https://splunk-hec.company.com:8088/services/collector?sourcetype=source?index=indexname'
json = _raw
auth_token = 'token id'
head = {'Authorization': auth_token}
response = requests.post(url, json=json, headers=head)
print(response)
print (response.reason)
print(response.json())
</code></pre>

<p>ERROR:-</p>

<pre><code>&lt;Response [400]&gt;
Bad Request
{u'text': u'No data', u'code': 5}
</code></pre>",,3,1,,2019-5-17 22:21:24,,2020-7-11 05:31:17,,,,,6645845.0,,1,0,python|curl|splunk,1275,12
283,258739,56222131,Finding multiple occurrences of a command in the Dashboard source,"<p>I've been trying to work with Regex in trying to find multiple occurrences of all the loadjob commands in the dashboard source code and have been struggling with this. Is this feasible?</p>

<p>Already tried knowing that loadjob savedsearch has ""| loadjob savedsearch=""user:app:jobname""</p>

<pre><code>| rex field=""source"" max_match=0 "".*(?:b: loadjob.*=.*:.*:(?P&lt;jobname&gt;))\|* .*""
</code></pre>

<p>Source example (this is treated as a single line):</p>

<pre><code>&lt;form script=""playground_v2.js"" stylesheet=""playground_v3.css""&gt; 
    &lt;label&gt;1st Line Clone&lt;/label&gt; 
&lt;init&gt; 
    &lt;set token=""PrioColor""&gt;0xfecb00&lt;/set&gt; &lt;set token=""Prio1Color""&gt;0xe60000&lt;/set&gt; 
    &lt;set token=""Prio2Color""&gt;0xa8b400&lt;/set&gt; &lt;set token=""Prio3Color""&gt;0x9c2aa0&lt;/set&gt; 
    &lt;set token=""Prio4Color""&gt;0xeb9700&lt;/set&gt; &lt;set token=""Prio5Color""&gt;0x00b0ca&lt;/set&gt; 
&lt;/init&gt; 
    &lt;earliest&gt;0&lt;/earliest&gt; &lt;/search&gt; 
    &lt;search id=""volume2""&gt; 
        &lt;query&gt; | loadjob savedsearch=""reporting:fs_reporting:1st_Line_Volumes"" | search Domain=$history_domain$ | eval month=if(""$report$""==""raised"",RaisedMonth,ResolvedMonth) | eval conta=if(""$report$""==""raised and resolved"" AND RaisedSameMonth==0,0,count) | where month&amp;gt;=""$start_time$"" AND month&amp;lt;=""$end_time$"" | eval _time=strptime(month.""/01 00:00:00"", ""%Y/%m/%d %H:%M:%S"") | stats sum(conta) as conta by _time, Priority 
        &lt;/query&gt; 
    &lt;/search&gt; 
    &lt;search id=""time2""&gt; 
        &lt;query&gt; | loadjob savedsearch=""reporting:fs_reporting:1st_Line_Times"" | search Domain=$history_domain$ | eval month=if(""$report$""==""raised"",RaisedMonth,ResolvedMonth) | where month&amp;gt;=""$start_time$"" AND month&amp;lt;=""$end_time$"" | eval _time=strptime(month.""/01 00:00:00"", ""%Y/%m/%d %H:%M:%S"") 
        &lt;/query&gt; 
    &lt;/search&gt; 
    &lt;search id=""sla2""&gt; 
        &lt;query&gt; | loadjob savedsearch=""reporting:fs_reporting:1st_Line_SLA"" | search Domain=$history_domain$ | eval month=if(""$report$""==""raised"",RaisedMonth,ResolvedMonth) | where month&amp;gt;=""$start_time$"" AND month&amp;lt;=""$end_time$"" | eval _time=strptime(month.""/01 00:00:00"", ""%Y/%m/%d %H:%M:%S"") 
        &lt;/query&gt; 
    &lt;/search&gt;
&lt;/form&gt;
</code></pre>

<pre><code>&lt;form script=""playground_v2.js"" stylesheet=""playground_v3.css""&gt; &lt;label&gt;1st Line Clone&lt;/label&gt; &lt;init&gt;    &lt;set token=""PrioColor""&gt;0xfecb00&lt;/set&gt; &lt;set token=""Prio1Color""&gt;0xe60000&lt;/set&gt; &lt;set token=""Prio2Color""&gt;0xa8b400&lt;/set&gt; &lt;set token=""Prio3Color""&gt;0x9c2aa0&lt;/set&gt; &lt;set token=""Prio4Color""&gt;0xeb9700&lt;/set&gt; &lt;set token=""Prio5Color""&gt;0x00b0ca&lt;/set&gt; &lt;/init&gt; &lt;earliest&gt;0&lt;/earliest&gt; &lt;/search&gt; &lt;search id=""volume2""&gt; &lt;query&gt; | loadjob savedsearch=""reporting:fs_reporting:1st_Line_Volumes"" | search Domain=$history_domain$ | eval month=if(""$report$""==""raised"",RaisedMonth,ResolvedMonth) | eval conta=if(""$report$""==""raised and resolved"" AND RaisedSameMonth==0,0,count) | where month&amp;gt;=""$start_time$"" AND month&amp;lt;=""$end_time$"" | eval _time=strptime(month.""/01 00:00:00"", ""%Y/%m/%d %H:%M:%S"") | stats sum(conta) as conta by _time, Priority &lt;/query&gt; &lt;/search&gt; &lt;search id=""time2""&gt; &lt;query&gt; | loadjob savedsearch=""reporting:fs_reporting:1st_Line_Times"" | search Domain=$history_domain$ | eval month=if(""$report$""==""raised"",RaisedMonth,ResolvedMonth) | where month&amp;gt;=""$start_time$"" AND month&amp;lt;=""$end_time$"" | eval _time=strptime(month.""/01 00:00:00"", ""%Y/%m/%d %H:%M:%S"") &lt;/query&gt; &lt;/search&gt; &lt;search id=""sla2""&gt; &lt;query&gt; | loadjob savedsearch=""reporting:fs_reporting:1st_Line_SLA"" | search Domain=$history_domain$ | eval month=if(""$report$""==""raised"",RaisedMonth,ResolvedMonth) | where month&amp;gt;=""$start_time$"" AND month&amp;lt;=""$end_time$"" | eval _time=strptime(month.""/01 00:00:00"", ""%Y/%m/%d %H:%M:%S"") &lt;/query&gt; &lt;/search&gt; &lt;/form&gt;
</code></pre>

<pre><code>| rest /servicesNS/-/-/data/ui/views 
| table Type author title eai:acl.app eai:data 
| eval Type=""Dashboards"" 
| rename author as Owner title as Name eai:acl.app as AppName eai:data as source search as source
| rex field=""source"" max_match=0 "".*(?:b: loadjob.*=.*:.*:(?P&lt;jobname&gt;))\|* .*""
</code></pre>

<p>What I wanted is to get All instances in dashboard source of ""loadjob"" and get its job name into field</p>

<p>Currently i'm not getting any results with it</p>",,2,3,,2019-5-20 13:38:24,,2019-5-21 09:46:44,2019-5-20 14:30:23,,1779359.0,,1779359.0,,1,0,regex|rest|regex-group|splunk-query,48,7
284,258740,56260312,"Do you need to extract the time field in Splunk, or is that automatic?","<p>I'm using the UI to insert some log data into Splunk. When I press ""Extract Fields"" in Splunk, I'm able to highlight the time field and enter in a name for that field, like ""timestamp.""</p>

<p>However, I've already specified the timestamp field in the source type. Does this mean I do not have to manually extract the timestamp field per event? In the ""Extract Fields > Select Fields"" UI, can I safely ignore and leave the timestamp field unhighlighted without a name?</p>",56263357.0,1,0,,2019-5-22 15:29:43,,2019-5-22 19:02:48,,,,user499054,,,1,0,splunk,89,7
285,258741,56326952,log4net: udp appender to splunk only logs first character,"<p>I have a simple application which should forward all logs to a splunk server. For this I use log4net with the udp appender.</p>

<p>The problem is only the first character seems logged in splunk:</p>

<p><img src=""https://imgur.com/7BULi1l"" alt=""Splunk"">
Image: <a href=""https://imgur.com/7BULi1l"" rel=""nofollow noreferrer"">https://imgur.com/7BULi1l</a></p>

<p>I traced the request with wireshark:</p>

<pre><code>¬)yElb!
J
ÿ-X,Ñ[date=2019-05-27 15:00:27,489] [level=INFO ] [environment=xxx] [hostname=xxx] [type=web] [logger=SplunkLogger] [message=Working on 634726506325099884]
</code></pre>

<p>There is another tool we use that is using log4net too and this one works without problems. Sadly I have no access to the source. So I think the splunk server is configured correctly.</p>

<p>log4net.conf</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;log4net&gt;

  &lt;root&gt;
    &lt;level value=""INFO"" /&gt;
    &lt;appender-ref ref=""SplunkAppender"" /&gt;
    &lt;appender-ref ref=""FileAppender"" /&gt;
  &lt;/root&gt;

  &lt;logger name=""SecurityLogging""&gt;
    &lt;level value=""ERROR"" /&gt;
  &lt;/logger&gt;



    &lt;appender name=""SplunkAppender"" type=""log4net.Appender.UdpAppender""&gt;
      &lt;threshold value=""INFO"" /&gt;
      &lt;remoteAddress value=""xxxx"" /&gt;
      &lt;remotePort value=""xxxx"" /&gt;
      &lt;layout type=""log4net.Layout.PatternLayout""&gt;
        &lt;conversionPattern value=""[date=%date] [level=%-5level] [environment=xxxx] [hostname=%property{log4net:HostName}] [type=web] [logger=%logger] [message=%message]"" /&gt;
      &lt;/layout&gt;
    &lt;/appender&gt;

    &lt;appender name=""FileAppender"" type=""log4net.Appender.FileAppender""&gt;
      &lt;file value=""log-file.txt"" /&gt;
      &lt;appendToFile value=""true"" /&gt;
      &lt;layout type=""log4net.Layout.PatternLayout""&gt;
        &lt;conversionPattern value=""[date=%date] [level=%-5level] [environment=xxxx] [hostname=%property{log4net:HostName}] [type=web] [logger=%logger] [message=%message]"" /&gt;
      &lt;/layout&gt;
    &lt;/appender&gt;

&lt;/log4net&gt;
</code></pre>",56355737.0,1,1,,2019-5-27 13:22:07,,2019-5-29 08:02:19,,,,,1951424.0,,1,1,.net-core|log4net|splunk,160,11
286,258742,56361993,Azure App Service - cannot store logs in azure storage,"<p>I would like to configure my diagnostic logs to be redirected to Blob or Table Storage. However the only option I see is Filesystem:
<a href=""https://i.stack.imgur.com/FI8EL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FI8EL.png"" alt=""App Service logs""></a></p>

<p>My goal is to collect these logs in splunk</p>",,2,0,,2019-5-29 13:40:06,,2021-11-27 08:32:18,,,,,1116499.0,,1,1,azure|logging|azure-web-app-service|azure-storage|splunk,201,9
287,258743,56367775,Accepting Splunk license agreement using Ansible-Playbook,"<p>I am new to using Ansible-Playbooks, and I am running into issues in accepting license agreements with Splunk.</p>

<p>Any time I have shell run: </p>

<pre><code>""/opt/splunkforwarder/bin/splunk start --accept-license --answer-yes""
</code></pre>

<p>I get a continuous lockup that forces me to terminate the program.</p>

<pre><code>TASK [acceptlicense] ****************************************************************************************************************

^C
</code></pre>

<p>Going into the box and running the command manually I am told the following:</p>

<pre><code>[root@##########-lab_env]# /opt/splunkforwarder/bin/splunk start --accept-license --answer-yes

This appears to be your first time running this version of Splunk.

Create credentials for the administrator account.
Characters do not appear on the screen when you type the password.
Password must contain at least:
   * 8 total printable ASCII character(s).
Please enter a new password:
</code></pre>

<p>I have gone through several forums online that give help in answering what to do when met with specific prompts like this, but anytime I make an adjustment I am told something along the lines of:</p>

<blockquote>
  <p>ERROR! '_______' is not a valid attribute for a Task</p>
</blockquote>

<p>At this point I am pretty stuck and unsure of how to continue.</p>

<p>My code snippet is as follows:</p>

<pre><code>- hosts: ""{{hostName}}""
  become: true
  become_user: root
  become_method: sudo

  tasks: 

    - name: copy_splunk
      shell: cp splunkforwarder-7.1.3-51d9cac7b837-linux-2.6-x86_64.rpm /opt/.; date; ls -l /opt
      args:
        chdir: /tmp
      register: run_ll

    - debug: var=run_ll.stdout_lines

    - name: install rpm package
      shell: rpm -ivh splunkforwarder-7.1.3-51d9cac7b837-linux-2.6-x86_64.rpm
      args:
        chdir: /tmp
      ignore_errors: True
      register: install_rpm

    - debug: var=install_rpm.stdout_lines

    - name: acceptlicense
      tags:
        - install
      shell: /opt/splunkforwarder/bin/splunk start --accept-license --answer-yes
      register: accept_l

    - debug: var=accept_l.stdout_lines
</code></pre>

<p>I have only done a couple of playbooks in the past, so this error is something new to me.</p>

<p>Does anyone have any insight?</p>",56369792.0,3,0,,2019-5-29 20:06:44,,2021-4-16 01:33:51,2019-5-30 03:18:22,,2370483.0,,11574891.0,,1,1,ansible|splunk,1925,15
288,258744,56371025,Getting XML value from attribute dispatchState with python and request,"<p>I'm doing a request to splunk for jobs using:</p>

<pre><code> response = requests.post('https://xxxx.net:8089/services/search/jobs/' + job_id, auth=(splunk_user, splunk_pass), verify=False)
</code></pre>

<p>Now i look at the output using <code>print(response.text)</code>. I want to be able to print just the value of <code>&lt;s:key name=""dispatchState""&gt;DONE&lt;/s:key&gt;</code> and see if it's ""DONE"" or whatever the value is.</p>",,1,1,,2019-5-30 02:49:00,,2019-5-30 04:52:18,2019-5-30 04:22:47,,9070080.0,,11575905.0,,1,0,python|python-3.x|splunk|splunk-sdk,46,6
289,258745,56427192,Example logging to Splunk w/ React 16+,"<p><strong>I am looking for examples showing how to write log messages to Splunk from a React JS 16+ application.</strong>  Splunk has a TypeScript snippet in their docs, but I am interested in seeing how others have tied it into the React framework so everything is configured and initialized properly for use in the application.</p>

<p>Note: I've considered using log4js with a custom appender for Splunk but haven't found any examples for that either!</p>",,0,0,,2019-6-3 12:15:42,,2019-6-3 12:15:42,,,,,173281.0,,1,3,reactjs|splunk|splunk-sdk,306,9
290,258746,56451608,Splunk Conditional Dedup and Other FIlters,"<p>Is it possible to conditionally apply a filter, for example <code>dedup</code> based on an <code>if</code> statement or a <code>where</code></p>

<p>For example:</p>

<pre><code>(initial search)
| dedup field(s) | where var1=true
| table field(s)
</code></pre>

<p>Or:</p>

<pre><code>(initial search)
| if(var1=true, dedup field(s))
| table field(s)```
</code></pre>",,1,0,,2019-6-4 21:30:16,,2019-6-6 22:19:11,,,,,8216464.0,,1,0,splunk|splunk-query,593,11
291,258747,56458027,regex operator in Splunk is not working to match results,"<p>I am writing a code to simply match a regex in my search to match index field which matches app1_<em>, app2_</em>, etc</p>

<p>However my search below works</p>

<pre><code>| eventcount summarize=false index=app1_*| dedup index 
</code></pre>

<p>But when I use it like below it doesn't, it would be required to be done using regex since I would like to use an ""OR"" in the regex:</p>

<pre><code>| eventcount summarize=false |regex index=""app1_*""| dedup index
</code></pre>",,1,0,,2019-6-5 09:44:27,,2019-6-5 10:42:51,2019-6-5 09:45:32,,3832970.0,,5609139.0,,1,0,regex|splunk,340,11
292,258748,56473093,Divide two timecharts in Splunk,"<p>I want to divide two timecharts (ideally to look also like a timechart, but something else that emphasizes the trend is also good).</p>

<p>I have two types of URLs and I can generate timecharts for them like this:</p>

<pre><code>index=my-index sourcetype=access | regex _raw=""GET\s/x/\w+"" | timechart count

index=my-index sourcetype=access | regex _raw=""/x/\w+/.*/\d+.*\s+HTTP"" | timechart count
</code></pre>

<p>The purpose is to emphasize that the relative number of URLs of the second type is increasing and the relative number of URLs of the first type is decreasing.</p>

<p>This is why I want to divide them (ideally the second one by the first one).</p>

<p>For example, if the first series generates <code>2, 4, 8, 4</code> and the second one generates <code>4, 9, 20, 12</code> I want to have only one dashboard showing somehow the result <code>2, 2.25, 2.5, 3</code>.</p>

<p>I just managed to get together those information by doing this, but not to generate a timechart and not to divide them:</p>

<pre><code>index=my-index sourcetype=access 
| eval type = if(match(_raw, ""GET\s/x/\w+""), ""new"", if(match(_raw, ""/x/\w+/.*/\d+.*\s+HTTP""), ""old"", ""other"")) 
| table type 
| search type != ""other"" 
| stats count as ""Calls"" by type
</code></pre>

<p>I also tried some approaches using <code>eval</code>, but none of them work.</p>",56486256.0,1,0,,2019-6-6 07:50:16,1.0,2019-6-7 15:30:30,2019-6-7 15:30:30,,3885376.0,,3885376.0,,1,0,splunk|splunk-query,326,11
293,258749,56507392,Is there a way to access Splunk using SQL instead of SPL?,"<p>I've been having a very hard time using SPL to query data in Splunk... I wish to replace all of that with some simple SQL, Is that possible ? If so how ? </p>

<p>I don't want to pay a lot just to get Splunk training... would rather use my SQL skills :) </p>

<p>Hope you all agree and can help me find a solution !</p>

<p>Cheers!</p>",,2,0,,2019-6-8 14:44:05,2.0,2020-7-15 04:25:27,,,,,11607719.0,,1,1,sql|sql-server|splunk|splunk-query,717,12
294,258750,56508749,Tableau on Splunk.. Possible?,"<p>My BI team would like to access my Splunk data using Tableau, we don't want to use Splunk visualizations and would rather use our BI tools... </p>

<p>We tried to use Splunk connectors but that failed and the volume of data is too big to show on tableau... is there any solution for this problem ? how can that be achieved and what are your recommnendations on this subject?</p>

<p>Thank you.</p>",,1,0,,2019-6-8 17:39:46,,2019-6-12 11:04:28,,,,,11607719.0,,1,0,business-intelligence|splunk|tableau-api,53,6
295,258751,56554581,How to display python output in splunk?,"<p>I am writing a code in python which will run sql select query and return the result. How do I display the output from python script in Splunk?</p>

<p>Currently I just have a python script running sql query and have tried importing <code>import splunklib.client as client</code>which fails as <code>[pylint] E0401:Unable to import 'splunklib.client'</code> </p>

<p>Tried this:</p>

<pre><code>import mysql.connector
import splunklib.client as client

#splunk credentials
HOST = ""localhost""
PORT = 8089
USERNAME = ""admin""
PASSWORD = ""yourpassword""

# Connect to splunk and log in 
service = client.connect(
    host=HOST,
    port=PORT,
    username=USERNAME,
    password=PASSWORD)
</code></pre>

<p>But it gives an error while importing library </p>

<p>I expect the output of python script which will be something like :</p>

<pre><code>   STAGED = 1
   FAILED = 2
   VALIDATED =1
</code></pre>

<p>to be displayed in SPLUNK using python script itself.</p>",,1,0,,2019-6-12 04:14:10,,2019-6-12 11:01:27,,,,,9305541.0,,1,0,python|splunk,269,9
296,258752,56561425,Logs with stack traces,"<p>I work on service which is based on spring boot 2 , for tracing logs I use spring sleuth and splunk for collecting logs itself. 
But if there are stack traces in logs it looks weird and stack traces are not marked with tracedID. </p>

<p>I have 2 questions:
1. Should I exclude stack traces from production logs at all ?
2. Or how can I marked stack traces with traceID?</p>",,1,2,,2019-6-12 11:43:02,,2019-6-12 11:48:30,,,,,9299052.0,,1,0,spring-boot|logging|splunk|spring-cloud-sleuth,215,9
297,258753,56626799,How to use/do where in column of a lookup in Splunk Search Query,"<blockquote>
  <p>I want the search with a field which match with any of the values in
  look up table.</p>
</blockquote>

<p>For now, I have used below where in query. But, I still want to query with <strong>Look up table</strong> instead of manually putting all those values in double quotes using the <strong>in</strong> clause.</p>

<pre><code>|where in(search,""abcd"",""bcda"",""efsg"",""zyca"");
</code></pre>",56708244.0,1,0,,2019-6-17 07:41:16,1.0,2019-6-21 17:46:47,2019-6-17 21:17:37,,5371862.0,,5371862.0,,1,4,where|lookup|splunk|splunk-query|in-operator,586,12
298,258754,56634167,How can I get a panel in Splunk to show the results of an external API call?,"<p>Specifically, I have an API that returns some JSON. What I want to do is have the user go on the dashboard, type the input (in this case an IP address) and have the results of the external API call(for example a number pulled from a field in the response) be displayed in the panel. Is this doable? If so how? Bonus points for doing it without needing access to the backend.</p>",,1,0,,2019-6-17 15:13:20,,2019-6-20 08:15:44,,,,,5748353.0,,1,0,splunk,219,9
299,258755,56637937,cURL post to Splunk HEC on Jenkins,"<p>I'm sending a post request with cURL to a Splunk HTTP Event Collector (HEC) in Jenkins.  </p>

<p>When I log into the Jenkins server and execute the curl command directly on the command line, it works.  However, when constructing the command with Java and sending it through Jenkins, it's not logging anything.</p>

<pre class=""lang-java prettyprint-override""><code>new ProcessBuilder().command(""curl"", ""-k"", ""-X"", ""POST"", environment.getVar(""SPLUNK_URL""),
            environment.getVar(""SPLUNK_CHANNEL""), ""-H"", ""'Authorization:Splunk"", environment.getVar(""SPLUNK_TOKEN""), ""'"",
                ""-H"", ""'Cache-Control:no-cache'"", ""-d"",""'{\""sourcetype\"":\""json\"",\""event\"":"", toSimpleJSON(output),
                ""}'"").start().waitFor(5, TimeUnit.SECONDS);
</code></pre>

<p>I expect that the command which works on the Jenkins host would also work with Java's ProcessBuilder.</p>",,1,0,,2019-6-17 19:44:38,,2019-6-23 23:51:43,,,,,11545802.0,,1,0,java|http|jenkins|curl|splunk,124,8
300,258756,56657861,Need a count for a field from different timezones (have multiple fields from .csv uploaded file),"<p>I am little confused, as i have some events ingesting from .csv file in splunk from different different timezones china, pacific, eastern, europe etc...
I have fields like start time, end time, TimeZone, TimeZoneID, sitename, conferenceID &amp; hostname.....etc</p>

<p>for your info(conferenceID=131146947830496273, 130855971227450408......)
was wondering if i have to do a "".......|stats count of conferenceID"" for particular time interval(ex., 12:00 pm to 15:00 pm today ) by sitting on pacific timezone, using the start time and end time from the events search should collect all events sorting from there originating timezones time interval but not the taking splunk timezone time interval.</p>

<p>below are some samples of logs which I have</p>

<blockquote>
  <p>testincsso,130878564690050083, Shona,""GMT-07:00, Pacific (San Francisco)"",4,06/17/2019 09:33:17,06/17/2019 09:42:23,10,0,0,0,0,0,0,9,0,0,1,0,0,1,1
  host = usloghost1.example.com sourcetype =webex_cdr
  6/17/19
  12:29:03.060 AM </p>
  
  <p>testincsso,129392485072911500,Meng,""GMT+08:00, China (Beijing)"",45,06/17/2019 07:29:03,06/17/2019 07:59:22,31,0,0,0,0,0,0,0,0,30,1,1,0,0,1
  host = usloghost1.corp.example.com sourcetype = webex_cdr
  6/17/19
  12:19:11.060 AM </p>
  
  <p>testincsso,131121310031953680,sarah ward,""GMT-07:00, Pacific (San Francisco)"",4,06/17/2019 07:19:11,06/17/2019 07:52:54,34,0,0,0,0,0,0,0,0,34,3,3,0,0,2
  host = usloghost1.corp.example.com sourcetype = webex_cdr
  6/17/19
  12:00:53.060 AM </p>
  
  <p>testincsso,130878909569842780,Patrick Janesch,""GMT+02:00, Europe (Amsterdam)"",22,06/17/2019 07:00:53,06/17/2019 07:04:50,4,0,0,0,0,0,0,4,0,2,3,2,0,1,2
  host = usloghost1.corp.example.com sourcetype = webex_cdr</p>
</blockquote>

<p>update:</p>

<p>there is 2 fields in the events start time and end time for every conference it held in there local timezone(event originating TZ).
also _time refers the splunk time which I don't need in this case. what I need is there is date_hour, date_minutes, date_seconds...etc which shows events local timezone time(china, europe, asia...etc).
so when i sit here pacific TZ and try searching for 
index=test ""testincsso"" | stats count(conferenceID) by _time 
taking timeinterval last 4 hours then the output should display the count of Cenferences by taking the count from all events by comparing with there local TZ's time for last 4 hours.
so do I need to use ""| eval hour = strftime(_time,""%H"")"" or ""| eval mytime=_time | convert timeformat=""%H ctime(mytime)"" before stats.
thanks</p>

<p>-also changing timepicker default behavior may give correct results. 
I have events with fields ""start time"" and ""end time"" from different TZ. so when I try to search events ex., date range ""06-16-2019"" using time-picker I should get all events by seeing the field ""start time"" in events not the ""_time"" of Splunk.
I want change my splunk time picker default behavior and gives output by sieng events fields(ex., ""start time"" &amp; ""end time"". below the query I changed in source xml.</p>

<p>index=test sourcetype=webex] ""testinc"" | eval earliest = $toearliest$ | eval latest=if($tolatest$ &lt; 0, now(),$tolatest$) | eval datefield = strptime($Time$, ""%m/%d/%Y %H:%M:%S"")|stats count(Conference)</p>",,1,3,,2019-6-18 22:33:07,,2019-6-19 22:54:58,2019-6-19 22:54:58,,11575386.0,,11575386.0,,1,0,splunk|timestamp-with-timezone,96,7
301,258757,56740222,extract the last 2 fields regardless of size,"<p>I have been trying to get the last ""two fields"" of the following strings:</p>

<pre><code>cc-api-data.bar.bar
external-atl3-1.xx.fbcdn.net
fbcdn.net
</code></pre>

<p>for the first 2 strings, I would like to only get the ""bar.bar"" and ""fbcdn.net."" However, for the last string, I want it to match the whole thing since it has all i want.</p>

<p>I am pretty confident i could do this in a simple script but I am trying to use regex in this case. I can only get the last part of the string on the last string but not the whole thing. And I cannot tell the regex which field to take.
I literally just want the last two fields, no matter how many delimiters there are.</p>

<p>Any suggestions or is it even possible</p>",56740304.0,1,2,,2019-6-24 15:59:37,1.0,2019-6-24 17:48:09,,,,,11693245.0,,1,0,regex|splunk,37,7
302,258758,56746057,Sending TCP packets to Fluentd,"<p>In my Mac OS, With <code>fluentd</code> I'm trying to read tcp events and write it to local directory. </p>

<p>Below is the <code>td-agent.conf</code> that I have created to open TCP port and writing to local. </p>

<pre class=""lang-xml prettyprint-override""><code>  &lt;!-- td-agent.conf--&gt;
  &lt;source&gt;
    @type tcp
    @log_level ""trace""
    tag ""tcp.events""
    port 2201
    bind ""0.0.0.0""
    delimiter ""\\n""
    &lt;parse&gt;
      @type ""regexp""
      expression ""/^(?&lt;field1&gt;\\d+):(?&lt;field2&gt;\\w+)$/""
    &lt;/parse&gt;
  &lt;/source&gt;
  &lt;match tcp.events&gt;
    @type file
    path ""/Users/logs/outputlog""
    &lt;buffer time&gt;
      path ""/Users/logs/outputlog""
    &lt;/buffer&gt;
  &lt;/match&gt;
</code></pre>

<p>To test, I was sending the tcp packages to the port (<code>2201</code>) using tools like <code>telnet</code> and <code>netcat</code>. But the terminal don't return after connecting to the ports. It stays there with out any response. </p>

<p>Checked the verbose of telnet / netcat. No luck.</p>

<p>I expect <code>TCP</code> to connect and get the data logged in <code>fluentd</code> logs. But connection is getting established to the port but data is not written into log or the control of tcp connection is not returned back to terminal.</p>",,1,1,,2019-6-25 02:28:48,,2020-4-22 18:14:45,2019-6-25 02:34:25,,6598041.0,,6598041.0,,1,0,tcp|splunk|fluentd,635,11
303,258759,56798317,Filtering duplicate entries from Splunk events,"<p><strong>I am new to splunk and have got some splunk events as below</strong></p>

<p><code>2019-06-26 23:45:36 INFO ID 123456 | Response Code 404
2019-06-26 23:55:36 INFO ID 123456 | Response Code 404
2019-06-26 23:23:36 INFO ID 258080 | Response Code 404</code></p>

<p>Is there way to filter out the first two events as they have the same ID <code>123456</code> and view them as one event?
I tried something which I know is completely wrong, suggestions might be very useful on this.</p>

<p><code>index=myindex ""Response Code 404""  | rex field=ID max_match=2 ""(?&lt;MyID&gt;\b(?:123456)\b)"" | stats count by ID MyID | where count &gt; 1</code></p>",56799835.0,2,0,,2019-6-27 20:51:29,,2020-3-4 15:10:16,,,,,11015749.0,,1,0,splunk|splunk-query,1889,14
304,258760,56815269,Unable to connect to Splunk Enterprise on Azure,"<p>I have deployed Splunk Enterprise app on Azure and it has given me a DNS to connect to Splunk as http://{domainname}.southeastasia.cloudapp.azure.com but when i am using this to connect, i am receiving ""This site can’t be reached"" error. Please help !
Thanks in advance</p>

<p>U</p>",,1,0,,2019-6-29 05:26:44,,2019-7-1 09:16:17,,,,,10348230.0,,1,-1,azure|splunk,38,6
305,258761,56844187,Splunk How can I do this in python SSL,"<p>I need to your help. When I am using python to <code>Splunk Connection</code>, I saw this error. I'm using python 2.7</p>

<p><code>SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:618)</code></p>

<p>So, I saw that reference pages like this 
<a href=""https://docs.splunk.com/DocumentationStatic/PythonSDK/1.6.5/client.html#splunklib.client.Indexes"" rel=""nofollow noreferrer"">https://docs.splunk.com/DocumentationStatic/PythonSDK/1.6.5/client.html#splunklib.client.Indexes</a></p>

<p>But I still can't resolve the above error.</p>

<p>When verifying connection, it returns <code>False</code>.</p>

<p>Do I need to add more into the settings?</p>

<p>Thank you.</p>

<pre><code> service = client.connect(
        host=host,
        port=port,
        username=username,
        password=password,
        verify=False
    )   
</code></pre>",,0,0,,2019-7-2 01:22:30,,2019-7-2 03:08:14,2019-7-2 03:08:14,,7928883.0,,6936440.0,,1,1,python|ssl|connection|certificate|splunk,259,9
306,258762,56849742,How to Freeze table header in Splunk,"<p>I want to Freeze the table header in Splunk and also want to add vertical scrolling in the table. Let me know if there is a way to achieve this in Splunk.</p>

<p>TIA !!</p>",,1,2,,2019-7-2 09:50:39,,2020-8-2 17:08:36,2019-7-2 11:58:46,,7751604.0,,7751604.0,,1,0,splunk,303,9
307,258763,56856845,How do i optimize the following Splunk query?,"<p>I have results like below:  </p>

<pre><code> 1. DateTime=2019-07-02T16:17:20,913 Thread=[],  Message=[Message(userId=124, timestamp=2019-07-02T16:17:10.859Z, notificationType=CREATE, userAccount=UserAccount(firstName=S, lastName=K, emailAddress=abc@xyz.com, status=ACTIVE), originalValues=OriginalValue(emailAddress=null)) Toggle : true]


 2. DateTime=2019-07-02T16:18:20,913 Thread=[],  Message=[Message(userId=124, timestamp=2019-07-02T16:17:10.859Z, notificationType=CREATE, userAccount=UserAccount(firstName=S, lastName=K, emailAddress=abc@xyz.com, status=ACTIVE), originalValues=OriginalValue(emailAddress=new@xyz.com)) Toggle : true]


 3. DateTime=2019-07-02T16:19:20,913 Thread=[],  Message=[Message(userId=124, timestamp=2019-07-02T16:17:10.859Z, notificationType=CREATE, userAccount=UserAccount(firstName=S, lastName=K, emailAddress=abc@xyz.com, status=ACTIVE), originalValues=OriginalValue(emailAddress=new@xyz.com)) Toggle : true]
</code></pre>

<p>And I am trying to group results where the contents of the entire <code>""Message""</code> field is same and <code>""emailAddress=null""</code> is not contained in the Message.</p>

<pre><code>So in the results above 2 and 3 should be the output.
</code></pre>

<p>The following query works fine for me but I need to optimize it further according to the following conditions:</p>

<p><strong>Working Query</strong>: <code>index=app sourcetype=appname host=appname* splunk_server_group=us-east-2 | fields Message | search Message= ""[Message*"" | regex _raw!=""emailAddress=null"" |  stats count(Message) as count by Message | where count &gt; 1</code></p>

<p><strong>Conditions to optimize</strong></p>

<blockquote>
  <ul>
  <li>Cannot rex against raw</li>
  <li>Message key/value pair needs to be in the main search, not a sub-search</li>
  </ul>
</blockquote>",,1,0,,2019-7-2 16:39:14,,2019-7-2 17:48:28,,,,,5755464.0,,1,0,splunk|splunk-query|splunk-sdk|splunk-formula,83,7
308,258764,56858050,Can you combine multivalue fields to form a consolidated Splunk alert?,"<p>I have a Splunk search which returns several logs of the same exception, one for each ID number (from a batch process). I have no problem extracting the field from the log with reg-ex and can build a single alert for each ID number easily.</p>

<pre><code>Slack Message: ""Reference number $result.extractedField$ has failed processing.""
</code></pre>

<p>Since the error happens in batches, sending out an alert for every reference ID that failed would clutter up my Slack channel very quickly. Is it possible to collect all of the extracted fields and set the alert to send only one message? Like this...</p>

<pre><code>Slack Message: ""Reference numbers $result.listOfExtractedFields$ have failed to process."" 
</code></pre>",56934496.0,1,2,,2019-7-2 18:13:05,,2019-7-8 16:23:32,2019-7-8 16:23:32,,7733523.0,,7733523.0,,1,0,splunk,190,10
309,258765,56868303,Splunk query to list out by stats from json kind of log,"<p>I have a splunk log where the log will be in JSON format or as raw data. Need to write a splunk query using stats command.</p>

<p>index=* application_name=abc type=imp | stats count by status</p>

<p>Tried with 'stats count by status' command, but noyhing worked. Also tried as 'stats count by message_text:data' , 'stats count by message_text:data:status'</p>

<p>Log as listed as below,</p>

<p>{""application_name"":""abc"",""type"":""imp""},""box"":""dev"",""message_text"":""{\""data\"":{\""error\"":""invalid"",\""status\"":""200""}}</p>

<p>Need to get the count by status and type</p>",,1,3,,2019-7-3 10:42:17,,2019-7-4 07:39:30,2019-7-3 13:34:30,,11232481.0,,11232481.0,,1,0,splunk|splunk-query,1365,12
310,258766,56873687,Splunk: Extracting values for table,"<p>I have events in my logs that look like </p>

<pre><code>{
     linesPerSec:    1694.67    
     message:    Status:    
     rowCount:   35600000   
     severity:   info
}   
</code></pre>

<p>when i make a search like:</p>

<pre><code>index=""apps""  app=""my-api"" message=""*Status:*"" | table  _time,  linesPerSec, rowCount
</code></pre>

<p>This is what my table ends up looking like
<a href=""https://i.stack.imgur.com/9cR4W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9cR4W.png"" alt=""This is what my table ends up looking like""></a></p>

<p>How do I get the number value away from the key for both linesPerSec and rowCount? I want to see all instances. I tried using values(linesPerSec) but that seemed to aggregate only unique.</p>

<p>Thanks,</p>

<p>Nate</p>",56911453.0,2,0,,2019-7-3 15:38:47,,2019-7-6 05:22:14,,,,,2008240.0,,1,0,search|splunk|splunk-query,132,9
311,258767,56931845,Splunk integration with Hadoop,<p>How to integrate splunk with Hadoop (MapR distribution)?</p>,,1,0,,2019-7-8 09:21:07,,2019-7-8 16:09:09,2019-7-8 16:09:09,,4420967.0,,11695368.0,,1,0,integration|splunk|mapr,37,6
312,258768,56956840,Need shell script to trigger .py(python script) which is in remote machine,"<p>I'm having total 2 servers:</p>

<ul>
<li><strong>Server1</strong> -> Splunk server(Splunk Monitoring tool)</li>
<li><strong>Server2</strong> -> Server that has python scripts.</li>
</ul>

<p>In <code>Server1</code> if any alter triggers there is an option to run a script. So when alert triggers I need a shell script which can login into <code>Server2</code>(which has python code located in C:\abc\pythonscripts\xyz.py) and execute it. </p>

<p>I don't have any coding knowledge and I need help in getting shell query which can trigger python script which is located in different server.</p>",,0,2,,2019-7-9 16:20:25,,2019-7-9 16:53:20,2019-7-9 16:53:20,,9978223.0,,11760388.0,,1,0,python|shell|splunk,45,6
313,258769,56961149,How to make a graph of rex values in Splunk where all data points are shown,"<p>In my logs I am printing out <code>""total_time:1.2334""</code> (for example). 
I am able to grab the time values with Splunk by using rex: <code>| rex ""total_time:(?&lt;time&gt;.*)"" |</code></p>

<p>I want to display each total_time on a time chart (as either a line or a bar graph). However, when I try something like
<code>| timechart values(time)</code> it only shows a few of my data points (for example, only 3 columns are shown in the chart while there are 16 events).</p>

<p>Does anyone how I can display total_time best with the time on the Y axis and the occurrences on the X axis? Thanks. </p>",,1,0,,2019-7-9 22:17:10,,2019-7-10 11:35:18,,,,,3448228.0,,1,0,splunk|splunk-query,114,8
314,258770,56964961,"Many Case Regex1 Extract Values, How Can I?","<p>I need some your help.</p>

<p>I using Regex1 about data extract.
However many cases how can I do that?</p>

<p>Example</p>

<pre><code>hOme = 0.00
AbC= 0.50
KEN123 =0.80
4  =  1.00
5=200
</code></pre>

<p>Can you advice to me?</p>

<p>Just I extract using</p>

<pre><code>hOme\s\=\s(?&lt;Home&gt;[\d\.]+) ....
</code></pre>

<p>Thanks a lot..</p>",,1,0,,2019-7-10 06:56:18,,2019-7-10 09:12:42,2019-7-10 09:12:42,,372239.0,,6936440.0,,1,0,regex|splunk,20,5
315,258771,56966804,Splunk Enterprise pie-chart with count from different search criteria,"<p>I'm trying to create a pie chart where i'v 2 search result sets from different condition and different source. But i'm not able to join the result set into one pie chart.</p>

<pre><code>index=A sourcetype=B host=C | rex ""pattern1"" | chart count(field1) AS result1
index=A sourcetype=B host=C | rex ""pattern2"" | chart count(field2) AS result2
index=A sourcetype=B host=D | rex ""pattern3"" | chart count(field3) AS result3;
</code></pre>

<p>I'm able to get data for pattern1 and pattern2 as they have same index/sourcetype/host but cant join data from 3rd one.</p>

<pre><code>PieChart should represent resullt1, result2, others(resul3 - result1 - result2) out of result3
</code></pre>",,1,0,,2019-7-10 08:44:37,,2019-7-31 02:55:30,,,,,8490352.0,,1,0,pie-chart|splunk,206,9
316,258772,56978459,Is there a way to sort the output of the top command lexicographically within the count strata,"<p>I'm writing a Splunk query to retrieve error codes that feature a specific identifier token, then using the top command to figure sort them by occurrence. But now, I'm trying to put the output of the top command in lexicographic order.</p>

<p>I've tried reading through some of the documentation for Splunk's SPL, and I wasn't able to find a command or option that allows me to do this.</p>

<pre><code>message=SplunkLogging::* | top limit=0 userQuery
</code></pre>

<p>For instance if you have the following counts: A - 2, B - 5, C - 1, D - 2, I would want the results to be ordered as such: B - 5, A - 2, D - 2, C - 1.</p>",56980255.0,1,0,,2019-7-10 20:51:29,,2019-7-12 06:46:24,,,,,7901933.0,,1,0,splunk|splunk-query|splunk-formula,157,9
317,258773,56998003,How to make variables that are linked by a common correlation ID show on the same row of a Splunk table?,"<p>I am logging the time it takes to make external service calls from various clients within my application. These come as various events in Splunk that I search for and then extract the time the call took. For a single execution of the application several clients will be used and there is a unique correlation ID that links them all together.</p>

<p>Imagine an event in Splunk might look like this:</p>

<pre><code>&lt;RandomStuff, client1 time1: 3.2 , Random Stuff, correlation id: 250&gt;
</code></pre>

<p>and then another event would be:</p>

<pre><code>&lt;RandomStuff, client2 time2: 2.7 , Random Stuff, correlation id: 250&gt;
</code></pre>

<p>So, the client name (client2 time2 for example) will be different but the correlation ID is the same for a particular execution. </p>

<p>I do a Splunk search for time1, time2, timeN and then extract the time and correlation ID:</p>

<pre><code>&lt;my search&gt; 
| rex ""time1: (?&lt;t1&gt;.*)"" 
| rex ""time2: (?&lt;t2&gt;.*)"" 
| rex ""time3: (?&lt;t3&gt;.*)"" 
| rex ""correlation_id: (?&lt;corId&gt;.*)
</code></pre>

<p>This grabs all the relevant events and extracts the times (if they exist -- because one Splunk event will only have one of the three clients in it, there are three different events for time1, time2, and time3).</p>

<p>I then add <code>| table t1 t2 t3 corId</code> to the end of the search and I get a table that looks like this:</p>

<p><a href=""https://i.stack.imgur.com/w2h3o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w2h3o.png"" alt=""table1""></a></p>

<p>What I want, however, is something that looks like this:</p>

<p><a href=""https://i.stack.imgur.com/v8RZQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v8RZQ.png"" alt=""enter image description here""></a></p>

<p>Anyone have any ideas how to join t1, t2, and t3 together in the same row with the matching correlation ID?</p>

<p>I'm not very good with Splunk yet, so I think what might be happening is that when I do <code>rex time1</code> but it is the event with <code>rex time2</code> it is setting time1 as empty/null for that for that particular event. So it's actually displaying what I am telling it to. But since these are different event logs that I am trying to combine into one row with the linking correlation ID I'm not sure how to do that.</p>",,1,0,,2019-7-11 22:20:25,,2019-7-12 23:22:14,,,,,3448228.0,,1,0,splunk|splunk-query,88,7
318,258774,57001234,PyCharm doesn't Splunk SDK (splunklib),"<p>I have a simple python script that calls the Splunk API. For this, I include splunklib:
""import splunklib.client as client""</p>

<p>This script works very well, when I start it from a terminal window (MacOS): python3 main.py</p>

<p>For this, I had to install splunklib using ""pip3 install splunklib"".</p>

<p>Unfortunately I can't run the same script out of Pycharm.
I also successfully installed splunklib under settings/Project Interpreter/Package. splunklib is shown in the list of installed packages for the Project Interpreter (splunklib 1.0.0, Python 3.7).</p>

<p>When I run the script using Pycharm, I get the following error message:</p>

<p>import splunklib.client as client
ModuleNotFoundError: No module named 'splunklib.client'; 'splunklib' is not a package</p>

<p>Why can Pycharm not find Esplunklib, even though it's installed in the Project Interpreter?</p>",59293259.0,1,0,,2019-7-12 06:22:40,,2019-12-11 20:05:15,,,,,11648396.0,,1,2,python-3.x|macos|pycharm|splunk-sdk,1817,16
319,258775,57002074,Extract field embedded in log data,"<p>I want to extract a field from a string in Splunk. Here's an example piece of data, from which I want to extract the <code>vin</code> field.</p>

<pre><code>  {""timestamp"":""2147483647"",""time"":""2019-07-12T07:12:30Z"",""source_type"":""APP/PROC/WEB"",""source_instance"":""3"",""origin"":""rep"",""msg"":""2019-07-12 07:12:30.840  INFO 15 --- [ XNIO-2 task-95] f.c.g.m.c.m.r.r.GetCurrentLiteController : {\""transaction_summary\"":{\""vin\"":\""3FA6P0LU8JR126702\"",\""service\"":\""moduleinfo\"",\""api_call\"":\""getcurrentlite\"",\""requesting_system\"":\""CVFMA\"",\""start_time\"":\""1562915550829\"",\""end_time\"":\""1562915550840\"",\""response_time\"":\""11\"",\""http_response_code\"":\""200\"",\""app_status_code\"":\""200\"",\""trace_id\"":\""62b2e776-fd02-44c1-8f49-01930fc667db\"",\""userid\"":\""GVMS\"",\""x_b3_traceid\"":null,\""x_b3_spanid\"":null,\""x_span_export\"":\""true\""}}"",""message_type"":""OUT"",""level"":""info"",""job_index"":""0bfbe359-fe76-43e0-9a19-cea5dfd80856"",""job"":""diego_cell"",""ip"":""10.68.80.94"",""event_type"":""LogMessage"",""cf_space_name"":""Ford-GVMS_ECC_PROD"",""cf_org_name"":""Ford-GVMS_FMCC_PROD_ECC_Prod"",""cf_app_name"":""gvms-moduleinfo-api""}
</code></pre>

<p>What is the right way to do that?</p>",,1,3,,2019-7-12 07:25:28,,2019-7-16 21:46:56,2019-7-16 21:38:58,,182811.0,,2982258.0,,1,-1,splunk|splunk-formula,54,6
320,258776,57009442,Extracting values from json in Splunk using spath,"<p>I am trying following query</p>

<p><code>| makeresults | eval _raw=""{\""records\"":[{\""Name\"":\""name\""},{\""Name\"":\""worst_food\"",\""Value\"":\""salad\""},{\""Name\"":\""ex-wife\"",\""Value\"":\""Tammy\""}]}"" | spath
</code></p>

<p>this returns table as like below in Splunk.</p>

<p><code>records{}.name   records().value
 name             salad
 worst_food       Tammy
 ex-wife</code></p>

<p>But i am expecting value as like </p>

<p><code>records{}.name   records().value
 name
 worst_food       salad
 ex-wife          Tammy</code></p>

<p>Anyone experienced this issue? could you please share some knowledge that how to derive expected result. </p>",57038674.0,1,0,,2019-7-12 14:53:04,,2019-7-15 11:22:21,,,,,2395041.0,,1,0,json|splunk|splunk-query,4405,15
321,258777,57012268,json parsing using spath,"<p>I have a json log as shown below </p>

<pre><code>{
action: Get, 
applicationName: abc,
controller: Main, 
ip: 123.123.123.123, 
logLevel: INFO, 
loggerType: abcdef, 
machineName: windows, 
message: {""Value"":{""Data"":{""Items"":[{""FieldType"":""abc"",""Value"":""""},{""FieldType"":""abcd"",""Value"":""""},{""FieldType"":""123"",""Value"":""""}],""EncryptedDocKey"":""123456"",""Domain"":""Order"",""Partner"":""India"",""Carrier"":""Idea""},""RequestTrackerId"":""7894561230"",""Message"":""OK""},""Formatters"":[],""ContentTypes"":[],""DeclaredType"":null,""StatusCode"":null} 
principalId: 22222222-2222-2222-2222-222222222222 
requestMethod: POST 
requestUrl: https://abc123.com/api/v1/get 
responseData: {""Value"":{""Data"":{""Items"":[{""FieldType"":""abc"",""Value"":""""},{""FieldType"":""123"",""Value"":""""},{""FieldType"":""xyz"",""Value"":""""}],""EncryptedDocKey"":""123456789"",""Domain"":""Order"",""Partner"":""india"",""Carrier"":""idea""},""RequestTrackerId"":""7894561230"",""Message"":""OK""},""Formatters"":[],""ContentTypes"":[],""DeclaredType"":null,""StatusCode"":null} 
time: 2019-07-10 18:35:23.3893, 
traceId: 12345678963525, 
userName: abc/12345 
}
</code></pre>

<p>All the fields are indexed correctly. I am looking to extract json data in message element. I would like to extract FieldType,EncryptedDocKey,Domain,Partner,Carrier,RequestTrackerId in to its own fields using spath .</p>

<p>any other alternative options are also welcome. Thanks you for your help. </p>

<p>Tried regex but it did not work</p>",,0,3,,2019-7-12 18:22:51,,2019-7-13 19:20:19,2019-7-13 19:20:19,,4520025.0,,11760667.0,,1,1,regex|splunk|splunk-calculation|splunk-formula,111,8
322,258778,57039333,How to differentiate messages with pattern /bank/*/accounts/ vs /bank/4/accounts/a1 in splunk query?,"<p>I am trying to write a splunk query to monitor messages grouped by the API endpoints they belong to. 
I have 2 endpoints to differentiate from:</p>

<ol>
<li>/<strong>bank</strong>/*/<strong>accounts</strong>/</li>
<li>/<strong>bank</strong>/*/<strong>accounts</strong>/<strong>a1-b2-c3</strong></li>
</ol>

<p>My sample messages look as follows:</p>

<pre><code>2019-07-15 11:42:10 [INFO] method='GET' path='/bank/4/accounts/' status='200'
</code></pre>

<pre><code>2019-07-15 11:44:10 [INFO] method='GET' path='/bank/4/accounts/a1-b2-c3' status='200'
</code></pre>

<p>When I use following splunk query, I get messages which belong to both endpoints. </p>

<pre><code>index=my_index host=my_host GET /bank/*/accounts/  | rex field=_raw ""path=(?&lt;path&gt;.*)""  
</code></pre>

<p>I tried appending following command to query, but was not successful in isolating the results:</p>

<pre><code>| rex field=_raw "".*/accounts/(?&lt;accountid&gt;\w+)""
</code></pre>",,2,0,,2019-7-15 12:01:17,,2019-7-22 12:44:16,2019-7-15 12:10:42,,11786290.0,,11786290.0,,1,0,splunk|splunk-query,29,5
323,258779,57054077,Conditional Truncation of a URL with REGEX,"<p>I'm establishing a list of base URL's distilled from Splunk data. The definition of a ""base url"" is to ignore any parameters and then truncate so that everything to the left of the last slash remains.</p>

<p>Unfortunately, not all URL's contain parameters and I'm stuck on how to incorporate a conditional part in the regex. This is what I have so far:</p>

<pre><code>| makeresults&lt;br&gt;
| eval url=""www.google.com/search?q=best+something&amp;rlz=1C1GCEA_enNL789NL790&amp;oq=best+something&amp;aqs=chrome..69i57j0l5.4104j0j8&amp;sourceid=chrome&amp;ie=UTF-8""&lt;br&gt;
| rex field=url ""^(?&lt;url1&gt;[^\?]*)\?.*$""&lt;br&gt;
| rex field=url1 ""^(?&lt;base_url&gt;.*)\/.*$""
</code></pre>

<p>So this works perfectly for a URL containing parameters, the above gives ""www.google.com"" for the base_url. But if I remove the parameters, it returns nothing - e.g. <code>base_url=""""</code>. So I need to check first if the URL contains a question mark, if so, remove everything to the right, if not, don't do anything. I've been trying things to no avail so any help would be much appreciated!</p>",57675171.0,2,0,,2019-7-16 09:25:18,,2019-8-27 12:54:37,2019-7-16 10:53:59,,11790455.0,,11790455.0,,1,1,regex|splunk,138,9
324,258780,57059724,Search to Compare and generate a new table result,"<p>In need of finding a way to search to compare and generate a communication-relation table which apparently seem to involve some complex logic.</p>

<p>I have a table with two columns where one lists the servers and the other shows the list of different servers that it communicates with (how it communicates is out of the scope of requirement).</p>

<p>Need to search for different servers that apparently communicate with one another in one or the other way. The sample image added here will explain the source and expected result tables.</p>

<p><a href=""https://i.stack.imgur.com/I07fy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I07fy.png"" alt=""enter image description here""></a></p>

<p>Please let me know if and/or how this could be achieved in SQL Server or Splunk. </p>",,0,7,,2019-7-16 14:33:02,,2019-7-16 14:46:35,2019-7-16 14:46:35,,6578080.0,,9445571.0,,1,0,sql|sql-server|search|pattern-matching|splunk,50,6
325,258781,57077755,Splunk Dashboard Based on the Content of the Log file,"<p>My log file ""app.log"" contains 20-30 lines of data, example..</p>

<pre><code>Total records 100. Successful batch size: 30. (Followed by some data here....)
Total records 100. Successful batch size: 40. (Followed by some data here....)
Total records 100. Successful batch size: 30. (Followed by some data here....)
</code></pre>

<p>I want to create a dashboard to display Total records (100) and display the sum of successful batch sizes (30+40+30) and if sum is equal to Total records need to display a green tick mark. </p>

<p>I am new to Splunk and I have tried to display the log data and stuck from there, can someone help me how to read and calculate? I tried with few docs in google but no luck, Thanks for your help.</p>",,0,4,,2019-7-17 14:02:40,,2019-7-17 14:02:40,,,,,1208258.0,,1,0,splunk|splunk-query|splunk-calculation,49,6
326,258782,57149307,Sending notification from Splunk to external application,"<p>Is it possible to send notifications from Splunk to external application?    </p>

<p>The user scenario is that Splunk is used and set up already so the business applications are monitored properly. We have an application (let's call it ACME) that needs to receive alerts from Splunk (e.g. if <em>OutOfMemoryException</em> occurs in the log file of the business application then an alert has to be sent to the ACME application).<br>
My question is how Splunk can notify (send the alert to) the ACME application that the error occured? Can Splunk call a REST service? Or via JMX?</p>

<p>Thank for the help!<br>
Regards,<br>
V.</p>",57150879.0,1,0,,2019-7-22 15:32:07,,2019-7-22 17:20:55,,,,,1269572.0,,1,0,splunk,23,5
327,258783,57161918,I want to extract the string from the string and use it under a field,"<p>I want to extract a string from a string...and use it under a field named source.</p>

<p>I tried writing like this bu no good.</p>

<pre><code>index = cba_nemis Status: J source = *AAP_ENC_UX_B.* |eval plan=upper
(substr(source,57,2)) |regex source = ""AAP_ENC_UX_B.\w+\d+rp""|stats
count by  plan,source
</code></pre>

<p>for example..</p>

<p>source=/p4products/nemis2/filehandlerU/encpr1/log/AAP_ENC_UX_B.az_in_aza_277U_ rp-20190722-054802.log
 source=/p4products/nemis2/filehandlerU/encpr2/log/AAP_ENC_UX_B.oh_in_ohf_ed_ph_ld-20190723-034121.log</p>

<p>I want to extract the string \
AAP_ENC_UX_B.az_in_aza_277U_ rp from 1st
and
AAP_ENC_UX_B.oh_in_ohf_ed_ph_ld from 2nd.</p>

<p>and put it under the column source along with the counts..</p>

<p>I want results like...</p>

<pre><code>           source                                   counts
AAP_ENC_UX_B.az_in_aza_277U_ rp                       1
AAP_ENC_UX_B.oh_in_ohf_ed_ph_ld                       1
</code></pre>",57162343.0,1,0,,2019-7-23 10:21:05,,2019-7-24 12:36:33,,,,user8281485,,,1,0,splunk|splunk-query,177,8
328,258784,57224551,Need to combine Splunk queries by feeding the results of one to the other,"<p>We are having to search through hundreds of alerts on a daily basis to test a new fraud system.  The problem is we have to review every alert which is very time consuming.</p>

<p>The 1st query gets the list of alerts and the details for each.</p>

<p>The 2nd query takes the used ID, and search for 3 specific events which can be 0 to many.
If the are no records, then add ""NULL"" to user ID, ""N"" to the device ID, and $0.00 to the amount </p>

<p>The results of both queries need to be put into a table for extract.</p>

<pre><code>index=mbank_p_database sourcetype=mbank_event EventTypeID=1095 
| dedup OLBUserID
| table _time, SessionID, EventTypeID, OLBUserID, score, risk_rating, reason_code 
| sort _time
</code></pre>

<pre><code>index=mbank_p_database sourcetype=mbank_event EventTypeID=1000 OR EventTypeID=1011 OR EventTypeID=1012 OLBUserID=&lt;Results from 1st query&gt;
| table UDID, Amount
</code></pre>

<p>If there are no results in the second query for the User ID, then make ""UDID""=""N"", and ""Amount""=$0.00</p>

<pre><code>``

Table layout of results of combined query:
--_time = Date\Timestamp (1st Query)
--SessionID = ""SessionID"" (1st Query)
--OLBUserID = ""UserID"" (1st Query)
--Deposit? = (""Y"" or ""N"") (2nd Query)
--score = ""Score"" (1st Query)
--risk_rating = ""Rating"" (1st Query)
--reason_code = ""Reason Code""  (1st Query)
</code></pre>",57260427.0,2,1,,2019-7-26 17:22:47,,2019-7-31 15:29:29,,,,,5660979.0,,1,0,splunk,243,9
329,258785,57225079,curl from Write_http plugin of collectd is not working,"<p>I am trying to enable to Write_http plugin in collectd for Splunk, the problem is when I try to curl externally </p>

<pre><code>curl http://10.xx.xxx.x:8088/services/collector/raw  -H ""Authorization: Splunk 2c396ad4-f518-416f-83cd-e37596228792"" -d {""test"":""value""}
</code></pre>

<p>It works. But when I do the same in
 collectd plugin</p>

<pre><code>&lt;Plugin write_http&gt;
&lt;Node ""node-http-1""&gt;
URL ""http://10.xx.xxx.x:8088/services/collector/raw?channel=2c396ad4-f518-416f-83cd-e37596228792""
Header ""Authorization: Splunk 2c396ad4-f518-416f-83cd-e37596228792""
Format ""JSON""
Metrics true
StoreRates true
</code></pre>

<p>
</p>

<p>I get</p>

<pre><code>[error] write_http plugin: curl_easy_perform failed with status 56: Recv failure: Connection reset by peer
</code></pre>

<p>is it because of the curl version of collectd?</p>",,1,0,,2019-7-26 18:05:33,,2020-2-19 12:01:33,,,,,10371049.0,,1,1,ssl|curl|splunk|collectd,508,11
330,258786,57256053,A single splunk event contains same field value pair multiple times. How to fetch single occurrence?,"<p><code>""Env=abc""</code> is the field/value pair that every single Splunk event contains multiple no. of times. How to fetch only one of them to display in the time chart?</p>

<p>Ex: </p>

<pre><code>Env=abc Env=abc Env=abc Env=abc Env=abc Env=abc Env=abc Env=abc
</code></pre>

<p><strong>I want ""abc"" to be displayed only once in my graph.</strong> However, I see my graph displaying like below:</p>

<p><code>abc abc abc abc abc etc</code>.</p>",,1,2,,2019-7-29 14:36:44,,2019-7-31 02:25:31,2019-7-29 14:43:36,,10961238.0,,11852764.0,,1,1,splunk,112,8
331,258787,57296236,Dynamic Reports in Splunk,"<p>I want to pass splunk query as token.</p>

<p>I have tried it.But the passed query stops at the first equal symbol.For eg if splunk query is index=cq*.Then it is passed only till index.Things from first equal symbol are not considered.Morever any query can contain multiple = signs.</p>",,1,1,,2019-7-31 17:34:57,,2019-7-31 21:18:03,2019-7-31 17:57:55,,8523324.0,,8523324.0,,1,0,reporting|splunk,89,7
332,258788,57319041,Parse Nested JSON Array into Splunk Table,"<p>I have the below JSON event with nested array in splunk -:</p>

<pre><code>{
""items"":
    [
        {
            ""parts"":
                [
                    {
                        ""code"":""1"",""var"":"""",""pNum"":""101"",""counter"":1019
                    },
                    {
                        ""code"":""0"",""var"":"""",""pNum"":""102"",""counter"":1029
                    }
                ],
            ""se"":""A1"",
            ""so"":""111""
        },
        {
            ""parts"":
                [
                    {
                        ""code"":""1"",""var"":"""",""pNum"":""301"",""counter"":3019
                    },
                    {
                        ""code"":""0"",""var"":"""",""pNum"":""302"",""counter"":3029
                    }
                ],
            ""se"":""A3"",
            ""so"":""333""
        },
        {
            ""parts"":
                [
                    {
                        ""code"":""0"",""var"":"""",""pNum"":""401"",""counter"":4019
                    }
                ],
            ""se"":""A4"",
            ""so"":""444""
        },
        {
            ""parts"":
                [
                    {
                        ""code"":""1"",""var"":"""",""pNum"":""501"",""counter"":5019
                    }
                ],
            ""se"":""A5"",
            ""so"":""555""
        }
    ],
""id"":""x.9110790"",
""cr"":""x-273169""
}
</code></pre>

<p>I would like to extract this JSON into the below Splunk table -:</p>

<p><a href=""https://i.stack.imgur.com/VGS3e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VGS3e.png"" alt=""Output Splunk Table Format""></a></p>

<p>I tried to use spath as below but it is only giving wrong results given below -:</p>

<p>|makeresults | eval _raw=""{
    \""items\"":
        [
            {
                \""parts\"":
                    [
                        {
                            \""code\"":\""1\"",\""var\"":\""\"",\""pNum\"":\""101\"",\""counter\"":1019
                        },
                        {
                            \""code\"":\""0\"",\""var\"":\""\"",\""pNum\"":\""102\"",\""counter\"":1029
                        }
                    ],
                \""se\"":\""A1\"",
                \""so\"":\""111\""
            },
            {
                \""parts\"":
                    [
                        {
                            \""code\"":\""1\"",\""var\"":\""\"",\""pNum\"":\""301\"",\""counter\"":3019
                        },
                        {
                            \""code\"":\""0\"",\""var\"":\""\"",\""pNum\"":\""302\"",\""counter\"":3029
                        }
                    ],
                \""se\"":\""A3\"",
                \""so\"":\""333\""
            },
            {
                \""parts\"":
                    [
                        {
                            \""code\"":\""0\"",\""var\"":\""\"",\""pNum\"":\""401\"",\""counter\"":4019
                        }
                    ],
                \""se\"":\""A4\"",
                \""so\"":\""444\""
            },
            {
                \""parts\"":
                    [
                        {
                            \""code\"":\""1\"",\""var\"":\""\"",\""pNum\"":\""501\"",\""counter\"":5019
                        }
                    ],
                \""se\"":\""A5\"",
                \""so\"":\""555\""
            }
        ],
    \""id\"":\""x.9110790\"",
    \""cr\"":\""x-273169\""
    }"" |spath |rename items as *  | table id, cr,items{}.*</p>

<p><a href=""https://i.stack.imgur.com/iIFJa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iIFJa.png"" alt=""splunk actual output""></a></p>

<p>I am trying to parse the JSON type splunk logs for the first time. So please help with any hints to solve this.
Thank you</p>",,1,0,,2019-8-2 02:03:10,,2019-8-2 07:38:52,,,,,5491910.0,,1,0,json|splunk|multivalue|splunk-query,3550,14
333,258789,57321475,Splunk - Can Fields be used as app identifier,"<p>I have an Splunk system receiving logs from various applications and the logs are reported to different indexes for each application.
Its becoming difficult to maintain indexes, my client has requested to club all the indexes into one.</p>

<p>I am thinking of defining single index for all the application and use field (app_id) to identify the logs coming from different applications. Does this looks like a logical solution. Any thoughts on this are welcome please.</p>",,1,0,,2019-8-2 07:06:42,,2019-8-2 11:26:10,,,,,4403581.0,,1,0,logging|splunk|indexer,27,5
334,258790,57334551,Trying To Peform Mathmetical Calculations Based off One Column,"<p>I'm new to Splunk and i'm having trouble with the following line of code. I think what i'm trying to do is pretty self explanatory. Essentially the data i'm working with is one column and the values in the column are ""0"" or ""1"". Any help would be greatly appreciated. Thank you!</p>

<pre><code>| stats sum(ACCESS_REVIEW_COMPLETE) \ count(ACCESS_REVIEW_COMPLETE)
</code></pre>",,1,0,,2019-8-2 23:59:48,,2019-8-3 11:39:50,,,,,5171347.0,,1,0,splunk|splunk-calculation,15,5
335,258791,57356977,Splunk index usage search adding column titled NULL to results,"<p>I'm running a fairly simple search to identify index usage on my Splunk install by source, as we're running through the Enterprise 30-day trial with the intention of using Splunk Free after it expires:</p>

<pre><code>index=_internal source=*license_usage.log | eval MB=b/1024/1024 | timechart span=1d sum(MB) by s where count in top50
</code></pre>

<p>The results for all of my data sources are returned as expected but there's an additional column titled ""NULL"" at the end of the results:</p>

<p><a href=""https://i.stack.imgur.com/XYJ02.png"" rel=""nofollow noreferrer"">Splunk index search NULL column</a>
<br/><br/>
All of my data has an input source and when I click on the column and choose to view the data, it brings back no results.</p>

<p>Can anyone help me understand what this NULL column is please? If it's correct it suggests I'm using over the 500MB/day limit for Splunk Free, which I need to address before the trial period ends.</p>",57359059.0,1,0,,2019-8-5 10:44:25,,2019-8-5 12:56:30,,,,,11663841.0,,1,0,splunk|splunk-query,169,8
336,258792,57391387,script Send command with SH,"<p>I would need a .sh script that allows me to read only the second line of a file and then send it to machine B.</p>

<p>Example file:</p>

<p><code>timestamp_pippo.csv</code></p>

<pre><code>""Row1_skipped""
""Row2_send_to_machine""
</code></pre>

<p>the file is in the path:</p>

<pre><code>C:\Program Files\Splunk\var\run\splunk\csv
</code></pre>

<p>only the second row ""<code>row2_send_to_machine</code>"" (contains a unix command) must be sent to machine B</p>

<p>once the command has been sent, the file timestamp_pippo.csv must be deleted.
can you help me? I'm not familiar with .sh</p>

<p>what I've managed to create so far is only this:</p>

<pre><code>for a in $(C:\Program Files\Splunk\var\run\splunk\csv cat timestamp_pippo.csv|grep -v Row1_skipped);do
 ssh unix_machine@11.111.111.11 $a
 done
</code></pre>",,2,2,,2019-8-7 09:41:27,,2019-8-8 12:35:07,2019-8-7 11:36:48,,7166136.0,,11894830.0,,1,0,bash|sh|splunk,792,12
337,258793,57391397,splunklib.binding.HTTPError: HTTP 400 Bad Request -- Unknown search command 'index',"<p>Error while running the search command</p>

<p>While using splunk enterprise, I want to run a search command from backend, keyword is ""index = "". 
When I am running this command I am getting the result but when I am adding this command in my code, i am getting ""splunklib.binding.HTTPError: HTTP 400 Bad Request -- Unknown search command 'index'.""
I am able to login to splunk enterprise and can run a basic search command ""search * | head 100""</p>

<pre><code>def normal_search():
    #searchquery_normal = ""search * | head 10""
    searchquery_normal = ""index = some_tool_name""
    kwargs_normalsearch = {""exec_mode"": ""normal""}
    job = service.jobs.create(searchquery_normal, **kwargs_normalsearch)

    # A normal search returns the job's SID right away, so we need to poll for completion
    while True:
        while not job.is_ready():
            pass
        stats = {""isDone"": job[""isDone""],
                 ""doneProgress"": float(job[""doneProgress""])*100,
                  ""scanCount"": int(job[""scanCount""]),
                  ""eventCount"": int(job[""eventCount""]),
                  ""resultCount"": int(job[""resultCount""])}

        status = (""\r%(doneProgress)03.1f%%   %(scanCount)d scanned   ""
                  ""%(eventCount)d matched   %(resultCount)d results"") % stats

        sys.stdout.write(status)
        sys.stdout.flush()
        if stats[""isDone""] == ""1"":
            sys.stdout.write(""\n\nDone!\n\n"")
            break
        sleep(2)

    # Get the results and display them
    for result in results.ResultsReader(job.results()):
        print result

    job.cancel()   
    sys.stdout.write('\n')
</code></pre>

<p>Expected: No error
Actual:splunklib.binding.HTTPError: HTTP 400 Bad Request -- Unknown search command 'index'</p>

<p>.</p>",,1,0,,2019-8-7 09:41:53,,2019-8-7 13:11:57,,,,,8156651.0,,1,0,python|splunk-sdk,1673,13
338,258794,57401039,Log entry to Splunk using python,"<p>In Splunk we have an url, index, token, host, source and sourcetype and with those detail need to post data in splunk using python.</p>

<p>I was able to write a code using requests with URL, index, token and it works</p>

<pre><code>import requests
url='SPLUNK_URL'
Header = {'Authorization': 'Splunk '+'1234567'}
json = {""index"":""xxx_yyy"", ""event"": { 'message' : ""Value"" } }
r = requests.post(url, headers=Header, json, verify=False)
</code></pre>

<p>But sometimes get this error ConnectionError: ('Connection aborted.', OSError(""(10054, 'WSAECONNRESET')"")). How to avoid this error ?</p>",,1,5,,2019-8-7 19:20:37,,2021-1-12 23:58:24,2019-8-7 20:21:42,,11616999.0,,11616999.0,,1,0,python-requests|splunk,424,10
339,258795,57417592,Splunk query returns 0 after using eval function,"<p>Query return 0 value for eval calculation. </p>

<p>index=* platform=PC browser_name=chrome OR browser_name=edge OR browser_name=safari | stats count(eval(player_event=""play"")) AS Play count(eval(error_event_type=""vsf"")) AS VSF count(eval((Play / VSF))) AS Rate by browser_name </p>

<p>I would expect this query return % rate of eval counts and display a timechart </p>",,1,1,,2019-8-8 17:06:59,,2019-8-8 20:23:48,,,,,11903508.0,,1,1,splunk|splunk-query,231,9
340,258796,57420494,query to display an error if appearing more than 15% of the total traffic in last 1 hour,"<p>I need to create an alert which will prompt whenever ""reason"": ""LOCKED"" appears more than 15% in previous 1 hour. checks to be made every 10m. this should happen only for ""operation"":""ENROLL"" and ""operation"":""BIND""</p>

<p>i have this query which gives me the locked transactions but if I combine it with operation:BIND or ENROLL then I dont get any results even though the application is throwing logs for these.</p>

<p>index=abc  cf_app_name=""stack-overflow""  ""reason"": ""LOCKED"" AND ""operation"":""ENROLL"" </p>

<p>below is the sample log</p>

<pre><code>{
    ""id"": ""c90f975cb368"",
    ""source"": {
        ""domain"": ""ABC"",
        ""version"": ""1.0.0"",
        ""environment"": ""stage""
    },
    ""namespace"": ""a.b.c"",
    ""resource"": ""CARD"",
    ""operation"": ""ENROLL"",
    ""state"": ""FAILED"",
    ""tags"": [""kpi""],
    ""createTime"": 156898900,
    ""context"": {
        ""correlationId"": ""0-6093d36""
    },
    ""data"": {
        ""dpaData"": {
            ""dpaId"": ""1d457051052e71730e71cc5a"",
            ""srctId"": ""526e1bcf-ca6ce85ee9cb"",
            ""durbinRights"": false
            },
        ""dcfData"": {},
        ""srciData"": {
            ""srcId"": ""526e1ca6ce85ee9cb"",
            ""name"": ""mcd
           },
        ""appInstanceData"": {
            ""userAgent"": ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36"",
            ""abcdefghijklmnopqrstuvwxyz\""}"",
            ""remoteIpAddress"": ""xx.yy.zz.aa"",
            ""httpXForwardedFor"": ""xx.yy.aa.zz""
        },
        ""authenticationData"": {
            ""expiration"": false,
            ""authenticationResult"": {
                ""reason"": ""LOCKED""
            },
            ""emailVerified"": false,
            ""phoneVerified"": false
        },
        ""consumerData"": {},
        ""error"": {
            ""reason"": ""LOCKED"",
            ""message"": ""Access is denied to the requested resource. The user account has been locked.,  card locked time: [166898828]"",
            ""http-response-code"": ""400""
        }
    }
}
</code></pre>

<p>I just need the query which will give the events where ""reason"": ""LOCKED"" under the field error appears along with  ""operation"": ""ENROLL""</p>",,1,0,,2019-8-8 20:57:20,,2019-8-9 07:39:39,,,,,7072033.0,,1,0,splunk,33,6
341,258797,57425576,Querying the logger through splunk based on a particular value condition,"<p>I need to query the logger through splunk, for a particular statement based on a value inside that statement which is > some value. Can someone please help me with below questions ?</p>

<ol>
<li><p>I tried below index to find out all the occurrences having <b>""Batch completed in xx (where xx is the value which must be > some specified value )""</b>. But the results are zero even though there are occurrences satisfying the condition, in the log file.</p>

<pre><code>index=""My_App_ID""  APP-NAME makeresults | eval _raw=""Batch completed in 23"" | rex field=_raw ""Batch completed in (?&lt;compltd_time&gt;.\d+)"" | where compltd_time &gt; 10
</code></pre></li>
<li><p>I need to write an index which could fetch me out all the occurrences whose value of 100/23 is > (some-value say 10) for the below log statement </p>

<pre><code>Batch completed in 23 seconds for 100 UUIDS
</code></pre></li>
</ol>",,1,0,,2019-8-9 07:44:40,,2019-12-19 21:19:11,2019-12-19 21:19:11,,13302.0,,3696393.0,,1,0,indexing|splunk|splunk-query,130,8
342,258798,57433519,How to ingest data from elastic search and query the data in Splunk?,"<p>I have a project that require me to pull index data from elasticsearch to Splunk. Is it possible? </p>

<p>I came up with the tool called Elasticseach data integrator but its not returning any event i query in Splunk</p>",,0,1,,2019-8-9 15:52:05,,2019-8-9 15:52:05,,,,,11721226.0,,1,1,elasticsearch|logstash|kibana|splunk,372,10
343,258799,57436559,"How to add ""Not"" in endswith tag of Transaction search in Splunk","<p>I am building a query in splunk to filter logs that start with ""INFO:<strong>main</strong>:TABLE:"" and does ""NOT"" endswith ""INFO:<strong>main</strong>: Done"" 
I want all the transactions that do not log ""Done"" in the end. ""!""/ ""Not"" does not seem to work. </p>

<p>Part of the query I am stuck with: 
<code>| rex field=log ""INFO:__main__:TABLE: (?&lt;table_name&gt;[A-Za-z_]*)"" 
| transaction container_name startswith=""INFO:__main__:TABLE:"" endswith=""INFO:__main__: Done""</code></p>

<p>The current query will give me the transactions that terminated correctly/with Done status. I am interested in the one with NOT ""DONE"" status.</p>

<p>I want something like:
<code>| transaction container_name startswith=""INFO:__main__:TABLE:"" endswith!=""INFO:__main__: Done""</code></p>",,2,0,,2019-8-9 20:11:02,0.0,2019-9-5 17:49:49,2019-8-9 20:35:01,,11718804.0,,11718804.0,,1,0,splunk|splunk-query|splunk-sdk,500,11
344,258800,57458412,How do I pass credential in custom search command Splunk,"<p>My custom search command required some credentials to work (third party lib auth) and I don't want to have it hardcoded.
What is the best practice to pass the credentials inside this custom search command?</p>

<p> I'm looking to do some similar to set-up page from add-on builder.</p>

<p>Any thought on this? </p>",,1,0,,2019-8-12 09:12:56,,2020-5-20 03:53:33,,,,,5686594.0,,1,1,splunk|splunk-sdk,36,6
345,258801,57467070,How can fluent-bit add custom metadata to each event message being sent to splunk,"<p>I'm using fluent-bit within Kubernetes to forward logs to Splunk. We'll be using the same Splunk index for multiple Kubernetes clusters, so I want to tag each event being forwarded from fluent-bit with the cluster that it comes from. </p>

<p>I tried using the modify functionality to ""Add"" or ""Set"" a new field in the event. </p>

<pre class=""lang-sh prettyprint-override""><code>fluent-bit-filter.conf: |-
   [FILTER]
       Name                kubernetes
       Match               kube.*
       Kube_Tag_Prefix     kube.var.log.containers.
       Kube_URL            https://kubernetes.default.svc:443
       Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
       Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
       K8S-Logging.Parser  On
       K8S-Logging.Exclude On
       Add cluster devcluster
</code></pre>

<p>Sample log that I actually receive (missing the newly added field ""cluster"")</p>

<pre class=""lang-sh prettyprint-override""><code>[305] kube.var.log.containers.calico-node-xzwnv_kube-system_calico-node-a4a6a2261a76ec419e9cf13ae39732b3e918726573cf1a0dece648e679011578.log: [1565578883.799679612, {""log""=&gt;""2019-08-12 03:01:23.799 [INFO][68] int_dataplane.go 830: Received interface update msg=&amp;intdataplane.ifaceUpdate{Name:""cali5d1a7318787"", State:""up""}
</code></pre>",,2,0,,2019-8-12 19:19:38,1.0,2019-9-5 06:32:12,,,,,3112104.0,,1,3,logging|kubernetes|splunk|fluent-bit,2732,24
346,258802,57472372,Setup Splunk alert if count is zero for any of interface in both servers,"<p>I wrote a Splunk query to get the results and to generate alerts accordingly.</p>

<p>The query I'm using is </p>

<pre><code>index=aws_instance_prod sourcetype=""alert_log"" host=""*"" File_Count=0
| stats count by Interface
</code></pre>

<p>When executing this query I'm getting the count of interface which has File_Count = 0 in any of the host servers.</p>

<p>But I want to generate an alert if the count of any interface is 0 in both hosts.</p>

<p>Please refer below screenshot.
<a href=""https://i.stack.imgur.com/6IsDN.jpg"" rel=""nofollow noreferrer"">Sample Result</a></p>

<p>As per this screenshot we could see that S and Y are interface having count = 2 means both interfaces have 0 count in both hosts.
So I want to raise an alert for these both interfaces.</p>

<p>Similarly if any interface returns count as 2 then raise an alert for that interface.</p>",,1,0,,2019-8-13 06:40:22,,2019-9-8 14:43:14,2019-8-13 19:28:39,,3961280.0,,4664742.0,,1,1,alert|splunk|splunk-query,556,13
347,258803,57474112,Office 365 Notable Events,"<p>Do correlation searches in Splunk with Office365 events generate notable events in incident review?</p>

<p>I have created an alert to trigger on Office365 logs, there are events generating but on the incident review dashboard no notables are presented</p>",,1,0,,2019-8-13 08:43:13,,2019-8-13 13:45:28,,,,,11921373.0,,1,0,office365|splunk,33,6
348,258804,57477090,input certain field from dropdown and then use another field from that row (csv file) in the search,"<p>I'm new to Splunk and I am in the process of writing a Splunk query which takes in a key from the dropdown option on a dashboard and I want to then extract a different row (specifically domain) associated with that key in the csv file and then use it in the search to filter it by domain.</p>

<p>The query which I currently wrote is:</p>

<pre><code>basequery 
| lookup tenant.csv key as tenant_key output domain as Domain
| search tenant_key = $selected_client$
| stats count
</code></pre>

<p>I just want to display a count filtered by domain associated with the key provided by the dropdown. I'm not quite sure what is wrong or how to go about it.</p>",,1,2,,2019-8-13 11:46:52,,2019-8-13 15:54:02,2019-8-13 15:54:02,,3961280.0,,4371306.0,,1,0,splunk|splunk-query,90,7
349,258805,57485801,"Failed to mount Splunk config On Kubernetes - ERROR: Couldn't read ""/opt/splunk/etc/splunk-launch.conf","<p>I'm using <a href=""https://hub.docker.com/r/splunk/splunk/"" rel=""nofollow noreferrer"">this</a> Splunk image on Kubernetes (testing locally with minikube).</p>

<p>After applying the code below I'm facing the following error:</p>

<blockquote>
  <p>ERROR: Couldn't read ""/opt/splunk/etc/splunk-launch.conf"" -- maybe
  $SPLUNK_HOME or $SPLUNK_ETC is set wrong?</p>
</blockquote>

<p>My Splunk deployment:</p>

<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: splunk
  labels:
    app: splunk-app
    tier: splunk
spec:
  selector:
    matchLabels:
      app: splunk-app
      track: stable
  replicas: 1
  template:
    metadata:
      labels:
        app: splunk-app
        tier: splunk
        track: stable
    spec:
      volumes:
      - name: configmap-inputs
        configMap:
           name: splunk-config
      containers:
      - name: splunk-client
        image: splunk/splunk:latest
        imagePullPolicy: Always
        env:
        - name: SPLUNK_START_ARGS
          value: --accept-license --answer-yes
        - name: SPLUNK_USER
          value: root
        - name: SPLUNK_PASSWORD
          value: changeme
        - name: SPLUNK_FORWARD_SERVER
          value: splunk-receiver:9997
        ports:
        - name: incoming-logs
          containerPort: 514
        volumeMounts:
          - name: configmap-inputs
            mountPath: /opt/splunk/etc/system/local/inputs.conf
            subPath: ""inputs.conf""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: splunk-config
data:
  inputs.conf: |
    [monitor:///opt/splunk/var/log/syslog-logs]
    disabled = 0
    index=my-index
</code></pre>

<p>I tried to add also this env variables - with no success:</p>

<pre><code>    - name: SPLUNK_HOME
      value: /opt/splunk
    - name: SPLUNK_ETC
      value: /opt/splunk/etc
</code></pre>

<p>I've tested the image with the following <strong>docker</strong> configuration - <strong>and it ran successfully</strong>:</p>

<pre><code>version: '3.2'
services:
    splunk-forwarder:
      hostname: splunk-client
      image: splunk/splunk:latest
      environment:
        SPLUNK_START_ARGS: --accept-license --answer-yes
        SPLUNK_USER: root
        SPLUNK_PASSWORD: changeme
      ports:
      - ""8089:8089""
      - ""9997:9997""
</code></pre>

<hr>

<p>Saw <a href=""https://answers.splunk.com/answers/728533/getting-error-couldnt-read-optsplunketcsplunk-laun.html"" rel=""nofollow noreferrer"">this</a> on Splunk forum but the answer did not help in my case. </p>

<p>Any ideas?</p>

<hr>

<p>Edit #1: </p>

<p>Minikube version: Upgraded from<code>v0.33.1</code> to <code>v1.2.0</code>.</p>

<p>Full error log:</p>

<pre><code>$kubectl logs -l tier=splunk

splunk_common : Set first run fact -------------------------------------- 0.04s
splunk_common : Set privilege escalation user --------------------------- 0.04s
splunk_common : Set current version fact -------------------------------- 0.04s
splunk_common : Set splunk install fact --------------------------------- 0.04s
splunk_common : Set docker fact ----------------------------------------- 0.04s
Execute pre-setup playbooks --------------------------------------------- 0.04s
splunk_common : Setting upgrade fact ------------------------------------ 0.04s
splunk_common : Set target version fact --------------------------------- 0.04s
Determine captaincy ----------------------------------------------------- 0.04s
ERROR: Couldn't read ""/opt/splunk/etc/splunk-launch.conf"" -- maybe $SPLUNK_HOME or $SPLUNK_ETC is set wrong?
</code></pre>

<p>Edit #2: Adding config map to the code (was removed from the original question for the sake of brevity). <strong>This is the cause of failure</strong>.</p>",57590064.0,3,9,,2019-8-13 21:58:19,,2021-2-16 12:11:35,2019-8-21 10:58:48,,1103953.0,,1103953.0,,1,4,docker|kubernetes|splunk,1262,17
350,258806,57498476,"How to convert ""OR"" result to line chart","<p>I have a query to get the browser info for every event.</p>

<pre><code>""http://"" index=web GET (Chrome OR Safari OR IE OR Firefox)
</code></pre>

<p>How can I convert the result to a line chart to get browser usage based on different browsers? I can create line chart for each browser but I want to show the usage on the same chart.</p>",,1,0,,2019-8-14 16:13:38,,2019-8-14 16:19:37,,,,,10397308.0,,1,1,splunk|splunk-query,29,7
351,258807,57523780,Get current process status for the whole events,"<p>Current status with events under the same id</p>

<p>I have tried to add up the status by eval case (could manage only one event )not all events under the same process.</p>

<pre><code>|eval Status = case (eventId=""endProcess""  ,""Completed"" ,eventId=""error"",""Terminated"") 
|stats earliest(when) AS startTime latest(when) AS endTime by mainprocessname ,resourceName , Status 
|eval startTime = strftime( strptime( startTime, ""%Y-%m-%dT%H:%M:%S.%7NZ""), ""%Y-%m-%d %H:%M:%S"")
|eval endTime = strftime( strptime( endTime, ""%Y-%m-%dT%H:%M:%S.%7NZ""), ""%Y-%m-%d %H:%M:%S"")

|table startTime, endTime , mainprocessname,resourceName, Status 
</code></pre>

<p>my result now </p>

<pre><code>startTime   endTime    mainprocessname  resourceName       Status
2019-08-16 06:15:16 2019-08-16 06:15:16 03 - SSam Scott    Completed
</code></pre>

<p>what I want to get the different time  (but I get exact same time because it is grouped by status over event ) any tip to get the Status for the whole process</p>",,1,0,,2019-8-16 11:22:59,,2019-8-16 14:36:46,2019-8-16 13:50:02,,7166136.0,,7427248.0,,1,0,splunk|splunk-query|splunk-formula,27,5
352,258808,57525439,How does exact function w.r.t standard deviation while configuring splunk alert work?,"<p>I have been trying to set up an alert to see the spike in traffic in terms of standard deviation. 
I am setting the upper bound as below:</p>

<pre><code>upperBound=(avg+stdev*exact(4)).
</code></pre>

<p>However, I am not able to understand what does avg+stdev<em>exact(4) mean in terms of spike and deviation. What will be spike the in traffic be if I write as (avg+stdev</em>exact(4))?</p>

<p>Hence, how are these below different? and what should I choose?</p>

<pre><code>avg+stdev*exact(5))

avg+stdev*exact(4))

avg+stdev*exact(3))

avg+stdev*exact(2))
</code></pre>",,0,3,,2019-8-16 13:22:16,,2019-8-16 15:29:38,2019-8-16 15:29:38,,3961280.0,,11925396.0,,1,0,splunk|stdev,56,6
353,258809,57580667,How to manually set timestamp for Splunk logs?,"<p>I have a database table full of logs, and I would like to upload that into Splunk by executing a query to get an array of logs, then iterating over it and running <code>console.log()</code> on each log. This almost works fine, however the timestamp that Splunk picks up for each log is based on whatever time I run the upload script. </p>

<p>Is there any way I can transform my log object so that Splunk will use the timestamps I provide? For example, one of my logs might look like: </p>

<pre><code>{
  id: 2910432221,
  log_timestamp: 2019-08-07T19:04:03.000Z,
  userId: 2331,
  actionId: 45
}
</code></pre>

<p>Ideally, I would be able to run something like this, and have the value of <code>splunk_timestamp</code> appear in the <code>Time</code> column in the Splunk dashboard.</p>

<pre><code>console.log({
 id: 2910432221,
 splunk_timestamp: 2019-08-07T19:04:03.000Z,
 userId: 2331,
 actionId: 45
})
</code></pre>

<p>Is anything like that possible?</p>",57589484.0,1,0,,2019-8-20 19:50:44,,2019-8-21 10:22:43,,,,,7932229.0,,1,1,javascript|logging|splunk,160,10
354,258810,57584667,Uploading a Html file and breaking it into events in splunk,"<p>How to upload a HTML file in Splunk and break it into events.</p>

<blockquote>
  <p>I see that there is no predefined source type for html in splunk</p>
</blockquote>",,1,0,,2019-8-21 04:57:59,,2019-8-21 08:06:12,,,,,8523324.0,,1,0,html|splunk,287,10
355,258811,57585589,Splunk search issue,"<p>I have a search query like below. </p>

<pre><code>index = abc_dev sourcetype = data RequestorSystem = * Description=""Request Receieved from Consumer Service"" 
  OR Description=""Total Time taken in sending response""
| dedup TId
| eval InBoundCount=if(Description=""Request Receieved from Consumer Service"",1,0)
| eval OutBoundCount=if(Description=""Total Time taken in sending response"",1,0)
| stats sum(InBoundCount) as ""Inbound Count"",sum(OutBoundCount) as ""Outbound Count""
</code></pre>

<p>I am not sure why inbound count is always showing as 0, outbound count works perfectly</p>",,1,1,,2019-8-21 06:26:05,,2019-8-21 12:40:18,2019-8-21 12:40:18,,773623.0,,10455015.0,,1,0,splunk,38,7
356,258812,57592151,Splunk - counting numeric information in events,"<p>I'm very new to Splunk and wanted to know if the following was possible:  I'm trying to set up a dashboard of how many times we had to retry a call to a service. I am currently logging the following text:</p>

<p><code>number of retries required 0</code></p>

<p>The number of retries required can vary from 0 to 3</p>

<p>Is there an easy way to query this and display how many times it was either 0, 1, 2 or 3?</p>

<p>Thanks.</p>",57592743.0,1,2,,2019-8-21 12:55:54,,2019-8-21 13:57:07,2019-8-21 13:57:07,,590848.0,,3781095.0,,1,0,splunk,45,9
357,258813,57596841,Splunk installation on top of Docker fails in AWS ec2 instance,"<p>I'm trying to install Splunk on top of Docker in an AWS ec2 instance using the following command.</p>

<pre><code>docker run -it -e DEBUG=true -e SPLUNK_START_ARGS=--accept-license -e SPLUNK_PASSWORD=&lt;password&gt; splunk/splunk:latest
</code></pre>

<p>and I'm getting the following errors.</p>

<pre><code>TASK [splunk_common : Create .ui_login] ****************************************
fatal: [localhost]: FAILED! =&gt; {""changed"": false, ""checksum"": ""da39a3ee5e6b4b0d3255bfef95601890afd80709"", ""msg"": ""Source /var/tmp/ansible-tmp-1566408647.18-207818394602441/source not found""}

PLAY RECAP *********************************************************************
localhost                  : ok=18   changed=1    unreachable=0    failed=1    skipped=6    rescued=0    ignored=0   

Wednesday 21 August 2019  17:30:47 +0000 (0:00:00.377)       0:00:04.605 ****** 
=============================================================================== 
splunk_common : Update Splunk directory owner --------------------------- 1.04s
Gathering Facts --------------------------------------------------------- 0.74s
splunk_common : Update /opt/splunk/etc ---------------------------------- 0.52s
splunk_common : Create .ui_login ---------------------------------------- 0.38s
splunk_common : Check for existing installation ------------------------- 0.30s
splunk_common : Find manifests ------------------------------------------ 0.29s
splunk_common : Check if /sbin/updateetc.sh exists ---------------------- 0.15s
splunk_common : Check for existing splunk secret ------------------------ 0.13s
Provision role ---------------------------------------------------------- 0.13s
splunk_common : Check if we are in a docker ----------------------------- 0.13s
splunk_common : include_tasks ------------------------------------------- 0.08s
splunk_common : include_tasks ------------------------------------------- 0.08s
splunk_common : include_tasks ------------------------------------------- 0.07s
splunk_common : include_tasks ------------------------------------------- 0.07s
splunk_common : Set current version fact -------------------------------- 0.04s
splunk_common : include_tasks ------------------------------------------- 0.04s
Determine captaincy ----------------------------------------------------- 0.04s
Execute pre-setup playbooks --------------------------------------------- 0.04s
splunk_common : Setting upgrade fact ------------------------------------ 0.04s
splunk_common : Set first run fact -------------------------------------- 0.04s
ERROR: Couldn't read ""/opt/splunk/etc/splunk-launch.conf"" -- maybe $SPLUNK_HOME or $SPLUNK_ETC is set wrong?

</code></pre>

<p>However, the same command works on an Ubuntu machine on my laptop. I'm missing anything?</p>",,1,2,,2019-8-21 17:50:07,,2019-8-22 08:17:50,,,,,2764789.0,,1,1,docker|amazon-ec2|ansible|splunk,160,10
358,258814,57603547,Splunk time calculation issues,"<p>I have a requirement like this:</p>

<ul>
<li>calculate the acknowledgement SLA - should be within 2 hours</li>
<li>calculate Response SLA - Should be before next business day 9 AM.</li>
</ul>

<p>I would like to understand the logic to calculate how many transactions missed ack SLA and response SLA.</p>

<p>Suppose, if I submit the request at 3 PM today, how I can calculate the ACK should be before 5 PM today  and response should be before 9 AM.</p>

<pre><code>Transaction submitted time-- 22-Aug-2019 12:00 
Transaction acknowledge time-- 22-Aug-2019 13:00 
response sent at - 22-Aug-2019 19:00
</code></pre>

<p>so we have to consider the above transaction as success.</p>

<pre><code>Transaction submitted time-- 22-Aug-2019 12:00 
Transaction acknowledge time-- 22-Aug-2019 13:00 
response sent at - 23-Aug-2019 19:00
</code></pre>

<p>in this case, response sla breached and count as failure.</p>",,1,0,,2019-8-22 06:53:09,,2019-8-22 15:16:15,2019-8-22 11:07:53,,590848.0,,10455015.0,,1,0,splunk,48,7
359,258815,57620314,Jenkins to Splunk send files by mapping different sourcetypes to different sources(files),"<p>I need to send files(ex: XML and CSV) from Jenkins to Splunk. 
For which,
I have integrated Jenkins and Splunk using below</p>

<p>1) Jenkins app in Splunk(Splunk app for Jenkins)</p>

<p>2) Splunk plug-in in Jenkins</p>

<p>In Jenkins I have configured ""Custom Metadata"" like this</p>

<blockquote>
  <p>[Data Source->'logfile', Config Item-> 'Source Type',
  Value->'custom_xml_sourcetype' ] [Data Source->'logfile', Config
  Item-> 'Source Type', Value->'csv' ]</p>
</blockquote>

<p>In Splunk, Indexed files(ex: a.xml and b.csv) has the same sourcetype ->'custom_xml_sourcetype'. Ideally both needs to be mapped to the exact sourcetypes. Which should be like this</p>

<blockquote>
  <p>a.xml -> 'custom_xml_sourcetype' </p>
  
  <p>b.csv->'csv'</p>
</blockquote>

<p>but, it is mapped like this,</p>

<blockquote>
  <p>a.xml -> 'custom_xml_sourcetype'</p>
  
  <p>b.csv->'custom_xml_sourcetype'</p>
</blockquote>

<p>It is not certain, how to map the sourcetypes to sources(file).
There is no job level config information available as well.</p>

<p>My Requirments is very simple to get different files generated from the jenkins build artifact to splunk with different sourcetypes.
Where the Jenkins is configured as Master -> Slave setup.</p>

<p>Is this possible in the Jenkin's Splunk plug-in app, or should I go with different approach?</p>

<p>Many thanks.</p>",58606471.0,1,0,,2019-8-23 05:28:16,,2019-10-29 11:21:38,2019-8-26 03:27:35,,3961280.0,,3098236.0,,1,0,jenkins|splunk,245,9
360,258816,57625220,"Three items (Current value, trend and YTD) in single panel in Splunk","<p>We've setup a new Splunk dashboard and I'm looking to improve the trend graphs/panels.</p>

<p>We now have three panels each telling us something about a specific type of event. These panels are shown in the screenshot below:</p>

<p><a href=""https://i.stack.imgur.com/egNT2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/egNT2.png"" alt=""Three separate panels""></a></p>

<p>Top to bottom, </p>

<ul>
<li>the first shows the number of events in the current rolling period (of 30 days).</li>
<li>The second shows a trend graph of the number of events every day over the past 3 months.</li>
<li>The third and last shows the number of events in this year to the current data (YTD).</li>
</ul>

<p>I'd like to clean this up and combine it into one panel. Ideally it would look something like:</p>

<p><a href=""https://i.stack.imgur.com/a7bPQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a7bPQ.png"" alt=""Data combined into panel""></a></p>

<p>I've been messing with CSS files, but that is not a sustainable solution. 
Also I've found the Splunk documentation on how to <a href=""https://docs.splunk.com/Documentation/Splunk/7.3.1/Viz/SingleValueGenerate"" rel=""nofollow noreferrer"">Generate a single value</a>. This give me part of what I'm looking for. But this lacks the Year to Data value and I need the trend line to be more prominent.</p>

<p><a href=""https://i.stack.imgur.com/aYnaR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aYnaR.png"" alt=""From Splunk, single value with trend but without YTD""></a></p>

<p>Any suggestions to how to make this work?</p>",,2,0,,2019-8-23 11:14:37,,2021-2-23 16:13:48,2021-2-23 16:13:48,,4418.0,,3257806.0,,1,0,splunk|splunk-dashboard,205,9
361,258817,57654613,How to use Splunk-Logging with NodeJS,"<p><br/>
I want to write Lambda Application for AWS Lambda with NodeJS. <br/>
I install forward dependencies.<br/>
- serverless<br/>
- serverless-offline --save-dev<br/>
- splunk-logging --save<br/>
- aws-sam-local<br/></p>

<p>I also install Splubk-Enterprise Light Version in my local computer.<br/>
My problem is, nodejs is working, splunk is working, lamda function is working good. Bud when I tried to logging in slunk locall I get error.</p>

<pre><code>const SplunkLogger = require('splunk-logging').Logger;
const serverConfig = require('./server_config');
const loggerConfig = {
//url: process.env.SPLUNK_HEC_URL,
//token: process.env.SPLUNK_HEC_TOKEN,
//url: ""http://localhost:8088"",
url: ""http://127.0.0.1:8089"",
token: serverConfig.token,
batchInterval: 1000,
maxBatchCount: 10, // Manually flush events
maxRetries: 3,    // Retry 3 times
maxBatchSize: 1024 // 1 Kb
};
const logger = new SplunkLogger(loggerConfig);
module.exports.f_data = async (event, context, callback) =&gt; {
    let body = JSON.parse(event.body);
    console.log(""Body -&gt;:"", body);
    console.log(body.processingTime);
let count = 0;
let arrayOCode = [""fpd"", ""kka"", ""lma"", ""traw""];
let data = {};
if( arrayOCode.includes(body.progressID) ) {
    console.log(""Request Data:"", JSON.stringify(body, null, 2));
    console.log(""Body clusterCode:"", body.clusterCode);
    data.progressID = body.progressID;
    data.fNumber = body.fNumber;
    data.fPrevious = body.fPrevious;
    data.stateCode = body.stateCode;
    data.processingTime = body.processingTime;
    let cluster = {};
    cluster.clusterCode = body.clusterCode;
    data.cluster = cluster;
    let ta = {};
    ta.aa = body.aa;
    ta.bdt = body.bdt;
    ta.bwl = body.bwl;
    ta.cu = body.cu;
    ta.est = body.est;
    data.ta = ta;
    let wb = {};
    wb.bma = body.bma;
    wb.bk = body.bk;
    wb.bt = body.bt;
    wb.cbm = body.cbm;
    data.wb = wb;
}
let payload = {
    message: data,
    metadata: {
    host: 'serverless',
    source: serverConfig.source,
    sourcetype: serverConfig.sourcetype,
    index: serverConfig.spunk_md_index,
    time: body.processingTime
},
severity: ""info""
};
console.log(""Payload Information_____"", JSON.stringify(payload, null, 2));
logger.send(payload);
console.log(""_____Data_____"", JSON.stringify(data, null, 2));
// Send all the events in a single batch to Splunk
logger.flush((err, resp, body) =&gt; {
if(err || (body &amp;&amp; body.code !== 0)) {
} else {
// If succeeded body will be { text: 'Success', code: 0 }
console.log(""Response from Splunk"", body);
console.log(`Successfully processed: ${count} record(s).`);
callback(null, count);
}
});
};

const configureLogger = (context, callback) =&gt; {
    // Override SplunkLogger default formatter
    loggerConfig.eventFormatter = (event) =&gt; {
    if(typeof event == 'object' &amp;&amp; !Object.hasOwnProperty.call(event: 'awsRequestId')) {
        event.awsRequestId = context.awsRequestId;
    }
    return event;
};
// Set common error handler for logger.send() and logger.flush()
logger.error = (error, payload) =&gt; {
    console.log('error', error, 'context', payload);
    callback(error);
};
};

const checkIfObjectKeytyExists = (obj, name) =&gt; {
    if(obj.hasOwnProperty(name)) {
        return true;
    }
    return false;
};

const isEmpty = (obj) =&gt; {
    if(Object.entries(obj).length === 0 &amp;&amp; obj.constructor === Object) {
        return true;
    }
    return false;
};

module.exports.logger = (event, context, callback) =&gt; {
    // print out the event information on the console (so that we can see it in the CloudWatch logs)
    console.log(`The following happend in the DynamoDB database table ""users"":\n${JSON.stringify(event.Records[0].dynamodb, null, 2)}`);
    callback(null, { event });
};
</code></pre>

<p>server_config.json</p>

<pre><code>{
    ""token"": &lt;myToken&gt;,
    ""url"": ""http://localhost:8088/services/collector"",
    ""spunk_index_1"": &lt;myIndex1&gt;,
    ""source"": ""f_data"",
    ""sourcetype"": ""httpevent"",
}
</code></pre>

<p>When I use port 8065 I get an Error<br/>
    ERROR: { Error: Unexpected response from Splunk. Request body was: This resource can be found at <a href=""http://127.0.0.1:8065/en-US/services/collector/event/1.0"" rel=""nofollow noreferrer""><a href=""http://127.0.0.1:8065/en-US/services/collector/event/1.0"" rel=""nofollow noreferrer"">http://127.0.0.1:8065/en-US/services/collector/event/1.0</a></a>.<br/>
        at Request._callback (D:\Entwicklung\nodejs-projects\DB_CARGO_IOT\node_modules\splunk-logging\splunklogger.js:478:35)<br/>
        at Request.self.callback (D:\Entwicklung\nodejs-projects\DB_CARGO_IOT\node_modules\request\request.js:185:22)<br/>
        at Request.emit (events.js:189:13)<br/>
        at Request.EventEmitter.emit (domain.js:441:20)<br/>
        at Request. (D:\Entwicklung\nodejs-projects\DB_CARGO_IOT\node_modules\request\request.js:1161:10)<br/>
        at Request.emit (events.js:189:13)<br/>
        at Request.EventEmitter.emit (domain.js:441:20)<br/>
        at IncomingMessage. (D:\Entwicklung\nodejs-projects\DB_CARGO_IOT\node_modules\request\request.js:1083:12)<br/>
        at Object.onceWrapper (events.js:277:13)<br/>
        at IncomingMessage.emit (events.js:194:15)<br/>
        at IncomingMessage.EventEmitter.emit (domain.js:441:20)<br/>
        at endReadableNT (_stream_readable.js:1125:12)<br/>
        at process._tickCallback (internal/process/next_tick.js:63:19) code: -1 }  CONTEXT { message: '', severity: 'info', metadata: {} }</p>

<p>Has anyone an idea, why I can not send any log to Splunk?</p>

<p>EDITED<br/>
And if I use port 8088, I get this error:<br/>
    ERROR: { Error: connect ECONNREFUSED 127.0.0.1:8088<br/>
        at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1097:14)<br/>
    errno: 'ECONNREFUSED',<br/>
    code: 'ECONNREFUSED',<br/>
    syscall: 'connect',<br/>
    address: '127.0.0.1',<br/>
    port: 8088 }<br/></p>

<p>Thank you</p>",,0,0,,2019-8-26 08:48:45,,2019-8-29 13:24:47,2019-8-29 13:24:47,,2069036.0,,2069036.0,,1,1,node.js|lambda|splunk|serverless,1553,12
362,258818,57689234,Sorting problem regarding Last Modified Date in Splunk query,"<p>I have a problem regarding sorting in SPLUNK.</p>

<p>I want to make automated reports and I want to sort in a calendar the amount of tickets one day.</p>

<p>A ticket has these time stamps:</p>

<pre><code>ACTUAL_END_DATE=""2018-10-29 01:00:00.0"", 
ACTUAL_START_DATE=""2018-10-29 00:00:00.0"", 
CLOSED_DATE=""2019-06-16 12:56:00.0"",  
COMPLETED_DATE=""2019-06-06 10:47:46.0"",  
EARLIEST_START_DATE=""2018-10-23 11:20:42.0"",  
LAST_MODIFIED_DATE=""2019-06-16 12:56:07.0"",  
RFA_DATE=""2018-10-23 11:20:42.0"", 
RFC_DATE=""2018-10-22 15:19:00.0"",  
SFA_DATE=""2019-06-06 10:47:02.0"", 
SFR_DATE=""2019-06-06 10:46:52.0"",  
SCHEDULED_DATE=""2019-06-06 10:47:06.0"", 
SCHEDULED_END_DATE=""2018-10-29 01:00:00.0"", 
SCHEDULED_START_DATE=""2018-10-29 00:00:00.0"",  
SUBMIT_DATE=""2018-10-22 15:18:53.0"",  
</code></pre>

<p>I sort by two tokens, the earliest is ""@mon"" and the latest is ""now"".</p>

<p>Unfortunately, it sorts by LAST_MODIFIED_DATE and I have 62 tickets in one day. All that have ACTUAL_START_DATE in different months, as you can change a ticket after it closed to add details.</p>

<p>This is my query:</p>

<pre><code> stats latest(STATUS_REASON) as STATUS_REASON latest(CHANGE_REQUEST_STATUS) as CHANGE_REQUEST_STATUS latest(_time) as _time latest(CHANGE_TIMING) as CHANGE_TIMING by INFRASTRUCTURE_CHANGE_ID 
| where CHANGE_REQUEST_STATUS !=""Cancelled"" 
| timechart count span=1D
</code></pre>

<p>How can I sort them and get rid of the count from LAST_MODIFIED_DATE and have them shown by ACTUAL_START_DATE?</p>",57690901.0,1,0,,2019-8-28 09:32:51,,2019-8-28 18:35:51,2019-8-28 18:35:51,,13302.0,,11988280.0,,1,0,splunk,168,8
363,258819,57719076,How to serach specific to date range in Splunk - In my case date modifier is not working for the splunk rest API - services/search/jobs/export?,"<p>I've written python code to download data from splunk for the given search and given date range but it seems date range is not working- I can see logs which are outside of the date that I've entered.</p>

<p>Here is my code snippet:</p>

<pre><code>def download_binary_file(self, url_path, output_file_path, auth, data):
       self.logger.debug(""Entering DatacenterSplunk.download_binary_file() for dc "" + self.datacenter)
       print(""Writing logs to file: "" + output_file_path)
       try:
           s = requests.Session()
           r = s.post(url_path, auth=auth, data=data, stream=True, verify=self.verify_cert)
           r.raise_for_status()
           with open(output_file_path, 'wb') as f:
               for chunk in r.iter_content(chunk_size=512):
                   if chunk:
                       f.write(chunk)
               f.close()
       except Exception as e:
           self.logger.error(""Exception encountered in DatacenterSplunk.download_binary_file():"" + str(e))
           self._handle_exception(e)
       self.logger.debug(""Leaving DatacenterSplunk.download_binary_file() for dc "" + self.datacenter)
</code></pre>

<p>here is the URL and Data that I am passing,</p>

<pre><code>URL : https://example-zone-ms.compnay.com:8089/services/search/jobs/export
data= {'search': 'search source=*FOO_access* http_apikey | fields - host,source,sourcetype, splunk_server, _time, index, _serial', 'output_mode': 'csv', 'earliest': '08/22/2019:0:0:0', 'latest': '08/22/2019:23:59:59'}
</code></pre>

<p>It works fine except, the date range issue, always I am getting last 7 days of log irrespective of the date range I entered. For this range
earliest=08/22/2019:0:0:0 -d latest=08/23/2019:0:0:0
I can getting from Aug 29 - Aug 22</p>",,1,0,,2019-8-30 00:21:27,,2019-8-30 16:40:29,2019-8-30 16:40:29,,6686944.0,,6686944.0,,1,1,json|rest|splunk|splunk-query,890,11
364,258820,57731352,Having trouble getting data from Splunk using rest api,"<p>I am using rest api to get the data from Splunk through command-line -interface. Below is my curl command to get the data.</p>

<pre><code>curl  -ku  admin:pwrd https://localhost:8080/servicesNS/admin/app/search/jobs -d search=""search index=policy sourcetype=smaccess source=""/app/CA/log/smaccess.log"" earliest=-1d | bin _time span=1d | eval date = strftime(_time, ""%m/%d/%Y"") | stats dc(uid) as ActiveUsers by date""

</code></pre>

<p>I'm getting below exception while trying with the above curl command.</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;response&gt;
  &lt;messages&gt;
    &lt;msg type=""ERROR""&gt;Unparsable URI-encoded request data&lt;/msg&gt;
  &lt;/messages&gt;
&lt;/response&gt;

</code></pre>

<p>It was working without this <code>eval date = strftime(_time, ""%m/%d/%Y"")</code>  in the search query.
Can someone please help me with this.</p>

<p>Thanks in advance.</p>",,0,2,,2019-8-30 18:00:18,,2020-9-12 22:27:15,2019-9-2 00:29:21,,11605011.0,,11605011.0,,1,1,api|curl|command-line-interface|splunk,832,11
365,258821,57755291,How to search in multiple indexes with same field,"<p>I want to search in two indexes. I need to extract one field's value from the first index and search for it in the second index, and then I need the count. It's hard to write it down, sorry.</p>

<p>Soo.. something like.:</p>

<pre><code>index=firstIndex someUniqueField=something | rex commonField=someregex | 
</code></pre>

<p>And the other search:</p>

<pre><code>index=secIndex someOtherUniqueField=something2 | commonfield= theRegexedStuff
</code></pre>

<p>And I want this to be in one query and get the count of it. Is it even possible?</p>

<p>One thing to add: 
The second search will usually have less results, since it is an error case. So actually I want a number as a result, where the first search's unique field is there AND for the common field there is a result in the second search AND in the second search the unique field is there. Ugh, cannot really explain it better.</p>

<p>Thanks in advance.</p>",,1,1,,2019-9-2 10:23:41,,2019-9-2 18:02:45,2019-9-2 10:38:31,,7770095.0,,7770095.0,,1,1,splunk,319,10
366,258822,57757733,How to return string and numeric values under 1 column,"<p>I am trying to extract the schema version of customers from a specific application. As per design customers can use any schema number they want. Some of the customers are inserting for example the value 6.0 and others '6.0', when I want to return the number of the results per schema, splunk returns those values as separate columns.
When I use convert or eval (tostring, tonumber) some of the values are not returned.</p>

<p>how can I achieve that none of the data are going to be lost but I will get 1 column for 6.0 and '6.0'?</p>

<p>stats count by customerAccount, schemaVersion | xyseries customerAccount schemaVersion count</p>",57761246.0,1,0,,2019-9-2 13:22:07,,2019-9-3 00:04:09,,,,,10687558.0,,1,0,xml|splunk,25,5
367,258823,57783236,Splunk chart function displaying zero values when trying to round off input,"<p>I have been trying to display a chart in splunk. I uploaded my json data through Splunk HTTP Forwarder and running the query:</p>

<p>After I uploaded the json data, I have got fields such as</p>

<pre><code>""message"":{""acplbuild"":""ACPL 1.20.1"",""coresyncbuild"":""4.3.10.25"",""testregion"":""EU_Stage"",""client"":""EU_Mac"",""date"":""2019-08-27"",""iteration"":""20"",""localCreateTime"":""6.672"",""createSyncTime"":""135.768"",""createSearchTime"":""0.679"",""filetype"":""CPSD"",""filesize"":""690_MB"",""filename"":""690MB_NissPoetry.cpsd"",""operation"":""upload"",""upload_DcxTime"":""133.196"",""upload_manifest_time"":""133.141"",""upload_journal_time"":""1.753"",""upload_coresync_time"":""135.225"",""upload_total_time"":142.44},""severity"":""info""}
</code></pre>

<p>I am trying to run the following query</p>

<pre><code>index=""coresync-ue1"" host=""acpsync_allacpl_7"" message.testregion=EU_STAGE message.client=EU_Mac message.operation=""upload""  |eval roundVal = round(message.upload_total_time, 2) | chart median(roundVal) by message.acplbuild
</code></pre>

<p>I am getting no values. It should display rounded off median values as a chart. Can someone point me if I am doing anything wrong here.</p>",57811390.0,1,1,,2019-9-4 07:11:59,1.0,2019-9-5 18:46:04,2019-9-4 07:18:39,,3487800.0,,3487800.0,,1,1,splunk,90,11
368,258824,57796488,Do not include certain source files,"<p>I have a folder containing all the log files, the filenames are colour-red, colour-green, colour-blue, colour-yellow, etc. I am writing the spl to include all the files except one, e.g. colour-white.</p>

<p>I know the * performs the wildcard search, and [^c] excludes specific character in the bracket. But I don't know how to combine them to exclude a certain word. On the other hand, I am not sure the same regrex rule apply for splunk.</p>

<pre><code>source= ""log/colour-*""
source= ""log/colour-[^w]""
</code></pre>

<p>The desired result of the query is to retrieve all the files, expect colour-white.</p>

<p>Maybe some filters can be applied to retrieve the desired result, but so far the filters I know are for the file contents, not the file names.</p>",57810188.0,2,1,,2019-9-4 22:55:51,,2019-9-5 17:12:45,,,,,7798124.0,,1,0,regex|splunk,34,8
369,258825,57801885,Log messages discarded when logging to Splunk using Serilog,"<p>We have a Windows service that creates a new thread and runs a scheduled task once per day. Logging is done with Serilog and sink is Splunk (""Serilog.Sinks.Splunk""). During a successful run we write eight information messages to the log (Log.Information("""") ). The messages are more or less identical from one run to another, apart from a timestamp and integer values. Four of the messages is logged before the actual job tasks are done and four after.</p>

<p>We have discovered that sometimes all eight messages turn up in Splunk, sometimes only the four last messages (those logged after the time consuming processing has been done) and sometimes none of the messages.</p>

<p>When we add another sink, writing to file (""Serilog.Sinks.File"") we always get all of the eight messages in the file.</p>

<p>Adding Serilog debug logging (Serilog.Debugging.SelfLog.Enable), when log messages are discarded we get the following debug message logged (once - not one per lost message):
""2019-08-30T11:28:03.9029821Z A status code of Forbidden was received when attempting to send to https://&lt;>/services/collector.  The event has been discarded and will not be placed back in the queue.""</p>

<p>Adding a Sleep (System.Threading.Thread.Sleep() ) first thing in the scheduled task we always get the logging done after the Sleep in Splunk, so it seems it takes some time to set up the connection to the Splunk endpoint and any messages sent before the connection is up is just discarded. Since three of the messages are logged by an external nuget package (Hangfire) before the execution enters into our code we frequently lose these three messages and it isn't ideal to have a Sleep() in our code.</p>

<p>Pseudo code (including the Sleep), as I described log messages 1-3 (and 6-8) are written by an external nuget package:</p>

<pre><code>    public Task DoJob()
    {
        var currentRunInformation = new RunInformation();
        try
        {
            System.Threading.Thread.Sleep(3000);
            Log.Information($""Log message 4"");

            //Get Data
            var jobData = GetJobData();
            //Do some calculations
            var calculated = DoCalculations(jobData);

            //Save result
            PersistResult(calculated);

            Log.Information($""Log message 4"");
            return Task.CompletedTask;
        }
        catch (Exception exception)
        {
            Log.Error(exception, $""Error log"");
            return Task.FromException(exception);
        }
    }
</code></pre>

<p>Is there any way we can make the logging wait for an open connection before sending messages? Or any other options to avoid having our logging discarded in an unpredictable manner?</p>",57814652.0,1,0,,2019-9-5 08:57:38,,2021-10-14 20:00:26,2021-10-14 20:00:26,,2756409.0,,760906.0,,1,1,c#|.net-core|splunk|serilog,1395,14
370,258826,57844935,How to get results for individual fields per second,"<p>I have the following query which gives me per second average results for the events. 
Is there a way I can modify it to produce the individual average results for each CLIENT? Thanks.</p>

<pre><code> index=some_some2_idx ns=something app_name=my-api 
 CLIENT IN (Apple, Orange, Banana) API IN (R_GET, T_GET) 
 | timechart span=1s count as requests_per_second | stats avg(requests_per_second)
</code></pre>

<p>This is currently outputting: </p>

<pre><code>avg(requests_per_second)
 18.39494
</code></pre>

<p>I am looking to get results as follows: </p>

<pre><code>Apple     Orange     Banana
 5.1       5.9        7.39
</code></pre>",57846405.0,1,0,,2019-9-8 19:01:27,,2019-9-9 02:42:56,,,,,9401029.0,,1,0,splunk|splunk-query,171,8
371,258827,57849869,How to mask secret values from log files in splunk UI?,"<p>I have my logs coming to /var/logs/messages file in Splunk UI. Now whenever the logs come, it also sends secret and access keys . I want to mask the secret and access keys with something like <code>AWS_SECRET_ACCESS_KEY= gjhsagd#########</code> . How can we update the value in SPLUNK UI.</p>",,1,0,,2019-9-9 07:54:43,,2019-9-9 09:07:54,,,,,9305541.0,,1,0,splunk|splunk-query,151,11
372,258828,57866381,Splunk : tstat count over nested fields using spath and groupby doesnt return result,"<p>On splunk, I have a data set as follows, under say index ""market-list"":</p>

<pre><code>{ Resource: {
        Fruit: mango
        Type: sweet 
       } 
        Attribute: {
         color: yellow
          from: { 
            place: argentina
            continent: southamerica
          } 
       }
    actions: [{ export : yes }]
 }

</code></pre>

<p>I want to use tstat as below to count all resources matching a given fruit, and also groupby multiple fields that are nested.</p>

<p>I tried:</p>

<pre><code>| tstats count | spath | rename ""Resource.Fruit"" as fruitname | search fruitname=mango where index=market-list groupby fruitname Attribute.from.place actions{}.export
</code></pre>

<p>expecting something on the lines of:</p>

<pre><code>| tstats count where ""Resource.Fruit""=mango index=market-list groupby fruitname Attribute.from.place actions{}.export 
</code></pre>

<p>This however does not return any result. Any suggestions on how to use spath with tstats for the above? I tried looking up but wasnt able to get the solution I expected. Thanks.</p>",,1,0,,2019-9-10 07:56:19,,2019-9-10 12:18:43,,,,,2605278.0,,1,0,splunk|splunk-query,380,10
373,258829,57884708,Duplicate field in Splunk Events,"<p>I have a very strange issue, in the same event there are two different values for the same field in the below format a.b.c="""" and a.b.c=""qwe123df"".I need to get the second value but when listing, first value is getting selected which is empty.Is there some way to get the non-empty value for this field? Remember '.' means concatenate in Splunk.I have tried to use rex but no luck.</p>",,1,0,,2019-9-11 08:04:02,,2019-9-11 12:28:53,,,,,5318252.0,,1,0,splunk|splunk-query,1041,13
374,258830,57923815,How to read splunk webhook payload from post request?,"<p>I am trying to get the JSON payload from post request using Splunk webhook as an alert action. My webhook url is <code>http://hostname/api/splunk_alert</code> </p>

<p>My nodejs server listening post request</p>

<pre><code>app.post('/api/splunk_alert', function(request, response){
    console.log(request.body);      // your JSON
     response.send(request.body);    // echo the result back
});
</code></pre>

<p>Can someone please let me know if I am in right direction?</p>

<p>Thanks</p>",,1,0,,2019-9-13 12:38:16,,2019-9-18 12:56:41,2019-9-18 12:56:41,,557657.0,,557657.0,,1,0,webhooks|splunk,1012,12
375,258831,57928682,Splunk - How to get results only if search field contains a word in the lookup table,"<p><strong><em>If I have a search result which has a field named ""Field1"" and It has values like :</em></strong></p>

<p>This is Word1 now.</p>

<p>This is Word2 now.</p>

<p>This is WordX now.</p>

<p>This is WordZ now.</p>

<p><strong><em>Below is the lookup table for Words.</em></strong></p>

<p><strong>Field1</strong></p>

<p>Word1</p>

<p>Word2</p>

<p>Word3</p>

<p>Word4</p>

<p>Word5</p>

<p>Word6</p>

<p><strong><em>How can I search so I get ONLY below results in the output because they contain ""Word1"" and ""Word2"" which are in the lookup table?</em></strong></p>

<p>This is Word1 now.</p>

<p>This is Word2 now.</p>",,1,0,,2019-9-13 18:28:08,,2019-9-15 13:11:19,,,,,255562.0,,1,0,splunk|splunk-query,1016,13
376,258832,58000083,"how to drill down or drill up in a dashboard, based on user selection, in splunk","<p>I am new to splunk and need some help. I need to create a dashboard to drill down and drill up based on selection. For eg, i have a company's org info along with their kpi scores.
If i have to drill down/drill up based on selection, how can i do it?
Here is the sample data for your reference.</p>
<blockquote>
<p>userid    KPI Score      Manager_id     Org_structure <br/></p>
<p>abcd12       4               Manager1             abcd12/Manager1/Director1/VP1/CIO<br/>
abcd34           3                 Manager1            abcd34/Manager1/Director1/VP1/CIO<br/>
abcd56        9         Manager1            abcd56/Manager1/Director1/VP1/CIO<br/>
abcd78           10      Manager1         abcd78/Manager1/Director1/VP1/CIO<br/>
abcd90           8        Manager1         abcd90/Manager1/Director1/VP1/CIO<br/>
user001   7        Manager2         user001/Manager2/Director1/VP1/CIO<br/>
user002    8             Manager2          user002/Manager2/Director1/VP1/CIO<br/>
user003    6       Manager2         user003/Manager2/Director1/VP1/CIO<br/></p>
</blockquote>
<p>Manager1 KPI will be an average of all his reportee's and so on.
Director1 KPI will be an average of all his direct reportees and their reportees. (In this case average of Manager1 and Manager 2 KPI).
Can someone please share how we can do it?</p>
<p>I expect the output of Manager1 KPI followed by other info. If i drill up, it should calculate all. Any idea how we can achieve this?</p>",,1,0,,2019-9-18 20:10:22,,2019-9-19 13:39:14,2020-6-20 09:12:55,,-1.0,,5231298.0,,1,-1,splunk,119,8
377,258833,58011069,Calculate the value of a field based on the values of other fields,"<p>I have a some fields like this:</p>

<pre><code>Group_servers|Name_server|Status**
Group1| server1|OK                
Group1| server2|OK  
Group2| server1|OK  
Group2| server1|No data  
Group2| server1|Yellow
Group2| server1|
</code></pre>

<p>I want to get the result as shown below</p>

<pre><code>Group_servers|Status
Group1|OK                
Group1| No data 
</code></pre>

<p>Сonditions for the formation of status groups are as follows:</p>

<pre><code>1. If at least one server in the group has the status ""No data"" or the field is empty, the status for the group is "" No data"" 
2. If at least one server in the group has the ""Yellow"" status, the status for the group is "" Yellow""
3. If all servers in the group have the status ""OK"", the status for the group is "" OK""
</code></pre>",,1,0,,2019-9-19 12:29:04,,2019-9-19 21:18:10,,,,,12089908.0,,1,1,splunk,30,6
378,258834,58054797,Splunk Alert on missing log with GUID,"<p>I am trying to create a Splunk alert that will be triggered if two events do not occur in a certain time window. The two events will be linked by a GUID and there may be multiple events occurring with different GUIDs simultaneously. </p>

<p>Can someone indicate where to start? </p>",,1,0,,2019-9-23 01:54:04,,2019-9-23 12:08:57,,,,,6290400.0,,1,0,splunk|splunk-query,88,8
379,258835,58062907,Missing forwarder Alert,"<p>DMC Alert - Missing forwarders alert not working properly.</p>

<p>We have provided the query as shown below and getting an output from different host servers and of different dates.I want only to alert when Universal forwarder stops working.</p>

<p>| inputlookup dmc_forwarder_assets | makemv delim="" "" avg_tcp_kbps_sparkline | eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb) | eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline) | eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps) | eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps) | eval forwarder_type = case(forwarder_type == ""full"", ""Heavy Forwarder"", forwarder_type == ""uf"", ""Universal Forwarder"", forwarder_type == ""lwf"", ""Light Forwarder"", 1==1, forwarder_type) | search NOT [| inputlookup dmc_assets | dedup ""servername"" | rename ""servername"" as hostname | fields hostname] status=missing</p>

<p>Expected Result : Get the alert when splunk forwarder stops or failed to sent any logs.</p>",,1,0,,2019-9-23 12:53:03,,2019-9-23 22:42:45,,,,,7206334.0,,1,0,splunk,346,10
380,258836,58087232,Splunk - Stats search count by day with percentage against day-total,"<p>The use-case I have is to provide the count of a certain error (searched by a certain pattern) by day and provide a percentage of such 'errored' requests against the total number of requests (searched without the error pattern) handled every day. Unable to form the appropriate query for it. The base queries are -</p>

<p>Get total counts for each day:</p>

<pre><code>index=my_index | bucket _time span=day | stats count by _time
</code></pre>

<p>Get just errors for each day:</p>

<pre><code>index=my_index ""Error-Search-Pattern"" | bucket _time span=day | stats count by _time
</code></pre>

<p>How do I combine the two counts to show up side-by-side and show the error:total percentage? </p>

<p>Thanks in advance.</p>",,1,0,,2019-9-24 19:46:05,,2019-9-24 23:48:58,2019-9-24 21:38:22,,3961280.0,,5688705.0,,1,2,splunk|splunk-query,2948,14
381,258837,58099556,If statements and extracting values,"<p>I have a result set that looks like </p>

<pre><code>{add=[44961373 (1645499799657512961), 44961374 (1645499799658561538), 44962094 (1645499799659610114), 44962095 (1645499799659610117), 44962096 (1645499799660658689), 44962097 (1645499799660658691), 44962098 (1645499799661707264), 44962099 (1645499799661707267), 44962100 (1645499799662755840), 44962101 (1645499799662755843), ... (592 adds)]}
</code></pre>

<p>If the add=[ array has more than 10 elements in it. Then it will put  (x adds) at the end of the statement to show how many actual adds there were. IF it has less than 10, then it wont put the  (x adds) statement. I am wanting timechart and also single value these outputs to a dashboard(separate modules). </p>

<p>I can get one or the other but I would like to use from logic to figure out which one to report. </p>

<pre><code>index=""index"" host=""host*"" path=/update | eval count=mvcount(add) | stats count
</code></pre>

<p>will get the count of the array</p>

<pre><code>index=""index"" host=""host*""  path=/update | stats sum(Adds)
</code></pre>

<p>will get the value of the (x adds).  Adds is a 'extracted field'. </p>

<p>How do I get either or? If add array >10, use sum(Adds), in the same breath.</p>",58108207.0,1,0,,2019-9-25 13:28:45,,2019-9-26 00:53:29,2019-9-25 13:30:34,,13860.0,,2008240.0,,1,0,regex|extract|splunk|splunk-query,48,7
382,258838,58180201,How to set different target values for different days in Splunk?,"<p>how can I change the target column in different target values you can see in the picture below. 
How is it possible to set different values for the different days?</p>

<p><a href=""https://i.stack.imgur.com/hita1.png"" rel=""nofollow noreferrer"">Result in Splunk with changes</a></p>

<p>index=*************
| bin _time span=1d
| stats count by _time
| eval target = 1000</p>",,1,1,,2019-10-1 07:45:41,,2019-10-2 00:45:46,,,,,10779796.0,,1,0,splunk,57,7
383,258839,58190560,"I am trying to test my splunk HEC token by ending an event to my splunk cloud instance, How can i verify i have the right HEC host name?","<p>I am using the curl command to send an event to Splunk cloud, but I am getting timeout errors and host not found errors. I believe it is the host name that is the problem.</p>

<p>I have tried the ""Server Name"" from the About section on our Splunk cloud UI. I have also tried pinging this server name but that didn't work.</p>

<p>I have tried the same with the URL in my Splunk cloud UI browser, no response from that.</p>

<p><code>curl -v -k https://input-??????:8088/services/collector -H ""Authorization: Splunk #######"" -d '{""sourcetype"": ""_json"", ""event"": ""Hello, world!""}'</code></p>

<p>I expect to see something like <code>{""text"": ""Success"", ""code"": 0}</code> and the event in my Splunk cloud UI.</p>",,1,0,,2019-10-1 18:42:08,,2019-10-2 08:22:13,2019-10-2 08:22:13,,9516173.0,,8066354.0,,1,-1,kubernetes|openshift|splunk|fluentd,230,9
384,258840,58204476,Can a report get manually emailed from splunk,<p>I am testing the email configuration in Splunk and instead of waiting for a scheduled report I would like to know if there is a way to manually push the report via email. </p>,,1,0,,2019-10-2 15:17:11,,2019-10-2 15:25:55,,,,,10891681.0,,1,0,splunk,12,4
385,258841,58214581,extract all occurrences of same field from request body splunk,"<p>I have a same field multiple times in one request body and need to find the value for each occurrence. like subTypeCodeId filed. result should have subTypeCodeId = 2
subTypeCodeId = 3</p>

<pre><code>{
  ""Items"": [
    {
      ""emailId"": ""@stny.com"",
      ""item"": {
        ""subTypeCodeId"": ""2""
      }
    },
    {
      ""emailId"": ""@comcast.com"",
      ""item"": {
        ""subTypeCodeId"": ""3""
      }
    }
  ]
}
</code></pre>

<p>splunk query: index=""gcp_prod_ecomm_cx_wallet"" ""1570081534220"" ""<em>API_NAME:wallet.addItemsToWalletBulk</em>"" |rex ""subTypeCodeId\x5C\"":\x5C\""(?.*)\""""</p>",,1,0,,2019-10-3 07:48:23,,2019-10-3 11:16:06,,,,,5659438.0,,1,0,splunk|splunk-query,116,8
386,258842,58260226,Need help in splunk regex field extraction,"<p>I have a splunk query(<strong>index=sat sourcetype=""sat_logs"" Message=""<em>application message published for</em>""</strong>)  which returns list of messages published by different applications.I need to extract specific field values from the messages.Please let me know the query to get the expected results. Thanks</p>

<p>Splunk query results:
Message:Alpha application message published for UserId: 12345678, UID: 92345678, Date: 2019-10-04, Message: {""Application"":""Alpha"",""ID"":""123""}  </p>

<p>Message:Beta application message published for UserId: 12345670, UID: 92345670,Date: 2019-10-03, Message: {""Application"":""Beta"",""ID"":""623""}   </p>

<p>Message:Zeta application message published for UserId: 12345677, UID: 92345677,Date: 2019-10-02, Message: {""Application"":""Zeta"",""ID"":""523""}   </p>

<p>Expected fields to be extracted and displayed as Table</p>

<p><strong>Application    UserId         UID           ID</strong>
Alpha          12345678       92345678      123</p>

<p>Beta           12345670       92345670      623</p>

<p>Zeta           12345677       92345677      523</p>",,1,0,,2019-10-6 18:39:47,,2019-10-6 23:41:17,,,,,11649645.0,,1,0,splunk|splunk-query,102,8
387,258843,58269635,Splunk extract fields from source,"<p>I have log entries all having a source file like:</p>

<blockquote>
  <p>/openshift/{openshift_container_id}/{openshift_container_name}/{openshift_image_name}/{openshift_pod_name}/{openshift_namespace}.{docker_stream}</p>
</blockquote>

<p>Is it possible to have parts of the log source as fields?
How can this be done?</p>",58276932.0,1,0,,2019-10-7 12:33:57,,2019-10-8 03:20:22,2019-10-7 12:38:44,,1115360.0,,479499.0,,1,0,splunk,112,10
388,258844,58293589,How do you get a Splunk forwarder to work with the main Splunk server?,"<p>I have installed Splunk version 7.3.2 on one server.  I have installed Splunk forwarder version 7.3.2 on a second server.  Both servers are running Ubuntu 18.  </p>

<p>On the Splunk forwarder I ran these commands:</p>

<pre><code>sudo /opt/splunkforwarder/bin/splunk add monitor /var/log/syslog -index main -sourcetype %app%

sudo /opt/splunkforwarder/bin/splunk add forward-server x.x.x.x:9997
</code></pre>

<p>(where x.x.x.x is the IP address of the Splunk server)</p>

<pre><code>sudo /opt/splunk/bin/splunk start
</code></pre>

<p>I tried rebooting both servers and restarting Splunk on both.</p>

<p>From the Splunk dashboard I want to see some indication of the logs from the server with the forwarder.  But I don't see any. I log into the web UI, I go to Settings -> Monitoring Console ->  Indexing -> Indexes and Performance.  There is an ""Instance"" drop down menu.  But the only option I see is the Splunk server.  I do not see the forwarder.</p>

<p>If I go to Data Inputs in the web UI, I cannot click ""Next.""</p>

<p>How can I see some evidence that the Splunk web UI is receiving data from the Splunk forwarder?</p>

<p>I check network connectivity, and nothing is blocking TCP/IP communication between the two.  I would expect to see some forwarded data in Splunk (on the main Splunk server), but I am not seeing that.  What should I do?</p>

<p>Edit: Through the web UI for Splunk I configured a listening port for the forwarding on port 9997.  By running <code>splunk enable listen 9997</code> I get </p>

<blockquote>
  <p>Failed to create. Configuration for port 9997 already exists.</p>
</blockquote>

<p>Update on 10/9/19</p>

<p>Connectivity is configured over port 9997 between the two servers.  I have used nmap to test over this port to and from both servers using internal and external IP addresses.  Nothing is filtered over this port.  Therefore I am quite sure no firewall rule or security mechanism is to blame.  In fact, the back end splunk log for the main splunk server has registered some activity of the Splunk forwarder.</p>

<p>I am trying to index /var/log/* on my Splunk forwarder server.  Here is an excerpt from /opt/splunkforwarder/var/log/splunk/splunkd.log: </p>

<blockquote>
  <p>10-10-2019 00:21:18.059 +0000 INFO  WatchedFile - Will begin reading
  at offset=4835872 for file='/var/log/sampleoct.log'. ... 10-10-2019
  00:22:10.944 +0000 WARN  FileClassifierManager - The file
  '/var/log/.test123.swp' is invalid. Reason: binary. 10-10-2019
  00:22:10.944 +0000 INFO  TailReader - Ignoring file
  '/var/log/.test123.swp' due to: binary 10-10-2019 00:22:10.945 +0000
  WARN  FileClassifierManager - The file '/var/log/.test123.swp' is
  invalid. Reason: binary. ... 10-10-2019 00:30:50.948 +0000 INFO 
  TailReader - Ignoring file
  '/var/log/journal/94b0369aaba948b4b6a6b43288cee7e6/system.journal' due
  to: binary</p>
</blockquote>

<p>In /var/log/ there is a non-binary file of text that I created.  It is 106 KB large.  On the Splunk server, I see no evidence that a second instance (e.g., the Splunk forwarder) is working.</p>

<p>On the Splunk server I see this in /opt/splunk/var/log/splunk/splunkd.log:</p>

<blockquote>
  <p>10-10-2019 00:21:09.930 +0000 WARN  DateParserVerbose - Accepted time
  (Tue Oct  8 16:59:22 2019) is suspiciously far away from the previous
  event's time (Wed Oct  9 23:48:21 2019), but still accepted because it
  was extracted by the same pattern. Context:
  source=/var/log/test123123|host=ip-123-123-123-1|%%app%%|125
  10-10-2019 00:22:31.288 +0000 WARN  AggregatorMiningProcessor -
  Breaking event because limit of 256 has been exceeded -
  data_source=""/var/log/test123"", data_host=""ip-123-123-123-1"",
  data_sourcetype=""%app%"" 10-10-2019 00:22:31.288 +0000 WARN 
  AggregatorMiningProcessor - Changing breaking behavior for event
  stream because MAX_EVENTS (256) was exceeded without a single event
  break. Will set BREAK_ONLY_BEFORE_DATE to False, and unset any
  MUST_NOT_BREAK_BEFORE or MUST_NOT_BREAK_AFTER rules. Typically this
  will amount to treating this data as single-line only. -
  data_source=""/var/log/test123"", data_host=""ip-123-123-123-1"",
  data_sourcetype=""%app%""</p>
</blockquote>

<p>Where in the web UI should I look for this log file?  It seems like it may be indexed, but I cannot find it.</p>",,3,0,,2019-10-8 20:22:43,,2020-6-16 03:32:44,2019-10-10 00:44:34,,10587072.0,,10587072.0,,1,2,splunk,1162,14
389,258845,58330639,How to remove '|' from json export in splunk?,"<p>So I am trying to export through python splunk queries to elasticsearch. I am using the json.dump() feature in python which is working and converting exactly like SPLUNK Web's convert feature. My issue with it though is that it's giving me one field, named _RAW, with pipes '|' information so elastic search doesn't see the individual fields yet clumps it all together like so:</p>

<pre><code>Data| nameId=""123123"" | exampleID='1234123' | fieldName=""Example"" ....etc
</code></pre>

<p>I want to be able to have a ""data"" field or a ""fieldName"" field not all of that clumped into one big field named ""raw""</p>",,2,0,,2019-10-10 20:29:48,,2019-10-21 18:34:35,,,,,12031516.0,,1,0,python|elasticsearch|splunk,128,10
390,258846,58346428,Break Apart String with Regex,"<p>I have a Splunk query that looks like</p>

<p><code>100.100.100.1 - - [11/Oct/2019:17:49:47 +0000] ""GET /someroute/rest/endpoint?param=2019-09-25 HTTP/1.1""</code></p>

<p>and I would really like to be able to break this apart using only regex so that I can get the </p>

<p><code>GET</code> and <code>/someroute/rest/endpoint</code> as two separate groups. Ignoring everything from the query parameters and on.</p>",58347047.0,1,5,,2019-10-11 18:05:30,,2019-10-11 19:03:01,2019-10-11 18:09:53,,6554544.0,,6554544.0,,1,-1,regex|splunk,29,6
391,258847,58358546,Splunk lookup csv file contains multiple occurrences of items. Need to query these items in an index for each unique time stamp range in csv,"<p>lookup <code>Test2.csv</code> in CSV format where EVENT_ID can have multiple SiteID fields and SiteID can have multiple EVENT_IDs. Only SiteID is a field in the splunk index.</p>

<pre><code>YEAR, SiteID, earliest_date, latest_date, EVENT_ID
2019, AB111, 1560988800, 1562112000, ABSE00350
2019, AB111, 1562198400, 1563321600, ABSE00351
2019, AB111, 1548892800, 1550016000, ABSE00352
2019, AB112, 1548892800, 1550016000, ABSE00352
</code></pre>

<p>I use the lookup to query an index, to calculate a KPI for each row.</p>

<p>Ideal query Output (KPI computed for unique combination of SiteID &amp; EVENT_ID) in index <code>pm_busy_half_hour</code>:</p>

<pre><code>SiteID, KPI, EVENT_ID
AB111, 68.4, ABSE00350
AB111, 74.3, ABSE00351
AB111, 22.1, ABSE00352
AB112, 34.5, ABSE00352
</code></pre>

<p>This is the top of my code, where I do the inputlookup, before proceeding to calculate the KPI from data in the index. However it only gives me a result with a single aggregate of the SiteID, not unique per row as desired.</p>

<pre><code>index=pm_busy_half_hour
[| inputlookup Test2.csv
| rename earliest_date as earliest, latest_date as latest
| table SiteID earliest latest
]
.....
.....
.....
</code></pre>

<p>Please advise</p>",,1,0,,2019-10-12 21:10:10,,2019-10-15 05:50:37,2019-10-15 05:50:37,,9395495.0,,12207291.0,,1,0,csv|datetime|splunk|splunk-query|multiple-entries,366,10
392,258848,58363685,How do ingesting value from xml/pdf/csv to splunk or solr?,"<p>I want to get the value from xml or csv or pdf file via using splunk or solr. How i import these format file into splunk and solr?</p>

<p>I have tried import xml file into solr but it doesnt fit solr schema.
Besides, i have no idea to import file into splunk.</p>",58368985.0,1,1,,2019-10-13 12:43:23,,2019-10-14 01:11:56,,,,,11229775.0,,1,0,xml|solr|splunk,897,11
393,258849,58364875,Splunk cloud Http Event Collector not working,"<p>I created a free Splunk cloud account and trying to push data to the main index of Splunk cloud by exposing HTTP rest endpoints by following <a href=""https://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Data/UsetheHTTPEventCollector"" rel=""nofollow noreferrer"">splunk doc</a></p>

<p>but I am getting a timeout exception for below endpoint</p>

<pre><code>curl -k  https://prd-p-f4txzc7qgv77.cloud.splunk.com:8088/services/collector/event -H ""Authorization: Splunk 07944f24-c69e-42fc-af3c-14035bddb085"" -d '{""event"": ""hello world""}'
</code></pre>

<p>Response :</p>

<pre><code>curl: Failed to connect to prd-p-f4txzc7qgv77.cloud.splunk.com port 8088: Operation timed out

</code></pre>

<p>Please help someone how to push data to Splunk cloud instance index.</p>",,1,0,,2019-10-13 15:11:46,,2019-10-13 15:42:47,,,,,7088245.0,,1,0,splunk,235,9
394,258850,58398918,How to extract field value in splunk,"<p>How to extract field values in Splunk using rex field=_raw </p>

<p><code>logAlias=Overall|logDurationMillis=1298|logTimeStart=2019-10-15_00:01:12.821|logTimeStop=2019-10-15_00:01:14.119|UniqueId=8aa984556db09592016dcd93b5a708ee</code></p>

<p>Path : /var/opt/pivotal/logs
Logfile : file.log</p>

<p>Splunk query:</p>

<p><code>index=main source=""/var/opt/pivotal/logs/file.log*"" 
| rex field=_raw ""logDurationMillis= (?&lt;numbr&gt;\d+)"" 
| where numbr&gt;950 
| stats count(numbr) As FailedFraudAPITxns</code></p>",,0,3,,2019-10-15 16:19:21,,2019-10-15 23:59:54,2019-10-15 23:59:54,,2227420.0,,3512605.0,,1,0,splunk,70,7
395,258851,58408617,Connecting to splunk hosted in azure using splunk sdk in python is giving timeout error,"<p><strong>I have my splunk instance hosted azure and i wanted to connect to it via splunkSDK but it is giving timeout error for 8089 port and 443 it is giving connection reset.</strong></p>

<pre><code>import splunklib.client as client
service = client.connect(host='IP-Address', port=8089,
                   username='admin', password='...')
</code></pre>

<p>I have checked firewall and also tried executing this in azure databricks, still issue is present and HTTPS on 443 is also enabled in azure hosted splunk instance. 
In web browser the ip is accessible via https like.. <a href=""https://ip-address"" rel=""nofollow noreferrer"">https://ip-address</a> But <a href=""http://ip-address:443"" rel=""nofollow noreferrer"">http://ip-address:443</a> is giving no response. Can someone help solve this issue.</p>",,1,0,,2019-10-16 08:10:56,,2019-10-16 08:29:00,,,,,6672384.0,,1,0,python|azure|port|splunk|splunk-sdk,191,9
396,258852,58433471,Using splunk to track memory dumps in sql server?,"<p>I'm a beginner and I am wondering if anyone who uses Splunk to monitor SQL server has successfully set up tracking for memory dumps.</p>

<p>As you may know, when a memory dump occurs in SQL Server, a file is created in the root of the SQL server log directory as a .mdmp or .dmp. All we would like to do is be able to keep track of when this memory dump happens and on what server, as indicated by the existence of these files. However, as far as I know, Splunk would not be able to track these files, since it would be scanning a folder looking for new .dmp files, and not indexing a log file that is then searched on.</p>

<p>We have indexes set up for wineventlog, perfmon, and mssql, but to my knowledge, a SQL server memory dump event is not actually logged in any of the related sources types like the general SQL server error log (a related event might, but it would not indicate itself as being related to a memory dump). I might be wrong about this though, and perhaps someone can correct me that this is logged somewhere common that Splunk would be able to consume.</p>

<p>I have also considered that there is a view (sys.dm_server_memory_dumps) that records these events, but we only know of two ways to get that into splunk. One is to set up a sql agent job that would query that table and output it as a file that splunk can then ingest, or to use the sql db connection plugin with splunk, but this has the issue that as far as I know it doesn't use a connection pool, which is a problem for us.</p>

<p>I am wondering how the community has approached this problem, any input appreciated, thank you!</p>",,0,1,,2019-10-17 13:09:20,,2019-10-17 13:09:20,,,,,12232861.0,,1,1,sql-server|memory|splunk|perfmon,44,6
397,258853,58435471,How to use REX command to extract multiple fields in splunk?,"<p>I want to be able to extract multiple fields in splunk using rex, but I am only able to extract 3 fields, then it stops working. An example of this is: </p>

<pre><code>rex field=_raw ""(?&lt;email&gt;\w+);(?&lt;OrderNumber&gt;\w+);(?&lt;shippingStreet&gt;\w+)"" 
</code></pre>

<p>That expression above shows 3 new fields in splunk which is perfect! but as soon as I add one more field, it doesn't show anything at all. is there a limit of 3 fields with rex? </p>",,1,1,,2019-10-17 14:55:32,,2019-10-18 10:42:54,,,,,12031516.0,,1,0,splunk,2075,13
398,258854,58456197,"Splunk query to get user, saved search name, last time the query ran","<p>From Splunk, I am trying to get the user, saved search name and last time a query ran ?
A single Splunk query will be nice.
I am very new to Splunk and I have tried these queries :-</p>

<pre><code>index=_audit action=search info=granted  search=* 
| search IsNotNull(savedsearch_name)  user!=""splunk-system-user""   
| table  user savedserach_name user search _time
</code></pre>

<p>The above query , is always empty for savesearch_name.</p>",,1,0,,2019-10-18 18:13:45,,2019-10-18 21:25:26,,,,,1921782.0,,1,0,splunk|splunk-query,720,12
399,258855,58499808,How can i display event (row) count in Splunk dashboard panel,"<p>I have created a dashboard panel for one of my SPL query which gives me list of results. For that i want to display the count of entries on the top of that panel.</p>

<p>I tried to set the token in the source XML but it didn't work.</p>

<pre><code>  &lt;label&gt;mytestdashboard&lt;/label&gt;
  &lt;row&gt;
    &lt;panel&gt;
      &lt;title&gt;TotalEvents $countResult$&lt;/title&gt;
      &lt;chart&gt;
        &lt;search&gt;
          &lt;progress&gt;
            &lt;set token=""countResult""&gt;$job.Count$&lt;/set&gt;
          &lt;/progress&gt;
          &lt;query&gt;host=""homework""&lt;/query&gt;
          &lt;earliest&gt;0&lt;/earliest&gt;
          &lt;latest&gt;&lt;/latest&gt;
          &lt;sampleRatio&gt;1&lt;/sampleRatio&gt;
        &lt;/search&gt;
        &lt;option name=""charting.chart""&gt;line&lt;/option&gt;
        &lt;option name=""charting.drilldown""&gt;none&lt;/option&gt;
        &lt;option name=""refresh.display""&gt;preview&lt;/option&gt;
      &lt;/chart&gt;
    &lt;/panel&gt;
  &lt;/row&gt;
&lt;/dashboard&gt;```
</code></pre>",58500069.0,1,0,,2019-10-22 08:13:45,,2019-10-22 08:30:05,,,,,12255716.0,,1,1,splunk,906,13
400,258856,58513943,Is it possible to run a curl command with a splunk dbxquery?,"<p>I am developing a dashboard that connects to Splunk via REST API and displays data on various charts/graphs etc.  In order to get the data I have to make a POST request via curl (node.js).  Everything is working great.  However when I try to make a Post request with a dbxquery, it fails and returns 'fatal dbxquery unknown command.'  I was wondering if anyone had encountered this before.</p>

<pre><code>curl -H 'Authorization: Basic auth token' -k https://devfg.com:8089/services/search/jobs  -d search="" | dbxquery query=\""SELECT count(*) FROM db.table\"" connection=\""connection\"""" -d output_mode=json 

</code></pre>",58514860.0,1,0,,2019-10-23 00:37:21,,2019-10-23 03:02:54,,,,,8761052.0,,1,0,rest|curl|splunk,248,11
401,258857,58516038,Need to get the values from json based on conditions in Splunk SPL,"<p>I have these three entries in an array and i want to get the cidr_ip of only those entries which have from_port = 22 . in this case that is second entry which has 5 cidr_ips</p>

<pre><code>{""grants"": [{""owner_id"": ""376456522198"", ""cidr_ip"": null}, {""owner_id"": ""376456522198"", ""cidr_ip"": null}], ""ipRanges"": """", ""from_port"": null, ""to_port"": null, ""groups"": ""\n "", ""ip_protocol"": ""-1""}
 {""grants"": [{""owner_id"": null, ""cidr_ip"": ""52.59.64.149/32""}, {""owner_id"": null, ""cidr_ip"": ""193.26.194.92/32""}, {""owner_id"": null, ""cidr_ip"": ""182.75.203.18/32""}, {""owner_id"": null, ""cidr_ip"": ""49.207.49.169/32""}, {""owner_id"": null, ""cidr_ip"": ""1.39.182.12/32""}], ""ipRanges"": ""\n "", ""from_port"": ""22"", ""to_port"": ""22"", ""groups"": """", ""ip_protocol"": ""tcp""}
 {""grants"": [{""owner_id"": null, ""cidr_ip"": ""52.59.64.149/32""}, {""owner_id"": null, ""cidr_ip"": ""182.75.203.18/32""}, {""owner_id"": null, ""cidr_ip"": ""193.26.194.92/32""}], ""ipRanges"": ""\n "", ""from_port"": ""3389"", ""to_port"": ""3389"", ""groups"": """", ""ip_protocol"": ""tcp""}
</code></pre>",58532055.0,1,2,,2019-10-23 05:30:05,,2019-10-24 11:46:03,2019-10-24 11:46:03,,9514676.0,,9514676.0,,1,0,amazon-web-services|splunk,185,10
402,258858,58530550,Splunk query to get non matching ID from two query,"<pre><code>Index=* sourcetype=""publisher"" namespace=""app_1"" | table ID message | where message=""published""


Index=* sourcetype=""consumer"" namespace=""app_1"" | table ID message | where message=""consumed""
</code></pre>

<p>I want to display non matching ID by comparing both the query, how can I achieve this.</p>

<p>If query 1 is giving 100 records and query 2 is giving 90 records and all 90 records are present in query 1 them I want to see 10 records which are not present in query 2.</p>",58531814.0,1,0,,2019-10-23 20:28:56,,2019-10-24 01:19:59,2019-10-24 01:19:59,,788833.0,,12265368.0,,1,0,splunk|splunk-query,231,10
403,258859,58555219,How do I send JSON files to Splunk Enterprise from JAVA?,"<p>I start by saying I'm a beginner.
I'm setting up a system where I collect some JSON files, I parse them in JAVA (Spring batch) and the part where I'm stuck is sending these files to the <strong>HTTP EVENT COLLECTOR (HEC)</strong> in Splunk enterprise. I tried crawling the web for some beginner-friendly guides but I couldn't find anything. I want to send POST to the Splunk enterprise with said files, so I can index them after they've been sent.
So far I could only connect to localhost:8089 like this:</p>

<pre><code>HttpService.setSslSecurityProtocol(SSLSecurityProtocol.TLSv1_2);

        ServiceArgs connectionArgs = new ServiceArgs();
        connectionArgs.setHost(""localhost"");
        connectionArgs.setUsername(""AdrianAlter"");
        connectionArgs.setPassword(""mypassword"");
        connectionArgs.setPort(8089);
        connectionArgs.put(""scheme"",""https"");
        // will login and save the session key which gets put in the HTTP Authorization header
        Service splunkService = Service.connect(connectionArgs);
        System.out.println(""Auth Token : "" + splunkService.getToken());

        Job info = splunkService.getJobs().create(""search index=main"");
        System.out.println(""Info: "");
</code></pre>",58585550.0,1,0,,2019-10-25 08:51:47,2.0,2020-12-30 09:21:15,2019-10-25 09:07:08,,12272949.0,,12272949.0,,1,0,java|json|rest|splunk,837,13
404,258860,58591082,how to setup a alert on machine learning toolkit for historical data,"<p>I am working on a splunk  time series forcasting poc and needed to show how splunk send alert when the prediction returns a result above threshold.
the search | inputlookup internet_traffic.csv | timechart span=120min avg(""bits_transferred"") as bits_transferred | eval bits_transferred=round(bits_transferred) , if predicts bits_trasferred above the condition given in alert should send email to mentioned id.
Currently the condition give is per result of the search. 
Kindly let me know how to set up the alert or which condition to setup.</p>",,2,0,,2019-10-28 12:38:16,,2019-10-28 23:24:54,,,,,12285792.0,,1,-1,machine-learning|splunk,54,6
405,258861,58592207,How to get the port number from a log (splunk) by regular expressions?,"<p>How to get the port number from a log (Splunk) by regular expressions:</p>

<pre><code>{""info"":{""seqno"":0,""evtType"":1,""oTime"":null,""links"":null,""id"":""9b0ae9a9-e424-11e9-a309-fd988b74a8c5"",""origin"":null,""relations"":[],""details"":"""",""severity"":5,""time"":1569918148265,""headId"":""9b0ae9a9-e424-11e9-a309-fd988b74a8c5"",""sa"":2},""desc"":{""alertId"":{""desc"":""The network port is down"",""label"":""Link down""},""pointId"":[{""desc"":""Type: openflow\nIP: a.b.c.d"",""label"":""device_name [a.b.c.d]""},{""desc"":"""",""label"":""""},{""desc"":""Network Interfaces"",""label"":""""},{""desc"":"""",""label"":""**eth-0-36**""}]},""id"":{""alertId"":""16"",""component"":1,""pointId"":[""a-b-c-d"",""dev"",""1"",""36""]}}
</code></pre>

<p>Port notation can be different depends on the device: </p>

<p>Eth1/1.2; Eth1/2.500; eth-0-19/4; eth-0-4; Eth1/4</p>

<p>I have tried <code>\W+((?i)Eth....(?-i))\W+</code> but it doesn't work in Splunk.</p>",58592343.0,2,0,,2019-10-28 13:53:48,,2019-10-28 15:40:52,2019-10-28 13:57:09,,418066.0,,10878505.0,,1,0,regex|splunk,75,7
406,258862,58618771,"Splunk Error: ""Could not look up HOME variable""","<p>Please help me to understand what this shell script does:</p>

<pre><code># main
for HOST in ${HOSTS}
do
URI=http://${HOST}:80
count=99
result=`/opt/splunk/bin/splunk search ""index=${INDEX} sourcetype=${SOURCETYPE} SPLUNK_HEALTH_CHECK |stats count"" -earliest_time ${EARLIEST} -latest_time ${LATEST} -uri ${URI} -auth ${USER}:${PASS} -preview F -output csv -timeout ${TIMEOUT} 2&gt;&amp;1 |grep -v count |tr -d '\n' |sed 's/""//g'`
if expr $result : '[0-9]*' &gt; /dev/null 2&gt;&amp;1; then

count=$result
result=""OK""
fi
date +""%Y-%m-%d %T sh=${HOST} status=\""${result}\"" delay_status=$count""
done
</code></pre>

<p>While calling this script from splunk, I get the following error:</p>

<blockquote>
  <p>status=""Could not look up HOME variable.  Auth tokens cannot be
  cached.10"" delay_status=99</p>
</blockquote>",,1,1,,2019-10-30 04:28:42,0.0,2019-10-31 05:57:28,2019-10-31 05:57:28,,6340496.0,,12295046.0,,1,-3,linux|shell|sh|splunk,298,9
407,258863,58622256,Assign a value to the variable in Splunk and use that value in the search,"<p>I have a use-case where I want to set the value to a variable based on the condition and use that variable in the search command.</p>

<p>Example:-
I want to check the condition </p>

<pre><code>    if account_no=818

    then var1=""vpc-06b""

    else var1=""*""
</code></pre>

<p>I tried </p>

<pre><code>...|eval val1=case(acc_no==818,""vpc-06b"",acc_no!=818,""*"")|search vpc_id=val1
</code></pre>

<p>but I am not getting any event. If I am trying </p>

<pre><code>...|search vpc_id=vpc-06b
</code></pre>

<p>then, as a result, I am getting the expected output.</p>",58652167.0,2,0,,2019-10-30 09:31:09,1.0,2019-10-31 22:58:14,2019-10-30 09:33:39,,7490367.0,,9514676.0,,1,3,splunk,3909,18
408,258864,58624693,How to extract in Splunk at index time (with tstats) json field with same child-key from different father-key using regex?,"<p>We have to model a regex in order to extract in Splunk (at index time) some fileds from our event. These fields will be used in search using the <code>tstats</code> command. The regex will be used in a configuration file in Splunk settings (<code>transformation.conf</code>). </p>

<p>The main aspect of the fields we want extract at index time is that they have the same json key but a different father json-key. </p>

<p>Is it possible modelling this extraction using regex?</p>

<p>This is an example of Splunk event having the structure described before (json by the way):</p>

<pre><code>{
   ""info"":{
      ""eventSource"":"""",
      ""sourceType"":""I/O"",
      ""status"":{
         ""code"":"""",
         ""msg"":"""",
         ""msgError"":""""
      },
      ""transactionId"":null,
      ""traceId"":null,
      ""timestampStart"":""2019-05-16T21:30:55.174Z"",
      ""timestampEnd"":""2019-05-16T21:30:55.174Z"",
      ""companyIDCode"":"""",
      ""channelIDCode"":"""",
      ""branchCode"":"""",
      ""searchFields"":{
         ""key_3"":""value"",
         ""key_2"":""value"",
         ""key_1"":""value""
      },
      ""annotation"":{},
      ""caller"":{
         ""id"":"""",
         ""version"":"""",
         ""acronym"":""""
      },
      ""called"":{
         ""id"":"""",
         ""version"":"""",
         ""acronym"":""""
      },
         ""storage"":{
            ""id"":"""",
            ""start"":"""",
            ""end"":""""
         }
      }
   },
   ""headers"":[],
   ""payLoad"":{
      ""input"":{
         ""encoding"":""1024"",
         ""ccsid"":""1024"",
         ""data"":""dati_in""
      },
      ""output"":{
         ""encoding"":""1024"",
         ""ccsid"":""1024"",
         ""data"":""dati_out""
      }
   }
}
</code></pre>

<p>The attended result is something like that:</p>

<ul>
<li>calledid -> aaa</li>
<li>callerversion -> 1</li>
<li>callerid -> bbb</li>
</ul>

<p>We tried something like that</p>

<pre><code>[calledid]
REGEX = (?&lt;=called).*""id"":""(?P&lt;calledid&gt;.*?)(?="")
FORMAT = calledid::""$1""
WRITE_META = true
</code></pre>

<p>but it  dowsn't work cause it matches until the last id he finds. Such as:</p>

<pre><code>"":{""id"":"""",""version"":"""",""acronym"":""""},""storage"":{""id"":""
</code></pre>

<p>Thanks in advance.</p>",,0,9,,2019-10-30 11:47:31,,2019-10-31 08:18:35,2019-10-31 08:18:35,,4178286.0,,4178286.0,,1,0,regex|splunk,32,6
409,258865,58629311,Setting a specific time interval in Splunk dashboard to get results from,"<p>I am currently trying to set up a splunk search query in a dashboard that checks a specific time interval. The job I am trying to set it up with runs three times a day. Once at 6am, once at 12:20pm, and once at 16:20(4:20pm). Currently the query just searches for the latest time and sets the background as to whether it received an error or not, but the users wanted the three times it runs per day to be displayed seperately so now I need to set up an interval of time for each of the three panels to display and I have tried a lot of things with no luck(I am new to splunk so I have been just randomly trying different syntax).</p>

<p>I have tried using a search command |search Time>6:00:00 Time&lt;7:00:00 and also tried using other commands that happen before the stats command that gets the latest time with no luck and I'm just stuck at this point and have no clue what to try.</p>

<p><strong>I have my index at the top here but don't think its necessary to show.</strong><br>
| rex field=_raw "".+EVENT:\s(?\S+)\s.+STATUS:\s(?\S+)\s.+JOB:\s(?\S+)""
| stats latest(_time) as Time by status 
| eval Time=strftime(Time, ""%H:%M:%S"") stats Time>6:00:00 Time&lt;7:00:00 
| sort 1 - Time 
| table Time status
| append [| makeresults | eval Time=""06:10:00""]
| eval range = case(status=""FAILURE"", ""severe"", status=""SUCCESS"", ""low"", 1==1, ""guarded"")
| head 1</p>",,1,0,,2019-10-30 15:56:25,,2020-7-14 16:41:19,,,,,8687327.0,,1,0,time|intervals|splunk-query,317,10
410,258866,58639390,How to extract in Splunk at indexed time json field with same child-key from different father-key using regex?,"<p>We have to model a regex in order to extract in Splunk (at index time) some fileds from our event. These fields will be used in search using the <code>tstats</code> command. The regex will be used in a configuration file in Splunk settings (<code>transformation.conf</code>). </p>

<p>The main aspect of the fields we want extract at index time is that they have the same json key but a different father json-key. </p>

<p>Is it possible modelling this extraction using regex?</p>

<p>This is an example of Splunk event having the structure described before (json by the way):</p>

<pre><code>{
   ""info"":{
      ""eventSource"":"""",
      ""sourceType"":""I/O"",
      ""status"":{
         ""code"":"""",
         ""msg"":"""",
         ""msgError"":""""
      },
      ""transactionId"":null,
      ""traceId"":null,
      ""timestampStart"":""2019-05-16T21:30:55.174Z"",
      ""timestampEnd"":""2019-05-16T21:30:55.174Z"",
      ""companyIDCode"":"""",
      ""channelIDCode"":"""",
      ""branchCode"":"""",
      ""searchFields"":{
         ""key_3"":""value"",
         ""key_2"":""value"",
         ""key_1"":""value""
      },
      ""annotation"":{},
      ""caller"":{
         ""id"":"""",
         ""version"":"""",
         ""acronym"":""""
      },
      ""called"":{
         ""id"":"""",
         ""version"":"""",
         ""acronym"":""""
      },
         ""storage"":{
            ""id"":"""",
            ""start"":"""",
            ""end"":""""
         }
      }
   },
   ""headers"":[],
   ""payLoad"":{
      ""input"":{
         ""encoding"":""1024"",
         ""ccsid"":""1024"",
         ""data"":""dati_in""
      },
      ""output"":{
         ""encoding"":""1024"",
         ""ccsid"":""1024"",
         ""data"":""dati_out""
      }
   }
}
</code></pre>

<p>The attended result is something like that:</p>

<ul>
<li>calledid -> aaa</li>
<li>callerversion -> 1</li>
<li>callerid -> bbb</li>
</ul>

<p>We tried something like that</p>

<pre><code>[calledid]
REGEX = (?&lt;=called).*""id"":""(?P&lt;calledid&gt;.*?)(?="")
FORMAT = calledid::""$1""
WRITE_META = true
</code></pre>

<p>but it  dowsn't work cause it matches until the last id he finds. Such as:</p>

<pre><code>"":{""id"":"""",""version"":"""",""acronym"":""""},""storage"":{""id"":""
</code></pre>

<p>Thanks in advance.</p>",,0,8,,2019-10-31 08:31:13,,2019-10-31 08:31:13,,,,,4178286.0,,1,0,json|regex|splunk,18,5
411,258867,58644451,How do I group rows by time unless there is a significant gap?,"<p>Problem domain: I (well, the business) have a wifi network that accepts public connections. We want to know how long each device is staying connected to each of the access points (AP). This is known as 'dwell time'. The problem is complicated as a device can, and usually does, move around between APs during the day and will often come back to many of them more than once.</p>

<p>We currently use Splunk as our data capture and reporting tool and it does this auto-magically but we are considering a move to AWS and so will need to re-form everything using a combination of ETL and SQL.</p>

<p>I have data that looks like the below:</p>

<pre><code>rowID clientMAC apMAC timeSeen
 100      1       a   12:01
 101      1       a   12:03
 102      1       a   12:05
 103      1       b   12:10
 104      1       b   12:20
 105      2       a   12:20
 106      2       a   12:22
 107      1       a   13:00
 108      1       a   13:02
 109      1       a   13:06
 110      1       a   13:12
</code></pre>

<p>My challenge is to report the duration of each example of clientAP+macAP, so for example, how long was <code>clientMAC=1</code> connected to <code>apMAC=a</code>.</p>

<p>I can't take the final <code>timeSeen</code> from the initial <code>timeSeen</code> as <code>clientMAC=1</code> connects to <code>apMAC=b</code> in the middle, so the result would include the time of that connection too.</p>

<p>The plain english logic of what I need to do is:</p>

<p>For each grouping of <code>clientMAC</code> and <code>apMAC</code>, determine the connection duration during the chosen time period. If there is a gap of, say 15 minutes between rows that are of the same combination, start a new duration calculation and close off the old one. Essentially, each set of a given <code>clientMAC</code> being seen at a given <code>apMAC</code> should be a seperate 'transaction' and reported as a single line.</p>

<p>So the desired output is something like:</p>

<pre><code>clientMAC apMAC Duration
    1      a      ...
    1      b      ...
    2      a      ...
    1      a      ...
</code></pre>",58672858.0,2,2,,2019-10-31 13:30:45,,2019-11-3 08:38:52,,,,,5907227.0,,1,1,sql|group-by|splunk|splunk-query,66,7
412,258868,58648724,How can I reliably get logs from Event Hub to Splunk,"<p>I want to guarantee delivery of messages from Event Hub to Splunk with no duplicates</p>

<p>I was looking into using Azure Functions because I like the serverless aspect of it, but it seems that Azure Functions progress the Event Hub checkpoint even if the function completes with errors. This would cause a loss of messages if our Splunk instance is down for an extended period of time (API Upgrades, or other unforeseen issues)
The retry policy on azure function triggers on Event Hub events also don't seem to be configurable</p>

<p>Are there other solutions to reliably get messages in Event Hub to Splunk?</p>",,1,0,,2019-10-31 17:43:02,,2019-10-31 22:53:48,,,,,12300408.0,,1,0,azure|azure-functions|splunk|azure-eventhub,874,11
413,258869,58650684,Splunk regex to match part of url string,"<p>I'm trying to use Splunk to search for all base path instances of a specific url (and maybe plot it on a chart afterwards).</p>

<p>Here are some example urls and the part I want to match for:</p>

<pre class=""lang-sh prettyprint-override""><code>http://some-url.com/first/  # match ""first""
http://some-url.com/first/second/ # match ""first""
http://some-url.com/first/second/third/  # match ""first""
</code></pre>

<p>Here's the regex I'm using, which works fine:</p>

<pre><code>http:\/\/some-url\.com\/(.*?)\/
</code></pre>

<p>What should my Splunk search be to extract the desired text? Is this even possible in Splunk?</p>",,2,0,,2019-10-31 20:22:07,,2019-11-1 02:36:02,,,,,6611672.0,,1,0,regex|splunk,4056,14
414,258870,58652759,How to accumulate counts from different searches into one (pie) chart?,"<p>I have 5 different searches I am doing in Splunk where I am getting the count of how many results from that search query.</p>

<p>I've had a look at this thread here:</p>

<p><a href=""https://answers.splunk.com/answers/757081/pie-chart-with-count-from-different-search-criteri.html"" rel=""nofollow noreferrer"">https://answers.splunk.com/answers/757081/pie-chart-with-count-from-different-search-criteri.html</a></p>

<p>but its not quite working for me, I'm not 100% sure if its what I want.</p>

<p>My search queries all look something like this:</p>

<p>index=A variable=""foo"" message=""Created*"" | stats count</p>

<p>index=A variable=""foo"" message=""Deleted*"" | stats count</p>

<p>I ideally want to assign each query to a keyword - such as created, deleted, etc, then do a pie chart based on the counts.</p>",58652808.0,1,0,,2019-11-1 00:30:58,,2019-11-1 02:33:00,,,,,12069869.0,,1,0,splunk|splunk-query,81,8
415,258871,58702084,How to create visualization from splunk stats in python?,"<p>I'm working on creating a slackbot that would talk to Splunk and get the trends/stats in visualization format.
I'm looking for some documentation/relevant links to understand how can we create a visualization from Splunk stats using Python.</p>",58706271.0,1,0,,2019-11-4 22:38:57,,2019-11-5 23:04:03,2019-11-5 23:04:03,user9755712,,user9755712,,,1,0,python|matplotlib|statistics|visualization|splunk,60,7
416,258872,58705501,Invalid key in stanza in Splunk,"<p>I'm getting the following error while starting splunk</p>

<pre><code>Invalid key in stanza [lmpool:auto_generated_pool_free] in /opt/splunk/etc/system/local/server.conf, line 25: defaultGroup  (value:  splunkssl).
</code></pre>",,1,0,,2019-11-5 06:17:49,,2019-11-5 10:19:29,,,,,11608960.0,,1,0,splunk,677,11
417,258873,58705525,How to use boto3 in splunk Enterprise,"<p>I am trying to create a custom app for my usecases in splunk. One of my usecase is to get some data from AWS for which I already have a working code written in python and I am using boto3 SDK. The same code I was trying in Splunk and it didn't work because Splunk doesn't have information of boto3. Any suggestions please.</p>

<p>The sample code is here.</p>

<pre><code>import boto3
import json

def ec2_client():
    client = boto3.client('ec2')
    """""" :type : pyboto3.ec2 """"""
    return client;

def perform_ec2_operation():
    ec2_interested_details = ec2_client().describe_instances()
    #print(ec2_interested_details)
    return ec2_interested_details;

if __name__ == '__main__':
    data = perform_ec2_operation()
    data = data['Reservations'][0]['Instances'][0]['NetworkInterfaces'][0]['Association']
    data = json.dumps(data);
    print(data)
</code></pre>",58720647.0,1,0,,2019-11-5 06:19:47,,2019-11-5 22:51:32,2019-11-5 09:10:12,,9514676.0,,9514676.0,,1,0,amazon-web-services|boto3|splunk,298,9
418,258874,58753413,Splunk Python SDK API job.results limited to 50k results. Trying to set an offset to pull multiple chunks of 50k but don't know how to get it to work,"<p>I have a job who's job['resultCount'] is 367k, but no matter what I do, I can't seem to pull more than the first 50,000 chunk. </p>

<p>I read this chunk of code off of an answer here for someone who had a similar end goal and setup: <a href=""https://answers.splunk.com/answers/114045/python-sdk-results-resultsreader-extremely-slow.html"" rel=""nofollow noreferrer"">https://answers.splunk.com/answers/114045/python-sdk-results-resultsreader-extremely-slow.html</a></p>

<pre><code>rs = job.results(count=maxRecords, offset=self._offset)
results.ResultsReader(io.BufferedReader(ResponseReaderWrapper(rs)))
</code></pre>

<p>I wrote the below code around that and I've fiddled around a bit with it, but I can't get offset=self._offset to do anything and I have no idea what it's supposed to be doing. </p>

<pre><code>class SplunkConnector(object):
def __init__(self, username, password, customerGuid):
    self.username = username
    self.password = password
    self.customerGuid = customerGuid
    flag = True
    while flag:
        try:
            self.service = client.connect(host=*****, port=8089, username=self.username, password=self.password, scheme='https')
            flag = False
        except binding.HTTPError as e:
            json_log.debug(str(e))

def search(self, query_dict):
    query = query_dict['search']
    label = query_dict['label']
    search_headers = query_dict['headers']
    customer = query_dict['customer']
    customerGuid = query_dict['customerGuid']
    try:
        earliest_time = query_dict['earliest_time']
        latest_time = query_dict['latest_time']
    except KeyError:
        earliest_time = '-1d@d'
        latest_time = '@d'
    json_log.debug('Starting %s customerGuid=%s' % (label, self.customerGuid))
    kwargs_normalsearch = {'exec_mode': 'normal', 'earliest_time': earliest_time, 'latest_time': latest_time, 'output_mode': 'csv'}
    job = self.service.jobs.create(query + ' | fillnull value=""---""', **kwargs_normalsearch)
    while True:
        try:
            while not job.is_ready():
                pass
            stats = {""isDone"": job[""isDone""],
                     ""label"": label,
                     ""customer"": customer,
                     ""customerGuid"": customerGuid,
                     ""doneProgress"": float(job[""doneProgress""]) * 100,
                     ""scanCount"": int(job[""scanCount""]),
                     ""eventCount"": int(job[""eventCount""]),
                     ""resultCount"": int(job[""resultCount""])}

            json_log.debug(stats)

            if stats[""isDone""] == ""1"":
                json_log.debug(""\n\nDone!\n\n"")
                break
            sleep(2)
            stats = {""isDone"": job[""isDone""],
                     ""label"": label,
                     ""customer"": customer,
                     ""customerGuid"": customerGuid,
                     ""doneProgress"": float(job[""doneProgress""]) * 100}

            json_log.debug(stats)

            if stats[""isDone""] == ""1"":
                json_log.debug('Search %s finished for customerGuid=%s'
                               % (label, customerGuid))
                break
            sleep(2)

        except binding.HTTPError as e:
            json_log.debug(str(e))
            pass
        except AttributeError:
            stats = {""isDone"": job[""isDone""],
                     ""label"": label,
                     ""customer"": customer,
                     ""customerGuid"": customerGuid,
                     ""doneProgress"": float(job[""doneProgress""]) * 100}

            json_log.debug(stats)

            if stats[""isDone""] == ""1"":
                json_log.debug('Search %s finished for customerGuid=%s'
                               % (label, customerGuid))
                break
            sleep(2)

    # Get the results and display them
    result_count = job['resultCount']
    rs = job.results(count=0)
    rr = results.ResultsReader(io.BufferedReader(rs))
    results_list = []
    for result in rr:
        if isinstance(result, results.Message):
            # Diagnostic messages may be returned in the results
            json_log.debug('%s: %s label=%s customerGuid=%s'
                           % (result.type, result.message, label, customerGuid))
        elif isinstance(result, dict):
            # Normal events are returned as dicts
            keys, values = [], []

            for header in search_headers:
                if header not in result.keys():
                    print(header)
                    result[header] = ''

            for key, value in result.items():
                if key in search_headers:
                    keys.append(str(key))
                    values.append(str(value))
            if not results_list == []:
                results_list.append(values)
            else:
                results_list.append(keys)
                results_list.append(values)

    output = io.BytesIO()
    writer = csv.writer(output, delimiter=',')
    writer.writerows(results_list)
    output_string = output.getvalue()
    assert rr.is_preview is False

    job.cancel()
    return [label, output_string.replace('\r\n', '\n').replace('---', '')]

    def searches(self, query_list):
        print(query_list)
        if type(query_list) == dict:
            query_list = [value for value in query_list.values()]
        with closing(ThreadPool(processes=len(query_list))) as pool:
            results = pool.map(self.search, query_list)
            pool.terminate()

        print(results)
        search_results = {item[0]: item[1] for item in results}
        print(search_results)
        return search_results
</code></pre>",58774373.0,1,0,,2019-11-7 16:44:24,,2019-11-8 22:10:28,2019-11-8 22:10:28,,7507925.0,,7507925.0,,1,1,python-3.x|python-2.7|api|splunk|splunk-sdk,697,14
419,258875,58769562,Splunk migration to S3 DataLake,"<p>We're looking at moving away from Splunk as our datastore and looking at AWS Data Lake backed by S3.</p>

<p>What would be the process of migrating data from Splunk to S3?  I've read lots of documents talking about archiving data from Splunk to S3 but not sure if this archives the data as a usable format OR if its in some archive format that needs to be restored to splunk itself?</p>",,2,1,,2019-11-8 15:34:10,,2019-11-15 20:57:45,,,,,4473858.0,,1,1,amazon-web-services|amazon-s3|splunk|data-lake,515,13
420,258876,58772448,Search string with dynamic value in Splunk,"<p>I have data in <code>splunk</code> like this:</p>

<pre><code>DEBUG demp.find - [Local-Log] Parameters name/city/isActive/: 3/XYZ/true/false
DEBUG demp.find - [Local-Log] Parameters name/city/isActive/: 4/AHJGS/true/false
DEBUG demp.find - [Local-Log] Parameters name/city/isActive/: 3/AJJ/true/false
DEBUG demp.find - [Local-Log] Parameters name/city/isActive/: 6/XYZ/true/false
DEBUG demp.find - [Local-Log] Parameters name/city/isActive/: 3/XYZ/true/false
DEBUG demp.find - [Local-Log] Parameters name/city/isActive/: 6/123/true/false
DEBUG demp.find - [Local-Log] Parameters name/city/isActive/: 3/HJG/true/false
</code></pre>

<p>I want to get the this result from a <code>splunk</code> query where <code>name</code> value is dynamic, I dont care about anything after that</p>

<p>like: <code>Parameters name/city/isActive/: {regex here to get name value}</code> and save that in new values to user further. Kindly guide me for this.</p>

<p>Thanks</p>",58774710.0,1,0,,2019-11-8 19:06:42,,2019-11-8 22:53:04,2019-11-8 19:14:37,,2185295.0,,4120870.0,,1,0,splunk,189,11
421,258877,58772682,Spring Actuator Metrics generate logs,"<p>I'm trying to get micrometer metrics data to Splunk. Each metric endpoint gives the current value of a metric, so I would need Splunk to send a http request to my application periodically, or I can write the metric values to a log file periodically. </p>

<p>So how do I get my application to write the metric values to logs?</p>",58772830.0,1,0,,2019-11-8 19:28:03,,2019-11-8 19:40:37,,,,,4104760.0,,1,0,spring|spring-mvc|splunk|micrometer|spring-micrometer,2176,20
422,258878,58784773,With Python Splunk SDK stop raise HTTPError(response) for 504 responses,"<p>Our Splunk environment sometimes throws 504 errors when we query via the API, which has been a known issue for a while. We recently moved a connector to using the Python sdk over a hacky script we had. In our API based one, we were able to catch the 504 errors that pop up randomly and loop back and try again when they happen. With the Python SDK, the best I've been able to find out to do is this:</p>

<pre><code>flag = True
while flag:
    try:
        job = self.service.jobs.(create|results|any jobs./job. call)(query, **kwargs_normalsearch)
        flag = False
    except binding.HTTPError:
        print('Splunk 504 Error')
    pass
pass
</code></pre>

<p>But this has a lot of problems, especially since if any other errors besides a 504 pop up that would actually indicate a real problem, I'm going to be stuck in a never ending loop. If there's a parameter or something for the jobs/job object that allows for ignoring specific HTTP errors? I'd really like to stop wrapping these calls in silly while True loops. If not is there a way to extract the status from the exception that's raised and at least be able to handle different status codes in different ways?</p>",,1,0,,2019-11-10 00:26:37,,2019-11-10 00:36:05,,,,,7507925.0,,1,0,python|python-2.7|python-requests|splunk|http-error,138,8
423,258879,58798145,Detect scheduled tasks with sysmon,"<p>How can I detect running scheduled tasks with sysmon in splunk?</p>

<p>There is a scheduled task running and I don't know since when it started how can I detect the scheduled task and when it first started?</p>

<p>I know that EventID 106 – stands for ""new scheduled job"" but is there a event id or something in the message that tells me that a process is comes from a scheduled task?</p>

<p>Thank you in advance</p>",,1,0,,2019-11-11 08:51:54,,2019-11-14 16:35:45,,,,,10493895.0,,1,0,scheduled-tasks|splunk,812,11
424,258880,58824840,Flume sink to Splunk?,"<p>Has anyone had success sinking data from flume to splunk?</p>

<p>I've tried the Thrift and Avro flume sinks, but they have issues.  Not really great formats for splunk, and flume keeps trying events over and over again after they've been sunk.</p>

<p>I'm looking into the flume HTTP sink to splunk's HEC, but I can't see how to set the HEC token in the header.  Has anyone configured the HEC token in header for flume http sink?</p>

<p>Considering just doing a file sink that is forwarded to Splunk, but would like to avoid this temporary file if possible.</p>

<p>Advice?</p>",59037614.0,1,0,,2019-11-12 18:44:57,,2019-11-25 17:58:18,,,,,2917835.0,,1,0,splunk|flume|flume-ng,104,8
425,258881,58847111,Alternative to subsearch to search more than million entries,"<p>Hi I have a sub search command which gives me the required results but is dead slow in doing so. I am having more than a million log entries that i need to search which is the reason why i am looking for an optimized solution. I have gone through answers asked for similar questions but not able to achieve what i need</p>

<p>I have a log which has transactions against an entry_id which always has a main entry and may or may not have subEntry</p>

<p>I want to find the count of version number for all the mainEntry log which has a subEntry</p>

<p>sample Query that i used</p>

<pre><code>index=index_a [search index=index_a ENTRY_FIELD=""subEntry""| fields Entry_ID] Entry_FIELD=""mainEntry"" | stats count by version
</code></pre>

<p>Sample data</p>

<pre><code>Index=index_a
1) Entry_ID=abcd Entry_FIELD=""mainEntry"" version=1
Entry_ID=abcd ENTRY_FIELD=""subEntry""
2)Entry_ID=1234 Entry_FIELD=""mainEntry"" version=1
3)Entry_ID=xyz Entry_FIELD=""mainEntry"" version=2
4)Entry_ID=lmnop Entry_FIELD=""mainEntry"" version=1
Entry_ID=lmnop ENTRY_FIELD=""subEntry""
5)Entry_ID=ab123 Entry_FIELD=""mainEntry"" version=3
Entry_ID=ab123 ENTRY_FIELD=""subEntry""
</code></pre>

<p>Please help in optimizing this</p>",58848105.0,1,0,,2019-11-13 23:22:24,,2020-4-22 16:35:46,,,,,1831420.0,,1,0,splunk,136,10
426,258882,58862934,Does Cron have a way to include all except one case,"<p><strong>Goal</strong> </p>

<p>Create a Cron expression that will run a task at 2 pm and 4 am every day to run a Splunk alert</p>

<p>Except for only run the 2 pm task on Thursday (don't run the task a 4 am on Thursday).</p>

<p><strong>Question</strong> </p>

<p>Is this an expression that can be represented in a single expression? (if so how).  </p>",,2,0,,2019-11-14 17:49:04,,2019-11-17 22:36:10,,,,,3808982.0,,1,0,cron|splunk,52,9
427,258883,58883082,PCRE: Optionally capture after last occurrence of character in the middle of a string,"<p>How can I modify this regex: </p>

<pre class=""lang-regex prettyprint-override""><code>(?:code: |handler[ :]+(?:\w+?code: )?)(?&lt;signature&gt;.+?(?= :| and)).+?: (?&lt;message&gt;.+?(?= :| +&gt;&gt;&gt;|$))
</code></pre>

<p>Such that <strong>iff</strong> the <code>signature</code> group is a stack-trace; I.E. dot-separated-path, like <code>App.Parent.Child.Method</code>; I only get <code>Method</code>? Currently, this expression works perfectly for all of my error messages, except in these stack-trace cases where I don't need the entire thing (I'm currently getting all of <code>App.Parent.Child.Method</code>).</p>

<hr>

<p>I've looked around, and all the examples I've seen rely on either starting a group with a known starting string, or anchoring to the start of the line. However, my string is in the middle of a longer string, and I also don't always know what it will be made of, these aren't really an option. I also can't use any code, since this is running as part of a Splunk search query / field extraction.</p>

<p>Here is an example of what I'm trying to capture from:</p>

<pre><code>&lt;&lt;&lt; WebContainer : [trace].WealthClientProfileService: Error code: TD.EBS.WCA.004.TEDS0001 : transaction failed. Error Level= 10  &gt;&gt;&gt;
</code></pre>

<p>I need to capture <strong>only</strong> ""<code>TEDS0001</code>"" from ""<code>TD.EBS.WCA.004.TEDS0001</code>"". However, since some of my error messages look like:</p>

<pre><code>&lt;&lt;&lt; WebContainer : [trace].Handler: ::Error from ISM with ID: 20178 and message: Client Information Not Found  &gt;&gt;&gt;
</code></pre>

<p>(in which case I am after the whole ""<code>Error from ISM with ID: 20178</code>""), I need this modification to only limit my capture group <strong>iff</strong> it has <code>.</code> in it. I feel like it's so simple but I just can't get it.</p>",58884383.0,1,0,,2019-11-15 18:56:10,,2019-11-15 20:40:04,,,,,10549827.0,,1,0,regex|pcre|splunk,41,6
428,258884,58915438,Searching for specific values in Splunk query,"<p>In this following query</p>

<pre><code>index=sne host=nwbsnep* sourcetype=sne_CAS_elilogs OR sourcetype=sne_CMS_elilogs ""Service.Operation""=""*""
</code></pre>

<p>Service Operation fetches 36 values out of which I need specific 7.</p>

<p>For this, I wrote another query</p>

<pre><code>index=sne host=nwbsnep* (sourcetype=sne_CAS_elilogs OR sourcetype=sne_CMS_elilogs) (""Service.Operation""=""A"" OR ""Service.Operation""=""B"" OR ""Service.Operation""=""C"" OR ""Service.Operation""=""D ""OR ""Service.Operation""=""E"" OR ""Service.Operation""=""F"" OR ""Service.Operation""=""G"" OR ""Service.Operation""=""H"")
</code></pre>

<p>This query is returning events of only <code>Service.Operation=F</code> and not of all the 7 events. Is there some mistake I am making with boolean expressions?</p>",,0,2,,2019-11-18 13:14:05,,2019-12-13 05:24:45,2019-12-13 05:24:45,,4842242.0,,8237369.0,,1,0,splunk|splunk-query,21,5
429,258885,58925210,DockerHub tag for running Splunk on CentOS 7,"<p>Does anyone know the tag I should use for running Splunk on CentOS 7? According to the following GitHub link, this host OS is supported:
<a href=""https://github.com/splunk/docker-splunk/tree/develop/base/centos-7"" rel=""nofollow noreferrer"">https://github.com/splunk/docker-splunk/tree/develop/base/centos-7</a></p>

<p>I've searched DockerHub (<a href=""https://hub.docker.com/r/splunk/splunk"" rel=""nofollow noreferrer"">https://hub.docker.com/r/splunk/splunk</a>), but the closest tag I found is listed below:
splunk/splunk:7.2.9-redhat</p>",,1,0,,2019-11-19 01:02:37,,2019-11-19 03:43:47,,,,,12394674.0,,1,0,dockerfile|centos7|splunk,40,6
430,258886,58940050,Splunk Rex: Extracting fields of a string to a value,"<p>I'm a newbie to SPlunk trying to do some dashboards and need help in extracting fields of a particular variable</p>

<p>Here in my case i want to extract only KB_List"":""KB000119050,KB000119026,KB000119036"" values to a column </p>

<pre><code>Expected output:

KB_List
KB000119050,KB000119026,KB000119036
</code></pre>

<p>i have tried:</p>

<pre><code>| rex field=_raw ""\*""KB_List"":(?&lt;KB_List&gt;\d+)\*""
</code></pre>

<p>highlighted the part below in the log </p>

<blockquote>
  <p>svc_log_ERROR"",""Impact"":4.0,""CategoryId"":""94296c474f356a0009019ffd0210c738"",""hasKBList"":""true"",""lastNumOfAlerts"":1,""splunkURL"":false,""impactedInstances"":"""",""highestSeverity"":""Minor"",""Source"":""hsym-plyfss01"",""reqEmail"":""true"",""AlertGroup"":""TIBCOP"",""reqPage"":"""",""KB_List"":""KB000119050,KB000119026,KB000119036"",""reqTicket"":""true"",""autoTicket"":true,""SupportGroup"":""TESTPP"",""Environment"":""UAT"",""Urgency"":4.0,""AssetId"":""AST000000000159689"",""LiveSupportGroup"":""TESTPP"",""sentPageTo"":""TESTPP""},""Notification"":{"""":{""requestId"":""532938335""}},"""":</p>
</blockquote>",,2,0,,2019-11-19 17:50:03,,2019-11-20 19:36:43,2019-11-20 19:36:43,,5001308.0,,5001308.0,,1,1,splunk|splunk-query|splunk-formula|rex|splunk-sum,5286,17
431,258887,58942580,Read from splunk source and write to topic - writing same record. not pulling latest records,"<p>same record is being written to topic. not pulling latest records from splunk. time parameters are set in start method to pull last one min data. Any inputs.</p>

<p>currently i dont set offset from source. when poll is run every time, does it look for source offset and then poll? in logs can we have time as offset.</p>

<pre><code>@Override
public List&lt;SourceRecord&gt; poll() throws InterruptedException {
    List&lt;SourceRecord&gt; results = new ArrayList&lt;&gt;();
    Map&lt;String, String&gt; recordProperties = new HashMap&lt;String, String&gt;();
    while (true) {
        try {
            String line = null;                
            InputStream stream = job.getResults(previewArgs);
            String earlierKey = null;
            String value = null;                                
            ResultsReaderCsv csv = new ResultsReaderCsv(stream);
            HashMap&lt;String, String&gt; event;    
            while ((event = csv.getNextEvent()) != null) {
                for (String key: event.keySet())   {                
                    if(key.equals(""rawlogs"")){
                        recordProperties.put(""rawlogs"", event.get(key));                                                        results.add(extractRecord(Splunklog.SplunkLogSchema(), line, recordProperties));
                        return results;}}}
            csv.close();
            stream.close();
            Thread.sleep(500);
        } catch(Exception ex) {
            System.out.println(""Exception occurred : "" + ex);
        }
    }
}
private SourceRecord extractRecord(Schema schema, String line, Map&lt;String, String&gt; recordProperties) {
    Map&lt;String, String&gt; sourcePartition = Collections.singletonMap(FILENAME_FIELD, FILENAME);       
    Map&lt;String, String&gt; sourceOffset = Collections.singletonMap(POSITION_FIELD, recordProperties.get(OFFSET_KEY));
    return new SourceRecord(sourcePartition, sourceOffset, TOPIC_NAME, schema, recordProperties);        
}

@Override
public void start(Map&lt;String, String&gt; properties) {
    try {
        config = new SplunkSourceTaskConfig(properties);
    } catch (ConfigException e) {
          throw new ConnectException(""Couldn't start SplunkSourceTask due to configuration error"", e);
    }
    HttpService.setSslSecurityProtocol(SSLSecurityProtocol.TLSv1_2);
    Service service = new Service(""splnkip"", port);
    String credentials = ""user:pwd"";
    String basicAuthHeader = Base64.encode(credentials.getBytes());
    service.setToken(""Basic "" + basicAuthHeader);       
    String startOffset = readOffset();
    JobArgs jobArgs = new JobArgs();
    if (startOffset != null) {
        log.info(""-------------------------------task OFFSET!NULL "");
        jobArgs.setExecutionMode(JobArgs.ExecutionMode.BLOCKING);
        jobArgs.setSearchMode(JobArgs.SearchMode.NORMAL);
        jobArgs.setEarliestTime(startOffset);
        jobArgs.setLatestTime(""now"");
        jobArgs.setStatusBuckets(300);
    } else {
        log.info(""-------------------------------task OFFSET=NULL "");
        jobArgs.setExecutionMode(JobArgs.ExecutionMode.BLOCKING);
        jobArgs.setSearchMode(JobArgs.SearchMode.NORMAL);
        jobArgs.setEarliestTime(""+419m"");
        jobArgs.setLatestTime(""+420m"");
        jobArgs.setStatusBuckets(300);
    }

    String mySearch = ""search host=search query"";
    job = service.search(mySearch, jobArgs);        
    while (!job.isReady()) {
        try {
            Thread.sleep(500);
        } catch (InterruptedException ex) {
            log.error(""Exception occurred while waiting for job to start: "" + ex);
        }
    }        
    previewArgs = new JobResultsPreviewArgs();
    previewArgs.put(""output_mode"", ""csv"");        
    stop = new AtomicBoolean(false);
}
</code></pre>",,0,10,,2019-11-19 20:49:24,,2019-11-22 03:24:45,2019-11-22 03:24:45,,2308683.0,,5381952.0,,1,0,java|apache-kafka|apache-kafka-connect|splunk,104,8
432,258888,58951670,How to count the total number of events in a splunk search result?,"<pre><code>index=apigee headers.flow_name=getOrderDetails  
| rename content.orderId as ""Order ID""
| table  ""Order ID"" 
| dedup ""Order ID""
</code></pre>

<p>I wish to count how many unique order IDs are received in the result.  </p>

<p>Appreciate any help!</p>",,1,1,,2019-11-20 10:07:42,,2019-12-5 18:24:38,2019-12-5 18:24:38,,3405171.0,,9510831.0,,1,0,splunk|splunk-query,1869,14
433,258889,58962680,"Extract this specific pattern using regEx ( ""message"":""TransactionRefNo =====> 37010072"") from splunk events using query?","<p>Below is the splunk event:</p>

<p>,""class"":""com.tmobile.supplychain.inventoryreceive.service.impl.ReceiveNotificationServiceImpl"",""sessionid"":""41870177"",""requesttimestamp"":""2019-11-20T07:18:04Z"",""message"":""TransactionRefNo =====> 37010072 CassandraGemfireWriteException in insertReceiveNotification RMA-105 | Error occurred while persisting receivenotification payload into Cassandra DB - Receive Notification.</p>

<p>I am trying to capture the Transref numbers for all the events in a splunk using regex. I am using this below query but it's not working.</p>

<pre><code>(index=scs_det sourcetype=scs_det_apps cf_org_name=""retail-inventory-serialization"" cf_space_name=production cf_app_name=""*"" cf_app_name=""soa-receive-service"" ""RMA-105"" 
| rex field=_raw ""transactionRefNo:(?&lt;transaction_ref_no&gt;\d+)"" 
| table _time transaction_ref_no 

index=scs_det sourcetype=scs_det_apps cf_org_name=""retail-inventory-serialization"" cf_space_name=production cf_app_name=""*"" cf_app_name=""soa-receive-service"" ""RMA-105"" 
| rex field=_raw ""message:(?&lt;transaction_ref_no&gt;\w+)"" 
| table _time transaction_ref_no 

index=scs_det sourcetype=scs_det_apps cf_org_name=""retail-inventory-serialization"" cf_space_name=production cf_app_name=""*"" cf_app_name=""soa-receive-service"" ""RMA-105"" 
| rex field=_raw ""message:(?&lt;Message:^.................................$&gt;) "" 
| table _time Message 
</code></pre>

<p>All are not working. Could you please some one help me how do i capture the trasref numbers using regex.</p>",,1,0,,2019-11-20 20:17:45,,2020-4-22 16:51:49,2020-4-22 16:51:49,,4418.0,,12406248.0,,1,0,splunk,110,9
434,258890,58978073,How to convert getStartMillis to normal time format in Selenium (Posting results to Splunk from Selenium code through jenkins),"<p>I am using this ITestResult interface and making use of methods within this. But I am running into a problem where StartTime and EndTime are showing up in a weird format like EndTime: 1574335356061 and    <strong>StartTime: 1574334748190</strong> in my splunk report. Same is the case with Total time   <strong>TotalTime: 607871</strong>.</p>

<p>Is there a way I can get Start time, End time and totak time in a normal format.</p>

<p>Below is the code I have written.</p>

<pre><code>public static void postLabTestResult(ITestResult test) {
        String testResult = test.getStatus()==1 ? ""PASS"" : ""FAIL"" ;
        String testFailReason = test.getThrowable() != null ? test.getThrowable().toString() : ""NotDefined"";
        SimpleDateFormat dateTimeFormat = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss"");
        String dateTime = dateTimeFormat.format(new Date());
        StringBuilder Entry = new StringBuilder();
        DesktopOSType desktopOS = TestRun.getOS();
        Entry.append(""{\""DateTime\"": "" + ""\"""" + dateTime + ""\"", "");
        Entry.append(""\""StartTime\"": "" + ""\"""" + test.getStartMillis() + ""\"", "");
        Entry.append(""\""EndTime\"": "" + ""\"""" + test.getEndMillis() + ""\"", "");
        Entry.append(""\""TotalTime\"": "" + ""\"""" + (test.getEndMillis() - test.getStartMillis()) + ""\"", "");
        Entry.append(""\""LabTestFailReason\"": "" + ""\"""" + testFailReason.replace(""\n"", """").replace(""\t"", """") + ""\""}]}"");

        //post to splunk
        Logger.logMessage(""LabTestData posted:  ""+ SplunkManager.postEvent(ConfigProps.SPLUNK_INDEX, Entry.toString()));
    }
</code></pre>

<p>The splunk report shows something like this </p>

<pre><code>   EndTime: 1574335356061
   LabTestFailReason: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
   LabTestName: verifyLoginTest
   LabTestResult: FAIL
   StartTime: 1574334748190
   TotalTime: 607871
</code></pre>

<p>Appreciate the help</p>",58978839.0,1,0,,2019-11-21 15:11:41,,2019-11-21 16:02:38,,,,,6323064.0,,1,0,selenium|jenkins|splunk,148,8
435,258891,58978673,Splunk: how to query nested values,"<p>I have a log below and I want to get the value of Description under :- Calling Checklist1003
How do I do that ??</p>

<pre><code>Message type: SBAWF13Info Code: 1001 dec, 3e9 hex
11/21/2019 09:21:53.297 Fault type: Application Severity: Info
11/21/2019 09:21:53.297 Description: This is a resubmission of a case that was underwritten using the
11/21/2019 09:21:53.297 UW_10.30 KB engine 
11/21/2019 09:21:53.297 
11/21/2019 09:21:53.297 UWROUTER service will be used for underwriting
11/21/2019 09:21:53.297 ----------------------------------------------------------------
11/21/2019 09:21:53.297 Message type: SBAWF13Info Code: 1001 dec, 3e9 hex
11/21/2019 09:21:53.297 Fault type: Application Severity: Info
11/21/2019 09:21:53.297 Description: This case will be underwritten using UWROUTER 1.0 
11/21/2019 09:21:53.297 
11/21/2019 09:21:53.297 **Calling Checklist1003**
11/21/2019 09:21:53.345 ----------------------------------------------------------------
11/21/2019 09:21:53.345 Message type: Code: 118310 dec, 1ce26 hex
11/21/2019 09:21:53.345 Fault type: Undefined Severity: Undefined
11/21/2019 09:21:53.345 **Description**: Hired From Date is missing for secondary employment for 
11/21/2019 09:21:53.345 applicant . 
11/21/2019 09:21:53.345 
11/21/2019 09:21:53.358 -----------------------------------------
</code></pre>",,1,0,,2019-11-21 15:44:23,,2019-11-21 20:29:31,2019-11-21 15:54:01,,3124333.0,,803860.0,,1,0,splunk-query,113,8
436,258892,58980589,Splunk Regex: Unable to extract data,"<p>what am I doing wrong here??</p>

<p>Splunk QUERY:</p>

<p>index=du sourcetype=""du:sbaservice-log""  du_service=""dugovt4.0""  ""ERROR="" | rex field=_raw </p>

<p>""INFO\=>CaseFileID\s+(?.*)"" | rex field=_raw ""INFO\=>Envelope\InstID\s\=\s(?</p>

<p>instID>\d+)""| rex field=_raw ""lenderCaseNo\s[(?\d+)]"" | rex field=_raw </p>

<p>""Originating\sID:\s+(?\S+)"" | rex field=_raw ""SBA\sCommand:\s+(?</p>

<p>\S+)"" | rex field=_raw ""Host:\s+(?\S+)"" | rex field=_raw "" Base\sGUID:\s+(?</p>

<p>\S+)"" | eval BTime = strptime(Begin_time, ""%H:%M:%S.%3N"")  | eval CTime = </p>

<p>strptime(Completion_time, ""%H:%M:%S.%3N"")  | eval ResTime=CTime-BTime</p>

<p>Also I am not getting the value of CASEFILEID data</p>

<p>ERROR:
  I get an error:-Error in 'rex' command: Encountered the following error while compiling the   </p>

<p>regex 'INFO\=>Envelope\InstID\s\=\s(?instID>\d+)': Regex: unrecognized character follows .</p>

<p>DATA in the LOG:</p>

<p>11/21/2019 12:22:01.817  INFO=>Executing workflow...</p>

<p>11/21/2019 12:22:01.817  INFO=><strong>CaseFileID</strong> 1427667459</p>

<p>11/21/2019 12:22:01.817  INFO=>Creating task 1003ToCLDF</p>

<p>11/21/2019 12:22:01.818  <strong>INFO=>Envelope InstID</strong> = 12006</p>",,1,0,,2019-11-21 17:29:04,,2019-11-21 18:17:50,2019-11-21 18:17:10,,803860.0,,803860.0,,1,0,regex|splunk-query,199,9
437,258893,58986326,Splunk Extract a Json Format Field In an Event,"<p>I have an event, and it was not json format but has an json format field <code>git_info</code>, like this</p>

<pre><code>2019-11-22 09:05:34.000, iterationName=""201903"", projectName=""project1"", pkg_id=""16505"", pkg_name=""NGTS_V1.1.9_SIM_20191122.zip"", pkg_size=""84.0 MB"", create_time=""2019-11-22 09:05:34"", git_info=""[
  {
    ""branch"": ""develop"",
    ""repoURL"": ""url1"",
    ""commitId"": ""2c977fa""
  }
]"", upload_type=""auto"", in_dev=""1"", in_snapshot=""0"", in_release=""0""
</code></pre>

<p>I want extract git_info field and stats the number of branch and repo and other statistics. 
How do I accomplish this?</p>",,1,0,,2019-11-22 01:50:28,,2019-11-25 02:39:18,2019-11-22 04:45:22,,1197605.0,,7270421.0,,1,1,json|events|splunk,546,10
438,258894,59001272,Kafka connect producer poll mechansim,"<p>I poll data from splunk logs and write to a topic. Every time poll runs, how does it know it should get new set of records. Do I need to store last read timestamp from logs and store it in sourceoffset and when poll is run next time, do I need to have logic to get data after that timestamp. </p>",,1,0,,2019-11-22 20:25:27,,2020-2-8 15:21:43,2020-2-8 15:20:36,,2308683.0,,5381952.0,,1,0,apache-kafka|apache-kafka-connect|splunk,39,6
439,258895,59033824,Resending Data to REST,"<p>I am <strong>very</strong> new to this and don't have much knowledge of the correct terms..  but I have rest api that sends data to Splunk.  I want to find a way to resend only data that has changed.  For example... I send 30 minutes worth of data to Splunk.  After the data has been sent to Splunk, one minute of the data has been changed (the user has edited the data or updated it of some sort).  I don't want to resend all 30 minutes of the data just to update the one minute that has changed.  Is there a way to only resend the new data? Maybe using a script or something?  </p>

<p>I would really appreciate some insight on this and maybe some example codes if the solution requires writing some code to do this.  </p>

<p>Thanks in advance!</p>",,0,2,,2019-11-25 14:15:12,,2019-11-25 14:15:12,,,,,11824947.0,,1,0,python|rest|splunk|data-retrieval,24,5
440,258896,59034927,Splunk alert if amount of logs for today 10% grater than average for the past three days per service,"<p>I would like to create an alert that is triggered when the number of logs for today 10% greater than average for the past three days per service.</p>

<p>For one specific service I can use next</p>

<pre><code>index=""some-index"" AND ""ctx.endpointname""=""service-name"" earliest=-3d@d |
timechart span=1d count |
timewrap d series=short |
eval threshold=(((s1+s2+s3)/3)+(((s1+s2+s3)/3)*0.1))'
</code></pre>

<p>But when I adding <code>by ctx.endpointname</code></p>

<pre><code>index=""some-index"" AND ""ctx.endpointname""=* earliest=-3d@d |
timechart span=1d count by ctx.endpointname|
timewrap d series=short |
eval threshold=(((s1+s2+s3)/3)+(((s1+s2+s3)/3)*0.1))'
</code></pre>

<p>the result fields names:</p>

<pre><code>service-name_s0
service-name_s1
service-name_s2
service-name_s3
</code></pre>

<p>and I can't calculate the threshold as I don't know how to refer to this field for each service</p>",,1,0,,2019-11-25 15:17:01,,2019-11-25 17:06:15,2019-11-25 17:06:15,,5919326.0,,5919326.0,,1,0,splunk|splunk-query,111,8
441,258897,59038753,Splunk produces a table with only one row,"<p>I am using a splunk dashboard to parse a key-value log file into a table.</p>

<p>This is the search i am using :</p>

<pre><code>...
&lt;row&gt;
    &lt;panel&gt;
      &lt;title&gt;Regdata Recon&lt;/title&gt;
      &lt;table&gt;
        &lt;search&gt;
          &lt;query&gt;REGDATA-RECON reconStartTime
          | eval reconStartTime1=strptime('reconStartTime', ""%Y-%m-%dT%H-%M-%S.%Q"")    &lt;-- this is the format from the log
          | eval reconEndTime1=strptime('reconEndTime', ""%Y-%m-%dT%H-%M-%S.%Q"")
          | eval reconStart=strftime('reconStartTime1', ""%Y-%m-%d %H:%M:%S"")
          | eval reconEnd=strftime('reconEndTime1', ""%Y-%m-%d %H:%M:%S"")
          | fields - reconStartTime1 - reconEndTime1
          | table environment,reconStart,reconEnd,Duration,result 
          | sort reconStart&lt;/query&gt;
          &lt;earliest&gt;$timeRange.earliest$&lt;/earliest&gt;
          &lt;latest&gt;$timeRange.latest$&lt;/latest&gt;
          &lt;sampleRatio&gt;1&lt;/sampleRatio&gt;
        &lt;/search&gt;
        ...
      &lt;/table&gt;
    &lt;/panel&gt;
&lt;/row&gt;
</code></pre>

<p>However, while there are many lines in the log file which satisfy this search, my dashboard gives me a table with only one row. I would like a table with rows for every line like that in the log.</p>

<p>What am i doing wrong ?</p>

<p>UPDATE :</p>

<p>It turns out that splunk views the whole .log file as one event. All the lines in the file are viewed together, instead of separately. </p>

<p>Something is off about uploading the log file to splunk, or about splunk interpreting the log file. (i said that the file should be viewed as sourcetype = log2metrics_keyvalue )</p>

<p>Please help.</p>",59053697.0,2,0,,2019-11-25 19:22:38,,2021-2-23 16:13:36,2021-2-23 16:13:36,,4418.0,,5663107.0,,1,1,splunk|splunk-dashboard,469,12
442,258898,59063320,Possible to integrate github probot with splunk?,"<p>Building my first git app and ... But i'm thinking of a github app that could post to splunk ... or integrate with the splunk github app and ""POST"" events to the system; wondering if this functionality is/ can be supported somehow;</p>

<p>The real question is - can I send selective information about the repository from a git app to splunk ? </p>

<p>Possible ?</p>",,1,0,,2019-11-27 05:04:53,1.0,2019-11-27 10:32:50,,,,,790551.0,,1,1,github|splunk|github-app,121,9
443,258899,59077155,How to compare a value with the number of matches for a second query?,"<pre><code>""daily unique entry text"" 
| spath input=stats 
| where ('expectedCount' != _???_)
</code></pre>

<p>I have one daily unique log entry that states the <code>expectedCount</code> of items that will be processed for that day.</p>

<p>Let's say this daily entry contains the unique text <code>daily unique entry text</code>, and each time an item gets processed successfully, I log <code>item processed</code>.</p>

<p>I'd like an alert that fires if <code>expectedCount</code> is not equal to the number of <code>item processed</code> log entries that follow, in that day. </p>

<p>Can this be accomplished with something at <code>_???_</code>? Or: what's the best way to do this? Thanks in advance!</p>",59077909.0,1,0,,2019-11-27 19:20:20,,2019-11-28 00:02:42,,,,,3735178.0,,1,0,splunk,23,6
444,258900,59104347,How do I install Splunklib for Python 3.7 on Windows?,"<p><code>Splunklib</code> for <code>Python 3.7</code> fails to install on my Windows machine. 
Since <code>pycrypto</code> was not ported to <code>Python 3</code>, I've uninstalled it and installed <code>pycrypodome</code> as replacement. 
Unfortunately, when trying to install <code>splunklib</code>, <code>pip</code> still tries to install <code>pycrypto</code>.</p>

<pre><code>*Installing collected packages: pycrypto, splunklib
Running setup.py install for pycrypto ... *
</code></pre>

<p>Is there a way to force <code>Python</code> or <code>pip</code> to use <code>pycryptodome</code> instead of <code>pycrypto</code>?</p>",59135232.0,1,0,,2019-11-29 11:57:54,2.0,2019-12-11 09:38:39,2019-12-2 09:21:17,,10857616.0,,8319506.0,,1,3,python|python-3.7|splunk|pycrypto|pycryptodome,3864,22
445,258901,59107583,Splunk query for matching lines that do not contain text,"<p>To find logging lines that contain ""gen-application"" I use this search query : </p>

<pre><code>source=""general-access.log"" ""*gen-application*""
</code></pre>

<p>How to amend the query such that lines that do not contain ""gen-application"" are returned ?</p>

<p>source=""general-access.log"" != ""<em>gen-application</em>"" returns error : </p>

<pre><code>Error in 'search' command: Unable to parse the search: Comparator '!=' has an invalid term on the left hand side: 
</code></pre>",59107724.0,1,0,,2019-11-29 15:32:24,,2019-11-29 15:43:43,,,,,470184.0,,1,2,splunk|splunk-query,3701,18
446,258902,59126076,How to aggregate logs to a persistent table in splunk?,"<p>My web application logs every user action.<br>
Every log entry contains the user id, the action (click, double-click etc...), timestamp and a short description.<br>
The logs for a specific user are stored for a few days,
hence I need to aggregate them to a processed report / data.<br>
I want to collect (and eventually display) a specific action (let's say double click)
of each user and its description.</p>

<p>For example, I want a table that gets updated for every log (or a few logs with some delay),<br>
that aggregates the data of a userId, the timestamp of all of his double clicks, the count of double<br>
 clicks and the description for each double click.</p>

<p>How can I solve this?<br>
What tools does splunk offer for something like aggregating log streams that gets removed?</p>",,2,0,,2019-12-1 13:41:36,,2019-12-1 17:27:07,,,,,11197814.0,,1,0,logging|aggregation|splunk,149,9
447,258903,59153959,Generating a report as Table in Splunk,"<p>I am new to very new to splunk. I need to generate a report in tabular format which has data as below.</p>

<pre><code>ClientIP | URL | Count
</code></pre>

<p>The csv file which I have uploaded contains logs as follows -</p>

<pre><code>127.0.0.1 - [25/Nov/2019:07:57:04 +0000] 7792:5011 CF-WRK: 874900139 : 17 ""DELETE https://Test123:5011/resources/1012 ?-"" 200 - 325240
127.0.0.1 - [25/Nov/2019:07:57:04 +0000] 7792:5011 CF-WRK: 874900139 : 19 ""GET https://Test123:5011/resources/697 ?-"" 200 580 151807
127.0.0.1 - [25/Nov/2019:07:57:05 +0000] 7792:5011 CF-WRK: 874900139 : 6 ""GET https://Demo123:5011/resources ?-"" 200 1927 152770
127.0.0.1 - [25/Nov/2019:07:57:06 +0000] 7792:5011 CF-WRK: 874900139 : 3 ""GET https://Demo123:5011/resources/ ?-"" 200 775 98234
</code></pre>

<p>What I want to achieve is to group similar URL together along with client ip and show their count.
So far I was able to list URL along with their count using this command</p>

<pre><code>sourcetype=access*  [search sourcetype=access*   | top URL | table URL]| stats count AS ""Count"" BY URL
</code></pre>

<p>Can someone please hep me here.</p>",,1,1,,2019-12-3 09:31:38,,2019-12-3 20:14:32,,,,,2092445.0,,1,0,splunk,29,5
448,258904,59256828,How to convert custom time into minutes in splunk,"<p>I have a field where it sends the time in either <code>1h3m45s</code> or <code>3m45s</code> format</p>

<p>I want to convert that into minutes to plot a graph out of it.</p>

<p>i have tried </p>

<pre><code>| eval ProcessingTime=replace(replace(replace(LoadTime,""h"","":""), ""m"","":""),""s"","""")
| eval ProcessingTime=strptime(ProcessingTime,""%H:%M:%S"")
</code></pre>

<p>But it gives blank field.</p>

<p>EDIT:</p>

<pre><code>| eval time=""1h34m56s""  | eval ProcessingTime=strftime(strptime(time, ""%Hh%Mm%Ss""),""%H:%M:%S"")|convert dur2sec(ProcessingTime)|  eval TimeinMin=(ProcessingTime)/60  | table time, ProcessingTime,TimeinMin
</code></pre>

<p>This gives the time in Min which I am expecting but whe I have time=34m56s it does not return anything</p>

<p>Could you please help?</p>",,3,0,,2019-12-9 21:21:46,,2019-12-12 22:51:35,2019-12-9 23:29:40,,9340011.0,,9340011.0,,1,0,datetime|splunk|splunk-query,1541,14
449,258905,59353555,Splunkorwarder using ansible,"<p>I would like to monitor multiple logs on the universal forwarder. How can i do this? Also when I set forward-server am running out in error. with Enable boot-start somehow i have to accept license manually to finish up the installation. Any suggestions, please?</p>

<pre><code>- name: connect forward server to Splunk server
  command: ""{{ splunkbin }} add forward-server {{ item }} -auth {{ splunkcreds }}""
  with_items: ""{{ splunkserver }}""
  when: splunkserver is defined
  notify: restart_splunk

- name: Enable Boot Start
  command: ""{{ splunkbin }} enable boot-start""

- name: add temporary monitor to create directory
  command: ""{{ splunkbin }} add monitor /etc/hosts -auth {{ splunkcreds }}""
  notify: restart_splunk
</code></pre>",,1,0,,2019-12-16 09:17:18,,2019-12-18 14:12:57,2019-12-16 09:20:46,,9629107.0,,9629107.0,,1,-1,linux|ansible|debian|splunk,177,7
450,258906,59410262,Export Splunk result 15000 output mail in CSV,"<p>My Splunk result set is giving output of 15000 record (sometimes more then that) but when I set query as an alert to send the result set in CSV file the result is getting limit to 10001 records only.
Can anyone help that how can I get all 15000 record in CSV file in a mail via alert setup.</p>",59423528.0,1,0,,2019-12-19 12:57:15,0.0,2019-12-20 10:16:10,,,,,12265368.0,,1,0,splunk|splunk-query,126,8
451,258907,59412002,Reformat XML data from junk tags using Python,"<p>I am new to Splunk tool, trying to write a code that having a XML data output which is coming from splunk, the output is having junk tags in between and format is also not in order.Is there any way to reformat the XML data using Modular Input splunk in python. </p>",,0,4,,2019-12-19 14:44:46,,2019-12-19 14:44:46,,,,,8774646.0,,1,0,python|xml|splunk,28,5
452,258908,59458400,JSON Combine Array with Array of Strings to get a cohesive name value pair,"<p>My application is utilizing a 3rd party application to obtain data (Splunk). The output that Splunks api endpoint returns is an Array containing all the row headers and an array of strings containing all the row's data. For example</p>

<pre><code>{
    ""fields"":[
        ""appID"",
        ""ApplicationName"",
        ""AppOwner"",
        ""AppOwnerID"",
        ""KnownIPS"",
        ""IP Count"",
        ""KnownFIDS"",
        ""FIDCount"",
        ""LastSeen"",
        ""TotalConnections""],
    ""rows"":[
        [
            ""123456"",
            ""HelloWorld"",
            ""Last,First"",
            ""E12345"",
            ""11.111.11.111,222.22.22.222"",
            ""2"",
            ""A67890,B12345,C67890"",
            ""3"",
            ""2019-12-08"",
            ""47937""
        ]
    ],
    ""id"":0
}
</code></pre>

<p>However I would like my output to be something like</p>

<pre><code>{
    Field[0]:row[0],
    Field[1]:row[1],
    etc..
}
</code></pre>

<p>Right now I am able to display the results on my web page using the following</p>

<pre><code> try {

        ArrayList&lt;String&gt; fieldslist = new ArrayList&lt;String&gt;();

        JSONObject json = new JSONObject(responseString);
        JSONArray fields = json.getJSONArray(""fields"");

        JSONArray jsonArray = json.getJSONArray(""rows""); // JSONArray is from the json.org library
        String[][] arrayOfArrays = new String[jsonArray.length()][];
        for (int i = 0; i &lt; jsonArray.length(); i++) {
            JSONArray innerJsonArray = (JSONArray) jsonArray.get(i);
            String[] stringArray = new String[innerJsonArray.length()];
            for (int j = 0; j &lt; innerJsonArray.length(); j++) {
                stringArray[j] = (String) innerJsonArray.get(j);
            }
            arrayOfArrays[i] = stringArray;
        }



        if (fields != null) {
            int len = fields.length();
            for (int i=0;i&lt;len;i++){
                fieldslist.add(fields.get(i).toString());

            }
        } ;

        appDetail.setFields(fieldslist);
        appDetail.setRows(arrayOfArrays);

    } catch (JSONException e) {
        e.printStackTrace();
    }

    return appDetail;
</code></pre>

<p>And my Model</p>

<pre><code>@JsonProperty(""fields"")
private List&lt;String&gt; fields = new ArrayList&lt;String&gt;();
@JsonProperty(""rows"")
private String[][] rows = new String[i][j];

@JsonProperty(""fields"")
public List&lt;String&gt; getFields() {
    return fields;
}

@JsonProperty(""fields"")
public void setFields(List&lt;String&gt; fields) {
    this.fields = fields;
}

public Model withFields(List&lt;String&gt; fields) {
    this.fields = fields;
    return this;
}

@JsonProperty(""rows"")
public String[][] getRows() {
    return rows;
}

@JsonProperty(""rows"")
public void setRows(String[][] rows) {
    this.rows = rows;
}

public Model withRows(String[][] rows) {
    this.rows = rows;
    return this;
</code></pre>

<p>I know I will have to update my model to properly display the correct results, but I cant seem to get the logic correct within the try catch.</p>",59463305.0,1,2,,2019-12-23 16:20:57,,2019-12-24 02:54:01,,,,,8408625.0,,1,0,java|json|splunk,90,8
453,258909,59530338,How to setup splunk summary index?,"<p>I'm a bit confused with setting up summary index in <code>splunk</code>.<br>
I have an index name <code>index_1</code> which receive logs from my app.<br>
There are much too many logs, and I need to save an aggregation of them.</p>

<p>I have tried setting up the summary index from <a href=""https://docs.splunk.com/Documentation/Splunk/8.0.1/Knowledge/Usesummaryindexing"" rel=""nofollow noreferrer"">here</a> to an index name <code>summary</code>,<br>
but when I search the index there are no log entries.</p>

<p>My search is as follow:</p>

<pre><code>index=index_1 ... level&gt;30
</code></pre>

<p>I couldn't understand when to use the <code>collect</code> command and when setting up from the web ui is enough.</p>",,1,0,,2019-12-30 11:48:30,,2019-12-30 22:20:45,,,,,11197814.0,,1,0,splunk,318,10
454,258910,59572740,"Splunk, how to output kv and tables based on templates","<p>I have a project needs to extract information from 16 input files, do some manipulation, output to a template file. The template format is something as below: The question is what splunk command I shall use to output a file based on template? </p>

<p><strong>Template file format:</strong></p>

<pre><code>field1 = $1
field2 = $2
:
:
fieldN=  $N

column1, column2, column3, ...  columnM
$data11, $data21, $data31,  ...  $dataM1


$data1K, $data2K, $data3K,  ...  $dataMK

</code></pre>",,1,0,,2020-1-3 03:12:53,,2020-1-3 22:21:58,,,,,84592.0,,1,0,splunk,28,6
455,258911,59593662,How to match a keyword against a value and then assign the value accordingly to a variable in Splunk?,"<p>I have a case where I need to monitor primary and backup sensors simultaneously. But before that I need to figure out the primary and backup sensors.</p>

<p>I've something like this :</p>

<pre><code>**sensor**

sensor1

sensor1_backup

sensor2

sensor2_backup

sensor3

sensor3_backup
</code></pre>

<p>I want to check against the sensor to see if its a backup sensor and then need to know whose backup it is.</p>

<p>I tried something like this :</p>

<pre><code>... | eval backup_sensor=if(match(sensor,*backup*,1,0),sensor) 
</code></pre>

<p>But, if match is true I can assign the sensor to backup. But how to assign the sensor to a new field ""primary_sensor"" if it fails. </p>

<p>Also, how can I do to identify whose backup it is.</p>

<p>For ex, I find one backup sensor ""sensor2_backup"", should I again need to use match statement to see if its a backup for sensor2 ?</p>",,1,0,,2020-1-4 18:32:27,,2020-1-4 21:45:36,,,,,5799636.0,,1,0,pattern-matching|conditional-statements|splunk|multiple-monitors,30,5
456,258912,59595838,Howto break text line into multiple events,"<p>I am new to Splunk and I'm trying to play a little bit with source type and the regex setting of it...Let's say I put following events into HEC:</p>

<pre><code>curl -k https://utu:8088/services/collector/event/1.0 -H ""Authorization: Splunk 21755979-ed43-4a1a-8962-e6e45ccf3ccf"" -d '{""event"": ""splunk splunk splunk dog"", ""sourcetype"": ""hec_st""}'

curl -k https://utu:8088/services/collector/event/1.0 -H ""Authorization: Splunk 21755979-ed43-4a1a-8962-e6e45ccf3ccf"" -d '{""event"": ""splunk splunk splunk cat"", ""sourcetype"": ""hec_st""}'
</code></pre>

<p><strong>hec_st</strong> is the source type with regex:</p>

<p>(splunk)\s+</p>

<p>with SHOULD_LINEMERGE=false</p>

<p>Please why mentioned settings doesn't break string ""splunk splunk splunk cat"" into multiple events </p>

<p>splunk<br>
splunk<br>
splunk<br>
cat</p>

<p>I'm able to find this string as one event always. Thanks a lot in advance</p>

<p>T.</p>",,2,1,,2020-1-4 23:49:44,,2020-1-11 21:55:01,,,,,5249424.0,,1,0,splunk,996,11
457,258913,59650275,How to structure a splunk query to generate a count of events where the field is either null or not null?,"<p>I'm working with some access logs that may or may not have a user_name field. I don't need to do anything fancy, I'd just like to generate a single query that returns a stats table containing a count of events where this field is either null or not null. For example, my log is structured like this:</p>

<pre><code>&lt;timestamp&gt;&lt;field1&gt;&lt;field2&gt;&lt;user_name&gt;&lt;field4&gt;
</code></pre>

<p>For anonymous connections, user_name is not logged, so these values are null. I can get all of the non-null values easily enough:</p>

<p><code>&lt;base_query&gt; user_name=""*"" | stats count</code></p>

<p>This gives me a nice table of the non-null user_name field:</p>

<pre><code>count
------
812093
</code></pre>

<p>I can also get a count of the null fields with a little more work, but this seems messy:</p>

<pre><code>&lt;base_query&gt; | fillnull user_name value=NULL| search user_name=NULL | stats count
</code></pre>

<p>And then I get a count of entries with a null user_name field. </p>

<pre><code>count
-----
31215
</code></pre>

<p>However, what I'm really looking for is a single query that combines both of these into a single stats table, ideally:</p>

<pre><code>not_null | null
----------------
812093   | 31215
</code></pre>

<p>Thanks!</p>",59652036.0,1,0,,2020-1-8 16:40:41,,2020-1-9 15:27:09,,,,,591182.0,,1,0,splunk|splunk-query,866,11
458,258914,59668890,How to make a stats count with a if-condition to specific value on the log,"<p>I'm newbie with Splunk and I'm trying make a query to count how many requests have a determinate value, but this counter must be incremented if a specific attribute is on the request.</p>

<p>Example:</p>

<pre><code>2020-01-09 13:51:28,802 INFO  [http-nio-8080-exec-8] class:ControllerV1, UA=[tokyo], GW=[api-gateway-id]
2020-01-09 13:51:31,865 INFO  [http-nio-8080-exec-9] class:ControllerV1, UA=[tokyo], GW=[api-gateway-id]
2020-01-09 13:51:32,922 INFO  [http-nio-8080-exec-10] class:ControllerV1, UA=[tokyo], GW=[api-gateway-id]
2020-01-09 13:51:36,939 INFO  [http-nio-8080-exec-2] class:ControllerV1, UA=[tokyo], GW=null
2020-01-09 13:51:48,614 INFO  [http-nio-8080-exec-1] class:ControllerV1, UA=[new-york], GW=[api-gateway-id]
2020-01-09 13:51:49,266 INFO  [http-nio-8080-exec-3] class:ControllerV1, UA=[new-york], GW=[api-gateway-id]
2020-01-09 13:51:57,533 INFO  [http-nio-8080-exec-4] class:ControllerV1, UA=[helsing], GW=[api-gateway-id]
</code></pre>

<p>For the example above, I must increment the counter if <code>GW != null</code>, so I've three counters, for <strong>tokyo</strong>, <strong>new-york</strong> and <strong>helsing</strong>. The results should be something like:</p>

<pre><code>tokyo | new-york | helsing
  3   |    2     |    1 
</code></pre>

<p>Tried:</p>

<p><code>source=""/logfiles.log"" | rex ""UA=(?&lt;user-agent&gt;\w+)"" | stats count(eval(user-agent=""[tokyo]"")) as TOKYO</code></p>

<p>But returns the error: <code>Error in 'rex' command: Encountered the following error while compiling the regex 'UA=(?&lt;user-agent&gt;\w+)': Regex: syntax error in subpattern name (missing terminator).
</code></p>

<p>I know that can't use <code>-</code> but I must do it and when I remove it, the results keeping null (0 results).</p>",,1,0,,2020-1-9 17:02:11,,2020-1-9 21:41:01,2020-1-9 17:55:43,,5140756.0,,5140756.0,,1,0,splunk|splunk-query,258,9
459,258915,59671036,Splunk - Adding pagination,"<p>I have these logs - </p>

<pre><code>2020-01-09 06:20:03,965 - INFO - field1=1 field2=1554 
field3=100 host=1

2020-01-09 06:25:03,965 - INFO - field1=2.43 field2=1999 
field3=188 host=2

2020-01-09 06:30:03,965 - INFO - field1=3.43 field2=2300 
field3=222 host 1

2020-01-09 06:30:03,965 - INFO - field1=4.43 field2=2200 
field3=201 host 3

2020-01-09 06:30:03,965 - INFO - field1=4.43 field2=2500 
field3=200 host 2
</code></pre>

<p>In splunk, I need to add a pagination table that would have ""host"" as 1st column and the corresponding (""field2""+""field3"") as the 2nd column.</p>

<p>Any suggestions on how to do that?</p>",,1,0,,2020-1-9 19:40:31,,2020-1-9 22:54:28,,,,user9755712,,,1,0,pagination|splunk,40,6
460,258916,59672870,Splunk - Multi-select - Show unique hosts,"<p>I want to populate the list of hosts in the multiselect input option in Splunk. </p>

<p><code>index=someIndexName * host!=""notThis*"" | stats values(host) as host</code></p>

<p>I can see the list of hosts getting populated in Splunk. However, they are not getting populated in multiselect list. It says ""populating"" and nothing shows up. </p>",59673267.0,2,0,,2020-1-9 22:12:38,,2020-1-10 20:00:11,,,,user9755712,,,1,0,splunk,582,11
461,258917,59686865,How to get two fields using rex from log file?,"<p>I'm newbie with Splunk. My goal is take two or more fields from logs. I must check if one field is <code>true</code> and so use another field to make a counter. The counter is about how many requests is make by client using <code>user-agent</code> attribute. </p>

<p>My logic desired:</p>

<pre><code>int count1, count2;
count1 = 0;
count2 = 0;

if (GW == true) {
  if (UA == ""user-agent1"") count1++;
  if (UA == ""user-agent2"") count2++;
}
</code></pre>

<p>At the moment I can get just one field and make a counter without <code>if-condition</code>.</p>

<p>This query works fine, and return the correct requests counter:</p>

<pre><code>source=""logfile.log"" | rex ""UA=(?&lt;ua&gt;\w+)"" | stats count(eval(ua=""user-agent1"")) as USER-AGENT1
</code></pre>

<p>But, when I try get the second field (<code>GW</code>) to make the logic, the query returns <code>0</code>.</p>

<pre><code>source=""logsfile.log"" | rex ""UA=(?&lt;ua&gt;\w+) GW=(?&lt;gw&gt;\w+)"" |stats count(eval(ua=""user-agent1"")) as USER-AGENT1
</code></pre>

<p>So, how I get more fields and how make <code>if-condition</code> on query? </p>

<p>Sample log:</p>

<pre><code>2020-01-10 14:38:44,539 INFO  [http-nio-8080-exec-8] class:ControllerV1, UA=user-agent1, GW=true
2020-01-10 14:23:51,818 INFO  [http-nio-8080-exec-3] class:ControllerV1, UA=user-agent2, GW=true
</code></pre>",59687094.0,1,5,,2020-1-10 18:04:49,,2020-1-10 18:31:13,,,,,5140756.0,,1,0,splunk|splunk-query,45,7
462,258918,59715471,How to do own encryption and decryption on splunk universal forwarder,"<p>I am trying to do custom encryption and decryption of data on the universal forwarders. I am trying to configure the Splunk UF to use own certificates and forward the encrypted data to the third-party system(Java socket). The reason I am doing this is to recover the Splunk event logs to the java socket connection by decrypting the event changelogs. I tried configuring Splunk UF to use my own certificates for encryption and decryption but it doesn't seem to be working when I tried decrypting the data on Java. Surely I am missing something but could not find the reason.</p>

<p>How can I do this on Splunk UF?</p>",59716179.0,1,0,,2020-1-13 11:10:15,,2020-1-13 11:54:38,,,,,12563214.0,,1,0,java|encryption|splunk,193,9
463,258919,59738135,How will an event without a year indicator be indexed?,"<p>I have a task to index a log file where timestamp looks like this: <code>Jan 13 03:43:31.662</code>, there is no year indication. I have no means of changing this, the application generating those logs does not permit it. </p>

<p>So, is Splunk able to append the current year?
Is there going to be a collision between the events in a year's time?</p>",,1,0,,2020-1-14 16:37:51,,2020-1-14 23:43:45,,,,,4855386.0,,1,1,splunk,22,7
464,258920,59743319,How to make searching for error in logs easier in Java?,"<p>Our team has a huge multi-module Java project which contains a whole bunch of duplicated and non-informative messages in logs, which are sometimes hard to find, even using Splunk. So I am wondering, is there any log wrapper or IDE extension which can give a unique ID for each log message? It will help us to search for the appropriate message/error though all the legacy heap of logs.</p>",,1,0,,2020-1-14 23:40:04,,2020-1-15 11:44:59,,,,,4978865.0,,1,0,java|logging|splunk,121,8
465,258921,59750663,Regex in Splunk Log to search,"<p>I have Splunk logs , whose entry looks as below:</p>

<pre><code>15/01/2020
10:34:29.076    
{ [-]
   app_module: testmodule
   environment: XXXX
   level:  INFO
   logger_name:                      project.stats
   message: Query execution time: [1222] app ID: [TEST] for user: [jhhsakjhsa]
   thread_name:    catalina-exec-371
   timestamp: 2020-01-15T05:04:29,076
   x-request-id: hdkhwqkjdhwqhdwqdo908109328182eh
}
Show as raw text
host = &lt;host&gt;,worker, Splunk--idx-d-i-0xssaxx4d0f95a8timestamp = 2020-01-15T05:04:29,076 
</code></pre>

<p>I need to make query where query execution time is in 4 digits. We have query execution time as log statement in message field in splunk log (<strong>Query execution time: [1222]</strong>)</p>

<p>What will be query for same.</p>",,1,0,,2020-1-15 11:38:07,,2020-1-15 20:57:33,,,,,2522819.0,,1,0,splunk|splunk-query,377,10
466,258922,59787161,Python Script - Passing a string parameter fails in requests module( Curl Command),"<p><b>Sample Curl Command:</b><br />
curl ""<a href=""https://www.test.org:8088/services/collector/raw"" rel=""nofollow noreferrer"">https://www.test.org:8088/services/collector/raw</a>"" -kH ""Authorization: Splunk 999"" -d '{""index"":""abc"",""sourcetype"":""dev"",""userName"":""user1"",""lastActiveDate"":1236472051.807}' <br /><br />
<b>Issue :</b> <br/>
1.When data=teststring is given (manually entered value), it works fine.<br />
2.But when data=completeinfowithsyntax is given , it fails to send data.</p>

<p><b>Python Program:</b></p>

<pre><code>import requests
from requests.packages.urllib3.exceptions import InsecureRequestWarning

import calendar, time
from datetime import datetime

import re
import json

def convert_utc_to_epoch(lastActiveDate):
    timestamp = datetime.strptime(lastActiveDate, ""%Y-%m-%dT%H:%M:%S.000+0000"")
    epoch = int(calendar.timegm(timestamp.utctimetuple()))
    return epoch

def getInfo():

    headers = {
    'Authorization': 'adc 12345'
    'Content-Type': 'application/json',
    }

    userName_list , lastActiveDate_list = [] , []

    requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
    response = requests.get('https://somewebsite.org', headers=headers, verify=False)
    Content = response.text

    userName = re.compile(r'.*""userName"":""(.*?)""')
    userName_list = (userName.findall(Content)) if userName else """"

    lastActiveDate = re.compile(r'.*""lastActiveDate"":""(.*?)""')
    lastActiveDate_list = (lastActiveDate.findall(Content)) if lastActiveDate else """"

    lastActiveDate = convert_utc_to_epoch(lastActiveDate_list[0])
    return userName_list[0],lastActiveDate

def sendDataToSplunk():

    headers = {
    'Authorization': 'Splunk 999',
    }

    RequiredData = getInfo()
    print(RequiredData)

    timeinttostr= str(RequiredData[1])
    completeinfo='""index"":""abc"",""sourcetype"":""dev"",""userName"":""'+RequiredData[0]+'"",""lastActiveDate"":'+timeinttostr
    print(completeinfo)
    completeinfowithsyntax=""'{""+completeinfo+""}'""
    print(type(completeinfowithsyntax))
    print(""completeinfowithsyntax"",completeinfowithsyntax)

    teststring='{""index"":""abc"",""sourcetype"":""dev"",""userName"":""user1"",""lastActiveDate"":1579010706}'

    response = requests.post('https://www.test.org:8088/services/collector/raw', headers=headers, data=completeinfowithsyntax, verify=False)
    print(""done"")

sendDataToSplunk()
</code></pre>",,0,9,,2020-1-17 12:14:41,,2020-1-17 15:53:34,2020-1-17 15:53:34,,12220341.0,,12220341.0,,1,0,python|splunk,71,7
467,258923,59800768,Splunk compare two rex queries from different source,"<p>I have two queries...</p>

<p><strong>(1)</strong> submit log query:</p>

<pre><code>index=xxx_prod host=""foo.org"" 5032 submit | rex ""id=PO:(?&lt;PO&gt;\d*)"" | dedup PO | table PO _time
</code></pre>

<p><strong>(2)</strong> saved log query:</p>

<pre><code>index=xxx_prod host=""bar.org"" | rex ""savePO.*POId=(?&lt;PO&gt;\d*).*\""responseCode\"":200"" | dedup PO | table PO _time
</code></pre>

<p>I want to compare both <code>PO</code> and <code>timestamp</code> (from _time field) results from different services one is a submit event, another one is a save event. and I want to show it as the following table</p>

<pre><code>   PO    | submit_date             | save_date               | elapse_time_min | isSave   
 1000001 | 2020-01-18 02:09:49.022 | 2020-01-18 02:51:51q289 | 41              | true
 1000002 | 2020-01-18 03:18:25.780 | 2020-01-18 03:59:08.695 | 49              | true
 1000003 | 2020-01-18 03:18:25.780 |                         |                 | false
</code></pre>",,2,0,,2020-1-18 12:52:51,,2020-1-21 09:27:02,2020-1-18 22:04:32,,13302.0,,2077479.0,,1,0,splunk|splunk-query|rex,141,8
468,258924,59823119,How can I run different tasks on different hosts?,"<p>I was trying to create an ansible playbook in such a way that the playbook first create an EC2 instance using host as local host</p>

<p>After that the instance created using above task must return IP of the new instance and on the newly created instance I wanted to install splunk can someone help me </p>

<pre><code>- hosts: localhost
  connection: local
  gather_facts: False

  tasks:
    - name: create a new ec2 key pair, returns generated private key
      ec2_key:
         name: my_keypair3
         force: false
         region: us-east-1
      register: ec2_key_result

    - name: Save private key
      copy: content=""{{ ec2_key_result.key.private_key }}"" dest=""./akey.pem"" mode=0600
      when: ec2_key_result.changed

    - name: Provision a set of instances
      ec2:
         key_name: my_keypair3
         group: SplunkSecurityGroup
         instance_type: t2.micro
         image: ami-04b9e92b5572fa0d1
         wait: true
         region: us-east-1
         exact_count: 1
         count_tag:
            Name: Demo
         instance_tags:
            Name: v3

    - name: Downloading Splunk
      get_url:
         url: ""https://www.splunk.com/bin/splunk/DownloadActivityServlet?architecture=x86_64&amp;platform=linux&amp;version=8.0.1&amp;product=splunk&amp;filename=splunk-8.0.1-6db836e2fb9e-linux-2.6-amd64.deb&amp;wget=true""
         dest: ~/splunk.deb
         checksum: md5:29723caba24ca791c6d30445f5dfe6
</code></pre>",,1,1,,2020-1-20 12:16:48,,2020-1-20 22:28:25,2020-1-20 22:28:25,,1264804.0,,12747863.0,,1,0,ansible|splunk,59,7
469,258925,59844895,Add values in Splunk if rows match,"<p>I have a splunk query that returns application traffic on a daily basis. </p>

<pre><code>AppID   AppBindID   ServerIP     HostName AppServer tree DailyCount
17159   cn=id1     123.45.678.10   server1  serverA LDAP    3   
17159   cn=id1     123.45.678.10   server1  serverA LDAP    7   
17159   cn=id2     123.45.678.11   server1  serverA LDAP    3
</code></pre>

<p>I want to be able to identify duplicates in the data from columns 1-6 and then add the values from column 7 to make a single unique row.</p>

<p>Desired output:</p>

<pre><code>AppID   AppBindID   ServerIP     HostName AppServer tree DailyCount
17159   cn=id1     123.45.678.10   server1  serverA LDAP    10  
17159   cn=id2     123.45.678.11   server1  serverA LDAP    3
</code></pre>",59849231.0,1,0,,2020-1-21 16:01:18,,2020-1-21 21:07:03,,,,,8408625.0,,1,0,splunk,48,7
470,258926,59863866,Combining 2 queries based on a common value,"<pre><code>1st query
ns=mynamespace* app_name=A-api API=GET_INITIAL_DATA NAME=*

2nd query
ns=mynamespace* app_name=B-api API=GET_FINAL_DATA NAME=*
</code></pre>

<p>I have the above 2 queries. Each is querying a micro service's logs. But I do not want to call them individually and looking to have a single query.
I want to be able to match 1st query against the 2nd query based on name. I am trying to get a % and also total count. Trying to achieve something like the following:</p>

<pre><code>GET_INITIAL_DATA Total count: 10000000
GET_FINAL_DATA count that matched NAME in 1st call : 8000000
Matching call Percentage : 80%
Non Matching call Percentage : 20%
</code></pre>

<p>and show that in a chart divided weekly over a 3 month period. Is there a way to do this? I am expecting millions of records thus it would not make sense for me to make the first query, get all the names (millions of em) and then use that data to make second call. Please assist. Thank you.</p>",,1,0,,2020-1-22 16:08:16,,2020-1-24 05:40:10,,,,,5460020.0,,1,2,splunk|splunk-query,29,6
471,258927,59870412,Can I use a single regular expression to parse key-value pairs into named capture groups?,"<p>My data looks like this:</p>

<pre><code>[ REPORT_PROFILE = Some text ] [ TIME_GENERATED = 1579734865 ] [ RECORD_NUMBER = 131757058 ]
</code></pre>

<p>My data might also contain <code>[ SOME_KEY = Some value]</code>.</p>

<p>I'd like to extract:</p>

<pre><code>| Key            | Value      |
|----------------|------------|
| SOME_KEY       | Some value |
| REPORT_PROFILE | Some text  |
| TIME_GENERATED | 1579734865 |
| RECORD_NUMBER  | 131757058  |
</code></pre>

<p>I could do this using multiple regexes e.g.</p>

<p><code>\[\s+REPORT_PROFILE = (?&lt;REPORT_PROFILE&gt;[^\]]+)\s+\]</code></p>

<p><code>\[\s+\TIME_GENERATED = (?&lt;TIME_GENERATED&gt;[^\]]+)\s+\]</code></p>

<p>But is there a way I can use a single regex to extract an arbitrary number of match groups, dynamically naming them based on a key name in the source text?</p>

<p>I'm using Splunk but it's just PCRE under the hood (not PCRE2, to clarify).</p>",,1,0,,2020-1-23 01:16:44,,2020-1-23 02:12:45,,,,,2803508.0,,1,0,regex|splunk,119,8
472,258928,59883160,[Splunk][Security] Is a fake alert app useless?,"<p>preparing for my master´s thesis my supervisor at the uni suggested to create an app that produces fake alerts with suspicious log files in splunk to maintain admins´s attention on security issues. L like at the airport security where regularly fake guns and knifes are displayed on the scanner to catch the guard´s attention. </p>

<p>However, after some research I get the feeling most admins have an opposite issue, having to many false alerts. As I have no experience with Splunk in a security context, I am looking for some opinions on that. Can someone give me some insights?</p>",,2,0,,2020-1-23 16:31:10,,2020-1-23 18:40:15,,,,,12769825.0,,1,0,security|logging|operating-system|admin|splunk,44,6
473,258929,59896946,How to download the raw log files from splunk,"<p>Where I work they just switched to splunk, but I just need the raw log file so I can work with it in notepad++ for debugging.  So my question is can I get the raw log file from splunk, I don't need any of the ""features"" of splunk I just need the raw log files.  So how can I get that.</p>",59906767.0,1,0,,2020-1-24 13:00:02,,2020-1-25 06:01:36,,,,,1316113.0,,1,2,logging|splunk,3179,15
474,258930,59897086,Send spark driver logs running in k8s to Splunk,"<p>I am trying to run a sample spark job in kubernetes by following the steps mentioned here: <a href=""https://spark.apache.org/docs/latest/running-on-kubernetes.html"" rel=""nofollow noreferrer"">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a>.</p>

<p>I am trying to send the spark driver and executor logs to Splunk. 
Does spark provide any configuration to do the same? 
How do I send the Splunk configurations like the HEC endpoint, port, token, etc in the spark-submit command?</p>

<p>I did try passing it as args to the the spark driver as </p>

<pre class=""lang-sh prettyprint-override""><code>bin/spark-submit
  --deploy-mode cluster
  --class org.apache.spark.examples.JavaSparkPi
  --master k8s://http://127.0.0.1:8001
  --conf spark.executor.instances=2
  --conf spark.app.name=spark-pi
  --conf spark.kubernetes.container.image=gcr.io/spark-operator/spark:v2.4.4
  --conf spark.kubernetes.authenticate.driver.serviceAccountName=&lt;account&gt;
  --conf spark.kubernetes.docker.image.pullPolicy=Always
  --conf spark.kubernetes.namespace=default
  local:///opt/spark/examples/jars/spark-examples_2.11-2.4.4.jar
  --log-driver=splunk
  --log-opt splunk-url=&lt;url:port&gt;
  -—log-opt splunk-token=&lt;token&gt;
  --log-opt splunk-index=&lt;index&gt;
  --log-opt splunk-sourcetype=&lt;sourceType&gt;
  --log-opt splunk-format=json
</code></pre>

<p>but the logs were not forwarded to the desired index.</p>

<p>I am using spark version 2.4.4 to run spark-submit.</p>

<p>Thanks in advance for any inputs!!</p>",,1,0,,2020-1-24 13:08:35,,2020-1-25 21:15:02,2020-1-25 01:49:39,,7947644.0,,12775390.0,,1,0,apache-spark|logging|kubernetes|splunk|spark-submit,629,12
475,258931,59899533,Install pyOpenSSL and boto3 manually? Drag and drop doesn't work because of missing dependencies,"<p>I have an app that I wrote for Splunk that has dependencies on boto3 and pyOpenSSL libraries. I haven't found a good way to get app dependencies into the apps bin folder other than drag/drop, which isn't working for boto3 and pyOpenSSL.</p>

<p>To this point in time, every time we needed to make a python module available to a single app in Splunk, we would drag and drop the python modules into $SPLUNK_HOME/etc/apps/APP_NAME/bin/MODULE . This has worked until we needed the pyOpenSSL and boto3 libraries which have lots of cryptography and single script dependencies that don't come over correctly.</p>

<p>What I've tried:</p>

<pre><code>1| python3 -m venv $SPLUNK_HOME/etc/apps/APP_NAME/
2| python3 -m pip install (pyOpenSSL, boxsdk, pyJWT, boto3) &lt; base dependencies
3| move $SPLUNK_HOME/etc/apps/APP_NAME/lib/python3.7/site-packages/ &gt; $/SPLUNK_HOME/etc/apps/APP_NAME/bin
4| Put all my app scripts in $/SPLUNK_HOME/etc/apps/APP_NAME/bin alongside all the modules I just installed to that folder using venv
5| Start Splunk
6| search | search_command arg=0
</code></pre>

<p>At this point, Splunk tells me that the enum34, ipaddress, chainmap, cryptography (_constant_time module buried in here somewhere doesn't exist where it should) modules don't exist.</p>

<p>I then shut down Splunk, redid steps 1-6 but also installing all those missing modules on step 2. The error I'm getting now is this:</p>

<pre><code>External search command 'boxfiles' returned error code 1. First 1000 (of 1456) bytes of script output: ""No module named constant_time ERROR ""Error 'No module named constant_time'. Traceback (most recent call last): File """"/Applications/Splunk/etc/apps/TA-box-connector/bin/box_connector/init.py"""", line 3, in from box_connector import BoxConnector File """"/Applications/Splunk/etc/apps/TA-box-connector/bin/box_connector/box_connector.py"""", line 10, in from OpenSSL import crypto File """"/Applications/Splunk/etc/apps/TA-box-connector/bin/OpenSSL/init.py"""", line 8, in from OpenSSL import crypto, SSL File """"/Applications/Splunk/etc/apps/TA-box-connector/bin/OpenSSL/crypto.py"""", line 12, in from cryptography import x509 File """"/Applications/Splunk/etc/apps/TA-box-connector/bin/cryptography/x509/init.py"""", line 8, in from cryptography.x509.base import ( File """"/Applications/Splunk/etc/apps/TA-box-connector/bin/cryptography/x509/base.py"""", line 18, in from cryptography.x509.extensions import Exte"".
</code></pre>

<p>I'd like to solve this error, but I've been working through this dependency issue for some time now, so if there's a better solution to getting these packages on here, I would love to hear about it.</p>",59923759.0,1,0,,2020-1-24 15:35:49,,2020-1-27 20:31:50,2020-1-26 19:32:31,,7507925.0,,7507925.0,,1,0,python|python-3.x|dependencies|boto3|splunk,215,10
476,258932,59901155,Splunk Load csv from GCP into a KVStore lookup using the Python SDK,"<p>We currently have a 45mb CSV file that we're going to be loading into a Splunk kvstore. I want to be able to accomplish this via the python SDK but I'm running into a bit of trouble loading the records.</p>

<p>The only way I can find to update a kvstore is the service.collection.insert() function which as far as I can tell only accepts 1 row at a time. Being that we have 250k rows in this file, I can't afford to wait for all lines to upload every day.</p>

<p>This is what I have so far:</p>

<pre><code> from splunklib import client, binding
 import json, pandas as pd
 from copy import deepcopy

 data_file = '/path/to/file.csv'

 username = 'user'
 password = 'splunk_pass'
 connectionHandler = binding.handler(timeout=12400)
 connect_kwargs = {
     'host': 'splunk-host.com',
     'port': 8089,
     'username': username,
     'password': password,
     'scheme': 'https',
     'autologin': True,
     'handler': connectionHandler
 }
 flag = True
 while flag:
     try:
         service = client.connect(**connect_kwargs)
         service.namespace['owner'] = 'Nobody'
         flag = False
     except binding.HTTPError:
         print('Splunk 504 Error')

 kv = service.kvstore
 kv['test_data'].delete()
 df = pd.read_csv(data_file)
 df.replace(pd.np.nan, '', regex=True)
 df['_key'] = df['key_field']
 result = df.to_dict(orient='records')
 fields = deepcopy(result[0])
 for field in fields.keys():
     fields[field] = type(fields[field]).__name__
 df = df.astype(fields)
 kv.create(name='test_data', fields=fields, owner='nobody', sharing='system')
 for row in result:
     row = json.dumps(row)
     row.replace(""nan"", ""'nan'"")
     kv['learning_center'].data.insert(row)
 transforms = service.confs['transforms']
 transforms.create(name='learning_center_lookup', **{'external_type': 'kvstore', 'collection': 'learning_center', 'fields_list': '_key, userGuid', 'owner': 'nobody'})
 # transforms['learning_center_lookup'].delete()
 collection = service.kvstore['learning-center']
 print(collection.data.query())
</code></pre>

<p>In addition to the problem of taking forever to load a quarter million records, it keeps failing on a row with nan as the value, and no matter what I put in there to try to deal with the nan, it persists in the dictionary value.</p>",59923689.0,1,0,,2020-1-24 17:28:49,,2020-1-26 23:27:33,2020-1-24 17:42:11,,2602913.0,,7507925.0,,1,0,python|python-3.x|dataframe|insert|splunk,407,11
477,258933,59925930,what regex command i can use in order to create a field in splunk,"<p>I have a data which splunk shows, but i dont see a field for what i wanted</p>

<p><code>""ag-somethin-id"":[""97234d506-E0ASD-4XXX-AXX0-ASD77757""]</code></p>

<p>I need to to create a field with <strong>ag-somethin-id</strong> which should actually give me the all the values under those events and it should show something like:</p>

<p><code>97234d506-E0ASD-4XXX-AXX0-ASD77757</code> under this field <strong>ag-somethin-id</strong></p>

<p>Till now I have tried using the below, but its not correct:</p>

<p><code>rex ""ag-somethin-id[\\\"":]*(?&lt;ag-somethin-id&gt;[^\\["":""]*)""</code></p>

<p>Please help in fixing this</p>",,1,0,,2020-1-27 06:15:33,,2020-1-27 11:58:12,,,,,4304029.0,,1,0,splunk-query,17,4
478,258934,59952137,read splunk jsonArray with spath,"<p>Here is my set:</p>

<pre><code>| makeresults
| eval _raw=""[[\""A\"",\""AA\""],[\""B\"",\""BB\""],[\""C\"",\""CC\""]]""
|spath  path={}{} output=data
</code></pre>

<p>I would like to have 3 distinct tuple the A together and B together and C together,
but i have all in one line with my request.</p>

<p>I can do something like </p>

<pre><code>|spath  path={0}{} output=data0
|spath  path={1}{} output=data1
|spath  path={2}{} output=data2
</code></pre>

<p>but i'm looking for something more dynamic :)</p>

<p>Any idea?</p>",60199955.0,1,0,,2020-1-28 15:34:59,,2020-2-13 03:20:28,,,,,12367059.0,,1,0,arrays|json|splunk,680,11
479,258935,59957456,how to export charts along with respective search query from splunk dashboard to pdf?,"<p>I have created a dashboard which has few input pickers and different panels. When I export to PDF, only the charts are getting exported. I need the URL getting generated on clicking the ""Open Search"" button available at the right bottom of interactive chart needs to get exported to PDF.</p>

<p>If possible the queries needs to be invisible in splunk dashboard.</p>

<p>Thanks in advance.</p>",,1,0,,2020-1-28 21:41:12,,2020-1-30 22:59:34,2020-1-30 22:59:34,,10659580.0,,10659580.0,,1,0,splunk,157,8
480,258936,59994261,Remove characters from Message field in splunk,"<p>I am searching for specific event codes in splunk, such that the first part of the message field starts with <code>""A member was added to a security-enabled global group""</code>. After that, it has a whole lot more information which, for my purposes, I do not need to see. I tried the following searches however I am not getting the results I want. </p>

<p>This search made no changes to the message: </p>

<pre><code>index=""win_evt"" EventCode=4728  | rex field=Message ""(?&lt;=A:)(?&lt;Notes&gt;.*)(?=.)"" | table  _time, Account_Name, Group_Name, Message, EventCode, Message
</code></pre>

<p>This search completely removed the message:</p>

<pre><code>index=""win_evt"" EventCode=4728  | eval Message = trim(replace(Message,"".*"","""")) | table  _time, Account_Name, Group_Name, Message, EventCode, Message
</code></pre>

<p>This does nothing as well:</p>

<pre><code>index=""win_evt"" EventCode=4728  | rex field=Message mode=sed ""s/\..*$//"" | table  _time, Account_Name, Group_Name, Message, EventCode, Message
</code></pre>

<p>All I want is for the <code>| table Message</code> to show is that first line.</p>",,1,0,,2020-1-30 20:47:26,,2020-1-31 03:00:37,2020-1-31 03:00:37,,5411817.0,,11584513.0,,1,0,regex|eval|splunk|rex,1042,12
481,258937,59995022,Splunk adding two values for a timechart,"<p>I have a SPLUNK search, and want to add the previous values for a third chart.</p>

<pre><code>&lt;br/&gt;
&amp;lt;dashboard&amp;gt;&lt;br/&gt;
  &amp;lt;label&amp;gt;Sum&amp;lt;/label&amp;gt;&lt;br/&gt;
  &amp;lt;fieldset submitButton=""false"" autoRun=""true""&amp;gt;&lt;br/&gt;
    &amp;lt;input type=""time"" token=""time""&amp;gt;&lt;br/&gt;
      &amp;lt;label&amp;gt;Time&amp;lt;/label&amp;gt;&lt;br/&gt;
      &amp;lt;default&amp;gt;&lt;br/&gt;
        &amp;lt;earliest&amp;gt;-1d@d&amp;lt;/earliest&amp;gt;&lt;br/&gt;
        &amp;lt;latest&amp;gt;-0d@d&amp;lt;/latest&amp;gt;&lt;br/&gt;
      &amp;lt;/default&amp;gt;&lt;br/&gt;
    &amp;lt;/input&amp;gt;&lt;br/&gt;
  &amp;lt;/fieldset&amp;gt;&lt;br/&gt;
  &amp;lt;row&amp;gt;&lt;br/&gt;
    &amp;lt;panel&amp;gt;&lt;br/&gt;
      &amp;lt;title&amp;gt;shipping&amp;lt;/title&amp;gt;&lt;br/&gt;
      &amp;lt;chart&amp;gt;&lt;br/&gt;
        &amp;lt;search&amp;gt;&lt;br/&gt;
          &amp;lt;query&amp;gt;&lt;br/&gt;
                        (source=""/var/fedex/foo/logs/foo.log"" )&lt;br/&gt;
                         (FOO ) AND 0000 &lt;br/&gt;
                         | timechart  span=1h count as ""Value1""&lt;br/&gt;
                         | append  [ &lt;br/&gt;
                         search  &lt;br/&gt;source=""/var/fedex/bar/logs/bar.log""
                          (BAR ) AND 0000  &lt;br/&gt;
                         | timechart  span=1h count as ""Value2""&lt;br/&gt;
                         ]&lt;br/&gt;
                         | append  [ &lt;br/&gt;
                          timechart  span=1h  eval(""Value1"" + ""Value2"") as ""Value3""&lt;br/&gt;
                                   ]&lt;br/&gt;
             &amp;lt;/query&amp;gt;&lt;br/&gt;
          &amp;lt;earliest&amp;gt;$time.earliest$&amp;lt;/earliest&amp;gt;&lt;br/&gt;
          &amp;lt;latest&amp;gt;$time.latest$&amp;lt;/latest&amp;gt;&lt;br/&gt;
        &amp;lt;/search&amp;gt;&lt;br/&gt;
        &amp;lt;option name=""charting.axisY.scale""&amp;gt;log&amp;lt;/option&amp;gt;&lt;br/&gt;
        &amp;lt;option name=""charting.axisY2.abbreviation""&amp;gt;auto&amp;lt;/option&amp;gt;&lt;br/&gt;
        &amp;lt;option name=""charting.chart""&amp;gt;line&amp;lt;/option&amp;gt;&lt;br/&gt;
        &amp;lt;option name=""charting.axisY.minimumNumber""&amp;gt;1000&amp;lt;/option&amp;gt;&lt;br/&gt;
        &amp;lt;option name=""charting.axisY.maximumNumber""&amp;gt;10000000&amp;lt;/option&amp;gt;&lt;br/&gt;
      &amp;lt;/chart&amp;gt;&lt;br/&gt;
    &amp;lt;/panel&amp;gt;&lt;br/&gt;
  &amp;lt;/row&amp;gt;&lt;br/&gt;
&lt;br/&gt;
gives me an error &lt;br/&gt;
 ""Error in 'timechart' command: The eval expression has no fields: 'Value1 + Value2'.""
&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;
I see to be running around in circles on this issue.  I am now getting suggestions to go back to my first approach.&lt;br/&gt;

                         | append  [ eval(""Value1"" + ""Value2"") as ""Value3"" &lt;br/&gt;
                          timechart  span=1h  ""Value3""&lt;br/&gt;
                                   ]&lt;br/&gt;
</code></pre>

<pre><code>see https://answers.splunk.com/answers/168646/timechart-wtih-eval-cant-recognize-fields.html&lt;br/&gt;
</code></pre>

<p>which gave me a <br/>
Error in 'eval' command: The 'eval' function is unsupported or undefined.</p>",,1,2,,2020-1-30 21:49:38,,2020-2-1 06:10:01,2020-1-31 19:29:57,,116158.0,,813186.0,,1,0,eval|splunk,1403,12
482,258938,59999482,"How to put conditional output like arthmemetic , value o/p should be greater than = 30k and less that = 30 k","<p><a href=""https://i.stack.imgur.com/RtWWi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RtWWi.png"" alt=""enter image description here""></a>enter image description heresourcetype=xxxxx ""connection from 17.129.249.164"" OR ""connection from 17.208.230.209"" OR ""connection from 10.41.84.33"" OR ""connection from 10.41.158.214"" OR ""connection from 10.41.88.162"" OR ""connection from 10.41.157.80"" OR ""connection from 10.41.88.198"" OR ""connection from 17.208.225.42"" OR ""connection from 10.41.92.81"" OR ""connection from 10.41.92.237"" | rex field=_raw ""connection from (?\d+.\d+.\d+.\d+):""| bin span=1d _time |chart max(value)  count by _time FROM_IP limit=0</p>",60049658.0,2,0,,2020-1-31 07:15:05,1.0,2020-2-4 02:12:55,2020-2-1 08:42:13,,11831790.0,,11831790.0,,1,-2,splunk|splunk-query,38,5
483,258939,60051611,Is it possible to make a dynamic splunk dashboard?,"<p>Pardon me if someone has already asked this question, but since I haven't found a satisfactory answer so posting this query .</p>

<p>I am a beginner in Splunk. Just wanted to know if what I am trying to do is feasible or not .</p>

<p>I am trying to make a Splunk dashboard where it has 3 panels. I want to pass the result of one panel to another ,where it can be used in the query .
Simply put, I want to find the count of an event : Firstly Month wise from a year(span=1mon). Then select the highest count month and find the count day wise(span=1d) for that month. From this month I want to select the day with highest count and then find the hour(span=1h) with highest event count. 
Right now I am doing this whole process manually. </p>

<ol>
<li>Find Month with max event count -> max_month</li>
<li>Find Day with max event count from the max_month -> max_day</li>
<li>Find Hour with max event count from max_day</li>
</ol>

<p>Is it possible to automate this process using a dashboard where it can automatically select the max month, day and hour ?</p>

<p>I tried using nested query but wasnt able to make any significant progress .</p>",,1,0,,2020-2-4 06:15:58,,2020-2-7 05:24:57,,,,,12836904.0,,1,0,splunk|splunk-query,902,12
484,258940,60066801,Does Docker Splunk logging driver support indexer ack,"<p>Docker splunk driver is used in my application. Here is the configuration.</p>

<pre><code>splunk-url: ""https://splunk-server:8088""
splunk-token: ""token-uuid""
splunk-index: ""my_index""
</code></pre>

<p>My token of splunk have index acknowledgement enabled, such that Http Event Collector (HEC) requires X-Splunk-Request-Channel in header.</p>

<p>I am sure that event can be sent via a HTTP client like postman to HEC with the header, but I cannot find the configuration option from docker splunk driver to set it.</p>

<p>Given that splunk index ack is required by my organisation. Is there any workaround?
cheers</p>",,0,0,,2020-2-4 23:02:58,,2020-2-4 23:02:58,,,,,769670.0,,1,1,docker|splunk,25,5
485,258941,60098764,Regex couldn't find presence of all failures,"<blockquote>
  <p>Status': 'Failure: DNS resolution failed: Rcode Domain(3)', 'CheckedTime': datetime.datetime(2017, 2, 1, 14, 47, 38, 382000, tzinfo=tzlocal())}}, {'Region': 'us-east-1', 'IPAddress': '01.000.2.12', 'StatusReport': {'Status': 'Success: DNS resolution Success: Rcode Domain(3)', 'CheckedTime': datetime.datetime(2017, 2, 1, 14, 47, 35, 371000, tzinfo=tzlocal())}}, {'Region': 'us-west-1', 'IPAddress': '01.000.14.10', 'StatusReport': {'Status': 'Failure: DNS resolution failed: Rcode Domain(3)', 'CheckedTime': datetime.datetime(2017, 2, 1, 14, 47, 34, 715000, tzinfo=tzlocal())}}, {'Region': 'us-west-2', 'IPAddress': '01.000.22.10', 'StatusReport': {'Status': 'Failure: DNS resolution failed: Rcode Domain(3)', 'CheckedTime': datetime.datetime(2017, 2, 1, 14, 47, 42, 801000, tzinfo=tzlocal())}}, {'Region': 'us-west-2', 'IPAddress': '01.000.18.10', 'StatusReport': {'Status': 'Failure: DNS resolution failed: Rcode Domain(3)', 'CheckedTime': datetime.datetime(2017, 2, 1, 14, 47, 25, 189000, tzinfo=tzlocal())}}, {'Region': 'us-east-1', 'IPAddress': '01.000.1.10', 'StatusReport': {'Status': 'Failure: DNS resolution failed: Rcode Domain(3)', 'CheckedTime': datetime.datetime(2017, 2, 1, 14, 47, 42, 293000, tzinfo=tzlocal())}}]}</p>
</blockquote>

<p><strong><em>Problem</em></strong>:</p>

<p>I need to find any failure in the string and the associated message and it should not look for any success in the message.</p>

<blockquote>
  <p>Status': 'Failure: DNS resolution failed: Rcode Domain(3)', 'CheckedTime': datetime.datetime(2017, 2, 1, 14, 47, 38, 382000, tzinfo=tzlocal())}}, {'Region': 'us-east-1', 'IPAddress': '01.000.2.12',
   'Status': 'Failure: DNS resolution failed: Rcode Domain(3)', 'CheckedTime': datetime.datetime(2017, 2, 1, 14, 47, 34, 715000, tzinfo=tzlocal())}}, {'Region': 'us-west-2', 'IPAddress': '01.000.22.10'
    etc.</p>
</blockquote>

<p><strong><em>What I tried:</em></strong> </p>

<pre><code>Status':.+Failure.*(?=Success)
</code></pre>

<p>and </p>

<pre><code>'Status':.+
</code></pre>

<p>but it doesn't give me what I want.</p>

<p>Please help!!</p>",60162369.0,1,4,,2020-2-6 15:45:44,0.0,2020-2-11 05:11:28,2020-2-6 20:29:07,,1678362.0,,10581832.0,,1,0,regex|splunk|regex-greedy,51,7
486,258942,60100460,Merge my required files from DEV branch to QA in Azure VSTS,"<p>This is regarding Splunk Phantom playbook code deployment. </p>

<p>Whenever we create a new playbook, in the repository it creates two files for each playbook (.json, .py). We have 3 different branches associated with one Repository (DEV, QA, PROD).</p>

<p>In our DEV branch, development work would continue and it has around 300 files(JSON and Python files, that means 150 playbooks in Phantom dev GUI).</p>

<p>Now, I don't want to merge all these files to my QA, I just need a few files those undergone changes in DEV needs to push to QA (cause out of 300 files in DEV, we have around 160 files in QA with same code).</p>

<p>When any changes would happen to DEV files, I have to pick those specific files and raise the pull request then merge (that means only those files with changes in DEV should move to QA {kinda Cherry-picking}).</p>

<p>I have followed below process, but I'm getting merge conflicts.</p>

<p>Process:</p>

<ol>
<li><p>Create a new branch</p>

<pre><code>git checkout -b branch_name
</code></pre></li>
<li><p>Remove all files for this branch</p>

<pre><code>git rm -rf .
</code></pre></li>
<li><p>Commit changes</p>

<pre><code>git commit -m ""create new branch"" 
</code></pre></li>
<li><p>Retrieve some files from master branch</p>

<pre><code>git checkout master -- file1 file2 file3 file4    
</code></pre></li>
<li><p>Commit changes</p>

<pre><code>git commit -m ""create new branch"" 
</code></pre></li>
</ol>

<p>After doing this process, I navigate to my VSTS UI, and apply cherry-pick on ""commit"" which I have done at 4th, 5th steps (I am able to successfully create commit with my required files and at the time of performing cherry-pick, I'm running into issues).</p>

<p>Below is a screenshot when I tried to do the cherry-pick - error message looks like this:</p>

<pre><code>There were conflicts when cherry-picking commit 60625f. This operation needs to be done locally
</code></pre>

<p><img src=""https://i.stack.imgur.com/e51LP.png"" alt=""""></p>",,1,4,,2020-2-6 17:24:50,,2020-2-7 02:41:22,2020-2-7 02:41:22,,4629442.0,,12076706.0,,1,0,git|splunk|git-cherry-pick,417,11
487,258943,60110812,Splunk props config for AWS Cloudtrail json logs,"<p>I need to ingest AWS cloudtrail logs pulled from S3. The files contain a single json payload that contains individual cloudtrail events. However splunk is not recognising the individual events and not splitting correctly. its just one big lump as a single event</p>

<p>Each file contains this format:</p>

<pre><code>  ""Records"": [
    {
      ""apiVersion"": ""2012-06-01"",
      ""awsRegion"": ""us-west-1"",
      ""eventID"": ""c-c245-2c4-32v6-vfff"",
      ""eventName"": ""DescribeLoadBalancers"",
      ""eventSource"": ""elasticloadbalancing.amazonaws.com"",
      ""eventTime"": ""2019-11-30T18:15:33Z"",
      ""eventType"": ""AwsApiCall"",
      ""eventVersion"": ""1.05"",
      ""recipientAccountId"": ""redacted"",
      ""requestID"": ""2xc454xc-2345-234cv5-2345"",
      ""requestParameters"": null,
      ""responseElements"": null,
      ""sourceIPAddress"": ""1.1.1.1"",
      ""userAgent"": ""aws-sdk-ruby3/3.75.0 jruby/2.3.3 java aws-sdk-elasticloadbalancing/1.19.0 cloudhealth"",
      ""userIdentity"": {
        ""accessKeyId"": ""redacted"",
        ""accountId"": ""redacted"",
        ""arn"": ""arn:aws:sts::redacted:assumed-role/team/AssumeRoleSession"",
        ""principalId"": ""redacted:AssumeRoleSession"",
        ""sessionContext"": {
          ""attributes"": {
            ""creationDate"": ""2019-11-30T17:45:06Z"",
            ""mfaAuthenticated"": ""false""
          },
          ""sessionIssuer"": {
            ""accountId"": ""redacted"",
            ""arn"": ""arn:aws:iam::redacted:team/company"",
            ""principalId"": ""redacted"",
            ""type"": ""Role"",
            ""userName"": ""redacted""
          },
          ""webIdFederationData"": {}
        },
        ""type"": ""AssumedRole""
      }
    },{
      ""apiVersion"": ""2012-06-01"",
      ""awsRegion"": ""us-west-1"",
      ""eventID"": ""c-c245-2c4-32v6-vfff"",
      ""eventName"": ""DescribeLoadBalancers"",
      ""eventSource"": ""elasticloadbalancing.amazonaws.com"",
      ""eventTime"": ""2019-11-30T18:16:33Z"",
      ""eventType"": ""AwsApiCall"",
      ""eventVersion"": ""1.05"",
      ""recipientAccountId"": ""redacted"",
      ""requestID"": ""2xc454xc-2345-234cv5-2345"",
      ""requestParameters"": null,
      ""responseElements"": null,
      ""sourceIPAddress"": ""1.1.1.1"",
      ""userAgent"": ""aws-sdk-ruby3/3.75.0 jruby/2.3.3 java aws-sdk-elasticloadbalancing/1.19.0 cloudhealth"",
      ""userIdentity"": {
        ""accessKeyId"": ""redacted"",
        ""accountId"": ""redacted"",
        ""arn"": ""arn:aws:sts::redacted:assumed-role/team/AssumeRoleSession"",
        ""principalId"": ""redacted:AssumeRoleSession"",
        ""sessionContext"": {
          ""attributes"": {
            ""creationDate"": ""2019-11-30T17:45:06Z"",
            ""mfaAuthenticated"": ""false""
          },
          ""sessionIssuer"": {
            ""accountId"": ""redacted"",
            ""arn"": ""arn:aws:iam::redacted:role/team"",
            ""principalId"": ""redacted"",
            ""type"": ""Role"",
            ""userName"": ""redacted""
          },
          ""webIdFederationData"": {}
        },
        ""type"": ""AssumedRole""
      }
    }
  ]
}
</code></pre>

<p>My Props looks like this</p>

<pre><code>[cloudtrail]
KV_MODE = json
</code></pre>",,1,0,,2020-2-7 09:42:35,1.0,2020-2-7 09:42:35,,,,,223290.0,,1,0,amazon-web-services|splunk|amazon-cloudtrail,213,9
488,258944,60158215,Splunk rex: extracting repeating keys and values to a table,"<p>I have some logs in Splunk for which I'm trying to extract a few values. My log entries look like this:</p>

<pre><code>host-03.company.local:9011[read 3617, write 120 bytes] host-05.company.local:9011[read 370658827, write 177471 bytes] host-07.company.local:9011[read 99, write 96 bytes] host-07.company.local:9011[read 96, write 96 bytes] host-05.company.local:9011[read 120, write 120 bytes] host-05.company.local:9011[read 120, write 120 bytes] host-03.company.local:9015[read 42955, write 120 bytes] host-05.company.local:9015[read 3048879, write 86677386 bytes] host-02.company.local:7035[read 120, write 120 bytes] host-03.company.local:9015[read 120, write 120 bytes] host-05.company.local:9015[read 809077, write 120 bytes] host-02.company.local:7035[read 120, write 120 bytes] host-03.company.local:9015[read 120, write 120 bytes] host-05.company.local:9015[read 120, write 120 bytes] host-02.company.local:7035[read 120, write 120 bytes]
</code></pre>

<p>The pattern these log entries follow is <code>host:port[read xxx, write yyy bytes]</code></p>

<p>There can be anywhere from 1 to about 20 host records in this log line.</p>

<p>What I'm hoping to do, in Splunk, is extract these fields to a table, such that the result looks like:</p>

<pre><code>hostname                   readBytes WriteBytes
-----------------------------------------------
host-03.company.local:9011      3617        120
host-05.company.local:9011 370658827     177471
host-07.company.local:9011        99         96
host-05.company.local:9011       120        120
</code></pre>

<p>Logic here being that I'm extracting the <code>read</code> and <code>write</code> entries for each host, such that each one becomes a line in this table.</p>

<p>I've made some progress in extracting the hosts, with the rex:</p>

<pre><code>index=myApplication &lt;mySearch&gt;
  | rex field=_raw ""(?&lt;hostsTmp&gt;([a-zA-Z0-9\-\.]+:[0-9]+))""
  | table hostsTmp
</code></pre>

<p>However, even this result seems wrong, some of the results are just blank lines. In addition, the <code>hostsTemp</code> field doesn't seem to be a multi-variable field. mvcount(hostsTemp) returns nothing for each entry.</p>

<pre><code>mvcount(hostsTmp)   len(hostsTmp)   hostsTmp
--------------------------------------------
    -                    -          host-05.company.local:9011
    -                    -          -
    -                    -          host-05.company.local:9011
    -                    -          -
    -                    -          host-05.company.local:9011
    -                    -          -
</code></pre>

<p>Note that I'm using the <code>-</code> character here to represent a lack of data in my table. Every other line is just completely blank, and the <code>mvcount</code> and <code>len</code> values for hostsTmp is always empty.</p>

<p>Relatively new to Splunk and not an expert in regex, so any help is appreciated.</p>",60159316.0,1,0,,2020-2-10 20:50:56,1.0,2020-2-10 22:20:25,,,,,591182.0,,1,0,regex|splunk|splunk-query,615,12
489,258945,60180024,Symbolic link is getting the permission error,"<p>I'm trying to get the symbolic link from other user.</p>

<p>My file is located in <code>/home/serviceA/logs/a.txt</code> And I want to create a symbolic link to <code>/home/centos/logs/a.txt</code>.</p>

<p>Here is my command I ran as root user:
<code>ln -s /home/serviceA/logs/a.txt /home/centos/logs/a.txt</code></p>

<p>I see the red color of filename. And I still get the permission denied error</p>

<p>The error is <code>lrwxrwxrwx 1 root root 47 Feb 12 01:49 /home/centos/logs/a.txt -&gt; /home/serviceA/logs/a.txt</code></p>

<p>Eventually, I want to forward the <code>/home/centos/logs/a.txt</code> log file to the Splunk.</p>

<p>Why am I getting the permission error after creating the symbolic link? And how do I fix it? (<code>chmod 777</code> didn't help)</p>",,1,1,,2020-2-12 01:58:17,,2020-2-13 02:30:15,2020-2-12 02:03:52,,7820956.0,,7820956.0,,1,0,unix|splunk,475,11
490,258946,60207053,Creating a REST Handler for any of Splunk's REST endpoints,"<p>How to create a Persistent(or any for that matter) REST HANDLER for any given(inbuilt) SPLUNK REST API Endpoint? How to use <strong>PersistentServerConnectionApplication</strong> class ?</p>

<p>I have gone through <em><a href=""https://gist.github.com/LukeMurphey/238004c8976804a8e79570d22721fd99"" rel=""nofollow noreferrer"">https://gist.github.com/LukeMurphey/238004c8976804a8e79570d22721fd99</a></em> but cant figure out where to start and how to make one.</p>",60217782.0,1,0,,2020-2-13 11:45:07,,2020-2-13 23:04:46,2020-2-13 11:58:52,,7665068.0,,7665068.0,,1,0,splunk|splunk-sdk,279,10
491,258947,60216405,Kafka Connect Splunk Sink Connector Issue,"<p>I am running into another issue with Splunk sink connector. When I use </p>

<pre><code>""splunk.hec.ssl.validate.certs"": ""true"" 
</code></pre>

<p>the error I get is -</p>

<blockquote>
  <p>javax.net.ssl.SSLHandshakeException:
  sun.security.validator.ValidatorException: PKIX path building failed:
  sun.security.provider.certpath.SunCertPathBuilderException: unable to
  find valid certification path to requested target</p>
</blockquote>

<p>When I use <code>""splunk.hec.ssl.validate.certs"": ""false""</code> the error becomes - </p>

<blockquote>
  <p>javax.net.ssl.SSLHandshakeException:
  java.security.cert.CertificateException: Certificates do not conform
  to algorithm constraints Caused by:
  java.security.cert.CertPathValidatorException: Algorithm constraints
  check failed on keysize limits. RSA 1024bit key used with
  certificate........</p>
</blockquote>

<p>Any idea what is causing it ?</p>",,0,6,,2020-2-13 21:01:58,,2020-2-13 21:13:08,2020-2-13 21:13:08,,2308683.0,,8692189.0,,1,0,ssl|apache-kafka|apache-kafka-connect|splunk,134,8
492,258948,60219585,how to speed up a splunk export?,"<p>I am using the <code>python 3</code> <code>splunk</code> API to export some massive logs.
My code essentially follows the <code>splunk</code> API guidelines:</p>

<pre><code>import splunklib.client as client
import splunklib.results as results
import pandas as pd

kwargs_export = {""earliest_time"": ""2019-08-19T12:00:00.000-00:00"",
                 ""latest_time"": ""2019-08-19T14:00:00.000-00:00"",
                 ""search_mode"": ""normal""}

exportsearch_results = service.jobs.export(mysearchquery, **kwargs_export)

reader = results.ResultsReader(exportsearch_results)    

df = pd.DataFrame(list(reader))
</code></pre>

<p>But this is extremely slow... </p>

<p>Ultimately I want to store the output of the search as a <code>csv</code> to disk. Is there any way to speed the export?</p>

<p>Thanks!</p>",,1,0,,2020-2-14 03:29:16,,2020-7-10 08:58:10,,,,,1609428.0,,1,0,python|splunk,456,10
493,258949,60227520,Fetching the result based on sourcetype,"<p>I have written a query to fetch the all java exception count wise in splunk. But this query fetch across all sourcetype.</p>

<pre><code>java.*.*Exception NOT warn  | rex ""(?&lt;rexexption&gt;java*.*Exception)""| stats count by rexexption | table count,rexexption | sort count | reverse
</code></pre>

<p>Now I want all these exceptions per <strong>sourcetype</strong> wise.</p>",,1,0,,2020-2-14 13:42:54,,2020-2-14 15:58:01,,,,,1120878.0,,1,0,splunk|splunk-query,18,5
494,258950,60241983,Not able to read nested json array in SPLUNK,"<p>I am using ""spath"" to read json structure from a log file.</p>

<pre><code>{""failure_reason"":null,""gen_flag"":""GENERATED"",""gen_date"":""2020-02-15"",""siteid"":""ABC"",""_action"":""Change"",""order"":""123""}
</code></pre>

<p>I am able to parse above json.</p>

<p>However, ""spath"" function is not able to read nested array inside that json:</p>

<pre><code>{""failure_reason"":""[{""module"":""Status Report"",""reason"":""Status Report is not available"",""statusCode"":""503""}]"",""gen_flag"":""GENERATED_PARTIAL"",""gen_date"":""2020-02-15"",""siteid"":""ABC"",""_action"":""Change"",""wonum"":""321""}.
</code></pre>

<p>please help!</p>",,1,0,,2020-2-15 18:43:11,,2020-2-16 04:41:55,2020-2-16 04:41:55,,1821558.0,,12904423.0,,1,0,splunk|splunk-query,368,10
495,258951,60266963,"I am trying to use regular expression for extracting the Filename filed in Splunk,I have attached the same text","<p>ID=6913&amp;Filename=<strong>C%3A%5CUsers%5CTHanse04%5CAppData%5CRoaming%5CDocumentum%5CViewed%5C181019_ERS_321_102_500857.pdf</strong>&amp;Download=65536&amp;DownloadSize=79243 HTTP/1.1"" 200 3 ""-"" ""Java/1.8.0_192""</p>

<p>I need to extract and after extract i need Thanse04 from it
Filename=C%3A%5CUsers%5C<strong>THanse04</strong>%5CAppData%5CRoaming%5CDocumentum%5CViewed%5C181019_ERS_321_102_500857.pdf</p>",60267287.0,2,0,,2020-2-17 16:44:33,,2020-2-17 18:59:51,,,,,12913947.0,,1,-2,regex|splunk|splunk-query,31,5
496,258952,60302586,Metrics for spring boot rest api,<p>I have a spring boot app with some REST APIs. I would like to collect usage metrics on these APIs. Metrics would be like how many times did a user hit the service in a time period. The user id is available in request header. Is there any framework that will help me with this reducing the boiler plate? Will splunk help?</p>,,3,0,,2020-2-19 14:28:18,,2020-11-30 01:04:48,,,,,3326694.0,,1,2,java|spring|spring-boot|metrics|splunk,230,13
497,258953,60343986,Is there a similar command to multisearch in Splunk for non-streaming searches?,"<p>I understand in Splunk that <strong>multisearch</strong> allows multiple searches to run in parallel. However, it's only for streaming searches. Is there a similar command or way to run non-streaming searches in parallel?</p>",,0,0,,2020-2-21 17:55:59,,2020-2-21 17:55:59,,,,,11142188.0,,1,2,splunk|splunk-query|splunk-sdk,257,9
498,258954,60391331,Read and write to same file simultaneously from Invoke-RestMethod in PowerShell,"<p>We have a program that controls door access on our premises. Whenever a person opens the door with their tag, an event is registered in the programs database. These events can be read by enabling a HTTP integration, which makes it possible to view them in the localhost web browser. </p>

<p>We want to export the events viewed from the HTTP URL to Splunk. To do that, I have been writing a PowerShell script that uses <code>Invoke-RestMethod</code> to fetch the data from the URL to a file on <code>C:\Scripts</code>, which Splunk then monitors. </p>

<p>Here's the PowerShell script I have so far:</p>

<pre><code>$getRestMethodParams = @{
    Uri = 'http://localhost:5004/eventexport?end_date=keep'
    Method = 'Get'
    Credential = $Creds
    OutFile = 'C:\Scripts\SplunkOutput.xml'
}
Invoke-RestMethod @getRestMethodParams
</code></pre>

<p>The URI used will keep the connection open with a heartbeat with <code>end_date=keep</code> so we are monitoring the events in real-time. The script will also output the results into the file <code>'C:\Scripts\SplunkOutput.xml'</code>. So far, so good.</p>

<p>However, the code will also always keep the file in an open/used state (because of the heartbeat parameter), which prevents Splunk from reading from the file until I terminate the script, which we don't want to do (well, we will have to at some point to prevent the file from growing too big, but that will be done later on).</p>

<p>A colleague suggested I tried to use <code>[System.IO.File]</code> to manipulate the file streams, but I only got so far. This is the code I used:</p>

<pre><code>$file = [System.IO.File]::Open('C:\Scripts\SplunkOutput.xml')

$getRestMethodParams = @{
    Uri = 'http://localhost:5004/eventexport?end_date=keep'
    Method = 'Get'
    Credential = $Creds
    OutFile = $file
}
Invoke-RestMethod @getRestMethodParams
</code></pre>

<p>Unfortunately, that gave me the output as:</p>

<pre><code>Cannot find an overload for ""Open"" and the argument count: ""1"".
At C:\Scripts\SplunkPoller1.ps1:12 char:1
+ $file = [System.IO.File]::Open('C:\Scripts\SplunkOutput.xml')
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (:) [], MethodException
    + FullyQualifiedErrorId : MethodCountCouldNotFindBest
</code></pre>

<p>I also tried with (from <a href=""https://stackoverflow.com/questions/34357609/powershell-closing-filestream"">PowerShell Closing FileStream</a>):</p>

<pre><code>$inFile = 'C:\Scripts\SplunkOutput.xml'
$inFS = New-Object FileStream($inFile, [FileMode]::Open)

$getRestMethodParams = @{
    Uri = 'http://localhost:5004/eventexport?end_date=keep'
    Method = 'Get'
    Credential = $Creds
    OutFile = $inFS
}
Invoke-RestMethod @getRestMethodParams
</code></pre>

<p>Which gave me:</p>

<pre><code>Unable to find type [FileMode].
At C:\Scripts\SplunkPoller1.ps1:11 char:40
+ $inFS = New-Object FileStream($inFile, [FileMode]::Open)
+                                        ~~~~~~~~~~
    + CategoryInfo          : InvalidOperation: (FileMode:TypeName) [], RuntimeException
    + FullyQualifiedErrorId : TypeNotFound
</code></pre>

<p>Any and all tips would be greatly appreciated on how to approach this issue! Thanks.</p>",60404377.0,1,2,,2020-2-25 09:29:34,,2020-2-25 22:54:33,,,,,5872381.0,,1,0,powershell|splunk,124,9
499,258955,60397835,Splunk : How to query splunk dashboard using CURL command and save data in to Json file,"<p>Help is needed,</p>

<p>I am running below command on CLI and I am getting an error </p>

<p>Curl command:</p>

<pre><code>curl -k -u xxxxx@url.com:xxxxx \
    https://splunk.com/en-US/app/search/search/jobs/export \
    -d search=""searchindex=test"" -d output_mode=json
</code></pre>

<p>output: </p>

<pre><code>&lt;html&gt;
    &lt;head&gt;&lt;title&gt;302 Found&lt;/title&gt;&lt;/head&gt;
    &lt;body bgcolor=""white""&gt;
        &lt;center&gt;&lt;h1&gt;302 Found&lt;/h1&gt;&lt;/center&gt;
        &lt;hr&gt;&lt;center&gt;openresty/1.11.2.2&lt;/center&gt;
    &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Any help would be appreciated. Thanks!</p>",,1,1,,2020-2-25 15:16:49,,2020-2-25 15:46:10,2020-2-25 15:46:10,,9288200.0,,11611074.0,,1,0,splunk-query,381,10
500,258956,60408626,drill down in azure sentinel workbook,"<p>In Splunk, we have the drill-down option in the dashboard so is that possible in azure sentinel workbook? consider I have one chart(tile or piechart) so when I click on that I want to open another tab. Is it possible in azure sentinel workbook?</p>",,2,0,,2020-2-26 07:17:57,,2020-4-7 01:44:06,2020-4-7 01:39:44,,13687.0,,9586444.0,,1,0,azure|splunk|azure-log-analytics|azure-sentinel|azure-monitor-workbooks,457,10
501,258957,60441450,How to write a splunk query to ensure that search is working?,"<p>I have a number of splunk alerts which fire if there are no results. However, occasionally our splunk search goes down for a minute, causing all of these alerts to fire.</p>

<p>I am not in charge of the splunk server, so I cannot do anything to improve it's ability for the search to stay up. I can however modify the queries of the alerts. I am not looking for suggestions for how to keep the search up. It is out of my hands.</p>

<p>Is there a way I can modify a search to check that search is working? Like perhaps a way to always return at least 1 result from the search and then I can modify my alerts to fire when there is just 1 result rather than 0.</p>

<p>so the pseudo query would be something like: query which always returns just 1 result if search is working | append these results with original splunk query that expects > 0 results</p>",,1,0,,2020-2-27 20:42:19,,2020-2-28 03:22:22,,,,,4504267.0,,1,0,splunk|splunk-query,390,10
502,258958,60446366,Fluent-bit inside Spark Container,"<p>I am trying to run <code>fluent-bit</code> inside <code>spark</code> container so that <code>spark driver container</code> which is writing the logs in a file <code>/var/log/sparkDriver.log</code> controlled by <code>spark log4j</code> properties, can be read or consumed by <code>fluent-bit</code>. I know that running multiple processes in one container is an AntiParttern but right now I have no choice. What configuration I need, to <strong>read</strong> this file (<code>/var/log/sparkDriver.log</code>) and forward the logs to our internal <code>splunk hec</code> server.</p>

<p>I know <code>fluent-bit</code> can be used as a <code>sidecar</code> in the pod but I am using simple <code>spark-submit</code> to submit my spark job to <code>K8S</code> and <code>spark-submit</code> doesn't have any way to tell <code>k8s</code> that I want to run a sidecar (fluent-bit) as well.</p>

<p>I also know that <code>fluent-bit</code> can installed as deamonSet in the cluster which will basically run on each node in the <code>k8s</code> cluster and forward logs from the container via node to <code>Splunk</code>. But this option is also not going to work for me.</p>

<p>So I thought if I could bake <code>fluent-bit</code> or <code>splunkforwarder</code> or even <code>fluentd</code> and read the logs from a file or <strong>stdout</strong>. I know that the other 2 options will inflate my <code>spark</code> docker image but I don't have an option right now.</p>

<p>Any help or suggestion will be really appreciated</p>

<p>I actually tried the <code>tail</code> and <code>splunk</code> but somehow I am not able to figure out the right configuration for <code>fluent-bit</code></p>

<p>Here is my log file which is <code>spark</code> logs using <code>log4j</code>:</p>

<p>I actually tried it but somehow I am not able to put the right configuration around it. Here is how my log files look:</p>

<pre><code>20/03/02 19:35:47 INFO TaskSetManager: Starting task 12526.0 in stage 0.0 (TID 12526, 172.16.7.233, executor 1, partition 12526, PROCESS_LOCAL, 7885 bytes)
20/03/02 19:35:47 DEBUG KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Launching task 12526 on executor id: 1 hostname: 172.16.7.233.
20/03/02 19:35:47 INFO TaskSetManager: Finished task 12524.0 in stage 0.0 (TID 12524) in 1 ms on 172.16.7.233 (executor 1) (12525/1000000)
20/03/02 19:35:47 TRACE MessageDecoder: Received message OneWayMessage: OneWayMessage{body=NettyManagedBuffer{buf=CompositeByteBuf(ridx: 5, widx: 1622, cap: 1622, components=2)}}
20/03/02 19:35:47 TRACE MessageDecoder: Received message OneWayMessage: OneWayMessage{body=NettyManagedBuffer{buf=PooledUnsafeDirectByteBuf(ridx: 13, widx: 1630, cap: 32768)}}
20/03/02 19:35:47 TRACE MessageDecoder: Received message OneWayMessage: OneWayMessage{body=NettyManagedBuffer{buf=PooledUnsafeDirectByteBuf(ridx: 13, widx: 2414, cap: 4096)}}
</code></pre>

<p>Here is my <code>fluent-bit</code> configuration:</p>

<pre><code>[INPUT]
    Name  tail
    Path  /var/log/sparklog.log

# nest the record under the 'event' key
[FILTER]
    Name nest
    Match *
    Operation nest
    Wildcard *
    Nest_under event

# add event metadata
[FILTER]
    Name      modify
    Match     *
    Add index myindex
    Add host  ${HOSTNAME}
    Add app_name ${APP_NAME}
    Add namespace ${NAMESPACE}

[OUTPUT]
    Name        Splunk
    Match       *
    Host        splunk.example.com
    Port        30000
    Splunk_Token XXXX-XXXX-XXXX-XXXX
    Splunk_Send_Raw On
    TLS         On
    TLS.Verify  Off
</code></pre>",,1,1,,2020-2-28 06:27:15,,2020-3-2 22:05:36,2020-3-2 22:05:36,,4905042.0,,4905042.0,,1,1,apache-spark|kubernetes|splunk|fluentd|fluent-bit,696,11
503,258959,60551904,Capture last element either between or after / and before?,"<p>Say I have the following urls:</p>

<pre><code>https://test.com/welcome/
https://sub.test.com/home/edit
https://test.com/home/view?view=column
https://test.com/home/view/?view=list
</code></pre>

<p>I would like to capture the following result:</p>

<pre><code>welcome
edit
view
view
</code></pre>

<p>Right now I have <code>(?:\/[^\/]+)+?\/(.*?)/{0,1}$</code>, <code>(?:\/[^\/]+)+?(?:.*\/)(.*?)\?{0,1}$</code>, and <code>(?:\/[^\/]+)+?(?:.*\/)(.*)/\?.*$</code> but they are complicated and I can't seem to combine them.</p>",60552265.0,3,8,,2020-3-5 18:29:28,,2020-3-7 10:02:18,2020-3-5 18:57:59,,3832970.0,,3903479.0,,1,2,regex|splunk,65,12
504,258960,60554918,splunk join 2 search queries,"<p>I am writing a splunk query to find out top exceptions that are impacting client. So I have 2 queries, one is client logs and another server logs query. Joined both of them using a common field, these are production logs so I am changing names of it. I am trying to find top 5 failures that are impacting client. below is my query.</p>

<pre><code>index=pirs sourcetype=client-* env=* (type=Error error_level=fatal) error_level=fatal serviceName=FailedServiceEndpoint | table _time,serviceName,xab,endpoint,statusCode | join left=L right=R where L.xab = R.xab [search index=zirs sourcetype=server-*  | rex mode=sed field=span_name ""s#\..*$##"" | search span_success = false spanName=FailedServiceEndpoint |  table _time,spanName,xab] | chart count over L.serviceName
</code></pre>

<p>I explicitly mentioned a service name in here, In the final query there wont be service name, because we need top 5 failures that are impacting client. </p>

<p>This query provides me with service name and count, I also need other columns like endpoint name, httpStatusCode I am not sure how to do that and also if there is anything refactoring required for splunk query? </p>",,1,0,,2020-3-5 22:31:21,,2020-3-6 01:00:39,,,,,11962948.0,,1,0,splunk|splunk-query,481,10
505,258961,60585696,Extract Splunk domain from payload_printable field with regex,"<p>I'm trying to extract a domain from the Splunk payload_printable field (source is Suricata logs) and found this regex is working fine for most of the cases:</p>

<pre><code>source=""*suricata*"" alert.signature=""ET JA3*"" 
| rex field=payload_printable ""(?&lt;dom&gt;[a-zA-Z0-9\-\_]{1,}\.[a-zA-Z0-9\-\_]{2,}\.[a-zA-Z0-9\-\_]{2,})""
| table payload_printable, dom
</code></pre>

<p>The regular expression is:</p>

<pre><code>(?&lt;dom&gt;[a-zA-Z0-9\-\_]{1,}\.[a-zA-Z0-9\-\_]{2,}\.[a-zA-Z0-9\-\_]{2,})
</code></pre>

<p>For example, if my printable_payload looks like this:</p>

<pre><code>...........^aO+.t....]......$.....mT*l.......&amp;.,.+.0./.$.#.(.'.
...........=.&lt;.5./.
...].........activity.windows.com..........
.................
.......................#...........
</code></pre>

<p>The domain ""activity.windows.com"" is successfully extracted. Now, it doesn't work for such a payload, because the regex matches another part that does not correspond to the domain:</p>

<pre><code>...........^aO+]v;.~........:.Y.zORw._I..K&gt;..&amp;.,.+.0./.$.#.(.'.
...........=.&lt;.5./.
...].........activity.windows.com..........
.................
.......................#...........
</code></pre>

<p>It extracts ""Y.zORw._I"".</p>

<p>Another example:</p>

<pre><code>...........^h.'`.o2...
.y.k&gt;..e.ef...]..8.G..&amp;.,.+.0./.$.#.(.'.
...........=.&lt;.5./.
...p.........arc.msn.com..........
.................
.......................#.........h2.http/1.1...................
</code></pre>

<p>I don't know how to do. Thank you for your help.</p>",,1,3,,2020-3-8 08:21:09,1.0,2020-3-11 17:03:17,2020-3-11 17:03:17,,1078742.0,,1078742.0,,1,0,regex|textfield|splunk|data-extraction,183,10
506,258962,60615462,splunk is reporting each line of stacktrace as a separate event,"<p>Our splunk instance is showing each line of a java stacktrace as a separate event. </p>

<p><a href=""https://i.stack.imgur.com/1lRVx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1lRVx.png"" alt=""enter image description here""></a></p>

<p>Can we fix it without changing our java code? Thank you.</p>",,1,0,,2020-3-10 10:13:05,,2020-3-10 11:24:37,,,,,674669.0,,1,1,java|splunk,845,13
507,258963,60633586,Need help in Splunk Pie chart search expression,"<p>I am new to splunk dashboard development, so far I am creating KPI's using just 'single value'.</p>

<p>I have three KPI's resulted 600, 250, 150</p>

<p>KPI 1 search expression - Result is 600 (example)</p>

<pre><code>index=indexname kubernetes.container_name=tpt
MESSAGE = ""Code request""
| spath output=message path=MESSAGE 
| table _time message
| stats count as count1
</code></pre>

<p>KPI 2 search expression - Result is 250 (example)</p>

<pre><code>index=indexname kubernetes.container_name=rsv
MESSAGE = ""pin in email""
| spath output=message path=MESSAGE 
| table _time message
| stats count as count2
</code></pre>

<p>KPI 3 search expression - Result is 150 (example)</p>

<pre><code>index=indexname kubernetes.container_name=rsv
MESSAGE = ""pin in sms""
| spath output=message path=MESSAGE
| table _time message
| stats count as count3
</code></pre>

<p>I have shown above KPI's as numbers in the dashboard. However I would like show a pie chart with 60%, 25% and 15% share for above numbers. What would be search expression to create this chart?</p>",60634103.0,1,0,,2020-3-11 10:12:21,,2020-3-12 01:44:49,2020-3-12 01:44:49,,472495.0,,11899725.0,,1,-2,splunk|splunk-query|splunk-calculation|splunk-formula,245,10
508,258964,60637584,Splunk LDAP authentication,"<p>It seems one of the LDAP strategies has stopped working for an unknown reason.  I have confirmed the password and the settings are correct.  I have also checked the <em>Map Groups</em> field and confirmed that the <em>user</em> role has been added and I am able to see all the user that should be in there under <em>LDAP Users</em>  I have also tried reloading authentication configuration with no luck.  Any help or suggestions would be greatly appreciated.  Below is the message I am getting.  Any help or tips would be greatly appreciated not sure where else to go from here.</p>

<blockquote>
<pre><code>3/11/20

8:30:46.318 AM

03-11-2020 08:30:46.318 -0500 ERROR UiAuth - user=myuser action=login
status=failure reason=user-initiated useragent=""Mozilla/5.0 (Windows
NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)
Chrome/80.0.3987.132 Safari/537.36"" clientip=123.123.123.123

host = abc001source = \Splunk\var\log\splunk\splunkd.logsourcetype =
splunkd

3/11/20

8:30:46.318 AM

03-11-2020 08:30:46.318 -0500 ERROR UserManagerPro - LDAP Login
failed, could not find a valid user=""myuser"" on any configured servers

host = abc001source = *\Splunk\var\log\splunk\splunkd.logsourcetype =
splunkd
</code></pre>
</blockquote>",,1,0,,2020-3-11 13:48:53,,2020-3-12 00:22:31,2020-3-12 00:22:31,,7480820.0,,13045653.0,,1,1,active-directory|ldap|splunk|splunk-formula,352,11
509,258965,60642641,How to specify a specific color to each column in Splunk bar chart?,"<p>I have bar chart with next search query:</p>

<pre><code>&lt;chart&gt;
  &lt;search&gt;
    &lt;query&gt;sourcetype=st AND logger=lg AND (message=""A"" OR message=""B"" OR message=""C"")
          | stats count by message
    &lt;/query&gt;
    &lt;earliest&gt;-48h&lt;/earliest&gt;
    &lt;latest&gt;now&lt;/latest&gt;
  &lt;/search&gt;
  &lt;option name=""
         •••
            &lt;/option&gt;
&lt;/chart&gt;
</code></pre>

<p>How to specify a specific color to each column with next logic:</p>

<pre><code>if (message=""A"")
  then color = 0xFF0000
else if (message=""B"")
  then color = 0x00FF00
else if (message=""C"")
  then color = 0x0000FF
</code></pre>

<p>Thanks!</p>",,0,1,,2020-3-11 18:45:18,,2020-3-11 18:45:18,,,,,4399478.0,,1,1,bar-chart|splunk|splunk-query,418,10
510,258966,60645376,how to send the local file using splunk forwarder docker image?,"<pre><code>    splunkuniversalforwarder:
        image: splunk/universalforwarder
        environment:
            - SPLUNK_START_ARGS=--accept-license
            - SPLUNK_FORWARD_SERVER=ops-splunkhead02.dop.sfdc.net:9997
            - SPLUNK_USER=root
            - SPLUNK_PASSWORD=xxxx
        ports:
            - 9997:9997
</code></pre>

<p>I store the log flie in <code>/var/logs/serviceLog.log</code> (Not in the container but in the local machine)</p>

<p>I don't see the parameter to pass the file path;;; Seems like the splunk forwarder is running in the background and I just realized I never pass the log source variable to the container!</p>

<p>Does anyone perhaps have an idea?</p>",63080752.0,1,0,,2020-3-11 22:45:05,1.0,2020-7-25 13:02:37,,,,,7820956.0,,1,0,docker|docker-compose|splunk,296,9
511,258967,60645940,Splunk configuration for dot net,<p>How do I configure log4net to push log message to splunk? I am using dot net 4.6 version. I have searched google a lot and not able to find any configuration for framework 4.6.</p>,,1,0,,2020-3-12 00:01:40,,2020-3-12 02:07:54,,,,,12963140.0,,1,0,c#|asp.net|log4net|splunk,229,9
512,258968,60653207,Azure Functions for Splunk,"<p>I am trying to set up Azure functions for Splunk
The steps are shown as below. After the Azure function has been deployed, I could actually see from the app service plan interface with the data in &amp; out figure.However, from the splunk side, I was told that they do not receive any data (I do not have the access right to splunk)
My question is: From where can I know if my settings for Azure function for splunk is working ? as I see it is running and there is changing in data-in and data-out volume...</p>

<p>The steps that I followed</p>

<ol>
<li>In Azure, configure the diagnostics profiles of the resources that you want to monitor.</li>
<li>In Splunk, switch on the HEC port and get the token value.</li>
<li>Gather the settings below.</li>
<li>Click the ""Deploy to Azure"" button below.
<a href=""https://github.com/sebastus/AzureFunctionDeployment/tree/SplunkVS"" rel=""nofollow noreferrer"">https://github.com/sebastus/AzureFunctionDeployment/tree/SplunkVS</a></li>
<li>Authenticate to the Azure Portal (if necessary)</li>
<li>Fill in the form with the setting values</li>
<li>Wait a few minutes for the function to be created and deployed
8 In the Splunk UI, watch for events to appear.</li>
</ol>",,0,1,,2020-3-12 11:27:50,,2020-3-12 11:27:50,,,,,13050888.0,,1,1,azure|azure-functions|splunk|azure-eventhub,480,10
513,258969,60711827,Querying about field with JSON type value,"<p>I've the follow log:</p>

<blockquote>
  <p>INFO  [http-nio-80-exec-30] class:ControllerV3,
  M=method, UA=ua, URI=/v3/transactions,
  QS=limit=21&amp;offset=0&amp;sort=-createDate, V=v3, P=3, RT=50,
  ET=25, ELAPSE-TIME=50,</p>
  
  <p>REQ={""userId"":98745569,""initialCreationDate"":""2020-03-13T00:00:00"",""finalCreationDate"":""2020-03-16T15:41:36"",""source"":""SOURCE"",""statusIds"":[2,3,4,5,6,7,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,79],""accountingEntryType"":""ENTRY_TYPE"",""considerPartialTransaction"":true},</p>
  
  <p>GW=false</p>
</blockquote>

<p>So I don't know how to get metrics and data about the <code>REQ</code> JSON field. I want know which values are passed on <code>statusIds</code>, <code>accountingEntryType</code>, <code>considerPartialTransaction</code> and the range of date of <code>initialCreationDate</code> and <code>finalCreationDate</code>. To get metric with normal field I use something like <code>| stats count by UA</code>. I'm newbie with Splunk and I don't know some functions to get the results.</p>",,1,0,,2020-3-16 19:03:49,,2020-3-16 22:56:57,,,,,5140756.0,,1,0,splunk|splunk-query,144,9
514,258970,60714046,How to install Splunk Universal Forward as a service?,"<p>I need to install Splunk Universal Forwarder in our AWS EC2 instance. I need it to be installed as a service so that it automatically starts when the instance starts. The Splunk Docs do not seem to cover that: </p>

<p><a href=""https://docs.splunk.com/Documentation/Forwarder/8.0.2/Forwarder/Installanixuniversalforwarder#Install_from_a_tar_file"" rel=""nofollow noreferrer"">https://docs.splunk.com/Documentation/Forwarder/8.0.2/Forwarder/Installanixuniversalforwarder#Install_from_a_tar_file</a></p>",,1,0,,2020-3-16 22:34:00,,2020-3-16 22:46:52,,,,,3885794.0,,1,0,splunk,120,8
515,258971,60714620,How to filter data collected in Event Hub before sending to an external SIEM Solution which is IBM QRADAR here,"<p>One of my customer is trying to integrate IBM QRADAR SIEM with Azure. They would like to send all data from various sources to Event Hub and the data would be related to Azure AD, Azure VMs, Key Vault etc. </p>

<p>But my customer only wants to send Security related data from Event Hub and discard all the other data and then send only the security related data to IBM QRADAR. What is the method to filter this data from Event Hub so that the SIEM solution doesn't get too much data which are not security related and choke the system.</p>",,1,0,,2020-3-16 23:49:47,,2020-3-17 00:58:24,,,,,8123477.0,,1,0,azure|splunk|azure-eventhub|azure-log-analytics|qradar,490,10
516,258972,60714694,Splunk: Stats from multiple events and expecting one combined output,"<p>I have below events</p>

<p>event_a has <code>time_a</code> and <code>MAS_A</code> fields</p>

<p>event_b has <code>time_b</code> and <code>MAS_B</code> fields</p>

<p>event_c has <code>time_c</code> and <code>MAS_C</code> fields </p>

<pre><code>sourcetype=""app"" eventtype in (event_a,event_b,event_c) 
| stats avg(time_a) as ""Avg Response Time"" BY MAS_A 
| eval Avg Response Time=round('Avg Response Time',2) 
</code></pre>

<p>Output I am getting from above search is two fields <code>MAS_A</code> and <code>Avg Response Time</code></p>

<p>I am trying to get this for <code>event_b</code> and <code>event_c</code> as well in same search SPL and expecting final output with two fields only 
<code>MAS_A_B_C</code> and <code>Avg Response Time</code></p>",60717710.0,1,0,,2020-3-17 00:00:46,,2020-3-17 07:10:35,,,,,11842751.0,,1,0,splunk,362,12
517,258973,60725157,splunk: View the alerts created by another user,"<p>We are using splunk enterprise in our organization. Is it possible to view the alerts created by another user?</p>

<p><a href=""https://i.stack.imgur.com/6bpwn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6bpwn.png"" alt=""enter image description here""></a></p>",,1,0,,2020-3-17 15:12:47,,2020-3-17 18:11:15,,,,,674669.0,,1,0,splunk,17,4
518,258974,60729791,Splunk: Search SPL with multiple Stats,"<p>I have below events</p>

<p>event_a has <code>time_a</code> and <code>MAS_A</code> fields</p>

<p>event_b has <code>time_b</code> and <code>MAS_B</code> fields</p>

<p>event_c has <code>time_c</code> and <code>MAS_C</code> fields </p>

<pre><code>sourcetype=""app"" eventtype in (event_a,event_b,event_c) 
| stats avg(time_a) as ""Avg_Res_Time_a"" BY MAS_A 
| eval Avg_Res_Time_a=round('Avg_Res_Time_a',2) 
</code></pre>

<p>Output I am getting from above search is two fields <code>MAS_A</code> and <code>Avg_Res_Time_a</code></p>

<p>How can i get 4 fields output as below</p>

<p><code>MAS_A_B_C</code>  (should contain values from MAS_A, MAS_C, MAS_C)</p>

<p><code>Avg_Res_Time_a</code> (stats avg(time_a) as ""Avg_Res_Time_a"" BY MAS_A)</p>

<p><code>Avg_Res_Time_b</code> (stats avg(time_b) as ""Avg_Res_Time_b"" BY MAS_B)</p>

<p><code>Avg_Res_Time_c</code> (stats avg(time_c) as ""Avg_Res_Time_c"" BY MAS_C)</p>

<p>Sample Events:</p>

<pre><code>04-03-2020 11.31.19 OFF   performance [WebContainer : 78]: USER_ID=HEIS MAS_A=3 TIME_A=5.898
04-03-2020 02.33.42 OFF   performance [WebContainer : 29]: USER_ID=MONA MAS_B=2 TIME_B=1.18 MAS_C=2 TIME_C=2.87
04-03-2020 12.31.19 OFF   performance [WebContainer : 30]: USER_ID=HEIB MAS_A=2 TIME_A=1.22
04-03-2020 02.38.42 OFF   performance [WebContainer : 33]: USER_ID=MONA MAS_B=3 TIME_B=2.20 MAS_C=20 TIME_C=29.03
</code></pre>

<p>Expected output</p>

<pre><code>MAS_A_B_C   Avg_Res_Time_a   Avg_Res_Time_b   Avg_Res_Time_c
2           1.22             1.18             2.87
3           5.898            2.20       
20                                            29.03
</code></pre>",,1,0,,2020-3-17 20:31:12,,2020-3-18 20:11:12,2020-3-18 20:11:12,,11842751.0,,11842751.0,,1,0,splunk|splunk-query,164,8
519,258975,60746289,splunk query to concatenate status code for every hour,"<pre><code>index=abc sourcetype=firststream-* env=* module=API type=Error error_level=fatal serviceName=MyService |bin _time span=1h | stats count by _time,serviceName,httpStatusCode
</code></pre>

<p>output is displayed for every httpStatuscode in that hour. Instead, I want to concatenate httpStatusCode for that hour and display in a single column.</p>",60749314.0,1,2,,2020-3-18 19:27:02,0.0,2020-3-19 00:29:07,,,,,11962948.0,,1,0,splunk|splunk-query,120,8
520,258976,60776003,Splunk error specify atleast one named group,"<p>I am executing below splunk query.</p>

<pre><code>index=api sourcetype=api-warn environ::api-prod* 
| bin _time span=1h
| rex mode=sed field=service_name ""s#\..*$##"" | rex field=requestPath ""https://api.com.org.net/(abc)/(def)""
| stats count(service_name) by _time,service_name
</code></pre>

<p>Getting below error: 
Error in 'rex' command: The regex '<a href=""https://api.com.org.net/(abc)/(def)"" rel=""nofollow noreferrer"">https://api.com.org.net/(abc)/(def)</a>' does not extract anything. It should specify at least one named group. Format: (?...).</p>

<p>Suppose, one of the url is:</p>

<pre><code>https://api.com.org.net/abc/def/some_number/?key=value&amp;key=value
</code></pre>

<p>My regular expression: </p>

<pre><code>https:\/\/(&lt;api\.com\.org\.net\/abc\/def&gt;*)
</code></pre>

<p>My regular expression does not match url, Could someone help out.</p>

<p>Not sure what is the issue here. I am using regex to match part of url abc/def in url'. Not sure what is going wrong. Can some one direct me in proper direction?</p>",,1,0,,2020-3-20 14:21:59,,2020-3-20 19:48:44,2020-3-20 17:35:22,,11962948.0,,11962948.0,,1,0,splunk|splunk-query,629,11
521,258977,60804117,Splunk Query to update a query,"<p>I am working on a Splunk requirement, which is like the splunk script is scheduled to run every 15 mints from Mon-Fri for 30m. However they have a new requirement to run this script for 60m on Sat and Sunday alone. </p>

<p>What are the changes to be done in the existing script to run every 60m for sat and sundays alone. Please help.</p>

<p>Thanks
Venkatesh</p>",,1,0,,2020-3-22 19:43:33,,2020-3-23 02:08:20,,,,,7768715.0,,1,0,splunk|splunk-query|splunk-sdk|splunk-calculation|splunk-formula,76,8
522,258978,60805870,regex parse multiple expressions,"<p>I have a requirement to parse  part of url with multiple expressions using regex, expressions are like /abc/def (or) /z/a (or) /v/g. I have a regex that satisfies single expression, but not sure how to do for multiple expression. </p>

<pre><code>(?&lt;volga&gt;.+?\/abc\/def.+?)
</code></pre>

<p>volga is a named capturing group.
Above regular expression satisfies anything before /abc and anything after /def, the same way url also has /z/a and /v/g. This is kind of either /abc/def/ or /z/a or /v/g can exist in url. How to write a regular expression that checks for all of above expressions.</p>

<p>Examples of url:</p>

<pre><code>/api/com/abc/def/710660716/847170/
</code></pre>

<pre><code>/api/com/z/a/
</code></pre>",60807566.0,1,4,,2020-3-22 22:53:31,,2020-3-23 03:21:37,2020-3-22 23:01:48,,1491895.0,,11962948.0,,1,0,regex|splunk,44,7
523,258979,60807456,Splunk nested queries,"<p>I have the following query on splunk</p>

<pre><code>index=""cusomerIndex"" source=*client-api* ""pending customer approval""
</code></pre>

<p>This query gives me the following result</p>

<pre><code>msg: pending customer approval for customer1`
</code></pre>

<p>I have another query on splunk </p>

<pre><code>index=""orderIndex"" source=*order-api* ""email notification sent""
</code></pre>

<p>this query gives me the following result in the customer field</p>

<pre><code>customerId: customer1
msg: email notification sent
</code></pre>

<p>I'm trying to come up with a query where I get all the cutomers who had a result for ""pending customer approval"" but don't have a result for ""email notification sent"".  I'm not an expert in splunk so not sure how to do it.</p>",,1,3,,2020-3-23 03:04:11,,2021-1-12 23:54:01,,,,,12848348.0,,1,0,splunk,101,8
524,258980,60807823,group by part of url using regex splunk,"<p>I have multiple url's all start with <code>/api/net</code>, I want to group by next couple of strings that are separated by / like</p>

<pre><code>/api/net/abc/def?key=value
/api/net/c/d?key1=value1
/api/net/j/h?key2=value2
</code></pre>

<p>I have below regular expression which parses all url's but I explicitly have to specify required in regular expression .</p>

<pre><code>| rex field=requestPath ""(?&lt;volga&gt;.+?(\/abc\/def)|(\/c\/d)|(\/j\/h).+?)""
</code></pre>

<p>volga is a named capturing group, I want to do a group by on volga without adding /abc/def, /c/d,/j/h in regular expression so that I would know number of expressions in there instead of hard coding.
There are other expressions I would not know to add, So I want to group by on next 2 words split by / after ""net"" and do a group by , also ignore rest of the url. Let me know if you did not understand, I could explain more.</p>",,1,0,,2020-3-23 04:06:46,,2020-3-23 13:18:57,,,,,11962948.0,,1,0,splunk|splunk-query,307,9
525,258981,60824511,"regular expression, take first three lines or all lines less than 3 lines in splunk","<p>In java, how to write a regex to take first 3 lines if there is more than 3 lines OR take all lines if there is less than or equal to 3 lines?</p>

<p>I used the <a href=""https://regexr.com/"" rel=""nofollow noreferrer"">https://regexr.com/</a> to verify my own solution, failed. Then I asked the online chat group IRC #regex, the expert gives me answer within 1 minute. So I believe I shall share the knowledge here.  </p>",60824516.0,1,2,,2020-3-24 03:19:13,,2020-3-25 05:17:35,2020-3-25 05:17:35,,84592.0,,84592.0,,1,0,regex|splunk,32,6
526,258982,60863060,How to authenticate an on demand API call made via splunk?,"<p>I am thinking of developing an API in Python using Flask library.
I learnt the below.
It has a <code>/auth</code> to just take any <code>username</code> and <code>password</code> and <code>save</code> it in database i.e. for registration.
It has a <code>/login</code> to take <code>username</code>, <code>password</code> and <code>validate</code> it with database and then send a <code>token</code>.
It then has a <code>/xx</code> call to take the <code>token</code>, <code>validate</code> the identity of user and return him with the requested <code>data</code>.</p>
<p>I am now trying to understand the below.<br />
If I make an API call via custom script execution on SPLUNK to this API that I will develop, how will I pass <code>Splunk</code> login credentials to the API and how will the API make sure that they are correct?</p>",,1,0,,2020-3-26 08:06:37,,2020-11-16 18:06:12,2020-11-16 18:06:12,,10064174.0,,10064174.0,,1,0,python|api|splunk,80,8
527,258983,60866448,Splunk count consecutive events based on value?,"<p>I am facing an availability monitoring issue here. We do have a heartbeat set up in splunk which tells whether the app is up or not with status = 0 or 1 every minute.
The thing is, that sometimes a fail occurs for one event (for no reason, since the app is running) and it lowers the availability rating of the application which is based on this.</p>

<p>Is it possible to set up a chart that ignores events if they did not happen 5 times in a row?</p>

<p>Example:</p>

<pre><code>_time   Status
00:01   1
00:02   1
00:03   1
00:04   0
00:05   1
00:06   1
00:07   1
00:08   1
00:09   1
00:10   1
00:11   1
</code></pre>

<p>If a 0 occurs, i want to check whether it apeared in 4 previous events aswell and only then count it into my chart - if not i want to treat it as false positive.</p>",60873937.0,1,0,,2020-3-26 11:41:45,,2020-3-26 18:31:11,,,,,12186398.0,,1,0,count|splunk,134,8
528,258984,60872109,getting the average duration over a group of splunk transactions,"<p>So I have some data in the format of </p>

<pre><code>Time                | UUID           |  event_name_status            | actual_important_log_time 
---------------------------------------------------------------------------------------------------------------
2020-03-26T12:00:00 | 123456789      |  car_end                      | 2020-03-25T16:50:30
2020-03-26T12:00:00 | 123456789      |  car_mid                      | 2020-03-25T16:40:30
2020-03-26T12:00:00 | 123456789      |  car_start                    | 2020-03-25T16:30:30
2020-03-26T12:00:00 | 123456788      |  car_end                      | 2020-03-25T15:50:30
2020-03-26T12:00:00 | 123456788      |  car_mid                      | 2020-03-25T15:20:30
2020-03-26T12:00:00 | 123456788      |  car_start                    | 2020-03-25T14:50:30
</code></pre>

<p>Which Is a consistent pattern with each transaction having a start, mid and end with a different UID per transaction (also different vehichles for other transactions).</p>

<p>I currently group them into transactions using the following search command.</p>

<pre><code>* | transaction UUID startswith=""car_start"" endswith=""car_end""
</code></pre>

<p>Which groups the transactions showing how many there were in the last X length of time (could be hundreds/thousands in a day.</p>

<p>I need to get the duration of each transaction using the <strong>actual_important_log_time</strong> field and then use these values to get the average </p>",,1,0,,2020-3-26 16:46:27,,2020-4-5 05:58:10,,,,,13130287.0,,1,0,splunk|splunk-query|splunk-calculation|splunk-formula,279,9
529,258985,60883353,[splunk]: Obtain a count of hits in a query of regexes,"<p>I am searching for a list of regexes in a splunk alert like this:</p>

<pre><code>... | regex ""regex1|regex2|...|regexn""
</code></pre>

<p>Can I modify this query to get a table of the regexes found along with their count. The table shouldn't show rows with 0 counts.</p>

<pre><code>regex2 17
regexn 3
</code></pre>",,2,0,,2020-3-27 09:36:38,,2020-3-28 20:22:40,2020-3-27 14:31:45,,674669.0,,674669.0,,1,0,devops|splunk,127,9
530,258986,60955107,Equivalent of Splunk's lookup in Kusto Query Language,"<p>I am trying to find the equivalent of Splunk Query Language's <strong>lookup</strong> command in Kusto Query Language.
Please help.</p>",60955252.0,1,0,,2020-3-31 16:15:43,,2020-3-31 16:23:23,,,,,7665068.0,,1,0,azure|splunk|splunk-query|kql,443,11
531,258987,61001189,Splunk Universal Forwarder not sending data to Indexer,"<p>I am reading different logs from same source folder. But not all files are getting read, one stanza works other don't.
If i restart the UF, all stanzas work, but changed data is not capturing by one stanza.</p>

<p>files i am planning to monitor below files</p>

<pre><code>performance_data.log
performance_data.log.1
performance_data.log.2
performance_data.log.3

performance.log
performance.log.1
performance.log.2

SystemOut.log
</code></pre>

<p>my input.conf file </p>

<pre><code>[default]
host = LOCALHOST

[monitor://E:\Data\AppServer\A1\performance_data.lo*]
source=applogs
sourcetype=data_log
index=my_apps

[monitor://E:\Data\AppServer\A1\performance.lo*]
source=applogs
sourcetype=perf_log
index=my_apps

[monitor://E:\Data\logs\ImpaCT_A1\SystemOu*]
source=applogs
sourcetype=systemout_log
index=my_apps
</code></pre>

<p>\performance_data.lo* and \SystemOu* stanzas working fine, but performance.lo* stanza not working. only sends data when i restart the UF (universal forwarder), but changes were not sending automatically like other stanzas did.
Anything i am doing wrong here ?</p>",,1,0,,2020-4-2 21:11:42,,2020-9-3 06:47:42,,,,,11842751.0,,1,0,splunk,1257,13
532,258988,61030294,Can I use splunk timechart without aggregate function?,"<p><a href=""https://docs.splunk.com/Documentation/Splunk/8.0.2/SearchReference/Timechart"" rel=""nofollow noreferrer"">https://docs.splunk.com/Documentation/Splunk/8.0.2/SearchReference/Timechart</a>
I tried several syntaxes but none is working. they all require aggregate function.
My goal is to display a line chart, representing the value of an event field over time.
Very simple, I don't need any max/min/sum/count at all.
I need the x-axis to be the time span(time range that I passed in as query timespan), every event will be a data point in that chart, y-axis is the value of a field that I choose, for example, fieldA, which is a double value field.
how to write my splunk query? 
search query ...| timechart fieldA?
(you don't have to use timechart, any command that can achieve my goal will be accepted)    </p>

<p>update: let me try to describe what I wanted using a data generation example:
| makeresults count=10 | streamstats count AS rowNumber
let's say the time span is last 24 hours, when running above query in splunk, it will generate 10 records data with the same _time field which is @now, and a rowNumber field with values from 1 to 10. what I want to see is a visualization, x-axis starts from (@now-24hours) to @now, and no data points for most of the x-axis, but at last second(the rightmost) I want to see 10 dots, the y-axis values of them is from 1 to 10.</p>",,2,0,,2020-4-4 15:20:37,,2020-4-5 05:37:24,2020-4-4 23:14:49,,2652281.0,,2652281.0,,1,0,splunk,1301,12
533,258989,61065705,Set priority in Splunk OpsGenie App that is accessible in OpsGenie,"<p>I have created some Splunk alerts and triggers OpsGenie succesfully via <a href=""https://splunkbase.splunk.com/app/3759/"" rel=""nofollow noreferrer"">Opsgenie App</a>, 
Now I would like to set priority in the alert. However a field related to priority is missing. These are the ones returned from the REST API:</p>

<pre><code>action.opsgenie.command
action.opsgenie.description
action.opsgenie.forceCsvResults
action.opsgenie.hostname
action.opsgenie.icon_path
action.opsgenie.is_custom
action.opsgenie.label
action.opsgenie.maxresults
action.opsgenie.maxtime
action.opsgenie.payload_format
action.opsgenie.track_alert
action.opsgenie.ttl
</code></pre>

<p>And within Splunk UI one can only set <code>api_url</code> (<code>action.opsgenie.param.api_url</code>)</p>

<p>How can I set priority or a custom field and then access it in the Splunk Integration in OpsGenie?</p>",,1,0,,2020-4-6 17:41:58,,2020-4-6 17:41:58,,,,,630269.0,,1,0,splunk|opsgenie,154,8
534,258990,61128169,Parse IBM MQ v9.1 Error Logs using Splunk,"<p>I'm forwarding my IBM MQ v9.1 error logs using splunk forwarder to a centralized cluster to see trends on common error occurring across my distributed messaging systems. </p>

<p>However I'm unable to parse the required fields, since the format of MQ error logs are varying i.e. the severity of the messages could be error, warning, informational, severe and termination  and each have different set of fields in itself and are not consistent.</p>

<p>Please let me know if anyone have used regex in splunk for parsing the fields of IBM MQ error logs for v9.1.</p>

<p>I have tried few regex patterns but it wasn't parsing as expected.</p>

<p>I have already referred below link, but that is for v8 and there is a different in format of error logs for v9,
<a href=""https://t-rob.net/2017/12/18/parsing-mq-error-logs-in-splunk/"" rel=""nofollow noreferrer"">https://t-rob.net/2017/12/18/parsing-mq-error-logs-in-splunk/</a></p>

<p>Also the splunk user is unable to access the error logs. I have updated below stanza in qm.ini 
Filesystem: 
   ValidateAuth=No </p>

<p>also set chmod -R 755 to /var/mqm/qmgrs/qmName/errors folder. </p>

<p>Though the permissions for the ERROR logs doesn't change whenever it gets updated, when the logs rotate the permissions are revoked and splunk user is not able to read the logs. </p>

<p>Please let me know how to overcome this without adding splunk user to mqm group</p>",61129705.0,1,2,,2020-4-9 18:48:02,,2020-4-27 06:03:19,2020-4-27 06:03:19,,9882114.0,,9882114.0,,1,5,ibm-mq|splunk,576,15
535,258991,61129598,Send Kubernetes pod's logs to Splunk,"<p>I am using Amazon EKS and I have a server (consider it as X ) which is connected to the control node using kubectl.</p>

<p>I am able to get the pod logs from the server X by running the following command.
 kubectl logs -f podname -n=namespace</p>

<p>Now my goal is to send these pod logs to Splunk for which I am using <a href=""https://github.com/splunk/splunk-connect-for-kubernetes"" rel=""nofollow noreferrer"">splunk-connect-for-kubernetes</a></p>

<p>But as per the configurations of values.yaml file, kubernetes logs are forwarded to the Splunk instead of the pod logs.</p>

<p>I would specifically like to send the pod logs i.e. my application logs to the Splunk. Is there any way to achieve this?</p>",61130091.0,1,1,,2020-4-9 20:21:27,1.0,2020-4-9 21:58:52,,,,,9102740.0,,1,-1,kubernetes|kubernetes-helm|splunk|fluentd|amazon-eks,919,15
536,258992,61136636,Apache reverse proxy with cross origin (CORS): CentOS 7 / implement SSO (single sign on),"<p>I am using Apache(<code>2.4.23</code>) reserve proxy(<code>192.168.1.208</code>) in CentOS <code>7.2</code> to do crossing origin so that I could send cookies from one domain to the other. </p>

<p>Our application (location in <code>192.168.1.210</code>, using splunk Java sdk to sent request (<code>http://192.168.0.208/splunk</code>) to apache server (<code>192.168.0.208</code>). And apache reserver proxy will send request in turn to splunk server which listens to port <code>8000</code> in the same server. The purpose is to send cookies also to splunk server (<code>192.168.0.208:8000</code>) in order to make SSO. But I failed to send cookie to splunk server.</p>

<p>Our httpd.conf configuration is as follows:</p>

<pre><code>ServerRoot ""/usr/local/apache""

Listen 80

# Example:
# LoadModule foo_module modules/mod_foo.so
#
LoadModule authn_file_module modules/mod_authn_file.so
LoadModule authn_core_module modules/mod_authn_core.so
LoadModule authz_host_module modules/mod_authz_host.so
LoadModule authz_groupfile_module modules/mod_authz_groupfile.so
LoadModule authz_user_module modules/mod_authz_user.so
LoadModule authz_core_module modules/mod_authz_core.so
LoadModule access_compat_module modules/mod_access_compat.so
LoadModule auth_basic_module modules/mod_auth_basic.so
LoadModule socache_shmcb_module modules/mod_socache_shmcb.so
LoadModule reqtimeout_module modules/mod_reqtimeout.so
LoadModule filter_module modules/mod_filter.so
LoadModule mime_module modules/mod_mime.so
LoadModule log_config_module modules/mod_log_config.so
LoadModule env_module modules/mod_env.so
LoadModule headers_module modules/mod_headers.so
LoadModule setenvif_module modules/mod_setenvif.so
LoadModule version_module modules/mod_version.so
LoadModule proxy_module modules/mod_proxy.so
LoadModule proxy_connect_module modules/mod_proxy_connect.so
LoadModule proxy_ftp_module modules/mod_proxy_ftp.so
LoadModule proxy_http_module modules/mod_proxy_http.so
LoadModule proxy_ajp_module modules/mod_proxy_ajp.so
LoadModule lbmethod_byrequests_module modules/mod_lbmethod_byrequests.so
LoadModule unixd_module modules/mod_unixd.so
LoadModule status_module modules/mod_status.so
LoadModule autoindex_module modules/mod_autoindex.so
LoadModule vhost_alias_module modules/mod_vhost_alias.so
LoadModule dir_module modules/mod_dir.so
LoadModule alias_module modules/mod_alias.so
LoadModule rewrite_module modules/mod_rewrite.so

&lt;IfModule unixd_module&gt;
   User apache
   Group apache
&lt;/IfModule&gt;

ServerAdmin you@example.com

ServerName 192.168.1.208:80

&lt;Directory /&gt;
    AllowOverride All
    Require all denied
&lt;/Directory&gt;

DocumentRoot ""/usr/local/apache/htdocs""
&lt;Directory ""/usr/local/apache/htdocs""&gt;
    Options Indexes FollowSymLinks
    AllowOverride None
    Require all granted
&lt;/Directory&gt;

&lt;IfModule dir_module&gt;
    DirectoryIndex index.html
&lt;/IfModule&gt;

&lt;Files "".ht*""&gt;
    Require all denied
&lt;/Files&gt;

ErrorLog ""logs/error_log""

LogLevel debug
&lt;IfModule log_config_module&gt;
    LogFormat ""%h %l %u %t \""%r\"" %&gt;s %b \""%{Referer}i\"" \""%{User-Agent}i\"""" combined
    LogFormat ""%h %l %u %t \""%r\"" %&gt;s %b"" common

    &lt;IfModule logio_module&gt;
        LogFormat ""%h %l %u %t \""%r\"" %&gt;s %b \""%{Referer}i\"" \""%{User-Agent}i\"" %I %O"" combinedio
    &lt;/IfModule&gt;

    CustomLog ""logs/access_log"" common

&lt;/IfModule&gt;

&lt;IfModule alias_module&gt;
    ScriptAlias /cgi-bin/ ""/usr/local/apache/cgi-bin/""
&lt;/IfModule&gt;

&lt;IfModule cgid_module&gt;
&lt;/IfModule&gt;

&lt;Directory ""/usr/local/apache/cgi-bin""&gt;
    AllowOverride None
    Options None
    Require all granted
&lt;/Directory&gt;

&lt;IfModule mime_module&gt;
    TypesConfig conf/mime.types

    AddType application/x-compress .Z
    AddType application/x-gzip .gz .tgz
&lt;/IfModule&gt;

# Virtual hosts
Include conf/extra/httpd-vhosts.conf

&lt;IfModule proxy_html_module&gt;
    Include conf/extra/proxy-html.conf
&lt;/IfModule&gt;

&lt;IfModule ssl_module&gt;
    SSLRandomSeed startup builtin
    SSLRandomSeed connect builtin
&lt;/IfModule&gt;

&lt;location /api/move &gt;
    Order deny,allow
    Allow from all
&lt;/location&gt;
</code></pre>

<p>httpd-vhost.conf</p>

<pre><code>&lt;VirtualHost *:80&gt;
    ProxyRequests Off
    ProxyPreserveHost On

    &lt;Location ""/splunk""&gt;
        Options Indexes FollowSymLinks
        AllowOverride All
        Require all granted
        Header set Access-Control-Allow-Origin *
        Header set Access-Control-Allow-Methods ""GET, POST, OPTIONS""
        Header set Access-Control-Allow-Headers ""Content-Type""

        Order deny,allow
        Allow from all
        RewriteEngine on
        RewriteCond %{HTTP_COOKIE} ssouser=([^;]+) [NC]
        RewriteRule .* - [E=RU:%1]
        RequestHeader set REMOTE-USER %{RU}e

        ProxyPass http://192.168.1.208:8000/splunk
        ProxyPassReverse http://192.168.1.208:8000/splunk
    &lt;/location&gt;
&lt;/VirtualHost&gt;
</code></pre>

<p>In Splunk, I have server.conf and web.conf two configuation files in /opt/splunk/etc/system/local/ folder. </p>

<p>server.conf</p>

<pre><code>[general]
trustedIP = 192.168.1.208
serverName = Splunk_Core_02
pass4SymmKey = $7$RRvdYDdIlj4P2geQdtHluTRb7OfvZhTFTZGJ7z5JiZAkJ6Q1at6j0Q==
sessionTimeout = 30s

[sslConfig]
sslPassword = $7$m6pB5a0PWFg64VlNZGgunhGElO3qLiAc6NrhfLO+tpX2jR7WC7qm1Q==

[lmpool:auto_generated_pool_download-trial]
description = auto_generated_pool_download-trial
quota = MAX
slaves = *
stack_id = download-trial

[lmpool:auto_generated_pool_forwarder]
description = auto_generated_pool_forwarder
quota = MAX
slaves = *
stack_id = forwarder

[lmpool:auto_generated_pool_free]
description = auto_generated_pool_free
quota = MAX
slaves = *
stack_id = free

[license]
active_group = Enterprise

[diskUsage]
minFreeSpace = 1024

[lmpool:test_splunk]
quota = MAX
slaves = *
stack_id = enterprise
</code></pre>

<p>web.conf</p>

<pre><code>#   Version 7.2.4

[default]


[settings]
#SSO
SSOMode = permissive
trustedIP = 192.168.1.208
remoteUser = REMOTE-USER
tools.proxy.on = False

root_endpoint = /splunk

enableSplunkWebSSL = 0

httpport = 8000

mgmtHostPort = 127.0.0.1:8089

appServerPorts = 8065

splunkdConnectionTimeout = 30

enableSplunkWebClientNetloc = False

privKeyPath = $SPLUNK_HOME/etc/auth/splunkweb/privkey.pem
serverCert = $SPLUNK_HOME/etc/auth/splunkweb/cert.pem


sslVersions = tls1.2
cipherSuite = ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256
ecdhCurves = prime256v1, secp384r1, secp521r1

# external UI URIs
userRegistrationURL = https://www.splunk.com/page/sign_up
updateCheckerBaseURL = https://quickdraw.splunk.com/js/
docsCheckerBaseURL = https://quickdraw.splunk.com/help


showProductMenu = False
productMenuLabel = My Splunk
showUserMenuProfile = False
productMenuUriPrefix = https://splunkcommunities.force.com

x_frame_options_sameorigin = True

remoteUserMatchExact = 0
remoteGroupsMatchExact = 0
remoteGroupsQuoted = True

allowSsoWithoutChangingServerConf = 0
static_endpoint = /static
static_dir = share/splunk/search_mrsparkle/exposed
testing_endpoint = /testing
testing_dir = share/splunk/testing

rss_endpoint = /rss
embed_uri =
embed_footer = splunk&gt;
template_dir = share/splunk/search_mrsparkle/templates
module_dir = share/splunk/search_mrsparkle/modules
enable_gzip = True

use_future_expires = True

flash_major_version = 9
flash_minor_version = 0
flash_revision_version = 124

enable_proxy_write = True

js_logger_mode = None
js_logger_mode_server_end_point = util/log/js
js_logger_mode_server_poll_buffer = 1000
js_logger_mode_server_max_buffer = 100

ui_inactivity_timeout = 60

enable_insecure_login = True

simple_error_page = False

cacheBytesLimit = 4194304
cacheEntriesLimit = 16384
staticCompressionLevel = 9

enable_autocomplete_login = False

verifyCookiesWorkDuringLogin = True

login_content =

enabled_decomposers = plot

minify_js = True
minify_css = True

trap_module_exceptions = True
enable_pivot_adhoc_acceleration = True
pivot_adhoc_acceleration_mode = Elastic

jschart_test_mode = False
jschart_truncation_limit.chrome = 50000
jschart_truncation_limit.firefox = 50000
jschart_truncation_limit.safari = 50000
jschart_truncation_limit.ie11 = 50000
jschart_series_limit = 100
jschart_results_limit = 10000

choropleth_shape_limit = 10000

dashboard_html_allow_inline_styles = true
dashboard_html_allow_iframes = true

max_view_cache_size = 1000
pdfgen_is_available = 1
listenOnIPv6 = no

log.access_file = web_access.log
log.access_maxsize = 25000000
log.access_maxfiles = 5
log.error_maxsize = 25000000
log.error_maxfiles = 5
log.screen = True
request.show_tracebacks = True
engine.autoreload_on = False
tools.sessions.on = True
tools.sessions.timeout = 1
tools.sessions.restart_persist = True
tools.sessions.httponly = True
tools.sessions.secure = True
tools.sessions.forceSecure = False
response.timeout = 7200

tools.sessions.storage_type = file
tools.sessions.storage_path = var/run/splunk
tools.decode.on = True
tools.encode.on = True
tools.encode.encoding = utf-8

override_JSON_MIME_type_with_text_plain = True

job_min_polling_interval = 100
job_max_polling_interval = 1000

acceptFrom = *

maxThreads = 0
maxSockets = 0

dedicatedIoThreads = 0

keepAliveIdleTimeout = 7200
busyKeepAliveIdleTimeout = 12


forceHttp10 = auto
# Controls CORS headers sent with responses.  This only takes effect when appServerPorts is set to a non-zero value.
crossOriginSharingPolicy =

allowSslCompression = false
allowSslRenegotiation = true
sendStrictTransportSecurityHeader = false

enableWebDebug = true

allowableTemplatePaths =
enable_risky_command_check = true

loginCustomLogo =
customFavicon =
loginBackgroundImageOption = default
loginCustomBackgroundImage =

loginFooterOption = default
loginFooterText =
loginDocumentTitleOption = default
loginDocumentTitleText =
loginPasswordHint =
appNavReportsLimit = 500
</code></pre>

<p>When I click the link in our application (<code>192.1.168.210</code>)， it invokes splunk java sdk and send request (<code>http://192.168.1.208/splunk</code>) to (<code>http://192.168.1.208/splunk/en-GB/account/login?return_to=%2Fsplunk%2Fen-GB%2F</code>). I could not find cookie. I even could not enable SSO. How could it be?</p>

<p>I have SSO debug page shown below:</p>

<p><a href=""https://i.stack.imgur.com/jqY0U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jqY0U.png"" alt=""enter image description here""></a></p>",,0,7,,2020-4-10 08:32:48,,2020-4-22 06:24:06,2020-4-22 06:24:06,,84592.0,,84592.0,,1,1,apache|single-sign-on|reverse-proxy|splunk,205,9
537,258993,61205265,Calculate mean deviation with Splunk,"<p>I have a list of values in Splunk. I can use this list to calcualte <code>avg(vals)</code> and <code>stdev(vals)</code>. How do I calculate the mean deviation.</p>

<p>The mean deviation is the average absolute difference between the mean and each value in the list.</p>

<p><code>(Sum_x |mean-x|) / N</code></p>",,1,0,,2020-4-14 10:11:28,,2020-4-15 01:22:48,,,,,4005067.0,,1,0,statistics|splunk|splunk-calculation,223,10
538,258994,61210169,Send logs to splunk from datapower,<p>I have a log target which send logs to splunk from datapower. In splunk logs I am not able to see the host name from which device that log came. Is there any settings at datapower end which we can correct to display the host name in splunk.</p>,,2,2,,2020-4-14 14:31:31,,2020-4-15 05:34:55,,,,,9213815.0,,1,0,logging|splunk|ibm-datapower,423,10
539,258995,61234812,change splunk admin password from linux cli,"<p>How do I reset splunk admin password? I guess I need access to the file system that Splunk is running on in order to modify the password file.
The solution from here does not work somehow: <a href=""https://stackoverflow.com/questions/7088201/splunk-admin-password"">splunk admin password</a></p>",,2,0,,2020-4-15 17:23:40,,2020-4-15 23:31:24,,,,,5800982.0,,1,0,splunk,1555,11
540,258996,61238217,AWS CLI Ouput formats to SPLUNK,"<p>I'm using the AWS CLI to get some Kinesis metrics - part of that I'm able to specify the output format as one of the below: 
<a href=""https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration-format"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration-format</a></p>

<blockquote>
  <p>Output Format</p>
  
  <p>The Default output format specifies how the results are formatted. The
  value can be any of the values in the following list. If you don't
  specify an output format, json is used as the default.</p>

<pre><code>json – The output is formatted as a JSON
</code></pre>
  
  <p>string.</p>
  
  <p>yaml – The output is formatted as a YAML</p>
  
  <p>string. (Available in the AWS CLI version 2 only.)</p>
  
  <p>text – The output is formatted as multiple lines of tab-separated
  string values. This can be useful to pass the output to a text
  processor, like grep, sed, or awk.</p>
  
  <p>table – The output is formatted as a table using the characters +|- to
  form the cell borders. It typically presents the information in a
  ""human-friendly"" format that is much easier to read than the others,
  but not as programmatically useful.</p>
</blockquote>

<p>I've tried TEXT as that seems the most reasonable for splunk but I think the line separated data is messing up splunks ingest:</p>

<pre><code>METRICDATARESULTS   iteratorAgeMilliseconds itagemillis PartialData
METRICDATARESULTS   readProvisionedThroughputExceeded   itagemillis PartialData
TIMESTAMPS  2020-04-15T20:21:00+00:00
TIMESTAMPS  2020-04-15T20:20:00+00:00
TIMESTAMPS  2020-04-15T20:19:00+00:00
TIMESTAMPS  2020-04-15T20:18:00+00:00
TIMESTAMPS  2020-04-15T20:17:00+00:00
TIMESTAMPS  2020-04-15T20:16:00+00:00
VALUES  0.0
VALUES  0.0
VALUES  0.0
VALUES  0.0
VALUES  0.0
VALUES  0.0
METRICDATARESULTS   writeProvisionedThroughputExceeded  itagemillis PartialData
TIMESTAMPS  2020-04-15T19:36:00+00:00
TIMESTAMPS  2020-04-15T19:35:00+00:00
TIMESTAMPS  2020-04-15T19:34:00+00:00
TIMESTAMPS  2020-04-15T19:33:00+00:00
VALUES  0.0
VALUES  0.0
VALUES  0.0
VALUES  0.0
VALUES  0.0
VALUES  0.0
</code></pre>

<p>Any thoughts on either the AWS or splunk side on how best to handle ingesting this data ? </p>

<p>here's the CLI command  <code>aws cloudwatch get-metric-data --start-time 16:29 --end-time 23:59 --metric-data-queries file://metric-data-queries.json --output text</code>
and contents of metric-data-queries.json</p>

<pre><code>[
  {
    ""Id"": ""iteratorAgeMilliseconds"",
    ""MetricStat"": {
      ""Metric"": {
        ""Namespace"": ""AWS/Kinesis"",
        ""MetricName"": ""GetRecords.IteratorAgeMilliseconds"",
        ""Dimensions"": [
          {
            ""Name"": ""StreamName"",
            ""Value"": ""test.dev.com""
          }
        ]
      },
      ""Period"": 1,
       ""Stat"": ""Sum"",
        ""Unit"": ""Count""
    },
    ""Label"": ""itagemillis"",
    ""ReturnData"": true
  },
  {
    ""Id"": ""readProvisionedThroughputExceeded"",
    ""MetricStat"": {
      ""Metric"": {
        ""Namespace"": ""AWS/Kinesis"",
        ""MetricName"": ""ReadProvisionedThroughputExceeded"",
        ""Dimensions"": [
          {
            ""Name"": ""StreamName"",
            ""Value"": ""test.dev.com""
          }
        ]
      },
      ""Period"": 1,
       ""Stat"": ""Sum"",
        ""Unit"": ""Count""
    },
    ""Label"": ""itagemillis"",
    ""ReturnData"": true
  },
    {
    ""Id"": ""writeProvisionedThroughputExceeded"",
    ""MetricStat"": {
      ""Metric"": {
        ""Namespace"": ""AWS/Kinesis"",
        ""MetricName"": ""WriteProvisionedThroughputExceeded"",
        ""Dimensions"": [
          {
            ""Name"": ""StreamName"",
            ""Value"": ""test.dev.com""
          }
        ]
      },
      ""Period"": 1,
       ""Stat"": ""Sum"",
        ""Unit"": ""Count""
    },
    ""Label"": ""itagemillis"",
    ""ReturnData"": true
  },
    {
    ""Id"": ""putRecordSuccess"",
    ""MetricStat"": {
      ""Metric"": {
        ""Namespace"": ""AWS/Kinesis"",
        ""MetricName"": ""PutRecord.Success"",
        ""Dimensions"": [
          {
            ""Name"": ""StreamName"",
            ""Value"": ""test.dev.com""
          }
        ]
      },
      ""Period"": 1,
       ""Stat"": ""Sum"",
        ""Unit"": ""Count""
    },
    ""Label"": ""itagemillis"",
    ""ReturnData"": true
  },
    {
    ""Id"": ""putRecordsSuccess"",
    ""MetricStat"": {
      ""Metric"": {
        ""Namespace"": ""AWS/Kinesis"",
        ""MetricName"": ""PutRecords.Success"",
        ""Dimensions"": [
          {
            ""Name"": ""StreamName"",
            ""Value"": ""test.dev.com""
          }
        ]
      },
      ""Period"": 1,
       ""Stat"": ""Sum"",
        ""Unit"": ""Count""
    },
    ""Label"": ""itagemillis"",
    ""ReturnData"": true
  },
    {
    ""Id"": ""getRecordsSuccess"",
    ""MetricStat"": {
      ""Metric"": {
        ""Namespace"": ""AWS/Kinesis"",
        ""MetricName"": ""GetRecords.Success"",
        ""Dimensions"": [
          {
            ""Name"": ""StreamName"",
            ""Value"": ""test.dev.com""
          }
        ]
      },
      ""Period"": 1,
       ""Stat"": ""Sum"",
        ""Unit"": ""Count""
    },
    ""Label"": ""itagemillis"",
    ""ReturnData"": true
  }
]
</code></pre>",,1,2,,2020-4-15 20:39:35,,2020-4-15 23:16:58,2020-4-15 20:58:11,,800592.0,,800592.0,,1,0,amazon-web-services|command-line-interface|splunk|amazon-kinesis,127,8
541,258997,61243709,Splunk - Javascript with XML,"<p>I have developed the dashboard using xml coding. In XML, i have referenced the javascript.</p>

<p>XML:
I am passing the word input from the field as highlighttoken.</p>

<p>Javascript:
I am getting the word and processing the search in all the tables refereed in XML.I can able to search single word. But i want to search as multiple word string with comma separated. Please find the below coding and help me with the javascript.</p>

<p>Example: If i search the keyword as ""Splunk"" it will highlight but if i search as ""Splunk,web,access"" it will not display anything.</p>

<p>XML:</p>

<pre><code>&lt;form script=""highlightToken.js""&gt;
  &lt;label&gt;Highlight_text&lt;/label&gt;
  &lt;fieldset&gt;
    &lt;input type=""time"" token=""field1""&gt;
      &lt;label&gt;&lt;/label&gt;
      &lt;default&gt;
        &lt;earliest&gt;-5m@m&lt;/earliest&gt;
        &lt;latest&gt;now&lt;/latest&gt;
      &lt;/default&gt;
    &lt;/input&gt;
    &lt;input type=""text"" token=""highlightToken"" searchWhenChanged=""true""&gt;
      &lt;label&gt;Search Word&lt;/label&gt;
      &lt;default&gt;splunk*&lt;/default&gt;
      &lt;initialValue&gt;searchword&lt;/initialValue&gt;
    &lt;/input&gt;
  &lt;/fieldset&gt;
  &lt;row&gt;
    &lt;panel&gt;
      &lt;title&gt;$highlightToken$&lt;/title&gt;
      &lt;table id=""highlightTable1""&gt;
        &lt;search id=""highlightSearch1""&gt;
          &lt;query&gt;index=_internal
| stats count by sourcetype&lt;/query&gt;

          &lt;earliest&gt;$field1.earliest$&lt;/earliest&gt;
          &lt;latest&gt;$field1.latest$&lt;/latest&gt;
        &lt;/search&gt;

        &lt;option name=""drilldown""&gt;none&lt;/option&gt;
        &lt;option name=""refresh.display""&gt;progressbar&lt;/option&gt;
      &lt;/table&gt;
    &lt;/panel&gt;
    &lt;panel&gt;
      &lt;table id=""highlightTable2""&gt;
        &lt;search id=""highlightSearch2""&gt;
          &lt;query&gt;index=_internal 
| stats count by sourcetype
| head 5&lt;/query&gt;
          &lt;earliest&gt;$field1.earliest$&lt;/earliest&gt;
          &lt;latest&gt;$field1.latest$&lt;/latest&gt;
        &lt;/search&gt;
        &lt;option name=""drilldown""&gt;none&lt;/option&gt;
        &lt;option name=""refresh.display""&gt;progressbar&lt;/option&gt;
      &lt;/table&gt;
    &lt;/panel&gt;
  &lt;/row&gt;
  &lt;row&gt;
    &lt;panel&gt;
      &lt;table id=""highlightTable3""&gt;
        &lt;search id=""highlightSearch3""&gt;
          &lt;query&gt;index=_internal 
| stats count by sourcetype
| head 5&lt;/query&gt;
          &lt;earliest&gt;$field1.earliest$&lt;/earliest&gt;
          &lt;latest&gt;$field1.latest$&lt;/latest&gt;
        &lt;/search&gt;
        &lt;option name=""drilldown""&gt;none&lt;/option&gt;
        &lt;option name=""refresh.display""&gt;progressbar&lt;/option&gt;
      &lt;/table&gt;
    &lt;/panel&gt;
    &lt;panel&gt;
      &lt;table id=""highlightTable4""&gt;
        &lt;search id=""highlightSearch4""&gt;
          &lt;query&gt;index=_internal sourcetype=""splunkd""
| stats count by sourcetype
| head 5&lt;/query&gt;
          &lt;earliest&gt;$field1.earliest$&lt;/earliest&gt;
          &lt;latest&gt;$field1.latest$&lt;/latest&gt;
        &lt;/search&gt;
        &lt;option name=""drilldown""&gt;none&lt;/option&gt;
        &lt;option name=""refresh.display""&gt;progressbar&lt;/option&gt;
      &lt;/table&gt;
    &lt;/panel&gt;
  &lt;/row&gt;
  &lt;row&gt;
    &lt;panel&gt;
      &lt;table id=""highlightTable5""&gt;
        &lt;search id=""highlightSearch5""&gt;
          &lt;query&gt;index=_internal 
| stats count by sourcetype
| head 5&lt;/query&gt;
          &lt;earliest&gt;$field1.earliest$&lt;/earliest&gt;
          &lt;latest&gt;$field1.latest$&lt;/latest&gt;
        &lt;/search&gt;
        &lt;option name=""drilldown""&gt;none&lt;/option&gt;
        &lt;option name=""refresh.display""&gt;progressbar&lt;/option&gt;
      &lt;/table&gt;
    &lt;/panel&gt;
  &lt;/row&gt;
&lt;/form&gt;
</code></pre>

<p>Javascript|:</p>

<pre><code>require([
    ""underscore"",
    ""jquery"",
    ""splunkjs/mvc"",
    ""splunkjs/mvc/searchmanager"",
    ""splunkjs/mvc/tableview"",
    ""splunkjs/mvc/simplexml/ready!""
], function (_, $, mvc, SearchManager, TableView) {
    var defaultTokenModel = mvc.Components.get(""default"");
    // var strHighlightText = """"
    var strHighlightText = defaultTokenModel.get(""highlightToken"");
    var customCellRenderer = TableView.BaseCellRenderer.extend({
        canRender: function (cell) {
            return cell.field !== ""_time"";
        },
        render: function ($td, cell) {
            var strText = cell.value;
            if (strHighlightText !== ""*"" &amp;&amp; strHighlightText !== """") {
                var regEx = new RegExp(strHighlightText, ""gi"");
                strText = strText.replace(regEx, '&lt;b style=""background:#4dff4d;""&gt;$&amp;&lt;/b&gt;');
            }
            $td.addClass('string').html(_.template(strText));
        }
    });
    function highlightTable() {
        var maxCount = 6;
        for (i = 0; i &lt; maxCount; i++) {
            var tableItem = mvc.Components.get(""highlightTable"" + i);
            if (typeof (strHighlightText) !== ""undefined"" &amp;&amp; typeof (tableItem) !== ""undefined"") {
                var search = mvc.Components.get(""highlightSearch"" + i);
                if (search !== undefined) {
                    console.log(""highlightSearch:"", i)
                    search.startSearch();
                }
                console.log(""highlightToken:"", strHighlightText, "" tableId:"", i)
                tableItem.getVisualization(function (tableView) {
                    tableView.addCellRenderer(new customCellRenderer());
                });
            }
        }
    }
    defaultTokenModel.on(""change:highlightToken"", function (model, value, options) {
        if (typeof (value) !== ""undefined"" || value !== ""$value$"") {
            strHighlightText = value;
            highlightTable();
        }
    });
    highlightTable();
});
</code></pre>",,1,0,,2020-4-16 06:06:25,,2020-4-16 11:58:59,,,,,13327360.0,,1,1,javascript|xml|splunk,684,11
542,258998,61246919,Citrix/COTS application monitor - Splunk,"<p>Our COTS applications are configured on Citrix, is it possible to monitor COTS apps via Splunk?</p>",,1,0,,2020-4-16 09:26:57,,2020-4-17 13:26:27,2020-4-17 13:24:42,,4418.0,,3640692.0,,1,1,splunk|citrix|xendesktop|cots,26,5
543,258999,61246941,Triple backslashes in splunk,"<p>I have log4j properties file and when I try to put logs on splunk I see triple backslashes</p>

<pre><code>{\\\""v\\\"":\\\""1.0\\\"",\\\""category\\\"":\\\""APP\\\"",\\\""level\\\"":\\\""INFO\\\"",\\\""timeStamp\\\""
</code></pre>

<p>is that a splunk or can I modify that in log4j properties, so no \\\ is displayed?</p>

<p>Thank you</p>",,1,0,,2020-4-16 09:27:57,,2020-4-17 05:17:52,,,,,6904332.0,,1,0,log4j|splunk,67,8
544,259000,61252066,In splunk addition of two same column from 2 indexes,"<p>I have 2 indexes with one field(A) as common in both </p>

<p>Now I want the count of that same field(A) from both indexer in one panel .eg:</p>

<p>indexer 1= total event count of A=30<br>
indexer 2= total event count of A=20</p>

<p>now in a panel i want to show total count of A as 50</p>",,1,0,,2020-4-16 13:56:10,,2020-4-20 13:44:43,2020-4-16 17:28:05,,4418.0,,13330893.0,,1,1,splunk-query|splunk-calculation|splunk-formula,59,7
545,259001,61276675,Splunk - Lookup values + static search string = output with count,"<p>I want to perform a search where I need to use a static search string + input from a csv file with usernames:</p>

<ol>
<li><p>Search query-
               <code>index=someindex host=host*p* ""STATIC_SEARCH_STRING""</code></p></li>
<li><p>Value from users.csv where the list is like this- <em>Please note that User/UserList is NOT a field in my Splunk</em>:
<code>**UserList**
User1
User2
User3
.
.
UserN</code></p></li>
</ol>

<p>I have tried using multiple one of them being-
<code>| inputlookup users.csv | join [search index=someindex host=host*p* ""STATIC_SEARCH_STRING""] | lookup  users.csv UserList OUTPUT UserList as User| stats count by User</code></p>

<p>The above one just outputs the list of users with count as '1' - which I assume it is getting from the table itself.</p>

<p>When I try searching events for a single user like- 
<code>index=someindex host=host*p* ""User1"" ""STATIC_SEARCH_STRING""</code>. I get 100's of events for that user.</p>

<p>Can someone please help me with this?
Sorry if this is a noob question, I have been trying to learn splunk in order to reduce my workload and am stuck here.</p>

<p>Thanks in advance! </p>",61283130.0,2,3,,2020-4-17 16:49:43,,2020-4-18 01:19:02,,,,,12753027.0,,1,0,splunk|splunk-query,2100,13
546,259002,61280311,Splunk - Convert Categorical Field with High Cardinality into Numbers,"<p>Question: I am using the default Splunk UI Search screen in which I have a search containing a field of categorical values (e.g. host names) which I would like to convert to numbers. So far, the only solution I have found is to use the eval-case combination to identify and convert each name. This works great if you have a small finite list of names. My field has an undetermined number of names and I want to avoid using Splunk's built-in hot coding e.g. eval {field} = 1. This spawns a set of columns which can run into very high numbers depending on the cardinality of the field. </p>

<p>Below is an example using eval and case. Again, this works fine for a few known values. I am seeking a more dynamic approach to deal with a large number of values.</p>

<pre><code>| eval src_zone_num = case(src_zone == ""zone1"", 1, src_zone == ""zone2"", 2, src_zone == ""zone3"", 3, src_zone == ""zone4"", 4, src_zone == ""zone5"", 5)
</code></pre>",,1,0,,2020-4-17 20:40:19,,2020-4-18 01:01:15,,,,,2284452.0,,1,0,splunk|splunk-query,53,6
547,259003,61294297,How to multiply value from previous command to some constant?,"<p>I am doing a query such that it will take the total count of the a value and then multiply it by some constant. For example :</p>

<pre><code>source=""test.csv""  sourcetype=""csv"" | stats count(adId) 
</code></pre>

<p>I want to multiply the result returned by the count by 0.5. Suppose if the output by <code>stats count</code> is 124 I want to multiply it by 0.5 and report the output.</p>",61296421.0,1,0,,2020-4-18 18:37:25,,2020-4-18 21:49:19,2020-4-18 18:46:44,,4157124.0,,5368424.0,,1,1,splunk|splunk-query|splunk-calculation|splunk-formula,76,10
548,259004,61348615,write data into splunk using Spring Boot,"<p>I am new to Splunk and working on connecting to Splunk API through Splunk SDK, Here is the sample connectivity code</p>

<pre><code>try {
        ServiceArgs args = new ServiceArgs();
        args.setHost(""localhost"");
        args.setPort(8089);
        args.setScheme(""http"");
        args.setUsername(""admin"");
        args.setPassword(""welcome1"");
        service = Service.connect(args);
        receiver = service.getReceiver();
    } catch (Exception e) {
        throw new RuntimeException(""closing connection"", e);
    }
</code></pre>

<p>When i try to run above block i am getting below error so not sure if i am missing anything here, I didn't change any ports when i did the installation </p>

<pre><code>java.net.SocketException: Unexpected end of file from server
at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:851) ~[na:1.8.0_191]
at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678) ~[na:1.8.0_191]
at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:848) ~[na:1.8.0_191]
at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678) ~[na:1.8.0_191]
at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587) ~[na:1.8.0_191]
at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492) ~[na:1.8.0_191]
at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_191]
at com.splunk.HttpService.send(HttpService.java:467) ~[splunk-1.6.3.0.jar:1.6.3]
at com.splunk.Service.send(Service.java:1295) ~[splunk-1.6.3.0.jar:1.6.3]
at com.splunk.HttpService.post(HttpService.java:348) ~[splunk-1.6.3.0.jar:1.6.3]
at com.splunk.Service.login(Service.java:1124) ~[splunk-1.6.3.0.jar:1.6.3]
at com.splunk.Service.login(Service.java:1103) ~[splunk-1.6.3.0.jar:1.6.3]
at com.splunk.Service.connect(Service.java:189) ~[splunk-1.6.3.0.jar:1.6.3]
at com.parexel.integration.api.AuditController.helloParexel(AuditController.java:53) ~[classes/:na]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_191]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_191]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_191]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_191]
at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:190) ~[spring-web-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138) ~[spring-web-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:105) ~[spring-webmvc-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:879) ~[spring-webmvc-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:793) ~[spring-webmvc-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) ~[spring-webmvc-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1040) ~[spring-webmvc-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:943) ~[spring-webmvc-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006) ~[spring-webmvc-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:898) ~[spring-webmvc-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at javax.servlet.http.HttpServlet.service(HttpServlet.java:634) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883) ~[spring-webmvc-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at javax.servlet.http.HttpServlet.service(HttpServlet.java:741) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53) ~[tomcat-embed-websocket-9.0.33.jar:9.0.33]
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100) ~[spring-web-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) ~[spring-web-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93) ~[spring-web-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) ~[spring-web-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201) ~[spring-web-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) ~[spring-web-5.2.5.RELEASE.jar:5.2.5.RELEASE]
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:202) ~[tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96) [tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:541) [tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139) [tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92) [tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74) [tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343) [tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:373) [tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65) [tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:868) [tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1594) [tomcat-embed-core-9.0.33.jar:9.0.33]
at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) [tomcat-embed-core-9.0.33.jar:9.0.33]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_191]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_191]
at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-9.0.33.jar:9.0.33]
at java.lang.Thread.run(Thread.java:748) [na:1.8.0_191]
</code></pre>

<p>I did check for the port with netstat -a and don't see 8089 is running, so appreciate any help here</p>",,1,0,,2020-4-21 16:41:55,,2020-4-22 02:30:23,,,,,10270958.0,,1,0,spring|spring-boot|splunk|splunk-sdk,828,11
549,259005,61353530,Enable Proxy in Jenkins for Splunk Plugin,"<p>I've installed Splunk plugin for Jenkins and I went to Jenkins configuration to update ""Splunk for Jenkins Configuration"". After saving the changes, I clicked test connection and got ""Connection Refused"" error.</p>

<p>When I ran CURL for splunk URL with proxy I was able to get the response back and this confirms that Jenkins is trying to hit without proxy.</p>

<p>Is there any place in Jenkins where I can configure the proxy so that Jenkins can pick that up while it is trying to connect to splunk server or is there any other alternative approach to this.</p>",,1,0,,2020-4-21 21:25:59,,2020-4-22 04:06:52,,,,,2474322.0,,1,0,jenkins|jenkins-plugins|splunk|jenkins-groovy,89,7
550,259006,61357666,How can we write the Splunk Query to find subField2 is present or not and if present get the counts of all subFiled2,"<p>{
index:""myIndex"",
field1: ""myfield1"",
field2:  {""subField1"":""mySubField1"",""subField2"":145,""subField3"":500},
...
..
.
}</p>

<p>SPL : index:""myIndex"" eval result = if(field.subField2) .....
is the dot operator works in SPL ?</p>",,2,0,,2020-4-22 05:07:13,,2020-4-22 16:26:54,,,,,9547537.0,,1,0,splunk|splunk-query,41,6
551,259007,61358636,Query for calculating duration between two different logs in Splunk,"<p>As part of my requirements, I have to calculate the duration between two different logs using Splunk query.
For example:</p>

<p><strong>Log 2:</strong>
    <em>2020-04-22 13:12  ADD request received ID : 123</em></p>

<p><strong>Log 1 :</strong>
    <em>2020-04-22 12:12  REMOVE request received ID : 122</em></p>

<p>The common String between two logs is "" request received ID :"" and unique strings between two logs are ""ADD"", ""REMOVE"". And the expected output duration is 1 hour.</p>

<p>Any help would be appreciated. Thanks</p>",,2,0,,2020-4-22 06:30:08,1.0,2020-4-22 16:32:10,,,,,5537431.0,,1,1,splunk|splunk-query,834,15
552,259008,61403610,Splunk: How to enable Splunk SSO,"<p>I have splunk and try to enable splunk SSO instead of nornal authentiation. I have configuraitons as follows:</p>

<p>In /opt/splunk/etc/system/local/server.conf</p>

<pre><code>[general]
trustedIP = 192.168.1.208
serverName = Splunk_Core_02
pass4SymmKey = $7$RRvdYDdIlj4P2geQdtHluTRb7OfvZhTFTZGJ7z5JiZAkJ6Q1at6j0Q==
sessionTimeout = 30s

[sslConfig]
sslPassword = $7$m6pB5a0PWFg64VlNZGgunhGElO3qLiAc6NrhfLO+tpX2jR7WC7qm1Q==

[lmpool:auto_generated_pool_download-trial]
description = auto_generated_pool_download-trial
quota = MAX
slaves = *
stack_id = download-trial

[lmpool:auto_generated_pool_forwarder]
description = auto_generated_pool_forwarder
quota = MAX
slaves = *
stack_id = forwarder

[lmpool:auto_generated_pool_free]
description = auto_generated_pool_free
quota = MAX
slaves = *
stack_id = free

[license]
active_group = Enterprise

[diskUsage]
minFreeSpace = 1024

[lmpool:test_splunk]
quota = MAX
slaves = *
stack_id = enterprise
</code></pre>

<p>In /opt/splunk/etc/system/local/web.conf</p>

<pre><code>[settings]
#SSO
SSOMode = permissive
trustedIP = 192.168.1.208,192.168.2.15,127.0.0.1
remoteUser = REMOTE-USER
#tools.proxy.on = False

root_endpoint = /splunk

#SSL
enableSplunkWebSSL = 0

httpport = 8000

mgmtHostPort = 127.0.0.1:8089

appServerPorts = 8065

splunkdConnectionTimeout = 30

enableSplunkWebClientNetloc = False

# SSL certificate files.
privKeyPath = $SPLUNK_HOME/etc/auth/splunkweb/privkey.pem
serverCert = $SPLUNK_HOME/etc/auth/splunkweb/cert.pem

...
</code></pre>

<p>I see <a href=""http://192.168.1.208:8000/debug/sso"" rel=""nofollow noreferrer"">http://192.168.1.208:8000/debug/sso</a> page, I see SSO is not enabled. What's wrong with my configurations?</p>",,3,0,,2020-4-24 07:51:04,,2020-5-13 07:14:56,,,,,84592.0,,1,0,splunk,228,11
553,259009,61426758,Splunk integration with Spring Boot,"<p>Please help me out with Splunk integration with SpringBoot. It will be great if you provided any code snippets or references.</p>

<p>Thank You.</p>",,2,0,,2020-4-25 13:55:32,1.0,2021-3-9 18:13:59,,,,,8949390.0,,1,1,spring-boot|splunk,9194,17
554,259010,61450330,Splunk query to extract a number (time taken),"<p>I have a splunk query result such as below:</p>

<p><code>2020-04-09 23:33:51.120  INFO 1 --- [an-885-exec-8] c.z.q.p.serv.backendCall      : time taken 11793 in ms</code></p>

<p>I want to extract the number after string ""time taken"" and table the result</p>

<p>I tried the below query</p>

<p><code>index=my-platform "": time taken"" | rex "".*:\stime\taken\s(?P&lt;time_taken&gt;(\d+))\sin\sms"" | table time_taken</code></p>

<p>This query results in a blank table and prints nothing.</p>

<p>Can anyone help with the right query to extract just the number next to time taken? </p>",,1,2,,2020-4-27 01:24:09,,2020-4-27 13:00:11,,,,,9616763.0,,1,0,regex|splunk|splunk-query,104,8
555,259011,61453301,Fluentd with HEC on Centos,"<p>I am trying to configure fluentd using td-agent steps with HEC for logging on centos, i am able to do complete installation with no issues, but when trying to insert HEC Stanza in tdagent.conf and then restarting the service, fluentd is not starting and coming in failed state.</p>

<p>i have tried various solutions, but with no luck. (Tracked fluentd GitHub issues as well)</p>

<p>my conf file - /etc/td-agent/td-agent.conf</p>

<p>HEC block - </p>

<pre><code>&lt;match **&gt;
  @type splunk_hec
  hec_host http-wwww.example.com
  hec_port XXX
  hec_token 00000000-0000-0000-0000-000000000000
&lt;/match&gt;
</code></pre>

<p>After Restarting service status is falied with following warning ( Checked with sudo tail -f /var/log/td-agent/td-agent.log )</p>

<pre><code>2020-04-27 06:37:37 +0000 [info]: gem 'fluent-plugin-record-modifier' version '2.1.0'
2020-04-27 06:37:37 +0000 [info]: gem 'fluent-plugin-rewrite-tag-filter' version '2.2.0'
2020-04-27 06:37:37 +0000 [info]: gem 'fluent-plugin-s3' version '1.3.1'
2020-04-27 06:37:37 +0000 [info]: gem 'fluent-plugin-splunk-hec' version '1.2.1'
2020-04-27 06:37:37 +0000 [info]: gem 'fluent-plugin-systemd' version '1.0.2'
2020-04-27 06:37:37 +0000 [info]: gem 'fluent-plugin-td' version '1.1.0'
2020-04-27 06:37:37 +0000 [info]: gem 'fluent-plugin-td-monitoring' version '0.2.4'
2020-04-27 06:37:37 +0000 [info]: gem 'fluent-plugin-webhdfs' version '1.2.4'
2020-04-27 06:37:37 +0000 [info]: gem 'fluentd' version '1.10.2'
2020-04-27 06:37:37 +0000 [warn]: [output_td] Use different plugin for secondary. Check the plugin works with primary like secondary_file primary=""Fluent::Plugin::TreasureDataLogOutput"" secondary=""Fluent::Plugin::FileOutput""
</code></pre>

<p>I have installed fluentd-HEC plugin using <strong>sudo td-agent-gem install fluent-plugin-splunk-hec -v 1.2.1</strong></p>

<p>On removing HEC block from td-agent.conf file fluentd runs normally.</p>",,0,0,,2020-4-27 07:02:02,,2020-4-27 07:02:02,,,,,4981408.0,,1,2,logging|splunk|fluentd,263,9
556,259012,61464005,How to form Splunk query to split a field into separate fields as per the maximum number of partitions?,"<p>I have some strings like below returned by my <code>Splunk</code> base search.</p>
<pre><code>&quot;CN=aa,OU=bb,DC=cc,DC=dd,DC=ee&quot;
&quot;CN=xx,OU=bb,DC=cc,DC=yy,DC=zz&quot;
&quot;CN=ff,OU=gg,OU=hh,DC=ii,DC=jj&quot;
&quot;CN=kk,DC=ll,DC=mm&quot;
</code></pre>
<p>Note: CN,OU,DC could be 0 or many.</p>
<p>My ultimate goal is to find all OUs something like below.<br />
(The combinations also need to be unique.)<br />
(All blank lines can be excluded.)<br />
eg:</p>
<pre><code>bb     (blank)
gg      hh
(blank) (blank)
</code></pre>
<p>The query that am using currently is not nice and it is not generic.<br />
It will work if at least one of my split results into 5 parts (0,1,2,3,4).<br />
But, it will not work and give blank results if none of my split results into 5 parts (0,1,2,3,4) i.e. all of them result in less than 5 parts.</p>
<pre><code>index=xx sourcetype=yy
| fields s
| rex field=s mode=sed &quot;s/,DC=.*//g&quot;
| eval temp=split(s,&quot;,OU=&quot;)
| eval a=mvindex(temp,1)
| eval b=mvindex(temp,2)
| eval c=mvindex(temp,3)
| eval d=mvindex(temp,4)
| dedup a b c d
| table a,b,c,d
</code></pre>
<p>How to make it generic i.e. get the count of split and make fields as per maximum split length?</p>",,1,3,,2020-4-27 16:48:32,1.0,2020-11-16 17:59:14,2020-11-16 17:59:14,,10064174.0,,10064174.0,,1,1,split|splunk|splunk-query,4834,15
557,259013,61479397,Is there a tool for Splunk like Toad that allows for multiple queries in the same editor?,"<p>I have the standard Splunk query web interface that allows the user to enter a single query at a time that looks like this:</p>

<p><a href=""https://i.stack.imgur.com/6Lxib.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Lxib.png"" alt=""enter image description here""></a></p>

<p>Is there a tool that allows multiple queries in the same editor that can be selected one at a time and queried so that as I'm working I can write a series of queries and execute them one at a time by selecting a single query?</p>",,1,0,,2020-4-28 11:43:28,,2020-4-28 12:52:26,,,,,2670571.0,,1,0,splunk|splunk-query,35,8
558,259014,61488124,How do I send non-cloudwatch JSON events to Splunk from Kinesis Firehose?,"<p>I'm trying to send a non-cloudwatch event to Splunk from Kinesis Firehose. I am processing the event with a Lambda and feeding it back into the Firehose in the following format (<a href=""https://docs.amazonaws.cn/en_us/firehose/latest/dev/data-transformation.html"" rel=""nofollow noreferrer"">required for Firehose</a>):</p>

<pre><code>{ 
    ""records"": [
        {
          ""recordId"": ""2345678"",
          ""result"": ""Ok"",
          ""data"": [base64-encoded custom JSON]
        }
    ]
}
</code></pre>

<p>However, it's throwing a vague parsing error once it gets to Splunk, with a help link that goes nowhere:</p>

<pre><code>""errorCode"":""Splunk.InvalidDataFormat"",""errorMessage"":""The data is not formatted correctly. To see how to properly format data for Raw or Event HEC endpoints, see Splunk Event Data (http://dev.splunk.com/view/event-collector/SP-CAAAE6P#data)""
</code></pre>

<p>What am I missing here? It seems strange that the HEC endpoint wouldn't be able to parse the messages coming from Firehose in their standard format.</p>

<p>I am sending the message to an HEC Event endpoint, using the splunk_configuration block in an <a href=""https://www.terraform.io/docs/providers/aws/r/kinesis_firehose_delivery_stream.html#splunk-destination"" rel=""nofollow noreferrer"">aws_kinesis_firehose_delivery_stream Terraform module</a>.</p>",,1,0,,2020-4-28 19:12:39,,2020-4-28 21:00:08,,,,,4963278.0,,1,0,json|parsing|terraform|splunk|amazon-kinesis-firehose,403,11
559,259015,61492764,Public read only Splunk data for testing,"<p>Is there a public instance of Splunk that can be used to test queries?</p>

<p>I Googled ""public splunk test instance"" and didn't see anything there.  </p>",,2,0,,2020-4-29 01:33:47,,2020-4-29 12:01:07,,,,,2670571.0,,1,0,testing|splunk,30,7
560,259016,61493004,Splunk query to get all counts including events (_raw) where match does not exist,"<p>How do I get a count of all records for a given field including a count of all records where the field does not exist.  </p>

<p>For example:</p>

<p>Given data that generally looks something like this:</p>

<pre><code>{""source_host"":""host1"", ""msg"":""some message"", ""user"":""jack""}
{""source_host"":""host2"", ""msg"":""some other message"", ""user"":""jill""}
</code></pre>

<p>I can get a count of all records like this:</p>

<pre><code>index=""my_index"" sourcetype=my_proj:my_logs | table _raw | stats count(_raw)
</code></pre>

<p>I can get a count of records for a given field like this:</p>

<pre><code>index=""my_index"" sourcetype=my_proj:my_logs | stats count(_raw) by source_host
</code></pre>

<p>Gives a table like this</p>

<pre><code>host       count
host_1     89
host_2     57
</code></pre>

<p>But I would like the query to also count records where the field exists but is empty, like this:</p>

<pre><code>{""source_host"":"""", ""msg"":""some message"", ""user"":""jack""}
</code></pre>

<p>And also count messages like this:</p>

<pre><code>asdf asdf asdf asdf asd fasdfasdfafas 
foo bar
Some other Junk someone wrote to my log
</code></pre>

<p>To get a table like this</p>

<pre><code>host       count
host_1     89
host_2     57
null       1
no_def     3
</code></pre>",,3,0,,2020-4-29 02:03:29,,2020-4-29 14:01:14,,,,,2670571.0,,1,0,splunk|splunk-query,1844,16
561,259017,61494406,Can you map values of a token to another value?,"<p>I was wondering if it was possible if you could change the value of a token (dropdown menu) in a query. </p>

<p>For context: I have a dropdown menu - which has values 1,2,3. I am using these values in a search query. However, I am also using another search query with a different index on the same dashboard that uses a,b,c. Is there a way to map the values 1,2,3 -> a,b,c or do it within the search query using an eval or something?</p>

<p>Thanks</p>",61513927.0,2,0,,2020-4-29 04:45:19,,2020-4-30 00:29:36,,,,,12069869.0,,1,0,splunk|splunk-query,669,12
562,259018,61498661,Adding custom column / field into splunk result,"<p>i am new to splunk and i am trying thing out on my own.
This might be an elementary question to most of you , but please be patient in trying to help me out.</p>

<pre><code>| inputlookup ""Wsp.csv""
| eval Outage = if(PublisherStatus = ""Active"", ""1"",""0"")
| eval _time=strptime(_time, ""%Y-%m-%dT%H:%M:%S"")
| eval DayOfWeek=strftime(_time, ""%A"")
</code></pre>

<p>I am trying to add Outage and DayOfWeek to be displayed in the result.</p>

<p>i tried using field Outage and dayofweek but it doesn't display the rest of fields present in Wsp.csv</p>

<p>is it possible to display Wsp + Outage + dayofweek in the search result ? how ?</p>",,1,0,,2020-4-29 09:35:25,,2020-4-29 13:10:32,,,,,12291110.0,,1,0,splunk|spl,109,8
563,259019,61501642,Splunk Role Validation for dashboard creation,"<p>Which capability i should add/delete from  any role  if i don't want to give access to create dashboard to that role say splunk_user ..
Below capability doesn't help 
Splunk_user => edit_per_panel_filters</p>

<p>kindly suggest..</p>",,1,0,,2020-4-29 12:18:21,,2020-4-30 00:33:09,2020-4-29 15:03:33,,13330893.0,,13330893.0,,1,0,splunk|splunk-query,38,6
564,259020,61529156,Splunk indexer running in docker container overwrites inputs.conf on docker restart,"<p>I am trying to 'upgrade' Splunk from 7.2.5 to 8.0.3.  Splunk is running on a RHEL7 VM in a docker container from Splunk.  (We not actually upgrading Splunk, we are moving to a new container on a new VM.)  Through automation, we had modified our container's etc/system/local/inputs.conf to run with SSL according to the Splunk documentation, and in 7.2.5 this works.</p>

<p>In 8.0.3, we are finding that configuration entries in inputs.conf are being erased whenever we restart docker.  (/opt/splunk is a folder mounted in the container so that it persists.)  Splunk is not 'restoring' the file (for example, from the ../defaults folder) - from testing, we've discovered that some comments do survive, but the configuration entries for SSL are being deleted and Splunk 8 is not running using SSL.</p>

<p>server.conf is also getting clobbered.</p>

<p>Anyone else notice this behavior?</p>

<p>Before restart:</p>

<pre><code>[default]
host = edb999320984
# BEGIN ANSIBLE MANAGED BLOCK
[splunktcp-ssl:9997]
disabled = 0

[SSL]
serverCert = /opt/splunk/etc/...
requireClientCert=false
# END ANSIBLE MANAGED BLOCK
</code></pre>

<p>After restart, all that remains is:</p>

<pre><code>[splunktcp://9997]
disabled = 0
# BEGIN ANSIBLE MANAGED BLOCK
</code></pre>

<p>One other thing we notice is that with Splunk 7, the files are owned by 999:999.  In Splunk 8, the owner/group is 41812:41812.  However, adjusting for that, our config changes are still getting clobbered.</p>",,1,1,,2020-4-30 17:18:58,,2020-5-1 12:07:29,2020-5-1 12:07:29,,4880184.0,,4880184.0,,1,0,splunk,182,9
565,259021,61531805,What's the difference between StartTime and _time in Splunk?,"<p>I've been looking at a recent event in Splunk with sourcetype WinHostMon, and I see two different values for StartTime and _time:</p>

<ul>
<li>StartTime=""20200427223006.448182-300""</li>
<li>_time is recorded as 2020-04-28T15:38:13.000-04:00</li>
</ul>

<p>If the last part is timezone, there are two things that are strange about this:</p>

<ol>
<li>The timezone for StartTime is in the middle of the Atlantic.</li>
<li>The times don't actually match.</li>
</ol>

<p>Question: What is the actual time of this event, if such a thing can actually be determined, and what is causing the discrepancy between these two times?</p>

<p>(I tried to post this on Splunk Answers but they seem to have a labyrinth to stop people from signing up and I was unable to get an activated account.)</p>",61534905.0,2,4,,2020-4-30 20:03:44,,2020-5-1 16:59:07,2020-5-1 16:59:07,,40352.0,,40352.0,,1,1,splunk,166,10
566,259022,61535887,How to alert on 90% use of physical RAM,"<p>I am trying to create an alert when 90% of RAM is used.
I have determined that Committed bytes is the amount of RAM in use but I don't know how to get total RAM.
I am NOT trying to find memory but a percentage of physical ram in use.
Perfmon stats have been indexed for me. I am new to SPLUNK</p>",61537394.0,1,0,,2020-5-1 02:25:00,,2020-5-1 05:39:20,,,,,4797243.0,,1,-1,windows|splunk|perfmon,118,8
567,259023,61586328,Production grade methodology for alerts,"<h1>Background</h1>

<p>Our <strong>code</strong> is written with:</p>

<ol>
<li>Unit tests</li>
<li>End to end tests</li>
<li>Code review</li>
<li>Staging process</li>
<li>Deployment process</li>
</ol>

<p>On the contrary, our <strong>alerts</strong> are just written and then modified occasionally manually. No quality process at all.</p>

<p>This process is reasonable for simple threshold checks. However, our alerts are sometimes built on complicated queries. Sometimes composed of ~20 lines of a query.</p>

<p>If we accidentally break an alert, it could expose us to production instability since we won't know if some logic or component breaks.</p>

<h1>The question</h1>

<p>Is there a recommended methodology for validating the quality of complicated alerts?</p>

<h2>P.S.</h2>

<p>We're using Splunk alerts</p>",61591209.0,1,0,,2020-5-4 06:26:37,,2020-5-4 11:30:38,,,,,3828416.0,,1,0,alert|splunk,19,6
568,259024,61587784,"Splunk: search a string, if found only then look for another log with same request-id","<p>I want to find a string (driving factor) and if found, only then look for another string with same x-request-id and extract some details out of it.</p>

<pre><code>x-request-id=12345 ""InterestingField=7850373"" [this one is subset of very specific request]
x-request-id=12345 ""veryCommonField=56789"" [this one is a superSet of all kind of requests]
</code></pre>

<p>What I've tried:</p>

<pre><code>index=myindex ""InterestingField"" OR ""veryCommonField""
| transition x-request-id
</code></pre>

<p>But problem with above is this query join all those request as well which has only veryCommonField in it.
I want to avoid join as they are pretty low in performance.</p>

<p>What I need:
list InterestingField, veryCommonField</p>

<p>Example:
Below represents beginning of all kind of request. We get thousands of such request in a day.</p>

<pre><code>index=myIndex xrid=12345 ""Request received for this. field1: 123 field2: test""
</code></pre>

<p>Out of all above request below category falls under 100.</p>

<pre><code>index=myIndex xrid=12345 ""I belong to blahBlah category. field3: 67583, field4: testing""
</code></pre>

<p>I don't want to search in a super-set of 1000k+ but only in matching 100 requests. Because with increased time span, this search query will take very long.</p>",,1,0,,2020-5-4 08:12:57,,2020-5-4 12:41:26,2020-5-4 12:41:26,,8490352.0,,8490352.0,,1,0,splunk|splunk-query,499,11
569,259025,61596038,Splunk - RSS Scripted Input,"<p><a href=""https://i.stack.imgur.com/9NMmP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9NMmP.jpg"" alt=""enter image description here""></a><a href=""https://i.stack.imgur.com/4EjeM.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4EjeM.jpg"" alt=""enter image description here""></a>I have downloaded RSS Scripted Input app from Splunk and extracted it to Splunk etc apps</p>

<p>Can you please help me fixing the ""NameError: name 'administrator' is not defined"" in Splunkd log?</p>

<p>The base_url has been defined, username and password are provided in rssfeed2.py</p>

<p>==============</p>",,0,6,,2020-5-4 15:38:53,,2020-5-4 15:38:53,,,,,8832693.0,,1,0,splunk|rss-reader,49,6
570,259026,61596340,how to send json data to splunk HEC or splunk enterprise,"<p>I need to send JSON data from Jenkins pipeline to Splunk. I am able to make JSON data. I am referring 
<a href=""https://stackoverflow.com/questions/58555219/how-do-i-send-json-files-to-splunk-enterprise-from-java"">How do I send JSON files to Splunk Enterprise from JAVA?</a> this link.
I am getting an error when line no. 5 : httppost.setEntity(new StringEntity(eventStr); has been called . 
please help...</p>

<pre><code> DefaultHttpClient httpclient = new DefaultHttpClient();
 HttpPost httppost = new HttpPost(""https://&lt;SERVER&gt;:8088/services/collector/event"");
 httppost.addHeader(""Authorization"", "" Splunk &lt;token id&gt;"");
 String eventStr = ""{sourcetype=_json, index=main, event={ &lt;JSON&gt; }}""
 httppost.setEntity(new StringEntity(eventStr);
 HttpResponse response = httpclient.execute(httppost);
 HttpEntity entity = response.getEntity();
 System.out.println(""response: "" + entity);
</code></pre>",,1,4,,2020-5-4 15:54:28,,2020-5-4 16:52:35,,,,,4148303.0,,1,0,java|groovy|splunk,394,10
571,259027,61619503,Splunk logs in dashboard,"<p>I need to make splunk dashboards with Ubuntu system logs (mainly logging and system modifying).
How could I get those logs and what can I convert them into a dashboard?</p>",,1,0,,2020-5-5 17:41:00,,2020-5-5 22:52:15,,,,,13475761.0,,1,0,logging|splunk,162,8
572,259028,61635584,Search in Splunk in a lookup table with multivalue fields,"<p>I have a lookup table that looks like below:</p>

<p><a href=""https://i.stack.imgur.com/uJWA8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uJWA8.jpg"" alt=""enter image description here""></a></p>

<p>So I have a Splunk query that generates a table with IP addresses and I want to automatically populate the relevant DNS names.</p>

<p>I use the following but it does not work:</p>

<pre><code>Index=servers signature_id=4624
| stats count by IpAddress
**| lookup lookup.csv ""ip"" AS IpAddress OUTPUT ""dns"" AS server_name**
| stats count by server_name IpAddress 
</code></pre>

<p>Any idea how to solve it?</p>

<p>Maybe I need to make something like that before the lookup</p>

<pre><code>| makemv delim=""|"" ip | mvexpand ip  | fields ip dns ?
</code></pre>",,1,0,,2020-5-6 12:45:09,,2020-5-7 06:18:22,2020-5-7 06:18:22,,5192289.0,,13482844.0,,1,0,splunk,506,10
573,259029,61646035,Get Specified element in array of json - SPLUNK,"<p>I im newbie in splunk.
I have this json:</p>

<pre><code>""request"": {
    ""headers"": [
        {
            ""name"": ""x-real-ip"",
            ""value"": ""10.31.68.186""
        },
        {
            ""name"": ""x-forwarded-for"",
            ""value"": ""10.31.68.186""
        },
        {
            ""name"": ""x-nginx-proxy"",
            ""value"": ""true""
        }
</code></pre>

<p>I need to pick a value when the property name has ""x-real-ip"" value.</p>",61730302.0,2,2,,2020-5-6 21:50:19,,2020-12-30 18:22:40,2020-12-30 18:22:40,,10668052.0,,11605634.0,,1,1,javascript|json|analytics|splunk,1479,13
574,259030,61658107,How to build Splunk search query?,"<p>I am new to Splunk. Hence, i would require some support to build search query. </p>

<p>Below is how my log prints:</p>

<p>[181] xxxx-xx-xx xx:xx:xx INFO   (lots of text)RITM1234::FAILED BECAUSE ROOT CAUSE::Ticket was an Add, but there was no valid account named XYZ for user</p>

<p>[181] xxxx-xx-xx xx:xx:xx INFO   (lots of text)RITM1234::::FAILED BECAUSE::Account XYZ is not correct for user 1234. Will not close ticket.</p>

<p>I will like to have the output in below table format:</p>

<h2>RITM    |App|user|Error</h2>

<p>RITM1234|XYZ|1234|Ticket was an Add, but there was no valid account named XYZ for user</p>",,1,1,,2020-5-7 12:41:34,,2020-5-8 03:26:46,,,,,13490714.0,,1,0,splunk|splunk-query,124,8
575,259031,61658701,Splunk Dashboard Security,"<p>I am from splunk Team, we are noticing that people  who are not the part of splunk team , they are doing changes in existing dashboard , without notifying us .. how can we fix this? can we do something like get notification once any changes done on any dashboard if Yes, then how? please help.</p>",,2,0,,2020-5-7 13:13:17,,2020-5-7 17:57:49,,,,,13330893.0,,1,0,splunk|splunk-query|splunk-sdk|splunk-calculation|splunk-formula,75,7
576,259032,61740038,How to Splunk search for transaction types that have a median latency above 3 seconds,"<p>I have a table that shows latency data, now i want to write a query for an alert that will alert when requests (method + uri) have a higher median than 3000ms (3s)</p>

<p>The query i use for that latency table is:</p>

<pre><code>index=ms-app  environment=prod AND ""*""
| eval uri=replace(mvindex(split('request.uri', ""?""), 0), ""\/\d+[-+\w]+"", ""/:n""), methodOverride='request.headers.X-HTTP-Method-Override'
| eval methodOverrideStr = if(isnull(methodOverride) OR methodOverride==""null"", """", ""("" + methodOverride + "")"")
| eval request = 'request.method' + methodOverrideStr + "" "" + uri + "" "" + 'response.httpStatusCode'
| stats
min(stats.overallResponseTimeInMilliSeconds) as ""Min"",
avg(stats.overallResponseTimeInMilliSeconds) as avg_latency,
max(stats.overallResponseTimeInMilliSeconds) as ""Max"",
median(stats.overallResponseTimeInMilliSeconds) as ""Median"",
perc95(stats.overallResponseTimeInMilliSeconds) as ""95th %"",
count(request) as ""# req total"", count(eval('stats.overallResponseTimeInMilliSeconds' &gt; 3000)) as ""#&gt;3s"",
count(eval('stats.overallResponseTimeInMilliSeconds' &gt; 5000)) as ""#&gt;5s"",
count(eval('stats.overallResponseTimeInMilliSeconds' &gt; 10000)) as ""#&gt;10s"" by request
| eval ""Avg"" = round(avg_latency, 0)
| table request, ""Median""

</code></pre>

<p>This produces a table displaying the median latencies based on method + uri
For example:</p>

<ul>
<li>POST /first-endpoint    1000 </li>
<li>GET /second-endpoint    2000</li>
<li>DELETE /third-endpoint  1500</li>
<li>POST /fourth-endpoint   4000</li>
<li>GET /fifth-endpoint     4500</li>
</ul>

<p>Now i am trying to create a query that will show only the method +uris that have high median latency above 3s so that i can create an alert, to alert splunk which endpoints have high latency
This is what i tried: </p>

<pre><code>index=ms-app  environment=prod AND ""*""
| eval uri=replace(mvindex(split('request.uri', ""?""), 0), ""\/\d+[-+\w]+"", ""/:n""), methodOverride='request.headers.X-HTTP-Method-Override'
| eval methodOverrideStr = if(isnull(methodOverride) OR methodOverride==""null"", """", ""("" + methodOverride + "")"")
| eval request = 'request.method' + methodOverrideStr + "" "" + uri + "" "" + 'response.httpStatusCode'
| stats
median(stats.overallResponseTimeInMilliSeconds) as ""Median""
| table request, ""Median"" &gt; 3000
</code></pre>

<p>Which should display this: </p>

<ul>
<li>POST /fourth-endpoint   4000</li>
<li>GET /fifth-endpoint     4500</li>
</ul>

<p>However it just shows the same results as the first query </p>",61740997.0,1,2,,2020-5-11 21:52:09,,2020-5-11 23:11:24,,,,,4181830.0,,1,0,splunk|latency|splunk-query|splunk-calculation|splunk-formula,141,10
577,259033,61759709,How to trigger spunk alert for every stat that appears on my query,"<p>I currently have a query that results in a couple stats being shown, ""Statistics (5)""</p>

<p><a href=""https://i.stack.imgur.com/M68mT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M68mT.png"" alt=""enter image description here""></a></p>

<p>I use this query to get those Stats:</p>

<pre><code>index=ms-app  environment=prod AND ""*""
| eval uri=replace(mvindex(split('request.uri', ""?""), 0), ""\/\d+[-+\w]+"", ""/:n""), methodOverride='request.headers.X-HTTP-Method-Override'
| eval methodOverrideStr = if(isnull(methodOverride) OR methodOverride==""null"", """", ""("" + methodOverride + "")"")
| eval request = 'request.method' + methodOverrideStr + "" "" + uri + "" "" + 'response.httpStatusCode'
| stats
median(stats.overallResponseTimeInMilliSeconds) as ""Median""
| table request, ""Median"" &gt; 3000 | where Median &gt; 3000
</code></pre>

<p>I want to create an alert that will trigger every time one stat appears</p>

<p>Currently have my trigger set up like this: 
<a href=""https://i.stack.imgur.com/7zo2p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7zo2p.png"" alt=""enter image description here""></a></p>

<p>Then i have an action that it will go to a slack channel i created when triggered.</p>

<p>However i do not ever see it being triggered in my slack despite having results in the Statistics section of my query</p>",,1,0,,2020-5-12 18:51:01,,2020-5-12 23:28:18,,,,,4181830.0,,1,0,triggers|splunk|alerts|splunk-query|splunk-sum,128,8
578,259034,61765352,Regex XML with group name and including tags,"<p>I have a XML that looks like this </p>

<pre><code>Executing request: POST https://[website]:
&lt;?xml version=1.0 encoding=UTF-8 standalone=yes?&gt;&lt;request&gt;[data]&lt;/request&gt;
</code></pre>

<p>Id like to regex out everything, including the request open and closing tags and name the group raw_message but I can't figure out how to do that. I've used regex101 and it makes sense but doesn't match and the debugger hasn't helped me figure out what I'm doing wrong.</p>",,1,3,,2020-5-13 02:28:29,,2020-5-31 07:26:42,2020-5-31 07:26:42,,18771.0,,3302683.0,,1,0,regex|xml|splunk,28,6
579,259035,61774628,Splunk alert scheduling to stop running script on particular times,"<p>I have two splunk scripts one to Run at 30 minutes and another one at 120 minutes between 00.00 to 06.00.</p>

<p>Now I would like to hold 30 minutes script not to run between this timeframe. How to do this one from splunk.</p>",,1,0,,2020-5-13 12:26:09,,2020-5-13 15:19:43,2020-5-13 14:47:19,,4418.0,,13533541.0,,1,0,alert|splunk|job-scheduling|splunk-query,94,7
580,259036,61777139,Indexer grouping in dropdown,"<p>I have 20 indexes where we want to display them in drop down in grouping manner ..how can we group them in a query ?
for example:-</p>

<p>index1,Index2,index3 should come with name abc....
Index 4, index 2, index 5 should come with name efg...</p>

<p>so in drop down we should see only value as abc , efg. so once will select abc the below pannel should show graph accordingly..</p>",,1,3,,2020-5-13 14:23:18,,2020-5-14 03:18:36,,,,,13330893.0,,1,0,splunk|splunk-query|splunk-calculation|splunk-formula|splunk-sum,114,8
581,259037,61810785,Event viewer logs to .txt file non-encrypted,"<p>Is there any way to get event viewer logs to .txt file.
I tried going to event viewer properties and changing dir and .txt.</p>

<p>However when opening .txt it seems to be encrypted.
Want plain text so that I can send to splunk and have a dashboard set up.</p>

<p>Thanks</p>",,1,0,,2020-5-15 02:36:49,,2020-5-15 05:56:01,,,,,7865091.0,,1,0,splunk|event-viewer,42,8
582,259038,61811201,Splunk: Matching an error log and obtaining the count of it,"<p>I'm new to Splunk. I need to get a count of each of the error messages from our logs. I tried writing the below search query but it is not working as expected.</p>

<p>index=""my_index"" source=""<em>my_service</em>.log"" logger=""com.xyz.splunk.logger.*"" severity=""ERROR"" |eval errorType=case(Message==""<em>mandatory field field1 is null</em>"", ""missing field1"", Message==""<em>mandatory field field2 is null</em>"", ""missing field2"", Message==""<em>mandatory field field1 has invalid value</em>"", ""invalid field1"") | stats count by errorType</p>",61812572.0,1,0,,2020-5-15 03:24:51,,2020-5-15 05:48:52,,,,,2504082.0,,1,1,splunk|splunk-query,517,14
583,259039,61875845,Splunk : filter table data,"<p>I have the following lookup:</p>

<pre><code>C1 | C2| C3| C4
===|===|===|====
A  | 1 | x | test
===|===|===|====
A  | 2 | y | test
===|===|===|====
B  | 1 | x | test
===|===|===|====
B  | 1 | y | test
===|===|===|====
B  | 1 | z | test
</code></pre>

<p>I want this to be converted to:</p>

<pre><code>C1 | C2| C3| C4
===|===|===|====
B  | 1 | x | test
===|===|===|====
B  | 1 | y | test
===|===|===|====
B  | 1 | z | test
</code></pre>

<p>So the idea is that if for unique value of C1, if there are multiple values in C2, the such combinations of C1 + C2 should be filtered out.</p>

<p>What I tried was:</p>

<pre><code>| inputlookup LUT.csv
| fillnull value=""NULL""
| stats  list(*) as * dc(""C2"") as count by  ""C1""
| where count=1
</code></pre>

<p>but this results in:</p>

<pre><code>C1 | C2    | C3    | C4
===|=======|=======|==============
B  | 1,1,1 | x,y,z | test,test,test
</code></pre>

<p>I DONOT want comma separated values. I want different row.</p>",,1,0,,2020-5-18 17:50:48,,2020-5-18 21:00:55,,,,,1789355.0,,1,0,splunk|splunk-query,340,10
584,259040,61900141,Sum of count with Splunk,"<p>First let me say that I am very very very new to splunk. I am trying to find all the ""host"" that make up an index and get a total count of unique values. The purpose of this is to eventually get alerts on when the total ""host"" changes so I can tell when something that makes up and index stops working. </p>

<p>Here is my query so far which gives me the host names and the count however I cannot figure out how to get the sum of ""count""</p>

<pre><code>index=exchangesmtp | table host | dedup host | stats count by host | addtotals fieldname=count
</code></pre>",61900903.0,2,0,,2020-5-19 20:25:13,,2020-5-19 22:11:52,,,,,13341904.0,,1,0,sum|host|splunk,676,12
585,259041,61905345,Splunk alert schedule,"<p>I have created a couple of alerts. The first one is running every 30 minutes from Mon-Friday, and another one is running at 00.00 to 06.00 every 2 hours. What I want is for the 30-minute job to pause or stop while the 2-hour one is running, then have the 30-minute job resume after the 2-hour one is complete. Or, is there another way to put the 30-minute job on hold? Here is the schedule I used for 30M: - 0,30 0-6,12-23 * * 1-5 And this one for 2H: 120M - 0 2-6/2 * * * But I still I see some alerts were triggered during 120 minutes. Kindly help </p>

<p>If not I need to avoid the overlap of last 30 mints of 30 mints job in 2 hours.. </p>",,1,0,,2020-5-20 04:50:15,1.0,2020-5-20 05:49:58,,,,,13533541.0,,1,0,alert|splunk|splunk-query,145,8
586,259042,61914347,Search over multiple lines regex,"<p>I try to find logs via search that contains a pattern over multiple log entries. E.g.</p>

<pre><code>time n :Post Request xyz
time n1 :requestCode --&gt; 401
</code></pre>

<p>I tried to use regex </p>

<pre><code>conf_file=xyz | regex ""Post\sRequest\sxyz\r\n.*401""
</code></pre>

<p>I checked the regex with another editor and its working fine. However Splunk never finds a result. So my question is how can I search for that pattern over those two lines?</p>",61917966.0,2,0,,2020-5-20 13:29:39,,2020-5-20 18:51:40,2020-5-20 14:08:34,,3805467.0,,3805467.0,,1,0,splunk|splunk-query,72,8
587,259043,61915555,Splunk not recognizing regex,"<p>I'm struggling to make a regex work with splunk. It works with regex 101, but splunk doesn't seem to recognize it!</p>

<p><strong>Regex:</strong> <code>\""([\w]+)\"":([^,}]+)</code></p>

<p><strong>Log entry:</strong></p>

<pre><code>May 20 12:22:21 127.0.0.1 {""rootId"": ""AXIxikL8ao-yaSvA"", ""requestId"": ""f6a873jkjjkjk:-8000:5738"", 
""details"": {""flag"": false, ""title"": ""task 1"", ""status"": ""Waiting"", ""group"": """", ""order"": 0}, 
""operation"": ""Creation"", ""objectId"": ""AXIyCN5Oao-H5aYyaSvd"", ""startDate"": 1589977341890, 
""objectType"": ""case_task"", ""base"": true, ""object"": {""_routing"": ""AXIxikL8ao-H5aYyaSvA"", ""flag"": 
false, ""_type"": ""case_task"", ""title"": ""task 1"", ""createdAt"": 1589977341516, ""_parent"": ""AXIxikL8ao- 
H5aYyaSvA"", ""createdBy"": ""user"", ""_id"": ""AXIyCN5Oao-H5aYyaSvd"", ""id"": ""AXIyCN5Oao-H5aYyaSvd"", 
""_version"": 1, ""order"": 0, ""status"": ""Waiting"", ""group"": """"}}
</code></pre>

<p><strong>Regex 101 link:</strong>
<a href=""https://regex101.com/r/XBuz9Y/2/"" rel=""nofollow noreferrer"">https://regex101.com/r/XBuz9Y/2/</a></p>

<p>I suspect splunk may have a different regex syntax, but i don't really know how to adapt it.</p>

<p>Any help?</p>

<p>Thanks!</p>",,2,11,,2020-5-20 14:23:44,,2020-5-21 02:44:01,2020-5-20 14:41:49,,5424988.0,,3163755.0,,1,0,json|regex|parsing|splunk,288,10
588,259044,61919020,Splunk: How to Write regex for pathparam?,"<p>I have following two API's</p>

<pre><code>/rest/customer/{id}
/rest/customer/{id}/account
</code></pre>

<p>How can i write a spunk query to just get records with first Endpoint?</p>",,1,0,,2020-5-20 17:16:24,,2020-5-21 00:03:06,,,,,4629427.0,,1,0,splunk|splunk-query,58,8
589,259045,61938277,Splunk pick latest entry and group by Id,"<p>I am new to splunk query, could some one help with this please. I am trying to get the latest entry for each id</p>

<p>Sample data:</p>

<pre><code>id=Id1 p1=12 p2=32 time=10:13
id=Id2 p1=34 p2=54 time=10:14
id=Id1 p1=1 p2=99  time=11:33
id=Id2 p1=5 p2=67  time=13:00
</code></pre>

<p>expected output:</p>

<pre><code>Id1 1 99
Id2 5 67

</code></pre>",61939497.0,2,0,,2020-5-21 15:41:40,,2020-5-21 23:02:02,,,,,5946845.0,,1,0,splunk|splunk-query,271,8
590,259046,61938531,Using Splunk - extract fields from xml data in a log file using xpath,"<p>I am using Splunk to extract a number of fields from xml data this is contained in a log file.  So to limit the search to be MOSTLY the xml file I start the search with this: 
 sourcetype=""name of type here"" ""RULE"" </p>

<p>This returns:</p>

<pre><code>0123459 TripMessage.createMessage MsgSource &lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;tmsTrip xmlns=""http://ground.fedex.com/schemas/linehaul/trip"" xmlns:ns2=""http://ground.fedex.com/schemas/linehaul/TMSCommon""&gt;
</code></pre>

<p>...</p>

<p>The file is very large.  This is part of it.</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;tmsTrip xmlns=""http://ground.fedex.com/schemas/linehaul/trip"" xmlns:ns2=""http://ground.fedex.com/schemas/linehaul/TMSCommon""&gt;
   &lt;recordType&gt;PURCHASEDLINEHAUL&lt;/recordType&gt;
   &lt;eventType&gt;APPROVE&lt;/eventType&gt;
   &lt;tripId&gt;116029927&lt;/tripId&gt;
   &lt;legId&gt;104257037&lt;/legId&gt;
   &lt;tripNumber&gt;104257037&lt;/tripNumber&gt;
   &lt;tripLegNumber&gt;1&lt;/tripLegNumber&gt;
   &lt;updatedDateGMT&gt;2020-02-20T21:53:39.000Z&lt;/updatedDateGMT&gt;
.... more lines here that are not important
     &lt;purchasedCost&gt;
      &lt;purchasedCostTripSegment&gt;
         &lt;purchCostReference&gt;1587040&lt;/purchCostReference&gt;
         &lt;carrier&gt;FXTR&lt;/carrier&gt;
         &lt;vendorType&gt;DRAY&lt;/vendorType&gt;
         &lt;billingMethod&gt;RULE&lt;/billingMethod&gt;
         &lt;carrierTrailerType&gt;PZ1&lt;/carrierTrailerType&gt;
         &lt;origin&gt;
            &lt;ns2:numberCode&gt;923&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;RLTO&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;330 RESOURCE DRIVE&lt;/ns2:address1&gt;
            &lt;ns2:address2&gt;LH PHONE 877-851-3543&lt;/ns2:address2&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/origin&gt;
</code></pre>

<p>This query selects the xml part text in the logging file and some of the fields are extracted and I can add to a table. (not including the source and sourcetype..)</p>

<pre><code>| xmlkv | table purchCostReference, eventType, carrier, billingMethod
</code></pre>

<p>But need more fields that are child elements within the xml data. One of them is the <strong>numberCode</strong>. I am trying to use <strong>xpath</strong> to extract these additional fields.  </p>

<pre><code>| xmlkv | xpath
""//tmsTrip/purchasedCost/purchasedCostTripSegment/origin/ns2:numberCode"" outfield=Origin | table purchCostReference, eventType, carrier, billingMethod, Origin
</code></pre>

<p>But no Origin data is returned when I add the field to the table.  There is no error.  The Origin column is empty.
<a href=""https://i.stack.imgur.com/h7Ika.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h7Ika.png"" alt=""enter image description here""></a></p>

<p><strong>UPDATE</strong>
I think the problem is that I need to add the <strong>field</strong> parameter.  The xml file is within a log text file.  I limit the search to get the xml file but not only the xml.  So I think xpath is struggling with the other text that is not xml.</p>

<p><strong>UPDATE</strong>
I tried creating an extracted field using the wizard of the xml file that is within the logging statement.  The xml is huge and I can only select about 30% of it.  If anyone is good at regex, maybe they can give me some pointers as to how to complete the regex command to get all of the xml.  (I tried updating the props.conf file but do not have permission to add TRUNCATE = 0).
This is the xml file sample:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;tmsTrip xmlns=""http://ground.fedex.com/schemas/linehaul/trip"" xmlns:ns2=""http://ground.fedex.com/schemas/linehaul/TMSCommon""&gt;
   &lt;recordType&gt;PURCHASEDLINEHAUL&lt;/recordType&gt;
   &lt;eventType&gt;APPROVE&lt;/eventType&gt;
   &lt;tripId&gt;143642990&lt;/tripId&gt;
   &lt;legId&gt;129014817&lt;/legId&gt;
   &lt;tripNumber&gt;129014817&lt;/tripNumber&gt;
   &lt;tripLegNumber&gt;1&lt;/tripLegNumber&gt;
   &lt;updatedDateGMT&gt;2020-05-22T00:53:21.000Z&lt;/updatedDateGMT&gt;
   &lt;origin&gt;
      &lt;ns2:numberCode&gt;928&lt;/ns2:numberCode&gt;
      &lt;ns2:locAbbr&gt;ANAH&lt;/ns2:locAbbr&gt;
      &lt;ns2:address1&gt;590 E ORANGE THORPE AVENUE&lt;/ns2:address1&gt;
      &lt;ns2:city&gt;ANAHEIM&lt;/ns2:city&gt;
      &lt;ns2:stateProvince&gt;CA&lt;/ns2:stateProvince&gt;
      &lt;ns2:postalCode&gt;92801&lt;/ns2:postalCode&gt;
      &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
      &lt;ns2:numberType&gt;1&lt;/ns2:numberType&gt;
      &lt;ns2:timeZoneAbbr&gt;PST&lt;/ns2:timeZoneAbbr&gt;
      &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
   &lt;/origin&gt;
   &lt;destination&gt;
      &lt;ns2:numberCode&gt;89&lt;/ns2:numberCode&gt;
      &lt;ns2:locAbbr&gt;WOOD&lt;/ns2:locAbbr&gt;
      &lt;ns2:address1&gt;6000 RIVERSIDE DR&lt;/ns2:address1&gt;
      &lt;ns2:address2&gt;LH PHONE 732-512-5579&lt;/ns2:address2&gt;
      &lt;ns2:city&gt;KEASBEY&lt;/ns2:city&gt;
      &lt;ns2:stateProvince&gt;NJ&lt;/ns2:stateProvince&gt;
      &lt;ns2:postalCode&gt;08832&lt;/ns2:postalCode&gt;
      &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
      &lt;ns2:numberType&gt;2&lt;/ns2:numberType&gt;
      &lt;ns2:timeZoneAbbr&gt;EST&lt;/ns2:timeZoneAbbr&gt;
      &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
   &lt;/destination&gt;
   &lt;schedDispatchDateGMT&gt;2020-05-22T13:00:00.000Z&lt;/schedDispatchDateGMT&gt;
   &lt;estimatedArrivalDateGMT&gt;2020-05-26T06:00:00.000Z&lt;/estimatedArrivalDateGMT&gt;
   &lt;drop/&gt;
   &lt;hook/&gt;
   &lt;actualRoute&gt;
      &lt;routeNumber&gt;308229&lt;/routeNumber&gt;
      &lt;routeOrderNumber&gt;0&lt;/routeOrderNumber&gt;
      &lt;totalMiles&gt;2787&lt;/totalMiles&gt;
      &lt;runTime&gt;54.6&lt;/runTime&gt;
   &lt;/actualRoute&gt;
   &lt;standardRoute&gt;
      &lt;routeNumber&gt;308229&lt;/routeNumber&gt;
      &lt;routeOrderNumber&gt;0&lt;/routeOrderNumber&gt;
      &lt;totalMiles&gt;2787&lt;/totalMiles&gt;
      &lt;runTime&gt;54.6&lt;/runTime&gt;
   &lt;/standardRoute&gt;
   &lt;paidRoute&gt;
      &lt;routeNumber&gt;308229&lt;/routeNumber&gt;
      &lt;routeOrderNumber&gt;0&lt;/routeOrderNumber&gt;
      &lt;totalMiles&gt;2787&lt;/totalMiles&gt;
      &lt;runTime&gt;54.6&lt;/runTime&gt;
   &lt;/paidRoute&gt;
   &lt;settlement&gt;
      &lt;dispatchSettlementEligibility&gt;false&lt;/dispatchSettlementEligibility&gt;
   &lt;/settlement&gt;
   &lt;livePkgCount&gt;0.0&lt;/livePkgCount&gt;
   &lt;tripTollAmount&gt;0.0&lt;/tripTollAmount&gt;
   &lt;trailers&gt;
      &lt;ns2:trailer&gt;
         &lt;ns2:trailerNbr&gt;531823&lt;/ns2:trailerNbr&gt;
         &lt;ns2:trailerPrefix&gt;FDXU&lt;/ns2:trailerPrefix&gt;
         &lt;ns2:configOrderNbr&gt;1&lt;/ns2:configOrderNbr&gt;
         &lt;ns2:sealNbr&gt;60606220&lt;/ns2:sealNbr&gt;
         &lt;ns2:packageWeight&gt;9931.59&lt;/ns2:packageWeight&gt;
         &lt;ns2:unladenWeight&gt;13870.0&lt;/ns2:unladenWeight&gt;
         &lt;ns2:totalWeight&gt;23801.59&lt;/ns2:totalWeight&gt;
         &lt;ns2:packageNumber&gt;703&lt;/ns2:packageNumber&gt;
         &lt;ns2:percentCube&gt;1&lt;/ns2:percentCube&gt;
         &lt;ns2:hazmatFlag&gt;false&lt;/ns2:hazmatFlag&gt;
         &lt;ns2:originPlanned&gt;
            &lt;ns2:numberCode&gt;928&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;ANAH&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;590 E ORANGE THORPE AVENUE&lt;/ns2:address1&gt;
            &lt;ns2:city&gt;ANAHEIM&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;CA&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;92801&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;1&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;PST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/ns2:originPlanned&gt;
         &lt;ns2:nextSortLocation&gt;
            &lt;ns2:numberCode&gt;89&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;WOOD&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;6000 RIVERSIDE DR&lt;/ns2:address1&gt;
            &lt;ns2:address2&gt;LH PHONE 732-512-5579&lt;/ns2:address2&gt;
            &lt;ns2:city&gt;KEASBEY&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;NJ&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;08832&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;2&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;EST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/ns2:nextSortLocation&gt;
         &lt;ns2:destinationPlanned&gt;
            &lt;ns2:numberCode&gt;89&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;WOOD&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;6000 RIVERSIDE DR&lt;/ns2:address1&gt;
            &lt;ns2:address2&gt;LH PHONE 732-512-5579&lt;/ns2:address2&gt;
            &lt;ns2:city&gt;KEASBEY&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;NJ&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;08832&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;2&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;EST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/ns2:destinationPlanned&gt;
         &lt;ns2:loads&gt;
            &lt;ns2:load&gt;
               &lt;ns2:loadId&gt;103718801&lt;/ns2:loadId&gt;
               &lt;ns2:loadNumber&gt;1&lt;/ns2:loadNumber&gt;
               &lt;ns2:origin&gt;
                  &lt;ns2:numberCode&gt;928&lt;/ns2:numberCode&gt;
                  &lt;ns2:locAbbr&gt;ANAH&lt;/ns2:locAbbr&gt;
                  &lt;ns2:numberType&gt;1&lt;/ns2:numberType&gt;
               &lt;/ns2:origin&gt;
               &lt;ns2:destination&gt;
                  &lt;ns2:numberCode&gt;89&lt;/ns2:numberCode&gt;
                  &lt;ns2:locAbbr&gt;WOOD&lt;/ns2:locAbbr&gt;
                  &lt;ns2:address2&gt;LH PHONE 732-512-5579&lt;/ns2:address2&gt;
                  &lt;ns2:numberType&gt;2&lt;/ns2:numberType&gt;
               &lt;/ns2:destination&gt;
               &lt;ns2:openDateGMT&gt;2020-05-21T19:53:46.000Z&lt;/ns2:openDateGMT&gt;
               &lt;ns2:dueOverrideFlag&gt;false&lt;/ns2:dueOverrideFlag&gt;
               &lt;ns2:hazmatFlag&gt;false&lt;/ns2:hazmatFlag&gt;
            &lt;/ns2:load&gt;
         &lt;/ns2:loads&gt;
      &lt;/ns2:trailer&gt;
   &lt;/trailers&gt;
   &lt;dollys/&gt;
   &lt;purchasedCost&gt;
      &lt;purchasedCostTripSegment&gt;
         &lt;purchCostReference&gt;2625998&lt;/purchCostReference&gt;
         &lt;carrier&gt;BNSF&lt;/carrier&gt;
         &lt;vendorType&gt;RAIL&lt;/vendorType&gt;
         &lt;carrierTrailerType&gt;53PC&lt;/carrierTrailerType&gt;
         &lt;origin&gt;
            &lt;ns2:numberCode&gt;4022&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;BNSF&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;3770 EAST WASHINGTON AVENUE&lt;/ns2:address1&gt;
            &lt;ns2:city&gt;LOS ANGELES&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;CA&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;90040&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;PST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/origin&gt;
         &lt;destination&gt;
            &lt;ns2:numberCode&gt;4040&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;CROX&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;NORFOLK SOUTHERN RAILROAD&lt;/ns2:address1&gt;
            &lt;ns2:address2&gt;125 COUNTY ROAD&lt;/ns2:address2&gt;
            &lt;ns2:city&gt;CROXTON&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;NJ&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;07307&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;EST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/destination&gt;
         &lt;stopOff&gt;
            &lt;ns2:stopOffLocation&gt;
               &lt;ns2:numberCode&gt;9996&lt;/ns2:numberCode&gt;
               &lt;ns2:stateProvince&gt;DU&lt;/ns2:stateProvince&gt;
               &lt;ns2:postalCode&gt;00000&lt;/ns2:postalCode&gt;
               &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
               &lt;ns2:numberType&gt;1&lt;/ns2:numberType&gt;
            &lt;/ns2:stopOffLocation&gt;
         &lt;/stopOff&gt;
         &lt;schedDispatchDate&gt;2020-05-22T05:00:00.000Z&lt;/schedDispatchDate&gt;
         &lt;estimatedArrivalDate&gt;2020-05-26T00:59:00.000Z&lt;/estimatedArrivalDate&gt;
         &lt;billingMethod&gt;RULE&lt;/billingMethod&gt;
         &lt;STCCCode&gt;4711110&lt;/STCCCode&gt;
         &lt;planNumber&gt;065&lt;/planNumber&gt;
         &lt;powerType&gt;1X&lt;/powerType&gt;
         &lt;powerOnlyFlag&gt;false&lt;/powerOnlyFlag&gt;
      &lt;/purchasedCostTripSegment&gt;
      &lt;purchasedCostTripSegment&gt;
         &lt;purchCostReference&gt;2625998&lt;/purchCostReference&gt;
         &lt;carrier&gt;NS&lt;/carrier&gt;
         &lt;vendorType&gt;RAIL&lt;/vendorType&gt;
         &lt;carrierTrailerType&gt;53PC&lt;/carrierTrailerType&gt;
         &lt;origin&gt;
            &lt;ns2:numberCode&gt;4061&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;NSAU&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;6300 SOUTH INDIANA AVENUE&lt;/ns2:address1&gt;
            &lt;ns2:city&gt;CHICAGO&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;IL&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;60637&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;CST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/origin&gt;
         &lt;destination&gt;
            &lt;ns2:numberCode&gt;4040&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;CROX&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;NORFOLK SOUTHERN RAILROAD&lt;/ns2:address1&gt;
            &lt;ns2:address2&gt;125 COUNTY ROAD&lt;/ns2:address2&gt;
            &lt;ns2:city&gt;CROXTON&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;NJ&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;07307&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;EST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/destination&gt;
         &lt;stopOff&gt;
            &lt;ns2:stopOffLocation&gt;
               &lt;ns2:numberCode&gt;4040&lt;/ns2:numberCode&gt;
               &lt;ns2:locAbbr&gt;CROX&lt;/ns2:locAbbr&gt;
               &lt;ns2:address1&gt;NORFOLK SOUTHERN RAILROAD&lt;/ns2:address1&gt;
               &lt;ns2:address2&gt;125 COUNTY ROAD&lt;/ns2:address2&gt;
               &lt;ns2:city&gt;CROXTON&lt;/ns2:city&gt;
               &lt;ns2:stateProvince&gt;NJ&lt;/ns2:stateProvince&gt;
               &lt;ns2:postalCode&gt;07307&lt;/ns2:postalCode&gt;
               &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
               &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
               &lt;ns2:timeZoneAbbr&gt;EST&lt;/ns2:timeZoneAbbr&gt;
               &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
            &lt;/ns2:stopOffLocation&gt;
         &lt;/stopOff&gt;
         &lt;schedDispatchDate&gt;2020-05-22T05:00:00.000Z&lt;/schedDispatchDate&gt;
         &lt;estimatedArrivalDate&gt;2020-05-26T01:00:00.000Z&lt;/estimatedArrivalDate&gt;
         &lt;billingMethod&gt;LOCAL&lt;/billingMethod&gt;
         &lt;STCCCode&gt;4711110&lt;/STCCCode&gt;
         &lt;planNumber&gt;045&lt;/planNumber&gt;
         &lt;powerType&gt;1X&lt;/powerType&gt;
         &lt;powerOnlyFlag&gt;false&lt;/powerOnlyFlag&gt;
      &lt;/purchasedCostTripSegment&gt;
   &lt;/purchasedCost&gt;
   &lt;drivers/&gt;
&lt;/tmsTrip&gt;
</code></pre>

<p>This is how much the extracted field I can select:

http://ground.fedex.com/schemas/linehaul/trip\"" xmlns:ns2=\""<a href=""http://ground.fedex.com/schemas/linehaul/TMSCommon%5C"" rel=""nofollow noreferrer"">http://ground.fedex.com/schemas/linehaul/TMSCommon\</a>"">
   PURCHASEDLINEHAUL
   APPROVE
   143642990
   129014817
   129014817
   1
   2020-05-22T00:53:21.000Z
   
      928
      ANAH
      590 E ORANGE THORPE AVENUE
      ANAHEIM
      CA
      92801
      FDEG
      1
      PST
      true</p>

<p>This is the regex that Splunk creates to select the above xml</p>

<pre><code>^[^\$\n]*\$\d+\.\w+\s+\w+\s+(?P&lt;xmlMessage&gt;&lt;\?\w+\s+\w+=""\d+\.\d+""\s+\w+=""\w+\-\d+""\?&gt;\s+&lt;\w+\s+\w+=""\w+://\w+\.\w+\.\w+/\w+/\w+/\w+""\s+\w+:\w+=""\w+://\w+\.\w+\.\w+/\w+/\w+/\w+""&gt;\s+&lt;\w+&gt;\w+&lt;/\w+&gt;\s+&lt;\w+&gt;\w+&lt;/\w+&gt;\s+&lt;\w+&gt;\d+&lt;/\w+&gt;\s+&lt;\w+&gt;\d+&lt;/\w+&gt;\s+&lt;\w+&gt;\d+&lt;/\w+&gt;\s+&lt;\w+&gt;\d+&lt;/\w+&gt;\s+&lt;\w+&gt;\d+\-\d+\-\d+\w+:\d+:\d+\.\d+\w+&lt;/\w+&gt;\s+&lt;\w+&gt;\s+&lt;\w+:\w+&gt;\d+&lt;/\w+:\w+&gt;\s+&lt;\w+:\w+&gt;\w+&lt;/\w+:\w+&gt;\s+&lt;\w+:\w+&gt;\d+\s+\w+\s+\w+\s+\w+\s+\w+&lt;/\w+:\w+&gt;\s+&lt;\w+:\w+&gt;\w+&lt;/\w+:\w+&gt;\s+&lt;\w+:\w+&gt;\w+&lt;/\w+:\w+&gt;\s+&lt;\w+:\w+&gt;\d+&lt;/\w+:\w+&gt;\s+&lt;\w+:\w+&gt;\w+&lt;/\w+:\w+&gt;\s+&lt;\w+:\w+&gt;\d+&lt;/\w+:\w+&gt;\s+&lt;\w+:\w+&gt;\w+&lt;/\w+:\w+&gt;\s+&lt;\w+:\w+&gt;\w+&lt;/\w+:\w+&gt;)
</code></pre>

<p>So can I change the above regex to include the entire xml?</p>

<p><strong>UPDATE</strong>
I tried extracting a field from the xmlMessage extracted field.  The xmlMessage field is above. I used the <strong>xpath</strong> command to extract <strong>recordType</strong>.  Put the result in a table. This is the command</p>

<pre><code>| xmlkv | xpath field=xmlMessage
""//tmsTrip/recordType"" outfield=Origin | table Origin
</code></pre>

<p>It returned no results.  This xpath command does not work for the simplest of queries.
What am I doing wrong?</p>",62068742.0,3,4,,2020-5-21 15:54:39,,2020-5-28 15:41:02,2020-5-22 12:26:06,,1724560.0,,1724560.0,,1,0,xpath|splunk,963,11
591,259047,61958891,Splunk query does not return multiple instances of the field values,"<p>I have a Splunk query that identifies all of the fields extracted from the xml file but not all of the instances of the data.
There is a huge xml file that has multiple instances of the result but only identifies 1 result in the field list.
This is the query: </p>

<pre><code>source=""service.log"" sourcetype=""dispatchapp"" ""&lt;billingMethod&gt;RULE&lt;/billingMethod&gt;"" ""createMessage MsgSource"" ""&lt;purchCostReference&gt;2618252&lt;/purchCostReference&gt;"" ""&lt;eventType&gt;DISPATCH&lt;/eventType&gt;"" | xmlkv
</code></pre>

<p>This is the result:
<a href=""https://i.stack.imgur.com/UKAhF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UKAhF.png"" alt=""enter image description here""></a></p>

<p>The highlighted fields are the ones I am interested in.  There are 3 carrier tags and many ns:numberCodes.  But it is only showing 1.
Below is the xml that has the data</p>

<pre><code>&lt;purchasedCost&gt;
      &lt;purchasedCostTripSegment&gt;
         &lt;purchCostReference&gt;2618252&lt;/purchCostReference&gt;
         &lt;carrier&gt;NS&lt;/carrier&gt;
         &lt;vendorType&gt;RAIL&lt;/vendorType&gt;
         &lt;carrierTrailerType&gt;53RP&lt;/carrierTrailerType&gt;
         &lt;origin&gt;
            &lt;ns2:numberCode&gt;4061&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;NSAU&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;6300 SOUTH INDIANA AVENUE&lt;/ns2:address1&gt;
            &lt;ns2:city&gt;CHICAGO&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;IL&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;60637&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;CST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/origin&gt;
         &lt;destination&gt;
            &lt;ns2:numberCode&gt;4040&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;CROX&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;NORFOLK SOUTHERN RAILROAD&lt;/ns2:address1&gt;
            &lt;ns2:address2&gt;125 COUNTY ROAD&lt;/ns2:address2&gt;
            &lt;ns2:city&gt;CROXTON&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;NJ&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;07307&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;EST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/destination&gt;
         &lt;stopOff&gt;
            &lt;ns2:stopOffLocation&gt;
               &lt;ns2:numberCode&gt;4040&lt;/ns2:numberCode&gt;
               &lt;ns2:locAbbr&gt;CROX&lt;/ns2:locAbbr&gt;
               &lt;ns2:address1&gt;NORFOLK SOUTHERN RAILROAD&lt;/ns2:address1&gt;
               &lt;ns2:address2&gt;125 COUNTY ROAD&lt;/ns2:address2&gt;
               &lt;ns2:city&gt;CROXTON&lt;/ns2:city&gt;
               &lt;ns2:stateProvince&gt;NJ&lt;/ns2:stateProvince&gt;
               &lt;ns2:postalCode&gt;07307&lt;/ns2:postalCode&gt;
               &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
               &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
               &lt;ns2:timeZoneAbbr&gt;EST&lt;/ns2:timeZoneAbbr&gt;
               &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
            &lt;/ns2:stopOffLocation&gt;
         &lt;/stopOff&gt;
         &lt;schedDispatchDate&gt;2020-05-20T05:00:00.000Z&lt;/schedDispatchDate&gt;
         &lt;actualDispatchDate&gt;2020-05-21T08:20:53.000Z&lt;/actualDispatchDate&gt;
         &lt;estimatedArrivalDate&gt;2020-05-25T08:18:53.000Z&lt;/estimatedArrivalDate&gt;
         &lt;billingMethod&gt;LOCAL&lt;/billingMethod&gt;
         &lt;STCCCode&gt;4711110&lt;/STCCCode&gt;
         &lt;planNumber&gt;065&lt;/planNumber&gt;
         &lt;powerType&gt;1X&lt;/powerType&gt;
         &lt;powerOnlyFlag&gt;false&lt;/powerOnlyFlag&gt;
      &lt;/purchasedCostTripSegment&gt;
      &lt;purchasedCostTripSegment&gt;
         &lt;purchCostReference&gt;2618252&lt;/purchCostReference&gt;
         &lt;carrier&gt;BNSF&lt;/carrier&gt;
         &lt;vendorType&gt;RAIL&lt;/vendorType&gt;
         &lt;carrierTrailerType&gt;53RP&lt;/carrierTrailerType&gt;
         &lt;origin&gt;
            &lt;ns2:numberCode&gt;4022&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;BNSF&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;3770 EAST WASHINGTON AVENUE&lt;/ns2:address1&gt;
            &lt;ns2:city&gt;LOS ANGELES&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;CA&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;90040&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;PST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/origin&gt;
         &lt;destination&gt;
            &lt;ns2:numberCode&gt;4040&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;CROX&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;NORFOLK SOUTHERN RAILROAD&lt;/ns2:address1&gt;
            &lt;ns2:address2&gt;125 COUNTY ROAD&lt;/ns2:address2&gt;
            &lt;ns2:city&gt;CROXTON&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;NJ&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;07307&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;EST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/destination&gt;
         &lt;stopOff&gt;
            &lt;ns2:stopOffLocation&gt;
               &lt;ns2:numberCode&gt;9996&lt;/ns2:numberCode&gt;
               &lt;ns2:stateProvince&gt;DU&lt;/ns2:stateProvince&gt;
               &lt;ns2:postalCode&gt;00000&lt;/ns2:postalCode&gt;
               &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
               &lt;ns2:numberType&gt;1&lt;/ns2:numberType&gt;
            &lt;/ns2:stopOffLocation&gt;
         &lt;/stopOff&gt;
         &lt;schedDispatchDate&gt;2020-05-20T05:00:00.000Z&lt;/schedDispatchDate&gt;
         &lt;actualDispatchDate&gt;2020-05-21T08:20:53.000Z&lt;/actualDispatchDate&gt;
         &lt;estimatedArrivalDate&gt;2020-05-25T08:19:53.000Z&lt;/estimatedArrivalDate&gt;
         &lt;billingMethod&gt;RULE&lt;/billingMethod&gt;
         &lt;STCCCode&gt;4711110&lt;/STCCCode&gt;
         &lt;planNumber&gt;065&lt;/planNumber&gt;
         &lt;powerType&gt;1X&lt;/powerType&gt;
         &lt;powerOnlyFlag&gt;false&lt;/powerOnlyFlag&gt;
      &lt;/purchasedCostTripSegment&gt;
      &lt;purchasedCostTripSegment&gt;
         &lt;purchCostReference&gt;2618252&lt;/purchCostReference&gt;
         &lt;carrier&gt;NS&lt;/carrier&gt;
         &lt;vendorType&gt;RAIL&lt;/vendorType&gt;
         &lt;carrierTrailerType&gt;53RP&lt;/carrierTrailerType&gt;
         &lt;origin&gt;
            &lt;ns2:numberCode&gt;4061&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;NSAU&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;6300 SOUTH INDIANA AVENUE&lt;/ns2:address1&gt;
            &lt;ns2:city&gt;CHICAGO&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;IL&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;60637&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;CST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/origin&gt;
         &lt;destination&gt;
            &lt;ns2:numberCode&gt;4040&lt;/ns2:numberCode&gt;
            &lt;ns2:locAbbr&gt;CROX&lt;/ns2:locAbbr&gt;
            &lt;ns2:address1&gt;NORFOLK SOUTHERN RAILROAD&lt;/ns2:address1&gt;
            &lt;ns2:address2&gt;125 COUNTY ROAD&lt;/ns2:address2&gt;
            &lt;ns2:city&gt;CROXTON&lt;/ns2:city&gt;
            &lt;ns2:stateProvince&gt;NJ&lt;/ns2:stateProvince&gt;
            &lt;ns2:postalCode&gt;07307&lt;/ns2:postalCode&gt;
            &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
            &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
            &lt;ns2:timeZoneAbbr&gt;EST&lt;/ns2:timeZoneAbbr&gt;
            &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
         &lt;/destination&gt;
         &lt;stopOff&gt;
            &lt;ns2:stopOffLocation&gt;
               &lt;ns2:numberCode&gt;4040&lt;/ns2:numberCode&gt;
               &lt;ns2:locAbbr&gt;CROX&lt;/ns2:locAbbr&gt;
               &lt;ns2:address1&gt;NORFOLK SOUTHERN RAILROAD&lt;/ns2:address1&gt;
               &lt;ns2:address2&gt;125 COUNTY ROAD&lt;/ns2:address2&gt;
               &lt;ns2:city&gt;CROXTON&lt;/ns2:city&gt;
               &lt;ns2:stateProvince&gt;NJ&lt;/ns2:stateProvince&gt;
               &lt;ns2:postalCode&gt;07307&lt;/ns2:postalCode&gt;
               &lt;ns2:locType&gt;FDEG&lt;/ns2:locType&gt;
               &lt;ns2:numberType&gt;8&lt;/ns2:numberType&gt;
               &lt;ns2:timeZoneAbbr&gt;EST&lt;/ns2:timeZoneAbbr&gt;
               &lt;ns2:daylightSavingsFlag&gt;true&lt;/ns2:daylightSavingsFlag&gt;
            &lt;/ns2:stopOffLocation&gt;
         &lt;/stopOff&gt;
         &lt;schedDispatchDate&gt;2020-05-20T05:00:00.000Z&lt;/schedDispatchDate&gt;
         &lt;actualDispatchDate&gt;2020-05-21T08:20:53.000Z&lt;/actualDispatchDate&gt;
         &lt;estimatedArrivalDate&gt;2020-05-25T08:20:53.000Z&lt;/estimatedArrivalDate&gt;
         &lt;billingMethod&gt;LOCAL&lt;/billingMethod&gt;
         &lt;STCCCode&gt;4711110&lt;/STCCCode&gt;
         &lt;planNumber&gt;065&lt;/planNumber&gt;
         &lt;powerType&gt;1X&lt;/powerType&gt;
         &lt;powerOnlyFlag&gt;false&lt;/powerOnlyFlag&gt;
      &lt;/purchasedCostTripSegment&gt;
   &lt;/purchasedCost&gt;
</code></pre>

<p>How to extract the numberCode and carrier instances for each of the purchase cost segments?</p>",62071377.0,1,0,,2020-5-22 15:49:36,,2020-5-28 17:56:13,,,,,1724560.0,,1,0,xml|splunk-query,82,7
592,259048,61964601,Create regular expression using the rex command in Splunk to return xml child elements,"<p>I am trying to use the Splunk command <strong>rex</strong> which uses regular expressions to extract data from a log statement.  This is the part of the xml of interest:</p>

<pre><code>&lt;tmsTrip xmlns=""removed_for_security"" xmlns:ns2=""removed_for_security""&gt;
   &lt;recordType&gt;PURCHASEDLINEHAUL&lt;/recordType&gt;
   &lt;eventType&gt;DISPATCH&lt;/eventType&gt;
   &lt;updatedDateGMT&gt;2020-05-21T17:22:55.000Z&lt;/updatedDateGMT&gt;
   &lt;origin&gt;
      &lt;ns2:numberCode&gt;923&lt;/ns2:numberCode&gt;
      &lt;ns2:numberType&gt;2&lt;/ns2:numberType&gt;
   &lt;/origin&gt;
   &lt;destination&gt;
      &lt;ns2:numberCode&gt;72&lt;/ns2:numberCode&gt;
      &lt;ns2:numberType&gt;2&lt;/ns2:numberType&gt;
   &lt;/destination&gt;
</code></pre>

<p>I need the <strong>numberCode</strong> from the origin and destination.
This rex returns the first one, 923.</p>

<pre><code>rex field=_raw ""\&lt;ns2\:numberCode\&gt;(?P&lt;origin&gt;[^\&lt;]+)"" 
</code></pre>

<p>I need a rex to return the second one, 72.</p>",61965378.0,1,0,,2020-5-22 22:07:44,,2020-5-22 23:25:02,,,,,1724560.0,,1,0,regex|xml|splunk,265,11
593,259049,61974591,Owncloud: Log-Files for login-attempts,"<p>Does Owncloud log any files with successfull/failed login attempts? I would like to transfer these into Splunk to anaylze potential attacks against our system. 
Thanks a lot in advance! </p>",62025193.0,2,0,,2020-5-23 15:45:39,,2020-5-26 15:02:34,,,,,13603091.0,,1,0,security|splunk|owncloud,326,9
594,259050,61983347,"How to fix this error -""aria-label"" attribute accurately describe the element?","<p>I am trying to fix the aria-label issue in Splunk I have 3 dropdowns. After doing testing using Siteimprove I am facing this issue and also Splunk is not allowing aria-label in the label field</p>

<p>Can anyone suggest how to fix this issue?</p>

<pre class=""lang-html prettyprint-override""><code>&lt;fieldset submitButton=""false"" autoRun=""true""&gt;&lt;/fieldset&gt;
&lt;row&gt;
  &lt;panel&gt;
    &lt;input type=""dropdown"" token=""area"" searchWhenChanged=""true""&gt;
    &lt;label&gt;Area&lt;/label&gt;
    &lt;choice value=""*""&gt;ALL&lt;/choice&gt;
    &lt;fieldForLabel&gt;Area&lt;/fieldForLabel&gt;
    &lt;fieldForValue&gt;Area&lt;/fieldForValue&gt;
    &lt;search&gt;
      &lt;query&gt;| loadjob savedsearch=Test ""Area""&lt;/query&gt;
      &lt;done&gt;
        &lt;set token=""enabledownload""&gt;none&lt;/set&gt;
        &lt;unset token=""display_details""&gt;&lt;/unset&gt;
      &lt;/done&gt;
    &lt;/search&gt;
    &lt;default&gt;*&lt;/default&gt;
    &lt;initialValue&gt;*&lt;/initialValue&gt;
    &lt;/input&gt;
    &lt;input type=""dropdown"" token=""dm"" searchWhenChanged=""true""&gt;
    &lt;label&gt;DM&lt;/label&gt;
    &lt;choice value=""*""&gt;ALL&lt;/choice&gt;
    &lt;fieldForLabel&gt;DM/fieldForLabel&gt;
      &lt;fieldForValue&gt;DM&lt;/fieldForValue&gt;
      &lt;search&gt;
        &lt;query&gt;| loadjob savedsearch=Test ""DM""&lt;/query&gt;
      &lt;/search&gt;
      &lt;change&gt;
        &lt;set token=""enabledownload""&gt;none&lt;/set&gt;
        &lt;unset token=""display_details""&gt;&lt;/unset&gt;
      &lt;/change&gt;
      &lt;default&gt;*&lt;/default&gt;
      &lt;initialValue&gt;*&lt;/initialValue&gt;
      &lt;/input&gt;
      &lt;input type=""dropdown"" token=""al"" searchWhenChanged=""true""&gt;
      &lt;label&gt;AL&lt;/label&gt;
      &lt;choice value=""*""&gt;AL&lt;/choice&gt;
      &lt;fieldForLabel&gt;AL&lt;/fieldForLabel&gt;
      &lt;fieldForValue&gt;AL&lt;/fieldForValue&gt;
      &lt;search&gt;
        &lt;query&gt;| loadjob savedsearch=Test ""AL""&lt;/query&gt;
      &lt;/search&gt;
      &lt;change&gt;
        &lt;set token=""enabledownload""&gt;none&lt;/set&gt;
        &lt;unset token=""display_details""&gt;&lt;/unset&gt;
      &lt;/change&gt;
      &lt;default&gt;*&lt;/default&gt;
      &lt;initialValue&gt;*&lt;/initialValue&gt;
      &lt;/input&gt;
      &lt;input type=""time"" token=""date_range"" searchWhenChanged=""true""&gt;
      &lt;label&gt;Date Range&lt;/label&gt;
      &lt;default&gt;
        &lt;earliest&gt;-24h@h&lt;/earliest&gt;
        &lt;latest&gt;now&lt;/latest&gt;
      &lt;/default&gt;
      &lt;change&gt;
        &lt;set token=""enabledownload""&gt;none&lt;/set&gt;
        &lt;unset token=""display_details""&gt;&lt;/unset&gt;
      &lt;/change&gt;
      &lt;/input&gt;
  &lt;/panel&gt;
</code></pre>",,1,1,,2020-5-24 08:21:09,,2020-8-28 11:22:32,2020-5-24 09:40:48,,1804251.0,,10403194.0,,1,0,html|css|splunk|wcag,119,8
595,259051,62047550,How to find authentication SSH logs for Linux,"<p>I am trying to write a query via splunk to find SSH logs used for authentication in Linux. Any ideas as to the query needed to writer to achieve this? I am new to splunk so any information would help.</p>

<p>Here is what I have started but to no avail:</p>

<p>sshd ""Invalid user"" NOT port NOT ""preauth]"" | iplocation InvalidSSHIP</p>",,1,1,,2020-5-27 16:01:10,,2020-5-29 01:19:28,,,,,10405409.0,,1,0,splunk|splunk-query|splunk-calculation,204,9
596,259052,62061727,Splunk query filter out based on other event in same index,"<p>I have a index named <code>Events</code></p>

<p>It contains a bunch of different events, all events have a property called <code>EventName</code>.
Now I want to do a query where I return everything that matches the following:</p>

<p>IF AccountId exists in event with EventName <code>AccountCreated</code> AND there is at least 1 event with EventName <code>FavoriteCreated</code> with the same AccountId -> return all events where EventName == <code>AccountCreated</code></p>

<p>Example events:</p>

<p><strong>AccountCreated</strong></p>

<pre><code>{
   ""AccountId"": 1234,
   ""EventName"": ""AccountCreated"",
   ""SomeOtherProperty"": ""Some value"",
   ""Brand"": ""My Brand"",
   ""DeviceType"": ""Mobile"",
   ""EventTime"": ""2020-06-01T12:13:14Z""
}
</code></pre>

<p><strong>FavoriteCreated</strong></p>

<pre><code>{
   ""AccountId"": 1234,
   ""EventName"": ""FavoritesCreated,
   ""Brand"": ""My Brand"",
   ""DeviceType"": ""Mobile"",
   ""EventTime"": ""2020-06-01T12:13:14Z""
}
</code></pre>

<p>Given the following two events, I would like to create 1 query that returns the AccountCreated event.</p>

<p>I've tried the following but it does not work, surely I must be missing something simple?</p>

<pre><code>index=events EventName=AccountCreated 
  [search index=events EventName=FavoriteCreated | dedup AccountId | fields AccountId]
| table AccountId, SomeOtherProperty
</code></pre>

<p>Im expecting ~6000 hits here but Im only getting 2298 events. What am I missing?</p>

<p><strong>UPDATE</strong>
Based on the answer given by @warren below, the following query works. The only problem is that it's using a JOIN which limits us to 50K results from the subsearch. When running this query I get 5900 results in total = Correct.</p>

<pre><code>index=events EventName=AccountCreated AccountId=*
| stats count by AccountId, EventName
| fields - count
| join AccountId 
    [ | search index=events EventName=FavoriteCreated AccountId=*
    | stats count by AccountId ]
| fields - count
| table AccountId, EventName
</code></pre>

<p>I then tried to use his updated example like this but the problem seems to be that it returns FavoriteCreated events instead of AccountCreated. 
When running this query I get 25 494 hits = Incorrect.</p>

<pre><code>index=events AccountId=* (EventName=AccountCreated OR EventName=FavoriteCreated)
| stats values(EventName) as EventName by AccountId
| eval EventName=mvindex(EventName,-1)
| search EventName=""FavoriteCreated""
| table AccountId, EventName
</code></pre>

<p><strong>Update 2 - WORKING</strong>
@warren is awesome, here is a full working query that only returns data from the AccountCreated events IF 1 or more FavoriteCreated event exists.</p>

<pre><code>index=events AccountId=* (EventName=AccountCreated OR EventName=FavoriteCreated)
| stats 
    values(Brand) as Brand,
    values(DeviceType) as DeviceType,
    values(Email) as Email,
    values(EventName) as EventName
    values(EventTime) as EventTime,
    values(Locale) as Locale,
    values(ClientIp) as ClientIp
  by AccountId
| where mvcount(EventName)&gt;1
| eval EventName=mvindex(EventName,0)
| eval EventTime=mvindex(EventTime,0)
| eval ClientIp=mvindex(ClientIp,0)
| eval DeviceType=mvindex(DeviceType,0)
</code></pre>",62065360.0,2,0,,2020-5-28 09:48:40,,2020-6-1 06:37:39,2020-6-1 06:37:39,,1141089.0,,1141089.0,,1,2,splunk|splunk-query,726,13
597,259053,62069217,Need to create a table in Splunk that is a nested table or has multiple columns,"<p>I have a working Splunk search that extracts data from an xml file within a logging statement. The search creates a table with 14 columns.  Below is the query that creates this table</p>

<pre><code>&lt;sourcetype and other data&gt;..| xmlkv  | rex max_match=0 ""\&lt;ns2\:numberCode\&gt;(?P&lt;location&gt;[^\&lt;]+)""| eval Segment1_Origin =  mvindex(location, 7), Segment1_Destination = mvindex(location, 8), Segment2_Origin = mvindex(location, 10), Segment2_Destination = mvindex(location, 11), Segment3_Origin = mvindex(location, 13), Segment3_Destination = mvindex(location, 14)  |  rex max_match=0 ""\&lt;carrier\&gt;(?P&lt;carrier&gt;[^\&lt;]+)"" | eval Segment1_Carrier =  mvindex(carrier, 0), Segment2_Carrier = mvindex(carrier, 1), Segment3_Carrier = mvindex(carrier, 2) |  rex max_match=0 ""\&lt;billingMethod\&gt;(?P&lt;billingMethod&gt;[^\&lt;]+)"" | eval Segment1_BillingMethod =  mvindex(billingMethod, 0), Segment2_BillingMethod = mvindex(billingMethod, 1), Segment3_BillingMethod = mvindex(billingMethod, 2) | table purchCostReference, eventType, Segment1_Carrier, Segment1_BillingMethod, Segment1_Origin, Segment1_Destination, Segment2_Carrier, Segment2_BillingMethod, Segment2_Origin, Segment2_Destination, Segment3_Carrier, Segment3_BillingMethod, Segment3_Origin, Segment3_Destination | sort purchCostReference, eventType
</code></pre>

<p>The table looks like this (all columns not shown because of the size):
<a href=""https://i.stack.imgur.com/Ihepl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ihepl.png"" alt=""enter image description here""></a></p>

<p>I would like the table to have the table nested by Segments in some way.
<a href=""https://i.stack.imgur.com/YWPrR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YWPrR.png"" alt=""enter image description here""></a></p>

<p>Or like this:
<a href=""https://i.stack.imgur.com/hEOXP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hEOXP.png"" alt=""enter image description here""></a></p>

<p>Is either one of these table designs possible in Splunk?</p>",,1,0,,2020-5-28 16:02:03,,2020-5-29 15:20:58,,,,,1724560.0,,1,0,splunk,269,9
598,259054,62087406,Need a regex command to find a value in an xml file,"<p>I have a large xml file that is within a logging statement.  I am using Splunk to extract values from the xml file.  I have to use regex to find these values because I cannot change the config files. I requested the change but it is pending...
This is an example of the xml file:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;tmsTrip xmlns=""http://ground.fedex.com/schemas/linehaul/trip"" xmlns:ns2=""http://ground.fedex.com/schemas/linehaul/TMSCommon""&gt;
   &lt;tripNumber&gt;129271010&lt;/tripNumber&gt;
   &lt;tripLegNumber&gt;1&lt;/tripLegNumber&gt;
   &lt;origin&gt;
      &lt;ns2:numberCode&gt;5902&lt;/ns2:numberCode&gt;
  ...many more fields....
   &lt;/origin&gt;
   &lt;destination&gt;
      &lt;ns2:numberCode&gt;5087&lt;/ns2:numberCode&gt;
   ...many more fields....
   &lt;/destination&gt;
  ...many more fields....
   &lt;purchasedCost&gt;
      &lt;purchasedCostTripSegment&gt;
         &lt;purchCostReference&gt;2644025&lt;/purchCostReference&gt;
         &lt;carrier&gt;BNSF&lt;/carrier&gt;
         &lt;vendorType&gt;RAIL&lt;/vendorType&gt;
         &lt;carrierTrailerType&gt;53PC&lt;/carrierTrailerType&gt;
         &lt;origin&gt;
            &lt;ns2:numberCode&gt;4022&lt;/ns2:numberCode&gt;
...many more fields....
         &lt;/origin&gt;
         &lt;destination&gt;
            &lt;ns2:numberCode&gt;4040&lt;/ns2:numberCode&gt;
...many more fields....     
         &lt;stopOff&gt;
            &lt;ns2:stopOffLocation&gt;
               &lt;ns2:numberCode&gt;9996&lt;/ns2:numberCode&gt;
...many more fields....       
            &lt;/ns2:stopOffLocation&gt;
         &lt;/stopOff&gt;
         &lt;schedDispatchDate&gt;2020-05-27T05:00:00.000Z&lt;/schedDispatchDate&gt;
...many more fields....
      &lt;/purchasedCostTripSegment&gt;
      &lt;purchasedCostTripSegment&gt;
         &lt;purchCostReference&gt;2644025&lt;/purchCostReference&gt;
         &lt;carrier&gt;NS&lt;/carrier&gt;
         &lt;vendorType&gt;RAIL&lt;/vendorType&gt;
         &lt;carrierTrailerType&gt;53PC&lt;/carrierTrailerType&gt;
         &lt;origin&gt;
            &lt;ns2:numberCode&gt;4061&lt;/ns2:numberCode&gt;
...many more fields....
         &lt;/origin&gt;
         &lt;destination&gt;
            &lt;ns2:numberCode&gt;4040&lt;/ns2:numberCode&gt;
...many more fields....         
         &lt;/destination&gt;
         &lt;stopOff&gt;
            &lt;ns2:stopOffLocation&gt;
               &lt;ns2:numberCode&gt;4040&lt;/ns2:numberCode&gt;
 ...many more fields....      
            &lt;/ns2:stopOffLocation&gt;
         &lt;/stopOff&gt;
         &lt;schedDispatchDate&gt;2020-05-27T05:00:00.000Z&lt;/schedDispatchDate&gt;
 ...many more fields....     
      &lt;/purchasedCostTripSegment&gt;
   &lt;/purchasedCost&gt;
&lt;/tmsTrip&gt;
</code></pre>

<p>I need to identify the <strong>ns2:numberCode</strong> for the origin and destination for each of the <strong>purchasedCostTripSegment</strong>.  </p>

<p>I am doing this in Splunk so the regex might be particular to Splunk.
I am able to get find the origins and destinations if I use the function <strong>mvindex()</strong> and count the instance of the <strong>ns2:numberCode</strong>.  But then they are individual fields and do not display clearly in a table.
This is the regex command that will return the first origin of a PurchaseCostTripSegment:</p>

<pre><code>| rex max_match=0 ""\&lt;ns2\:numberCode\&gt;(?P&lt;location&gt;[^\&lt;]+)"" | eval Segment1_Origin =  mvindex(location, 7)
</code></pre>

<p>I need a regex that will return all of the origins of the PurchaseCostTripSegments
I tried this:</p>

<pre><code>| rex max_match=0 ""\&lt;purchasedCostTripSegment\&gt;*\&lt;origin\&gt;*\&lt;ns2\:numberCode\&gt;(?P&lt;Origin&gt;[^\&lt;]+)""
</code></pre>

<p>It returned no value.
How can I write the regex to find all of the ns2:numberCode values that are in this section of the xml:</p>

<pre><code>     &lt;purchasedCostTripSegment&gt;
             &lt;purchCostReference&gt;2644025&lt;/purchCostReference&gt;
             &lt;carrier&gt;BNSF&lt;/carrier&gt;
             &lt;vendorType&gt;RAIL&lt;/vendorType&gt;
             &lt;carrierTrailerType&gt;53PC&lt;/carrierTrailerType&gt;
             &lt;origin&gt;
                &lt;ns2:numberCode&gt;4022&lt;/ns2:numberCode&gt;
&lt;/purchasedCostTripSegment&gt;
      &lt;purchasedCostTripSegment&gt;
         &lt;purchCostReference&gt;2644025&lt;/purchCostReference&gt;
         &lt;carrier&gt;NS&lt;/carrier&gt;
         &lt;vendorType&gt;RAIL&lt;/vendorType&gt;
         &lt;carrierTrailerType&gt;53PC&lt;/carrierTrailerType&gt;
         &lt;origin&gt;
            &lt;ns2:numberCode&gt;4061&lt;/ns2:numberCode&gt;
&lt;/purchasedCostTripSegment&gt;
</code></pre>

<p>In the above instance, I want to return values, 4022 and 4061,</p>",62087985.0,1,3,,2020-5-29 13:44:15,,2020-5-29 14:15:42,,,,,1724560.0,,1,1,regex|splunk,82,8
599,259055,62091254,Using Splunk Dashboard how to create a filter for each column of a table,"<p>I have a table that I saved as a report and created a dashboard that displays the table.
I want to add inputs for each field on the table to be able to filter the table data.
This is the table
<a href=""https://i.stack.imgur.com/eHLur.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eHLur.png"" alt=""enter image description here""></a></p>

<p>I added a text box as the first input
<a href=""https://i.stack.imgur.com/Kb1eF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kb1eF.png"" alt=""enter image description here""></a></p>

<p>How do I link the text box that has a token called <strong>purchCostReferenceToken</strong> to the purchCostReference column on the table?</p>",62096191.0,1,0,,2020-5-29 17:11:18,,2020-5-29 23:55:26,,,,,1724560.0,,1,0,splunk,423,10
600,259056,62105546,Splunk: Unable to get the correct min and max values,"<p>I'm a newbie as far as Splunk is concerned with modest regex skills.</p>

<p>We have events with the following patterns:</p>

<pre><code>fallbackAPIStatus={api1=133:..., api2=472:...,api3=498:...}
fallbackAPIStatus={api1=3535:...}
fallbackAPIStatus={api2=252:...,api3=655:...}
</code></pre>

<p>The numeric value indicates the response times and the ellipsis inidcates fields that I'm not interested in.</p>

<p>The number of apis within the braces is dynamic (between 1 and 4)</p>

<p>I want to be able to create a table as follows:</p>

<pre><code>apiName   TotalRequests  Max-Response-Time  Min-Response-Time 
api1           2            3535           133
api2           2             472           252
api2           2             655           498
</code></pre>

<p>Here's my search:</p>

<pre><code>index=my_logs  sourcetype=my_sourcetype | rex field=_raw ""fallbackAPIStatus=\{(?P&lt;fallBackApis&gt;[^\}]+)\}"" | eval temp=split(fallBackApis,"","") | rex field=temp ""(?P&lt;apiName&gt;[a-zA-Z-]+)=(?P&lt;responseTime&gt;[0-9]+):""|stats count as TotalRequests  max(responseTime) as Max-Response-Time min(responseTime) as Min-Response-Time by apiName
</code></pre>

<p>I'm able to get the TotalRequests right but I'm not able to get the correct max and min response times</p>

<p>Can someone advise what I'm doing wrong here?</p>",,1,0,,2020-5-30 16:35:06,,2020-5-31 00:04:08,2020-5-30 16:41:50,,13648776.0,,13648776.0,,1,3,splunk|splunk-query,76,9
601,259057,62105685,How can I send input parameter data to a report on a dashboard in Splunk,"<p>I am using Splunk to create a dashboard.  I added a report to the dashboard that returns all of the data from the search into a table. I want to add a few input fields so the user to option to filter the data of the report.  The first input is a textbox field.  The default and initial values are set to with an *.  That I hope means everything.
This is the xml created from the dashboard:</p>

<pre><code>&lt;form&gt;
  &lt;label&gt;Thru Train Dashboard&lt;/label&gt;
  &lt;fieldset submitButton=""false"" autoRun=""true""&gt;
    &lt;input type=""text"" token=""purchCostReferenceToken"" searchWhenChanged=""true""&gt;
      &lt;label&gt;TMS Reference Number&lt;/label&gt;
      &lt;default&gt;*&lt;/default&gt;
      &lt;initialValue&gt;*&lt;/initialValue&gt;
    &lt;/input&gt;
  &lt;/fieldset&gt;
  &lt;row&gt;
    &lt;panel&gt;
      &lt;title&gt;Thru Train XML DATA&lt;/title&gt;
      &lt;table&gt;
        &lt;search ref=""ThruTrainReportNestedResults""&gt;&lt;/search&gt;
        &lt;option name=""drilldown""&gt;row&lt;/option&gt;
        &lt;option name=""rowNumbers""&gt;true&lt;/option&gt;
      &lt;/table&gt;
    &lt;/panel&gt;
  &lt;/row&gt;
&lt;/form&gt;
</code></pre>

<p>I know I have to use the token value <strong>purchCostReferenceToken</strong> as an input to the report <strong>ThruTrainReportNestedResults</strong>.  But not sure how to do that since the report search as no input parameters.</p>

<p>This is the search query that creates the report, <strong>ThruTrainReportNestedResults</strong></p>

<pre><code> sourcetype... | xmlkv | rex max_match=0 ""\&lt;purchasedCostTripSegment\&gt;(?P&lt;segment&gt;[^\&lt;]+)"" |eval Segments =  mvrange(1,mvcount(mvindex(segment, 0, 2))+1,1) | rex max_match=0 ""\&lt;carrier\&gt;(?P&lt;Carriers&gt;[^\&lt;]+)"" | rex max_match=0 ""\&lt;billingMethod\&gt;(?P&lt;BillingMethod&gt;[^\&lt;]+)"" | rex max_match=0 ""&lt;purchasedCostTripSegment&gt;[\s\S]*?&lt;origin&gt;\s*&lt;ns2:numberCode&gt;(?P&lt;Origin&gt;\d+)""  | rex max_match=0 ""&lt;purchasedCostTripSegment&gt;[\s\S]*?&lt;destination&gt;\s*&lt;ns2:numberCode&gt;(?P&lt;Destination&gt;\d+)"" | rex max_match=0 ""&lt;purchasedCostTripSegment&gt;[\s\S]*?&lt;stopOff&gt;\s*&lt;ns2:stopOffLocation&gt;\s*&lt;ns2:numberCode&gt;(?P&lt;StopOffLocation&gt;\d+)"" | eval Time =_time | convert timeformat=""%m-%d-%Y %H:%M:%S"" ctime(Time) | table purchCostReference, eventType, Time, Segments, Carriers, BillingMethod, Origin, Destination, StopOffLocation | sort Time
</code></pre>

<p>Is there a way to filter the results of this query from a dashboard using input data?  The input data that I would want to filter on would be the purchCostReference, eventType, and Segments</p>

<p><strong>UPDATE</strong>
I removed the reference to the report and added the search string that creates the report but am getting the error <strong>Invalid character in tag name</strong>.  The search string is very long...</p>

<pre><code>&lt;row&gt;
    &lt;panel&gt;
      &lt;title&gt;Thru Train XML DATA&lt;/title&gt;
      &lt;table&gt;
        &lt;search base=""baseSearch""&gt;
             &lt;query&gt;
                index... sourcetype=""..."" ""&lt;billingMethod&gt;RULE&lt;/billingMethod&gt;"" ""createMessage MsgSource"" | xmlkv | rex max_match=0 ""\&lt;purchasedCostTripSegment\&gt;(?P&lt;segment&gt;[^\&lt;]+)"" |eval Segments =  mvrange(1,mvcount(mvindex(segment, 0, 2))+1,1) | rex max_match=0 ""\&lt;carrier\&gt;(?P&lt;Carriers&gt;[^\&lt;]+)"" | rex max_match=0 ""\&lt;billingMethod\&gt;(?P&lt;BillingMethod&gt;[^\&lt;]+)"" | rex max_match=0 ""&lt;purchasedCostTripSegment&gt;[\s\S]*?&lt;origin&gt;\s*&lt;ns2:numberCode&gt;(?P&lt;Origin&gt;\d+)""  | rex max_match=0 ""&lt;purchasedCostTripSegment&gt;[\s\S]*?&lt;destination&gt;\s*&lt;ns2:numberCode&gt;(?P&lt;Destination&gt;\d+)"" | rex max_match=0 ""&lt;purchasedCostTripSegment&gt;[\s\S]*?&lt;stopOff&gt;\s*&lt;ns2:stopOffLocation&gt;\s*&lt;ns2:numberCode&gt;(?P&lt;StopOffLocation&gt;\d+)"" | eval Time =_time | convert timeformat=""%m-%d-%Y %H:%M:%S"" ctime(Time) | table purchCostReference, eventType, Time, Segments, Carriers, BillingMethod, Origin, Destination, StopOffLocation | sort Time
            &lt;/query&gt;
        &lt;/search&gt;
        &lt;option name=""drilldown""&gt;row&lt;/option&gt;
        &lt;option name=""rowNumbers""&gt;true&lt;/option&gt;
      &lt;/table&gt;
    &lt;/panel&gt;
  &lt;/row&gt;
</code></pre>

<p><strong>UPDATE</strong> I encoded the search query and added between the query tags with the token element as described in a previous comment.  I do not get an error but get no results either.  Just a blank table...</p>

<pre><code>&lt;query&gt;             ...sourcetype%....%22%20%22%3CbillingMethod%3ERULE%3C%2FbillingMethod%3E%22%20%22createMessage%20MsgSource%22%20%7C%20xmlkv%20%7C%20rex%20max_match%3D0%20%22%5C%3CpurchasedCostTripSegment%5C%3E%28%3FP%3Csegment%3E%5B%5E%5C%3C%5D%2B%29%22%20%7Ceval%20Segments%20%3D%20%20mvrange%281%2Cmvcount%28mvindex%28segment%2C%200%2C%202%29%29%2B1%2C1%29%20%7C%20rex%20max_match%3D0%20%22%5C%3Ccarrier%5C%3E%28%3FP%3CCarriers%3E%5B%5E%5C%3C%5D%2B%29%22%20%7C%20rex%20max_match%3D0%20%22%5C%3CbillingMethod%5C%3E%28%3FP%3CBillingMethod%3E%5B%5E%5C%3C%5D%2B%29%22%20%7C%20rex%20max_match%3D0%20%22%3CpurchasedCostTripSegment%3E%5B%5Cs%5CS%5D%2A%3F%3Corigin%3E%5Cs%2A%3Cns2%3AnumberCode%3E%28%3FP%3COrigin%3E%5Cd%2B%29%22%20%20%7C%20rex%20max_match%3D0%20%22%3CpurchasedCostTripSegment%3E%5B%5Cs%5CS%5D%2A%3F%3Cdestination%3E%5Cs%2A%3Cns2%3AnumberCode%3E%28%3FP%3CDestination%3E%5Cd%2B%29%22%20%7C%20rex%20max_match%3D0%20%22%3CpurchasedCostTripSegment%3E%5B%5Cs%5CS%5D%2A%3F%3CstopOff%3E%5Cs%2A%3Cns2%3AstopOffLocation%3E%5Cs%2A%3Cns2%3AnumberCode%3E%28%3FP%3CStopOffLocation%3E%5Cd%2B%29%22%20%7C%20eval%20Time%20%3D_time%20%7C%20convert%20timeformat%3D%22%25m-%25d-%25Y%20%25H%3A%25M%3A%25S%22%20ctime%28Time%29%20%7C%20table%20purchCostReference%2C%20eventType%2C%20Time%2C%20Segments%2C%20Carriers%2C%20BillingMethod%2C%20Origin%2C%20Destination%2C%20StopOffLocation%20%7C%20sort%20Time purchCostReference=$purchCostReferenceToken$
&lt;/query&gt;
</code></pre>

<p>This is a screen shot of the results:
<a href=""https://i.stack.imgur.com/JhXpS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JhXpS.png"" alt=""enter image description here""></a></p>

<p><strong>UPDATE</strong>
I removed the complexity of the search so that it only has the index, source and sourcetype that is the same as the report and a text field to limit the search. Below is the query string:</p>

<pre><code>&lt;query&gt;index=""indexname same as report"" source=""source name same as report"" sourcetype=""source type name same as report"" ""createMessage MsgSource""&lt;/query&gt;
</code></pre>

<p>It returns a blank dashboard with no error as pictured above.</p>

<p><strong>UPDATE</strong>
I recreated the dashboard using the report query and have the search returning all of the table results.  I have an input for the reference number as a text box.  The token name is: <strong>purchCostReferenceToken</strong>  </p>

<p>I want to limit the table results based on this token. This is the query:</p>

<pre><code>&lt;form&gt;
  &lt;label&gt;Thru Train Dashboard&lt;/label&gt;
  &lt;fieldset submitButton=""false"" autoRun=""true""&gt;
    &lt;input type=""text"" token=""purchCostReferenceToken"" searchWhenChanged=""true""&gt;
      &lt;label&gt;Enter a TMS Reference Number to Filter Table&lt;/label&gt;
      &lt;default&gt;*&lt;/default&gt;
      &lt;initialValue&gt;*&lt;/initialValue&gt;
    &lt;/input&gt;
  &lt;/fieldset&gt;
  &lt;row&gt;
    &lt;panel&gt;
      &lt;title&gt;Thru Train Data&lt;/title&gt;
      &lt;table&gt;
        &lt;search&gt;
          &lt;query&gt;index=... ""&amp;lt;billingMethod&amp;gt;RULE&amp;lt;/billingMethod&amp;gt;"" ""createMessage MsgSource"" | xmlkv | rex max_match=0 ""\&amp;lt;purchasedCostTripSegment\&amp;gt;(?P&amp;lt;segment&amp;gt;[^\&amp;lt;]+)"" |eval Segments =  mvrange(1,mvcount(mvindex(segment, 0, 2))+1,1) | rex max_match=0 ""\&amp;lt;carrier\&amp;gt;(?P&amp;lt;Carriers&amp;gt;[^\&amp;lt;]+)"" | rex max_match=0 ""\&amp;lt;billingMethod\&amp;gt;(?P&amp;lt;BillingMethod&amp;gt;[^\&amp;lt;]+)"" | rex max_match=0 ""&amp;lt;purchasedCostTripSegment&amp;gt;[\s\S]*?&amp;lt;origin&amp;gt;\s*&amp;lt;ns2:numberCode&amp;gt;(?P&amp;lt;Origin&amp;gt;\d+)""  | rex max_match=0 ""&amp;lt;purchasedCostTripSegment&amp;gt;[\s\S]*?&amp;lt;destination&amp;gt;\s*&amp;lt;ns2:numberCode&amp;gt;(?P&amp;lt;Destination&amp;gt;\d+)"" | rex max_match=0 ""&amp;lt;purchasedCostTripSegment&amp;gt;[\s\S]*?&amp;lt;stopOff&amp;gt;\s*&amp;lt;ns2:stopOffLocation&amp;gt;\s*&amp;lt;ns2:numberCode&amp;gt;(?P&amp;lt;StopOffLocation&amp;gt;\d+)"" | eval Time =_time | convert timeformat=""%m-%d-%Y %H:%M:%S"" ctime(Time) | table purchCostReference, eventType, Time, Segments, Carriers, BillingMethod, Origin, Destination, StopOffLocation | sort Time&lt;/query&gt;
          &lt;earliest&gt;-30d@d&lt;/earliest&gt;
          &lt;latest&gt;now&lt;/latest&gt;
        &lt;/search&gt;
        &lt;option name=""drilldown""&gt;none&lt;/option&gt;
      &lt;/table&gt;
    &lt;/panel&gt;
  &lt;/row&gt;
&lt;/form&gt;
</code></pre>

<p>Where do I add the token to limit the search?
I tried adding this to the end of the query and all results were returned:</p>

<pre><code>purchCostReference=$purchCostReferenceToken$ 
</code></pre>",,1,0,,2020-5-30 16:46:45,,2020-6-2 14:42:36,2020-6-2 14:42:36,,1724560.0,,1724560.0,,1,0,splunk|splunk-query,662,11
602,259058,62151072,How to extract fields from JSON string in Splunk,"<p>In Splunk after searching I am getting below result-</p>

<p><code>FINISH  OnDemandModel - Model: Application:GVAP RequestID:test_manifest_0003 Project:AMPS EMRid:j-XHFRN0A4M3QQ status:success</code></p>

<p>I want to extract fields like Application, RequestID, Project, EMRid and status as columns and corresponding values as those columns' values.</p>

<p>I am new to Splunk and not sure how to use spath or other search commands.</p>",62152137.0,1,2,,2020-6-2 11:34:44,1.0,2020-6-2 12:34:13,,,,,8675943.0,,1,1,splunk|splunk-query,1535,15
603,259059,62155324,How do I added a token to the query of a dashboard in Splunk?,"<p>I recreated the dashboard using the report query and have the search returning all of the table results.  I have an input for the reference number as a text box.  The token name is: <strong>purchCostReferenceToken</strong>  </p>

<p>I want to limit the table results based on this token. This is the query:</p>

<pre><code>&lt;form&gt;
  &lt;label&gt;Thru Train Dashboard&lt;/label&gt;
  &lt;fieldset submitButton=""false"" autoRun=""true""&gt;
    &lt;input type=""text"" token=""purchCostReferenceToken"" searchWhenChanged=""true""&gt;
      &lt;label&gt;Enter a TMS Reference Number to Filter Table&lt;/label&gt;
      &lt;default&gt;*&lt;/default&gt;
      &lt;initialValue&gt;*&lt;/initialValue&gt;
    &lt;/input&gt;
  &lt;/fieldset&gt;
  &lt;row&gt;
    &lt;panel&gt;
      &lt;title&gt;Thru Train Data&lt;/title&gt;
      &lt;table&gt;
        &lt;search&gt;
          &lt;query&gt;index=... ""&amp;lt;billingMethod&amp;gt;RULE&amp;lt;/billingMethod&amp;gt;"" ""createMessage MsgSource"" | xmlkv | rex max_match=0 ""\&amp;lt;purchasedCostTripSegment\&amp;gt;(?P&amp;lt;segment&amp;gt;[^\&amp;lt;]+)"" |eval Segments =  mvrange(1,mvcount(mvindex(segment, 0, 2))+1,1) | rex max_match=0 ""\&amp;lt;carrier\&amp;gt;(?P&amp;lt;Carriers&amp;gt;[^\&amp;lt;]+)"" | rex max_match=0 ""\&amp;lt;billingMethod\&amp;gt;(?P&amp;lt;BillingMethod&amp;gt;[^\&amp;lt;]+)"" | rex max_match=0 ""&amp;lt;purchasedCostTripSegment&amp;gt;[\s\S]*?&amp;lt;origin&amp;gt;\s*&amp;lt;ns2:numberCode&amp;gt;(?P&amp;lt;Origin&amp;gt;\d+)""  | rex max_match=0 ""&amp;lt;purchasedCostTripSegment&amp;gt;[\s\S]*?&amp;lt;destination&amp;gt;\s*&amp;lt;ns2:numberCode&amp;gt;(?P&amp;lt;Destination&amp;gt;\d+)"" | rex max_match=0 ""&amp;lt;purchasedCostTripSegment&amp;gt;[\s\S]*?&amp;lt;stopOff&amp;gt;\s*&amp;lt;ns2:stopOffLocation&amp;gt;\s*&amp;lt;ns2:numberCode&amp;gt;(?P&amp;lt;StopOffLocation&amp;gt;\d+)"" | eval Time =_time | convert timeformat=""%m-%d-%Y %H:%M:%S"" ctime(Time) | table purchCostReference, eventType, Time, Segments, Carriers, BillingMethod, Origin, Destination, StopOffLocation | sort Time&lt;/query&gt;
          &lt;earliest&gt;-30d@d&lt;/earliest&gt;
          &lt;latest&gt;now&lt;/latest&gt;
        &lt;/search&gt;
        &lt;option name=""drilldown""&gt;none&lt;/option&gt;
      &lt;/table&gt;
    &lt;/panel&gt;
  &lt;/row&gt;
&lt;/form&gt;
</code></pre>

<p>Where do I add the token to limit the search?
I tried adding this to the end of the query before the table command:</p>

<pre><code>...   | eval Time =_time | convert timeformat=""%m-%d-%Y %H:%M:%S"" ctime(Time) purchCostReference=$purchCostReferenceToken$ | table purchCostReference, eventType, Time, Segments, Carriers, BillingMethod, Origin, Destination, StopOffLocation | sort Time
</code></pre>

<p>I get an error...<strong>error in convert command: the argument purchCostReference- is invalid</strong></p>

<p>I would like to add filters in several of the table columns.  The purchCostReference value is an extracted field in the query using <strong>xmlkv</strong>  </p>",62158670.0,1,0,,2020-6-2 15:19:25,,2020-6-2 18:15:45,2020-6-2 15:25:01,,1724560.0,,1724560.0,,1,1,splunk|splunk-query,295,11
604,259060,62168518,"Getting Error as ""Regex: syntax error in subpattern name (missing terminator)."" in SPLUNK","<p>I have been extracting fields in Splunk and this looks to be working fine for all headers but for the header l-s-m, I am getting the error as  ""syntax error in subpattern name (missing terminator).""</p>

<p>I have done similar for other headers and all works but this is the only header with ""hypen"" sign that is giving this error, I have tried multiple times but this is not helping.</p>

<p>Headers:</p>

<pre><code>Content-Type: application/json
Accept: application/json,application/problem json
l-m-n: txxxmnoltr 
Accept-Encoinding:gzip

</code></pre>

<p>Regex I am trying is <code>""rex field=u ""l-m-n: (?&lt;l-m-n&gt;.*)""</code> in SPLUNK. Could you please guide me here?</p>",62207383.0,1,0,,2020-6-3 08:38:17,,2020-6-5 02:56:52,2020-6-4 09:45:29,,11925396.0,,11925396.0,,1,1,splunk|splunk-query,1617,14
605,259061,62171943,What is the reason of Corrupted fields problem in SPLUNK?,"<p>I have a problem on this search below for last 25 days:</p>

<p>index=syslog Reason=""Interface physical link is down"" OR Reason=""Interface physical link is up"" NOT mainIfname=""Vlanif*"" ""nw_ra_a98c_01.34_krtti""</p>

<p>Normally field7 values are like these ones:</p>

<p>Region field7 Date mainIfname Reason count
ASYA nw_ra_m02f_01.34pndkdv may 9 GigabitEthernet0/3/6 Interface physical link is up 3
ASYA nw_ra_m02f_01.34pldtwr may 9 GigabitEthernet0/3/24 Interface physical link is up 2</p>

<p>But recently they wee like this:</p>

<p>00:00:00.599 nw_ra_a98c_01.34_krtti
00:00:03.078 nw_ra_a98c_01.34_krtti</p>

<p>I think problem may be related to:</p>

<p>It started to happen after the disk free alarm. (-Cri- Swap reservation, bottleneck situation, current value: 95.00% exceeds configured threshold: 90.00%. : 07:17 17/02/20)
Especially This is not about disk, it's about swap space, the application finishes memory and then goes to swap use. There was memory increase before, but obviously it was insufficient, it is switching to swap again.
I need to understand: ''Why they use so many resources?''<a href=""https://i.stack.imgur.com/YYAZg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YYAZg.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/MJEiT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MJEiT.jpg"" alt=""enter image description here""></a></p>

<p>Problematic one:</p>

<p>Normal one:</p>",,1,0,,2020-6-3 11:39:38,,2020-6-3 23:16:05,,,,,9533997.0,,1,0,search|report|field|splunk|execute,38,7
606,259062,62180282,How to install splunkclient for Windows,"<p>I am trying to connect to my splunk server via Python on my WIndows laptop.</p>

<p>I downloaded splunklib and splunk-sdk. However, when I run </p>

<pre><code>import splunklib.client as client
</code></pre>

<p>I get an error of </p>

<pre><code>ModuleNotFoundError: No module named 'splunklib.client'; 'splunklib' is not a package
</code></pre>

<p>Any ideas on why this is occurring and suggestions on to how to fix this or the best way to access Splunk via Python?</p>",,2,0,,2020-6-3 18:37:17,,2020-7-27 01:24:13,,,,,10405409.0,,1,0,python-3.x|splunk|splunk-sdk,246,10
607,259063,62188204,How to creat a Splunk bubble diagram from timechart,"<p>I am trying to create a bubblechart based on this search, also seen in image below.</p>

<pre><code>source=""*wineventlog:security"" sourcetype=""*wineventlog:security"" EventCode=4624 OR 4625 OR 4649 OR 4724 OR 4732 OR 4740| timechart span=1h count(EventCode) by EventCode
</code></pre>

<p>I have tried different methods to create something similar to the edited bubblechart image below, but with no success so far. I hope someone here can possibly help me achieve this, if it is even possible?</p>

<p>I can see that i probably would need to get the eventcodes in a own columns, and probably the same with the count...but how?</p>

<p><a href=""https://i.stack.imgur.com/ZOkKQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZOkKQ.png"" alt=""Splunk search""></a></p>

<p><a href=""https://i.stack.imgur.com/zHcZC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zHcZC.png"" alt=""Bubblechart""></a></p>",,1,0,,2020-6-4 06:30:16,,2020-6-4 14:04:01,,,,,7420945.0,,1,0,splunk|splunk-query|splunk-calculation|splunk-formula,40,6
608,259064,62188478,Can not automatically make field on macOS,"<p>When I insert CSV data into Splunk. 
data can not be recognized in Splunk 
I tried to insert same data on windows laptop, that's no problem. 
my mac setted UTF-8  I don't know how can I do that 
data.csv is saved as UTF-8 also maybe my mac has problem with encoding characters</p>

<p><img src=""https://i.stack.imgur.com/b4mZN.png"" alt=""enter image description here""></p>

<p><img src=""https://i.stack.imgur.com/jK9k3.png"" alt=""enter image description here"">
<a href=""https://i.stack.imgur.com/jK9k3.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>macOS Catalina 10.15.4</p>",,1,0,,2020-6-4 06:48:23,,2020-6-5 05:31:27,2020-6-4 07:22:58,,12817074.0,,12817074.0,,1,0,splunk,26,5
609,259065,62202638,Writing a Splunk Query - Unique Count of Initial Access Key Usage from Cloudtrail,"<p>I have a use case to write a splunk query to display in a line or area chart the unique and initial AWS access key usage by IAM users in our org trending for the past year. Management want to be able to visually show increased cloud adoption numbers over time. Any ideas on how to display this? I feel like I'm almost there with stats but not quite.</p>

<p>index=blah sourcetype=blah user_type=SAMLuser | stats earliest(eventTime) by userIdentity.userName</p>

<p>This almost gets me there, but it won't depict the stats in a pretty line chart.</p>

<p>Thanks!</p>",,1,0,,2020-6-4 19:25:37,,2020-6-5 16:28:48,,,,,13682422.0,,1,0,amazon-web-services|cloud|splunk|splunk-query,43,7
610,259066,62247664,Splunk two SourceTypes same columns into multiline chart with 2 axis,"<p>SourceType=A | table EventTime NumOutages
SourceType=B | table EventTime NumOutages</p>

<p>table EventTime NumOutages_A NumOutages_B</p>

<p>X axis will be time by hour and Y axis will be the NumOutages one line per sourcetype</p>

<p>I tried join and a few other examples but for some reason it is not working for me. </p>",62249082.0,1,0,,2020-6-7 15:32:44,,2020-6-7 17:25:21,2020-6-7 15:43:48,,745402.0,,745402.0,,1,0,splunk,27,6
611,259067,62267198,Splunk message in a single string with filenames needs to split,"<p>Hi I have a splunk message that gets list of filenames with paths in a single string. I need to extract all the filenames into new line in a single row</p>

<p>Message: </p>

<pre><code>/opt/test/files/matched/test1.txt, /src/files/log/test.log, /opt/main/unmatched/test2.txt
</code></pre>

<p>Need to get the files names for a id</p>

<pre><code>ID        Filenames
1            test1.txt
             test.log
             test2.txt
2           &lt;another list of names&gt;
</code></pre>

<p>Tried using mvexpand but could not achieve it</p>

<pre><code>| eval FileNames=mvindex(split(split(Sourcefiles,"",""), ""/""),-1)| table Id, FileNames |mvexpand FileName
</code></pre>",,1,0,,2020-6-8 16:50:01,,2020-6-8 18:52:43,2020-6-8 18:52:43,,9340011.0,,9340011.0,,1,0,split|z-index|splunk,203,10
612,259068,62279650,Create container in splunk phantom using phantom rest api,"<p>I want to create container in splunk phantom using phantom rest api. I am using splunk phantom community version. This is the body I am passing using python post method </p>

<pre><code>payload={
            ""description"": ""this is Useful description of this container."",
            ""label"":""events"",
            ""name"":""xOkta event 110"",
            ""sensitivity"": ""red"",
            ""severity"":""medium"",
            ""source_data_identifier"": ""4"",
            ""status"": ""new"",
            ""container_type"": ""default"",
            ""run_automation"": ""False"",
            ""due_time"": ""2020-06-10T19:29:23.759Z"",
            }
</code></pre>

<p>and this is the code :</p>

<pre><code>requests.post(url, auth=(username, password),json=payload, verify=False)
</code></pre>

<p>GET method is working. I am just unable to create container using phantom rest api(I am getting http status code 400 when post called). Any help will be appreciated.</p>",,1,0,,2020-6-9 09:46:10,,2021-11-9 23:39:18,,,,,2323612.0,,1,1,python|api|splunk,211,9
613,259069,62281287,How to extract contents after the last slash in fields in splunk?,"<p>I am new to splunk..SO i have a log which has contents(events) in this format</p>

<p>tool_code: error_code (path1/path2/path3/filename1,line) path1.path2.path3.testname1</p>

<p>I wrote rex to extract filenames and testnames
rex is </p>

<pre><code>|rex field=_raw (?&lt;UNW&gt;\S+)\s+(?&lt;UNWA&gt;\S+)\s+(?&lt;FILE_NAME&gt;\S+)\s+(?&lt;TEST_NAME&gt;\S+)
</code></pre>

<p>this created  table of this format (by using this command<code>|table FILE_NAME, TEST_NAME</code>)</p>

<p>FILE_NAME                            --------------------------------------                TEST_NAME</p>

<p>path1/path2/path3/filename1,line    ------------                 path1.path2.path3.testname1</p>

<p>but i want FILE_NAME to hold only the name(filename1) and not the path(we should extract the contents before the last slash and after the comma) and similarly TEST_NAME should only have testname1 and not the path.  </p>

<p>kindly help me in achieving this</p>",62282541.0,3,0,,2020-6-9 11:13:51,,2020-6-9 12:29:17,,,,,13236984.0,,1,0,splunk|rex,136,9
614,259070,62284476,In splunk How to apply Multiple filter on splunk,"<p>in splunk if we want to add multiple filter how can we do that easily .
eg:-</p>

<p>index=indexer  action= Null NOT IP IN (10.34.67.32 , 87.90.32.10.. so on) </p>

<p>Now question is if i have 519 IP which i want to exclude from result how can we do that easily..</p>

<p>I  already tried below code but it was taking  more time to write query</p>

<p>index = indexer action =Null IP!=10.34.67.32 IP!=87.90.32.10 so on..</p>",,2,0,,2020-6-9 14:02:26,,2020-7-14 05:28:22,2020-6-9 15:12:40,,13330893.0,,13330893.0,,1,0,splunk|splunk-query|splunk-formula,267,12
615,259071,62295131,How to calculate the ratio of field and gropuby the field in Splunk,"<p>I have this table.</p>

<pre><code>Fruits  Result
--------------
Apple   sold
Apple   sold
Apple   instock
Apple   expired
Banana  sold
Banana  sold
Banana  sold
Orange  instock
Orange  instock
</code></pre>

<p>I have to generate report like below in Splunk. I'd like to count by Fruits type and calculate the ratio of results. </p>

<pre><code>Fruits  count  instock_ratio expired_ratio sold_ratio
----------------------------------------------------
Apple   4       0.25         0.25          0.5
Banana  3       0            0             1.0
Orange  2       1.0          0             0
</code></pre>

<p>In SQL, I can get this result.</p>

<pre><code>WITH src AS(
    SELECT
       Fruits,
       count(CASE WHEN result=""sold"" THEN Fruits ELSE null END) AS sold_count,
       count(CASE WHEN result=""instock"" THEN Fruits ELSE null END) AS instock_count,
       count(CASE WHEN result=""expired"" THEN Fruits ELSE null END) AS expired_count,
       count(Fruits) AS total_counts
    FROM table
    GROUP BY Fruits
)
SELECT
   Fruits,
   total_counts,
   sold_count/total_counts,
   instock_count/total_counts,
   expired_count/total_counts
FROM src
</code></pre>

<p>Can anyone help me with the splunk command?</p>",62301948.0,1,0,,2020-6-10 03:02:15,,2020-6-10 11:01:04,,,,,13717660.0,,1,0,splunk,216,9
616,259072,62297606,Delta between two Splunk search results,"<p>I am trying to find out delta between two searches.</p>

<pre><code>index=""xyz-index"" userId | rename attributes.privateGroups as privateGroups 
| join type=inner userId [ search index=""xyz-index"" userId | rename attributes.publicGroups as publicGroups]
| table userId, privateGroups, publicGroups
</code></pre>

<ol>
<li>I want to find out userIds which are in both privateGroups and publicGroups</li>
<li>I want to find out userIds only in privateGroups but not in publicGroups or vice versa</li>
</ol>

<p>For the one i tired with inner query as mentioned above but i get two different search results when i changed the search order.</p>

<p>Please help me in the second query as well?  Below are the 3 events, 101 user id is in two groups whereas 102 is only one group</p>

<pre><code>
{
    userId : 101
    levle : INFO
    timestamp : 2020-06-10
    attributes: {
        privateGroups :  JohnOrg
    }
}
{
    userId : 101
    levle : INFO
    timestamp : 2020-05-09
    attributes: {
        publicGroups :  DistrictOrg
    }
}
{
    userId : 102
    levle : INFO
    timestamp : 2020-05-09
    attributes: {
        publicGroups :  DistrictOrg
    }
}
</code></pre>",62307259.0,1,2,,2020-6-10 06:58:47,,2020-6-11 17:58:57,2020-6-10 15:14:39,,909792.0,,909792.0,,1,0,splunk|splunk-query,153,8
617,259073,62309649,Splunk - To search for concurrent run of processes,"<p>I want to check if there are multiple instances of a job/process running .</p>

<p>Ex: My Splunk search : </p>

<pre><code>index=abc &lt;jobname&gt; |  stats earliest(_time) AS earliest_time, latest(_time) AS latest_time count by  source | convert ctime(earliest_time), ctime(latest_time) | sort - count
</code></pre>

<p>returns :</p>

<pre><code>source   earliest_time       latest_time          count
logA     06/06/2020 15:24:09 06/06/2020 15:24:59      1
logB     06/06/2020 15:24:24 06/06/2020 15:25:12      2
</code></pre>

<p>In the above since logB indicates job run before logA completion time,  it is indication of concurrent run of process. I would like to generate a list of all such jobs if it is possible , any help is appreciated .
Thank you. </p>",62314922.0,1,0,,2020-6-10 17:33:14,,2020-6-10 23:50:27,,,,,6808782.0,,1,0,concurrency|transactions|splunk,160,9
618,259074,62312428,Python: .json data from splunk,"<p>Following problem: 
For my university project I uploaded a json file to splunk and now I want to use this in python as a dataframe object.</p>

<p>Code:</p>

<pre><code>import urllib3
import requests
import json
import pandas as pd

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


server = 'localhost'
port = 8089
username = 'testuid'
password = 'testpw'
url='https://'+ server +':' + str(port)
param = {'shortname', 'permissionId'}

search='?search=search source%3D%22events.json%22%20host%3D%22DESKTOP-9QDQ0FT%22%20index%3D%22projektseminar%22%20sourcetype%3D%22_json%22%20%7Chead%2020%20%7Ctable%20shortname%20permissionId'

output_type =  '&amp;output_mode=json'
search_url = url + '/servicesNS/nobody/search/search/jobs/export' + search + output_type

r = requests.get(search_url, auth=(username, password), verify=False)
</code></pre>

<p>Works well to this point. 
Now I want this specific ""r"" response object as an dataframe object with the 2 columns ""shortname"" and ""permissionId"". There are several problems I have with this.
First of all the json I get from the Rest API is with the columns ""preview"", ""offset"" and ""results"".
I want a dataframe with the columns ""shortname"" and ""permissionId"". 
The problem is I can't use things like <code>json.load(r)</code> or <code>r.json()</code> or similiar, there always comes ""Extra Data"" Error.
So I'm a beginner with splunk and python so maybe there is a better way to do so... 
Another idea I didn't tried yet is to use a csv output instead of json. 
Would be nice if you guys would've some suggestion how to solve this problem.</p>

<p>thx</p>",,1,0,,2020-6-10 20:17:35,,2020-6-11 22:56:11,,,,,11508722.0,,1,0,python|json|csv|splunk,992,12
619,259075,62325686,How to pass or set token when click on image in html panel in splunk?,"<p>I want to set or pass value to the token , after clicking on image in HTML panel</p>

<p>Here is my panel code:</p>

<pre><code>&lt;form script=""my.js""&gt;
&lt;row&gt;
&lt;panel&gt;
&lt;html&gt;
&lt;a id=""mydivId""&gt;
&lt;img src=""/static/app/My_app/bck_city.png""/&gt;
&lt;/a&gt;
&lt;/html&gt;
&lt;/panel&gt;
&lt;/row&gt;
&lt;/form&gt;
</code></pre>

<p>And after clicking on the image of above panel the below token should set with value
(I want to set token=""mytoken"", with some value after clicking on the image above)</p>

<pre><code>&lt;row&gt;
&lt;panel&gt;
&lt;title&gt;mypanel&lt;/title&gt;
&lt;event&gt;
&lt;search&gt;
&lt;query&gt;|makeresults
|eval result=$mytoken$
|table result&lt;/query&gt;
&lt;earliest&gt;-15m&lt;/earliest&gt;
&lt;latest&gt;now&lt;/latest&gt;
&lt;/search&gt;
&lt;option name=""list.drilldown""&gt;none&lt;/option&gt;
&lt;option name=""refresh.display""&gt;progressbar&lt;/option&gt;
&lt;/event&gt;
&lt;/panel&gt;
&lt;/row&gt;
</code></pre>

<p>I also tried setting token value by .js</p>

<pre><code> require(['underscore',
'jquery',
'splunkjs/mvc',
'splunkjs/mvc/utils',
'splunkjs/mvc/tokenutils',
'splunkjs/mvc/simplexml/ready!'], function ($) {
    var utils = require(""splunkjs/mvc/utils"");
    $(document).ready(function () {
    $(""#mydivId"").on(""click"", function (){
    var tokens = mvc.Components.get(""default"");
    var tokenValue = tokens.get(""mytoken"");
    tokens.set(""mytoken"", ""cheese"");
        });
    });
});
</code></pre>

<p><a href=""https://i.stack.imgur.com/S2fvM.png"" rel=""nofollow noreferrer"">enter image description here</a></p>",62330618.0,1,0,,2020-6-11 13:35:54,,2020-6-11 18:16:36,,,,,11428944.0,,1,0,javascript|xml|visualization|splunk|splunk-query,395,11
620,259076,62327904,Kubernetes logs spamming Splunk,"<p>I'm having issues with Kubernetes containers spamming Splunk with hundreds of gigabytes of logs sometimes. I would like to put together a search to track containers that have a sudden log spike and generate an alert. </p>

<p>More specifically:</p>

<ol>
<li>look at the average rate of events </li>
<li>find the peak </li>
<li>decide a percentage of that peak </li>
<li>and then trigger an alert when a container has breached the threshold.</li>
</ol>

<p>The closest I have come up with is the below search, which has an average rate and standard deviation of that rate by hour:</p>

<pre><code>index=""apps"" sourcetype=""kube""
| bucket _time span=1h
| stats count as CountByHour by _time, kubernetes.container_name
| eventstats avg(CountByHour) as AvgByKCN stdev(CountByHour) as StDevByKCN by kubernetes.container_name
</code></pre>",,0,0,,2020-6-11 15:28:47,,2020-6-16 19:35:28,2020-6-16 19:35:28,,3025856.0,,4455463.0,,1,1,splunk-query,53,6
621,259077,62332487,Connect to GKE cluster dynamically using terraform,"<p>I am using Terraform script to spin up a GKE cluster and then use helm 3 to install splunk connector on the cluster. </p>

<p>How do I connect to the newly created cluster in terraform kubernetes provider dynamically ? </p>",62332797.0,1,0,,2020-6-11 19:54:19,,2020-6-11 20:14:53,,,,,11472725.0,,1,0,kubernetes|terraform|google-kubernetes-engine|kubernetes-helm|splunk,109,8
622,259078,62342333,Jenkins Pipeline stage view plugin not showing stage logs,"<p>Below is the environment I'm currently working on,</p>

<p>Jenkins Version : 2.222.1,</p>

<p>Pipeline: Stage View Plugin  : 2.13,</p>

<p>Splunk-devops plugin version: 1.9.3,</p>

<p>Splunk-devops-extend plugin version: 1.9.3.</p>

<p>Issue 1: Jenkins stage view plug-in supposed to show particular stage logs in pop-up window when clicked 'log', but it is not showing anything.
<a href=""https://i.stack.imgur.com/C0e2x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C0e2x.png"" alt=""noStageLogs""></a></p>

<p>Issue 2: Not sure if it because of above issue, jenkins stage's children information is not sent to Splunk. Earlier we were able to see these information.  For example stages{}.children{}.duration.</p>

<p>Any help would be more appreciated. Thanks.</p>",,0,0,,2020-6-12 10:26:55,,2020-6-12 10:26:55,,,,,5550497.0,,1,1,jenkins|jenkins-pipeline|jenkins-plugins|splunk,441,10
623,259079,62355772,Splunk integration with Oozie,"<p>We need to capture Oozie workflow Failure events in Splunk - HTTP Event Collector (Example: <a href=""https://qa.splunk.organization.com/services/collector"" rel=""nofollow noreferrer"">https://qa.splunk.organization.com/services/collector</a>). </p>

<p>To achieve this, we created a separate Oozie Java action to log the failure event to Splunk. The problem with this approach is, we have more than 100 oozie workflows and adding a new workflow action for Splunk is not feasible. </p>

<p>Is there any better approach to capture Oozie workflow failure in Splunk HTTP Event Collector?</p>

<blockquote>
  <p>Workflow.xml
      
      </p>
</blockquote>

<pre><code>    &lt;start to=""Input_Check"" /&gt;
    &lt;decision name=""Input_Check""&gt;
        &lt;switch&gt;
            &lt;case to=""Input_Move""&gt; ${fs:dirSize(source_hdfs_path) gt 0} &lt;/case&gt;
            &lt;default to=""end""/&gt;
        &lt;/switch&gt;
    &lt;/decision&gt;

    &lt;action name=""Input_Move""&gt;
        &lt;java&gt;
            &lt;main-class&gt;com.org.FileMove&lt;/main-class&gt;
            &lt;arg&gt;${source_hdfs_path}/&lt;/arg&gt;
            &lt;arg&gt;${destination_hdfs_path}&lt;/arg&gt;
        &lt;/java&gt;
        &lt;ok to=""Process_File"" /&gt;
        &lt;error to=""Splunk_Log"" /&gt;
    &lt;/action&gt;

    &lt;action name=""Process_File""&gt;
        &lt;java&gt;
            &lt;main-class&gt;com.org.FileProcessor&lt;/main-class&gt;
            &lt;arg&gt;inputPath=${destination_hdfs_path}/&lt;/arg&gt;
            &lt;arg&gt;outputPath=${output_hdfs_path}/&lt;/arg&gt;
        &lt;/java&gt;
        &lt;ok to=""end"" /&gt;
        &lt;error to=""Splunk_Log"" /&gt;
    &lt;/action&gt;

    &lt;action name=""Splunk_Log""&gt;
        &lt;java&gt;
            &lt;main-class&gt;com.org.SplunkLog&lt;/main-class&gt;
            &lt;arg&gt;https://qa.splunk.organization.com/services/collector&lt;/arg&gt;
            &lt;arg&gt;auth-token&lt;/arg&gt;
            &lt;arg&gt;Workflow Failed&lt;/arg&gt;
        &lt;/java&gt;
        &lt;ok to=""fail""/&gt;
        &lt;error to=""fail""/&gt;
    &lt;/action&gt;

    &lt;kill name=""fail""&gt;
        &lt;message&gt;Test Splunk Workflow failed&lt;/message&gt;
    &lt;/kill&gt;
    &lt;end name=""end"" /&gt;
&lt;/workflow-app&gt;
</code></pre>

<blockquote>
  <p>Java Class</p>
</blockquote>

<pre><code>import java.io.*;
import java.net.HttpURLConnection;
import java.net.URL;

public class SplunkLog {

    public static void main(String[] args) throws Exception {

        if(args.length!=3){
            System.exit(-1);
        }

        String URL = args[0];
        String authToken = args[1];
        String data = args[2];

        String jsonData = ""{\""event\"": \"""" + data + ""\"", \""sourcetype\"": \""manual\""}"";

        URL url = new URL (URL);
        HttpURLConnection connection = (HttpURLConnection) url.openConnection();
        connection.setDoOutput(true);
        connection.setDoInput(true);
        connection.setRequestProperty(""Content-Type"", ""application/json; charset=UTF-8"");

        connection.setRequestProperty  (""Authorization"", ""Splunk "" + authToken);
        connection.setRequestMethod(""POST"");
        connection.connect();

        OutputStream outputStream = connection.getOutputStream();
        BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(outputStream, ""UTF-8""));
        writer.write(jsonData);
        writer.close();
        outputStream.close();

        if (connection.getResponseCode()==HttpURLConnection.HTTP_OK) {
            System.out.println(""SUCCESS"");
        }else{
            System.out.println(""data : "" + jsonData + ""token : "" + authToken);
            System.out.println(""ERROR : "" + connection.getResponseMessage());
        }

    }
}
</code></pre>",,0,4,,2020-6-13 05:06:31,0.0,2020-6-13 14:34:14,2020-6-13 14:34:14,,6835509.0,,6835509.0,,1,1,oozie|splunk,120,8
624,259080,62366635,Splunk left jion is not giving as exepcted,"<p>Requirement: I want to find out, payment card information used in a particular day are there any tele sales order placed with the same payment card information.</p>

<p>I tried with below query it is supposed to give me all the payment card information from online orders and matching payment info from telesales.   But i am not giving  correct results basically results shows there are no telesales for payment information, but when i search splunk i am finding telesales as well.  So the query wrong.  </p>

<pre><code>index=""orders"" ""Online order received"" earliest=-9d latest=-8d 
    | rex field=message ""paymentHashed=(?&lt;payHash&gt;.([a-z0-9_\.-]+))"" 
    | rename timestamp as  onlineOrderTime 
    | table payHash, onlineOrderTime 
    | join type=left payHash [search index=""orders""  ""Telesale order received"" earliest=-20d latest=-5m | rex field=message ""paymentHashed=(?&lt;payHash&gt;.([a-z0-9_\.-]+))"" | rename timestamp as TeleSaleTime | table payHash, TeleSaleTime] 
    | table payHash, onlineOrderTime, TeleSaleTime
</code></pre>

<p>Please help me in fixing the query or a query to find out results for my requirement.</p>",62390609.0,3,0,,2020-6-13 23:15:59,,2020-6-15 14:38:00,2020-6-14 19:12:33,,909792.0,,909792.0,,1,0,splunk|splunk-query,1045,10
625,259081,62381716,Splunk interesting field exclusion,"<p>i have 4 fields (<code>Name</code> , <code>age</code>, <code>class</code>, <code>subject</code>) in one index (Student_Entry) and i want to add total events but i want to exclude those events who has any value in subject field.</p>

<p>I tried the below two ways </p>

<pre><code>index=Student_Entry   Subject !=* | stats count by event
index=Student_Entry   NOT Subject= * | stats count by event
</code></pre>",,3,0,,2020-6-15 05:11:12,,2020-6-15 14:39:43,2020-6-15 07:13:49,,5104596.0,,13330893.0,,1,0,splunk|splunk-query|splunk-sdk|splunk-calculation,350,12
626,259082,62391068,Logging k8s kubectl commands related activities by user profiles in Splunk,"<p>Disclaimer: I am neither K8s expert and not K8s Administrator and I have limited knowledge in Splunk logs how to access data using Splunk query. So please ignore if you can't help and DON'T close it without understanding what is the ask and I am happy to clarify. This will help people benefited who is running with same questions ask in future.
+++++++++++++++++++++++++++++++++++</p>

<p>We are using K8s on-Prem and there are tons of namespaces and users access pretty much every namespaces. Somebody accidentally can issue kubectl delete command to delete anything , it could be pod / service , roles or cluster. My objective in this thread is , is there anyway we can trace who is running every kubectl operations ?</p>

<p>I found below link which can help auditing k8s: <a href=""https://kubernetes.io/docs/tasks/debug-application-cluster/audit/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/tasks/debug-application-cluster/audit/</a></p>

<p>If Audit is enabled in k8s how can we trace that who has executed every kubectl command operation ? as I said I am neither k8s admin but want to know if there is clear path and way to trace this in logs from k8s back to Splunk ?</p>

<p>Our K8s Admin said audit has been setup already but kubectl command with user details are NOT flowing from Rancher / Fluentd to Splunk . Do we need any specific configuration to turned it on ? which K8s Admin needs to set . Any help would be appreciated .</p>

<p>thanks
N.B: - this is open thread from closed one.</p>",,1,1,,2020-6-15 14:59:21,,2020-6-26 19:08:34,,,,,5129186.0,,1,1,kubernetes|kubectl|splunk|fluentd|rancher,125,8
627,259083,62420621,Splunk 7.2.9.1 Universal forwarder on SUSE Linux12.4 not communicating and forwarding logs to Indexer after certain period of time,"<p>I have noticed Splunk 7.2.9.1 Universal forwarder on SUSE Linux12.4 is not communicating to deployment server and forwarding logs to indexer after certain period of time. ""splunkd"" process appears to be running while this issue persists. 
I have to restart UFW for it to resume communication to deployment and forward logs. But this will again stop communication after certain period of time.</p>

<p>I cannot see any specific logs in splunkd.log while this issue occurs.
However, i noticed below message from watchdog.log</p>

<p>06-16-2020 11:51:09.055 +0200 ERROR Watchdog - No response received from IMonitoredThread=0x7f24365fdcd0 within 8000 ms. Looks like thread name='Shutdown' is busy !? Starting to trace with 8000 ms interval.</p>

<p>Can somebody help to understand what is causing this issue.</p>",,1,0,,2020-6-17 02:41:52,,2020-6-17 11:36:53,,,,,13720773.0,,1,0,splunk|suse,476,10
628,259084,62448319,How to set a flag in splunk using lookup,"<p>I am trying to achieve below logic
trying to set a flag called &lt;<strong>adminuser</strong>>  the current user ID is present in the lookup (in the lookup 4 ID is there AAP1 APP2 AAP3)
if adminuser is False, then filter where Requestor in the event is  else do not filter
Only the 4 ids can see the user details. and rest can see only there request.</p>

<p>My XMl code is</p>

<p>index=* sourcetype=""testapp"" |eval split=split(Requestor, ""@""), Requestor=mvindex(split, 0)
| eval ""Requested Date"" = strftime(_time,""%Y-%m-%d %H:%M:%S"")
| Get current user ID = (| rest /services/authentication/current-context splunk_server=local | rename username as Requestor |eval split=split(Requestor, ""@""), Requestor=mvindex(split, 0))
| want to use flag  if current user ID is present in lookup
| if adminuser is False, then filter where Requestor in event is  else do not filter
|table ""Requested Date"" ""ID"" ""Requestor"" ""MD"" ""SM""  ""SL""  Status</p>",,1,0,,2020-6-18 10:53:23,,2020-6-18 23:35:51,,,,,10403194.0,,1,0,lookup|splunk|splunk-query,89,7
629,259085,62461168,rabbitmq integration to Splunk,"<p>Recently I installed Rabbit MQ in Centos8 for my company. We also using Splunk Enterprise so we wants to integrate our Rabbit MQ to Splunk and we wants to see, search, check our logs which is coming from Rabbit MQ to in Splunk . How can I do that I don't know. I google it but I didn't get info about it. May anybody help to me for this goal ? Thank you</p>",,2,0,,2020-6-18 23:50:44,,2020-6-19 14:26:23,,,,,11892800.0,,1,0,rabbitmq|splunk,614,11
630,259086,62473520,Create search query in Splunk through API (python code),"<p>I am trying to do a search query in splunk through an API. I found a reference code on <a href=""https://docs.splunk.com/Documentation/Splunk/8.0.4/RESTTUT/RESTsearches"" rel=""nofollow noreferrer"">https://docs.splunk.com/Documentation/Splunk/8.0.4/RESTTUT/RESTsearches</a>. I tried running the code on jupyter notebook. </p>

<p>But it gives a Connection refused error. </p>

<p>Code:</p>

<pre><code>    from __future__ import print_function
    from future import standard_library
    standard_library.install_aliases()
    import urllib.request, urllib.parse, urllib.error
    import httplib2
    from xml.dom import minidom

    baseurl = 'https://localhost:8089'
    userName = 'username'
    password = 'password'

    searchQuery = '| inputcsv foo.csv | where sourcetype=access_common | head 5'

    # Authenticate with server.
    # Disable SSL cert validation. Splunk certs are self-signed.
    serverContent = httplib2.Http(disable_ssl_certificate_validation=True).request(baseurl +                         '/services/auth/login','POST', headers={}, body=urllib.parse.urlencode({'username':userName,         'password':password}))[1]

    sessionKey = minidom.parseString(serverContent).getElementsByTagName('sessionKey')        [0].childNodes[0].nodeValue

    # Remove leading and trailing whitespace from the search
    searchQuery = searchQuery.strip()

    # If the query doesn't already start with the 'search' operator or another
    # generating command (e.g. ""| inputcsv""), then prepend ""search "" to it.
    if not (searchQuery.startswith('search') or searchQuery.startswith(""|"")):
searchQuery = 'search ' + searchQuery

            print(searchQuery)

    # Run the search.
    # Again, disable SSL cert validation.
    print(httplib2.Http(disable_ssl_certificate_validation=True).request(baseurl +         '/services/search/jobs','POST',
        headers={'Authorization': 'Splunk %s' % sessionKey},body=urllib.parse.urlencode({'search': searchQuery}))[1]) '''
</code></pre>

<p>Error:</p>

<pre><code>    ConnectionRefusedError: [Errno 111] Connection refused
</code></pre>

<p>Any help would be highly appreciated. </p>",,1,0,,2020-6-19 15:26:34,,2020-6-23 11:29:02,,,,,10779137.0,,1,0,python|api|splunk|connection-refused,490,10
631,259087,62506677,"Splunk: Escaping ""<"" "">"" from the dashboard's source code","<p>I am trying to put below search query as base search in the dashboard's source code.Getting &quot;unexpected close tag&quot; error because of &quot;&gt;&quot; and &quot;&lt;&quot; which encloses new field name &quot;Env&quot; extracted from rex.</p>
<pre><code>&lt;search id=&quot;base_search&quot;&gt;
&lt;query&gt;index=_internal earliest=-1d latest=now | rex field=host &quot;(?P&lt;Env&gt;[[:alpha:]]{2})\-[[:alpha:]]+&quot; &lt;/query&gt;
&lt;/search&gt;
</code></pre>
<p>Using backslash is not fixing.Can someone help me out here.</p>",62507872.0,2,1,,2020-6-22 02:38:48,,2020-6-23 12:50:47,,,,,4744396.0,,1,0,regex|splunk|splunk-query,598,12
632,259088,62560099,regex match between characters,"<p>the strings are like</p>
<pre><code>/fruit] 

/animal/lion

/plant/flower/rose
</code></pre>
<p>and i want to get only first word without slash and blacket which means fruit, animal, plant.</p>
<p>I did until <code>[^\/]*[a-z] *</code>
but i have no idea what is next step. can somebody help me?</p>
<p>Thanks</p>",,2,3,,2020-6-24 16:46:25,,2020-6-29 19:00:54,2020-6-29 19:00:54,,4418.0,,12960606.0,,1,1,regex|splunk,70,8
633,259089,62588370,How do i set a token based on the dropdown options in splunk dashboard,"<p>i am trying to create a Splunk dashboard, where I want to set a value to a token based on the two dropdown values(service dropdown and environment dropdown)</p>
<pre><code>&lt;input type=&quot;dropdown&quot; token=&quot;service&quot; searchWhenChanged=&quot;true&quot;&gt;
      &lt;label&gt;service&lt;/label&gt;
      &lt;choice value=&quot;capi&quot;&gt;capi&lt;/choice&gt;
      &lt;choice value=&quot;crapi&quot;&gt;crapi&lt;/choice&gt;
      &lt;choice value=&quot;oapi&quot;&gt;oapi&lt;/choice&gt;
      &lt;default&gt;capi&lt;/default&gt;
      &lt;initialValue&gt;capi&lt;/initialValue&gt;
  &lt;/input&gt;
 
  &lt;input type=&quot;dropdown&quot; token=&quot;environment&quot; searchWhenChanged=&quot;true&quot;&gt;
  &lt;label&gt;Environment&lt;/label&gt;
  &lt;choice value=&quot;prod&quot;&gt;prod&lt;/choice&gt;
  &lt;choice value=&quot;ppe&quot;&gt;ppe&lt;/choice&gt;
  &lt;choice value=&quot;pte&quot;&gt;pte&lt;/choice&gt;
  &lt;choice value=&quot;dev&quot;&gt;dev&lt;/choice&gt;
  &lt;default&gt;prod&lt;/default&gt;
  &lt;initialValue&gt;prod&lt;/initialValue&gt;
  &lt;/input&gt;
</code></pre>
<p>above are the 2 dropdowns, now i want to set a value to token &quot;endpoint&quot; based on value selected in service and environment dropdown values.</p>
<p>i tried using condition match, but i am not getting it right</p>
<pre><code>&lt;condition match=&quot;$service$==capi AND $environment$==ppe&quot;&gt;
&lt;set token = endpoint&gt;&quot;/capi/ppe&quot;&lt;/set&gt;
&lt;/condition&gt;
</code></pre>",,1,0,,2020-6-26 05:09:21,,2021-2-23 16:13:26,2021-2-23 16:13:26,,4418.0,,5256836.0,,1,0,splunk|splunk-dashboard,740,11
634,259090,62588765,How can i set config.node in webpack encore,"<p>I'm using Symfony 4 with webpack encore.
I installed the splunk-sdk via npm install splunk-sdk.
now encore dev --watch throw this error</p>
<pre><code>Running webpack ...


webpack is watching the files…

 ERROR  Failed to compile with 2 errors                                                                                                                                      7:04:33

This dependency was not found:

* fs in ./node_modules/request/lib/har.js, ./node_modules/splunk-sdk/lib/utils.js

To install it, you can run: npm install --save fs
</code></pre>
<p>I install fs, but the same error remains.</p>
<p>I read to insert folowing config in my webpack.config.js</p>
<pre><code>config.node = {
        fs: 'empty'
    };
</code></pre>
<p>How can i do this with Encore?</p>",,1,0,,2020-6-26 05:48:51,,2020-8-11 10:07:28,,,,,13151171.0,,1,0,symfony|webpack|symfony4|splunk|webpack-encore,328,12
635,259091,62591828,curl -u to javascript fetch(),"<p>I have this curl command</p>
<pre><code>curl -k -u username:password https://HOST:8089/services/search/jobs
</code></pre>
<p>I want get the data via Javascript.</p>
<p>Is fetch() the right method?</p>
<p>How to i convert the curl command into the fetch method?</p>",,1,1,,2020-6-26 09:27:53,,2020-6-26 10:01:43,,,,,13151171.0,,1,0,javascript|curl|fetch|splunk,154,9
636,259092,62599535,Splunk - Extract multiple values not equaling a value from a string,"<p>In Splunk I'm trying to extract multiple parameters and values that do not equal a specific word from a string. For example:</p>
<p>Anything in this field that does not equal &quot;negative&quot;, extract the parameter and value:</p>
<p>Field:</p>
<pre><code>field={New A=POSITIVE, New B=NEGATIVE, New C=POSITIVE, New D=BAD}
</code></pre>
<p>Result:</p>
<pre><code>New A=POSITIVE
New C=POSITIVE
New D=BAD
</code></pre>",,2,0,,2020-6-26 17:02:12,,2020-6-29 03:32:11,,,,,6146494.0,,1,0,splunk,321,10
637,259093,62602450,Splunk query: retrieve top 5 previous _raw events for each matching search event,"<p>I am trying to retrieve the top 5 previous _raw events followed by a matching search in Splunk.</p>
<p>Let's say, following is my search query and the output:</p>
<p>Query:</p>
<pre><code>index=my_index &quot;unhandled error&quot;
</code></pre>
<p>Result:</p>
<pre><code>_time1 header: unhandled error
_time2 header: unhandled error
_time3 header: unhandled error
</code></pre>
<p>Expectation:</p>
<pre><code>_time  header: interested_logs1
_time  header: interested_logs2  
_time  header: interested_logs3
_time  header: interested_logs4
_time  header: interested_logs5
_time1 header: unhandled error
_time  header: interested_logs1
_time  header: interested_logs2  
_time  header: interested_logs3
_time  header: interested_logs4
_time  header: interested_logs5
_time2 header: unhandled error
_time  header: interested_logs1
_time  header: interested_logs2  
_time  header: interested_logs3
_time  header: interested_logs4
_time  header: interested_logs5
_time3 header: unhandled error
</code></pre>
<p>In between those filtered logs, I have lots of unwanted logs plus few interested logs on the top as follows, without any filter:</p>
<pre><code>......lots of unwanted logs....
_time  header: interested_logs1
_time  header: interested_logs2  
_time  header: interested_logs3
_time  header: interested_logs4
_time  header: interested_logs5
_time1 header: unhandled error
    ......lots of unwanted logs....
_time  header: interested_logs1
_time  header: interested_logs2  
_time  header: interested_logs3
_time  header: interested_logs4
_time  header: interested_logs5
_time2 header: unhandled error
    ......lots of unwanted logs....
_time  header: interested_logs1
_time  header: interested_logs2  
_time  header: interested_logs3
_time  header: interested_logs4
_time  header: interested_logs5
_time3 header: unhandled error
</code></pre>
<p>Note: there is no correlation between unhandled error and interested logs. I just wanted to grab the reason for the unhandled error which are the top 5 _raw logs on the top.</p>
<p>The query should give for 3 <code>unhandled error</code> 3*5=15 matches or lines.</p>
<p>Thank you. Any suggestions are appreciated!</p>",,1,0,,2020-6-26 20:33:09,1.0,2020-6-29 04:10:31,2020-6-26 20:50:27,,6004598.0,,6004598.0,,1,0,splunk,211,10
638,259094,62621541,Is there a way to send Data from Splunk to Salesforce?,"<p>I need some help and guidance related to sending data from SPLUNK to Salesforce.</p>
<p>Basically, I am trying to extract few features and information from Splunk dashboard and I need to send that data to salesforce while creating a case. Such that the case is created with all those details automatically with the single click of the submit button from SPLUNK dashboard.</p>
<p>I read/tried the SPLUNK add on for Salesforce, however, this is more about pulling the salesforce data to SPLUNK but not about pushing the data from SPlUNK to salesforce.</p>
<p>Could you please guide me, how can we achieve this?</p>",,1,0,,2020-6-28 10:56:19,,2020-6-29 03:11:23,,,,,11925396.0,,1,0,splunk|salesforce-lightning,146,8
639,259095,62670458,json object raw data field into php array,"<p>I have a JSON object data extract from splunk, one of the fields <code>_raw</code> contains a string of data but since it's value is not a valid json string I can't seem to convert it to a php array using <strong>json_decode()</strong></p>
<p>Any ideas how I might be able to convert the value of <code>_raw</code> into a php array?</p>
<p>This is my json data</p>
<pre><code>{
    &quot;result&quot;: {
        &quot;_raw&quot;: &quot;Jun 30 06:51:04 blablabla.com apache2: event_id=&quot;something&quot; event=&quot;something&quot; app=&quot;testapp&quot; serial_number=&quot;066f1cda&quot; revoke_reason=&quot;key compromise&quot; revoke_comment=&quot;xxxxxx&quot; delay_revoke=&quot;15&quot; url=&quot;/blablabla.php&quot; account_id=&quot;123456&quot; user_id=&quot;xxxxxxx&quot; staff_id=&quot;xxxx&quot; staff_name=&quot;todd&quot; ip_address=&quot;123.123.123.123&quot;&quot;,
        &quot;splunk_server&quot;: &quot;splunkin02.localhost.com&quot;
    },
    &quot;results_link&quot;: &quot;https://splunk.sffew.com&quot;,
    &quot;app&quot;: &quot;search&quot;,
    &quot;search_name&quot;: &quot;TEST&quot;,
    &quot;owner&quot;: &quot;toodles@sfsfe.com&quot;,
    &quot;sid&quot;: &quot;scheduler_blahlahlah&quot;
}
</code></pre>
<p>Want to be able to reference each value within the string, like so</p>
<pre><code>Array (
     [event_id] =&gt; something
     [event] =&gt; something
     [app] =&gt; testapp
     [serial_number] =&gt; 066f1cda
     [revoke_reason] =&gt; key compromise
     [revoke_comment] =&gt; xxxxxx
     [delay_revoke] =&gt; 15
     [url] =&gt; /blablabla.php
     [account_id] =&gt; 123456
     [user_id] =&gt; xxxxxxx
     [staff_id] =&gt; xxxx
     [staff_name] =&gt; todd
     [ip_address] =&gt; 123.123.123.123
)
</code></pre>",62670688.0,2,2,,2020-7-1 05:49:32,,2020-7-1 06:31:09,2020-7-1 06:16:53,,283366.0,,3436467.0,,1,1,php|json|splunk,207,12
640,259096,62683868,How to check if the multi-value field contains the value of the other field in Splunk,"<p>I need to set the field value according to the existence of another event field (e.g. a field) in a multivalued field of the same event (e.g. mv_field)</p>
<p>Here is an example query, which doesn't work as I expected, because the ext_field always has the value &quot;value_if_true&quot;</p>
<pre><code>| ...
| eval ext_field = if(in(mv_field, field), &quot;value_if_true&quot;, &quot;value_if_false&quot;)
| ...
</code></pre>
<p>Could You please, tell me what am I doing wrong?</p>
<p>Thanks!</p>",62685525.0,1,0,,2020-7-1 19:12:05,,2021-11-26 04:45:12,,,,,13180241.0,,1,1,splunk|splunk-query,1675,15
641,259097,62691991,Splunk spath vs plain search performance,"<p>Assuming that I have json logs formatted like</p>
<pre><code>{
    level: INFO,
    logger: com.mantkowicz.test.TestLogger,
    message: Just a simple test log message
}
</code></pre>
<p>what is the difference between such two searches:</p>
<pre><code>A) ... | message = &quot;Just a simple test log message&quot;
B) ... | spath message | search message = &quot;Just a simple test log message&quot;
</code></pre>
<p>Is there any performance drawback? Should I prefer one of these?</p>",,2,0,,2020-7-2 08:18:09,1.0,2020-7-2 13:50:11,,,,,4153426.0,,1,1,json|logging|splunk,589,13
642,259098,62694385,Ingest logs into Splunk through AWS SNS,"<p>I'm planing to ingest my <code>Trend Micro Deep Security Agent</code> events into Splunk and only the possible option is forwarding events to AWS SNS topic. My question is, what is the recommended way to do this there after getting the events to SNS? As I could see, I need to create subscription in my <code>SNS topic</code> to send events to <code>AWS SQS</code> and then use the <code>Splunk Add-on for AWS</code> to pull the events. Are there anyone else have better idea on this or any other recommendation?</p>",,1,0,,2020-7-2 10:35:58,,2020-7-2 11:15:46,,,,,6482719.0,,1,0,amazon-sns|splunk|splunk-query,322,10
643,259099,62705376,Search using Lookup from a single field CSV file,"<p>I have a list of usernames that I have to monitor and the list is growing every day. I read Splunk documentation and it seems like lookup is the best way to handle this situation.</p>
<p>The goal is for my query to leverage the lookup function and prints out all the download events from all these users in the list.</p>
<p>Sample logs</p>
<pre><code>index=proxy123 activity=&quot;download&quot;

{
&quot;machine&quot;:&quot;1.1.1.1&quot;,
&quot;username&quot;:&quot;ABC@xyz.com&quot;,
&quot;activity&quot;:&quot;download&quot;
}

{
&quot;machine&quot;:&quot;2.2.2.2&quot;,
&quot;username&quot;:&quot;ASDF@xyz.com&quot;,
&quot;activity&quot;:&quot;download&quot;
}

{
&quot;machine&quot;:&quot;3.3.3.3&quot;,
&quot;username&quot;:&quot;GGG@xyz.com&quot;,
&quot;activity&quot;:&quot;download&quot;
}
</code></pre>
<p>Sample Lookup (username.csv)</p>
<pre><code>users
ABC@xyz.com
ASDF@xyz.com
BBB@xyz.com
</code></pre>
<p>Current query:</p>
<pre><code>index=proxy123 activity=&quot;download&quot; | lookup username.csv users OUTPUT users | where not isnull(users)
</code></pre>
<p>Result: 0 (which is not correct)</p>
<p>I probably don't understand lookup correctly. Can someone correct me and teach me the correct way?</p>",62706644.0,1,0,,2020-7-2 21:34:59,,2020-7-3 00:10:21,,,,,6945824.0,,1,0,splunk|splunk-query,466,13
644,259100,62711382,bandwidth in splunk Enterprise,"<p>I want to calculate the bandwidth beetwen a sender and a reciver in real time using Splunk Enterprise.</p>
<p>I would really really appreciate some help with this. I have searched previous questions, but can't seem to find the answer.</p>
<p>Kind Regards</p>",,1,0,,2020-7-3 08:23:49,,2020-7-6 00:43:16,,,,,13858202.0,,1,0,splunk|bandwidth,39,7
645,259101,62736562,Best method to keep lookup file value fresh,"<p>Say, I have to monitor users' activities from 3 specific departments: Science, History, and Math.</p>
<p>The goal is to send an alert if any of the users in any of those departments download a file from site XYZ.</p>
<p>Currently, I have a lookup file for all the users from those three departments.</p>
<pre><code>users
----------------------
user1@organization.edu
user2@organization.edu
user3@organization.edu
user4@organization.edu
user5@organization.edu
</code></pre>
<p>One problem: users can join, leave, or transfer to another department anytime.</p>
<p>Fortunately, those activities (join and leave) are tracked and they are Splunk-able.</p>
<pre><code>index=directory status=*
-----------------------------------------------
{
&quot;username&quot;:&quot;user1@organization.edu&quot;,
&quot;department&quot;:&quot;Science&quot;,
&quot;status&quot;:&quot;added&quot;
}
{
&quot;username&quot;:&quot;user1@organization.edu&quot;,
&quot;department&quot;:&quot;Science&quot;,
&quot;status&quot;:&quot;removed&quot;
}
{
&quot;username&quot;:&quot;user2@organization.edu&quot;,
&quot;department&quot;:&quot;History&quot;,
&quot;status&quot;:&quot;added&quot;
}
{
&quot;username&quot;:&quot;user3@organization.edu&quot;,
&quot;department&quot;:&quot;Math&quot;,
&quot;status&quot;:&quot;added&quot;
}
{
&quot;username&quot;:&quot;MRROBOT@organization.edu&quot;,
&quot;department&quot;:&quot;Math&quot;,
&quot;status&quot;:&quot;added&quot;
}
</code></pre>
<p>In this example, assuming I forgot to update the lookup file, I won't get an alert when MRROBOT@organization.edu downloads a file, and at the same time, I will still get an alert when user1@organization.edu downloads a file.</p>
<p>One solution that I could think of is to update the lookup manually via using inputlookup and outputlook method like:</p>
<pre><code>inputlookup users.csv | users!=user1@organization.edu | outputlookup users.csv
</code></pre>
<p>But, I don't think this is an efficient method, especially there's high likely I might miss a user or two.</p>
<p>Is there a better way to keep the lookup file up-to-date? I googled around, and one suggestion is to use a cronjob CURL to update the list. But, I was wondering if there's a simpler or better alternative than that.</p>",62741547.0,1,0,,2020-7-5 03:38:18,,2020-7-5 13:48:15,,,,,6945824.0,,1,0,splunk|splunk-query,103,9
646,259102,62750320,How to search splunk query which includes double quotes in the string to search,"<p>I am trying to search for a pattern(see below) in the logs using splunk. The String which I am going to search includes double quotes.</p>
<p>Below info log is printed in the logger..</p>
<pre><code>INFO: o.l.k.SomeClass: {&quot;function&quot;: &quot;delete&quot;, &quot;tenenId&quot;:&quot;15897&quot;,.......}
</code></pre>
<p>And the string i want to search is
<strong>&quot;function&quot;: &quot;delete&quot;</strong></p>
<p>The splunk query I am trying to execute is.,</p>
<pre><code>index=&quot;12585&quot; &quot;\&quot;function\&quot;: \&quot;delete\&quot;&quot;
</code></pre>
<p>I am not quite sure if this is going to work. Any suggestions?</p>",,3,0,,2020-7-6 06:22:25,,2020-10-29 00:52:06,,,,,3696393.0,,1,2,splunk|splunk-query,1197,13
647,259103,62782910,How to calculate duration between logs in Datadog?,"<p>Splunk has <code>transaction</code> command which can produce <code>duration</code> between logs grouped by id:</p>
<pre><code>2020-01-01 12:12 event=START id=1
2020-01-01 12:13 event=STOP  id=1
</code></pre>
<p>as it is decribed on</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/61358636/query-for-calculating-duration-between-two-different-logs-in-splunk"">Query for calculating duration between two different logs in Splunk</a></li>
<li><a href=""https://stackoverflow.com/questions/45551991/splunk-duration-between-two-different-messages-by-guid?rq=1"">Splunk - duration between two different messages by guid</a></li>
<li><a href=""https://community.splunk.com/t5/Splunk-Search/transaction-time-between-events/td-p/50279"" rel=""nofollow noreferrer"">transaction time between events</a></li>
</ul>
<p>How to calculate duration between events in Datadog?</p>",,0,0,,2020-7-7 19:52:50,,2020-7-7 23:11:06,2020-7-7 23:11:06,,2227420.0,,5962766.0,,1,4,logging|monitoring|datadog|splunk-query,453,10
648,259104,62791791,ansible loop over list,"<p>I have an inventory (this can be changed):</p>
<pre><code>index:
  - indexName: &quot;AAA&quot;
    homePath: &quot;$SPLUNK_DB/AAA/db&quot;
    coldPath: &quot;$SPLUNK_DB/AAA/colddb&quot;
    thawedPath: &quot;$SPLUNK_DB/AAA/thaweddb&quot;
    repFactor: &quot;auto&quot;
  - indexName: &quot;BBB&quot;
    homePath: &quot;$SPLUNK_DB/BBB/db&quot;
    coldPath: &quot;$SPLUNK_DB/BBB/colddb&quot;
    thawedPath: &quot;$SPLUNK_DB/BBB/thaweddb&quot;
    repFactor: &quot;auto&quot;
</code></pre>
<p>I want to loop over the indexes, but also want to use key value. like this:</p>
<pre><code>- name: Write paths for the index
  ini_file:
    dest: &quot;{{ splunk.home }}/etc/master-apps/_cluster/local/indexes.conf&quot;
    section: &quot;{{ index.indexName }}&quot;
    option: &quot;{{ item.key }}&quot;
    value: &quot;{{ item.value }}&quot;
  with_items:
    - { key: &quot;homePath&quot;, value: &quot;{{ index.homePath | default('', true) }}&quot; }
    - { key: &quot;thawedPath&quot;, value: &quot;{{ index.thawedPath | default('', true) }}&quot; }
    - { key: &quot;coldPath&quot;, value: &quot;{{ index.coldPath | default('', true) }}&quot; }
    - { key: &quot;repFactor&quot;, value: &quot;{{ index.repFactor | default('', true) }}&quot; }
</code></pre>
<p>Is this possible?</p>",62792833.0,2,0,,2020-7-8 09:35:01,,2020-7-8 14:11:39,,,,,3511320.0,,1,1,ansible|splunk,84,9
649,259105,62794444,Splunk query output formating to JSON format,"<p>I have ingested some logs to Splunk which now looks like below when searching from search header.</p>
<p><code>{\&quot;EventID\&quot;:563662,\&quot;EventType\&quot;:\&quot;LogInspectionEvent\&quot;,\&quot;HostAgentGUID\&quot;:\&quot;11111111CE-7802-1111111-9E74-BD25B707865E\&quot;,\&quot;HostAgentVersion\&quot;:\&quot;12.0.0.967\&quot;,\&quot;HostAssetValue\&quot;:1,\&quot;HostCloudType\&quot;:\&quot;amazon\&quot;,\&quot;HostGUID\&quot;:\&quot;1111111-08CF-4541-01333-11901F731111109\&quot;,\&quot;HostGroupID\&quot;:71,\&quot;HostGroupName\&quot;:\&quot;private_subnet_ap-southeast-1a (subnet-03160)\&quot;,\&quot;HostID\&quot;:85,\&quot;HostInstanceID\&quot;:\&quot;i-0665c\&quot;,\&quot;HostLastIPUsed\&quot;:\&quot;192.168.43.1\&quot;,\&quot;HostOS\&quot;:\&quot;Ubuntu Linux 18 (64 bit) (4.15.0-1051-aws)\&quot;,\&quot;HostOwnerID\&quot;:\&quot;1111112411\&quot;,\&quot;HostSecurityPolicyID\&quot;:1,\&quot;HostSecurityPolicyName\&quot;:\&quot;Base Policy\&quot;,\&quot;Hostname\&quot;:\&quot;ec2-11-11-51-45.ap-southeast-3.compute.amazonaws.com (ls-ec2-as1-1b-datalos) [i-f661111148a3f6]\&quot;,\&quot;LogDate\&quot;:\&quot;2020-07-08T11:52:38.000Z\&quot;,\&quot;OSSEC_Action\&quot;:\&quot;\&quot;,\&quot;OSSEC_Command\&quot;:\&quot;\&quot;,\&quot;OSSEC_Data\&quot;:\&quot;\&quot;,\&quot;OSSEC_Description\&quot;:\&quot;Non standard syslog message (size too large)\&quot;,\&quot;OSSEC_DestinationIP\&quot;:\&quot;\&quot;,\&quot;OSSEC_DestinationPort\&quot;:\&quot;\&quot;,\&quot;OSSEC_DestinationUser\&quot;:\&quot;\&quot;,\&quot;OSSEC_FullLog\&quot;:\&quot;Jul  8 11:52:37 ip-172-96-50-2 amazon-ssm-agent.amazon-ssm-agent[24969]:   \\\&quot;Document\\\&quot;: \\\&quot;{\\\\n    \\\\\\\&quot;schemaVersion\\\\\\\&quot;: \\\\\\\&quot;2.0\\\\\\\&quot;,\\\\n    \\\\\\\&quot;description\\\\\\\&quot;: \\\\\\\&quot;Software Inventory Policy Document.\\\\\\\&quot;,\\\\n    \\\\\\\&quot;parameters\\\\\\\&quot;: {\\\\n        \\\\\\\&quot;applications\\\\\\\&quot;: {\\\\n            \\\\\\\&quot;type\\\\\\\&quot;: \\\\\\\&quot;String\\\\\\\&quot;,\\\\n            \\\\\\\&quot;default\\\\\\\&quot;: \\\\\\\&quot;Enabled\\\\\\\&quot;,\\\\n            \\\\\\\&quot;description\\\\\\\&quot;: \\\\\\\&quot;(Optional) Collect data for installed applications.\\\\\\\&quot;,\\\\n            \\\\\\\&quot;allowedValues\\\\\\\&quot;: [\\\\n                \\\\\\\&quot;Enabled\\\\\\\&quot;,\\\\n     </code></p>
<p>How can I format this correctly to show in JSON format when searing in searcher header. I'm pretty new to Splunk, hence have less idea on this.</p>
<p>My <code>file_monitor</code> &gt; <code>props.conf</code> looks like below</p>
<pre><code>[myapp:data:events]
pulldown_type=true
INDEXED_EXTRACTIONS= json
KV_MODE=none
category=Structured
description=data
disabled=false
TRUNCATE=88888
</code></pre>",,0,5,,2020-7-8 12:05:42,,2020-7-9 13:21:57,2020-7-9 13:21:57,,4418.0,,6482719.0,,1,0,json|splunk|splunk-query,582,11
650,259106,62849260,Splunk Host header overrides host key from log messages,"<p>How can I stop Splunk considering hostname &quot;host&quot; more important than &quot;host&quot; key?</p>
<p>Let's suppose that I have the following logs:</p>
<blockquote>
<p>color = red ; host = localhost<br />
color = blue ; host = newhost</p>
</blockquote>
<p>The following query works fine:</p>
<pre><code>index=myindex | stats count by color
</code></pre>
<p>but the following doesn't:</p>
<pre><code>index=myindex | stats count by host
</code></pre>
<p>because instead of considering &quot;host&quot; being the key from the log, it sees the <strong>Host header</strong> as &quot;host&quot;.</p>
<p>How can I deal with this?</p>",,1,0,,2020-7-11 12:38:57,1.0,2020-7-12 15:39:29,,,,,3885376.0,,1,0,splunk|splunk-query|hostheaders,51,8
651,259107,62929823,Splunk Dashboard can be built using which of the following external component ? a)HTML b)CSS 3)XML 4)JavaScript,"<p>Splunk Dashboard can be built using which of the following external component ?
a)HTML
b)CSS
3)XML
4)JavaScript</p>",,1,1,,2020-7-16 07:41:24,,2021-2-23 16:12:29,2021-2-23 16:12:29,,4418.0,,13940477.0,,1,-2,splunk|splunk-dashboard,1505,13
652,259108,62939087,"How to connect to Splunk API via Python, receiving javascript error","<p>I am trying to connect to Splunk via API using python. I can connect, and get a 200 status code but when I read the content, it doesn't read the content of the page. View below:</p>
<p><a href=""https://i.stack.imgur.com/0E597.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0E597.png"" alt=""enter image description here"" /></a></p>
<p>Here is my code:</p>
<pre><code>import json
import requests
import re

baseurl = 'https://my_splunk_url:8888'
username = 'my_username'
password = 'my_password'

headers={&quot;Content-Type&quot;: &quot;application/json&quot;}

s = requests.Session()
s.proxies = {&quot;http&quot;: &quot;my_proxy&quot;}

r = s.get(baseurl, auth=(username, password), verify=False, headers=None, data=None)

print(r.status_code)
print(r.text)
</code></pre>
<p>I am new to Splunk and python so any ideas or suggestions as to why this is happening would help.</p>",,2,1,,2020-7-16 16:19:29,1.0,2021-3-15 23:48:04,2020-7-17 03:27:04,,8075540.0,,10405409.0,,1,1,python|json|python-requests|splunk|splunk-sdk,1658,13
653,259109,63003181,last 4 month/week data,"<p>How can we get timechart for last 4 month i have tried the below query but its giving me fixed last 4 month data like only for MAR,APR,MAY,JUN .. how can i get for AUG SEP OCT and NOV ..</p>
<p>PFB tried query..</p>
<pre><code>index=foo earliest=-1mon@mon latest=-0mon@mon Technology=&quot;Sourcefire&quot; 
| timechart span=1day count AS JUN-2020 
| appendcols [search index=SI earliest=-2mon@mon latest=-1mon@mon Technology=&quot;Sourcefire&quot; 
  | timechart span=1day count AS MAY-2020] 
| appendcols [search index=SI earliest=-3mon@mon latest=-2mon@mon Technology=&quot;Sourcefire&quot; 
  | timechart span=1day count AS APR-2020] 
| appendcols [search index=SI earliest=-4mon@mon latest=-3mon@mon Technology=&quot;Sourcefire&quot; 
  | timechart span=1day count AS MAR-2020] 
| table _time JUN-2020 MAY-2020 APR-2020 MAR-2020
</code></pre>
<p>also can you please help to get last 4 week data ..
i tried below which is not working..</p>
<pre><code>index=Foo earliest=-1w@w1 latest=-0w@w1 
| timechart span=1hour count by  RuleAction  
| appendcols [search index=FOO_1 | timechart span=1hour count by blocked ]
  appendcols [search index=Foo earliest=-2w@w1 latest=-1w@w1 
| timechart span=1hour count by  RuleAction 
| appendcols [search index=FOO_1  
| timechart span=1hour count by blocked ]
 appendcols [search index=Foo earliest=-3w@w1 latest=-2w@w1 
  | timechart span=1hour count by  RuleAction 
  | appendcols [search index=FOO_1  
  | timechart span=1hour count by blocked ]
 appendcols [search index=FOO earliest=-4w@w1 latest=-3w@w1 
| timechart span=1hour count by  RuleAction 
| appendcols [search index=ngss*_sourcefire_seceventFOO_1 
  | timechart span=1hour count by blocked ]
</code></pre>",,1,0,,2020-7-20 20:17:21,,2020-7-20 22:07:47,2020-7-20 20:25:50,,14419.0,,13330893.0,,1,1,splunk|splunk-formula,137,9
654,259110,63008679,Splunk MongoDB addon: do we need root priviledge to monitoring MongoDB data?,"<p>Experts：</p>
<p>If I am going to monitor following data /1/, we are using Splunk MongoDB addon, do we need root privilege to get the moniotoring data?</p>
<p>/1/</p>
<pre><code>Number of commands
Number of asserts users
Number of asserts messages
Number of available connections
Number of current connections
Connection Growth
Number of network bytes in
Number of network bytes out
Number of network requests
Mapped memory size 
Virtual memory size 
Resident memory size 
Replica Set availability   
Member status
</code></pre>",63009078.0,1,0,,2020-7-21 06:33:40,,2020-7-21 07:01:00,,,,,84592.0,,1,1,mongodb|splunk,110,10
655,259111,63012459,How to integrate splunk with spring cloud sleuth in Spring Boot project?,"<p>we are developing microservices using spring boot. we want to track the progress of microservices using Spring Cloud Sleuth and wants to show that logs using Splunk on some dashboard etc.
For demo purpose we have are building three services, Service A calls service B which in turns call Service C. we want to trace their calls using slueth and consolidate them in splunk.
how to implement distributed tracing with Sleuth and Splunk?
any help would be greatly appreciated.</p>",,0,4,,2020-7-21 10:24:53,1.0,2020-7-21 11:03:32,2020-7-21 11:03:32,,12964338.0,,12964338.0,,1,0,spring-boot|spring-cloud|splunk|spring-cloud-netflix|spring-cloud-sleuth,1137,12
656,259112,63018629,MuleSoft Log Forwarding from On-Premise to Splunk,"<p>I have a mulesoft on-premise standalone server and i want the applications to forward the logs to splunk , i got the splunk url ( http) with port 8088 (default one)</p>
<p>Url: http://:8088/services/collector/raw</p>
<p>and i deployed a mule app by updating the log4j2.xml under src/main/resources , on the server it throws this error</p>
<p>Tried to followed this article : <a href=""https://dzone.com/articles/recipe-to-implement-splunk-enterprise-on-premise-f"" rel=""nofollow noreferrer"">https://dzone.com/articles/recipe-to-implement-splunk-enterprise-on-premise-f</a></p>
<pre><code>ERROR 2020-07-21 11:05:48,067 [pool-58-thread-2] [event: ] com.mulesoft.agent.common.internalhandler.splunk.transport.HECTransport: There was an error executing the request.
java.util.concurrent.ExecutionException: org.asynchttpclient.exception.RemotelyClosedException: Remotely closed
    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_252]
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_252]
    at org.asynchttpclient.netty.NettyResponseFuture.get(NettyResponseFuture.java:213) ~[?:?]
    at com.mulesoft.agent.common.internalhandler.splunk.transport.HECTransport.send(HECTransport.java:127) ~[?:?]
    at com.mulesoft.agent.common.internalhandler.AbstractSplunkInternalHandler.flush(AbstractSplunkInternalHandler.java:173) ~[?:?]
    at com.mulesoft.agent.buffer.BufferedHandler.flushBuffer(BufferedHandler.java:241) ~[?:?]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_252]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_252]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_252]
Caused by: org.asynchttpclient.exception.RemotelyClosedException: Remotely closed
    at org.asynchttpclient.exception.RemotelyClosedException.INSTANCE(Unknown Source) ~[?:?]
INFO  2020-07-21 11:05:48,083 [pool-58-thread-2] [event: ] com.mulesoft.agent.buffer.BufferedHandler: Trying to retry flushing on buffer for EventTrackingSplunkInternalHandler. Remaining attempts: 0
</code></pre>
<p>Added below sections in log4j2 file</p>
<pre><code> &lt;Http name=&quot;Splunk&quot; url=&quot;http://xxxx-xxxxx-xxx:8088/services/collector/raw&quot;&gt;
            &lt;Property name=&quot;Authorization&quot; value=&quot;Splunk e052f3fa-xxxxxxxxxx&quot; &gt;&lt;/Property&gt;
            &lt;PatternLayout pattern=&quot;[%d{MM-dd HH:mm:ss}] %-5p %c{1} [%t]: %m%n&quot;&gt;&lt;/PatternLayout&gt;
        &lt;/Http&gt;
</code></pre>
<p>and</p>
<pre><code> &lt;AsyncRoot level=&quot;INFO&quot;&gt;
            &lt;AppenderRef ref=&quot;Splunk&quot; &gt;&lt;/AppenderRef&gt;
        &lt;/AsyncRoot&gt;
</code></pre>",,1,0,,2020-7-21 16:12:42,,2020-7-21 17:26:14,2020-7-21 16:26:08,,7699912.0,,7699912.0,,1,0,logging|splunk|mulesoft|mule4,323,11
657,259113,63022116,Does Splunk offer any such solution to make a call to SOAP REST HTTP URL and to test their availability?,"<p>I need to create some kind of health check in Splunk that calls a Rest URL every hour and check if the response returns HTTP code 200 and send an alert in case there is an error like http code 400 or http code 500.</p>
<p>For example Splunk should make an http call to the URL of my application every hour and check if the URL of my application returns HTTP code 200. In case the response from the URL has a different code than 200 then send a notification email telling that something is wrong.</p>
<p>is that possible?
Please help.</p>",,2,0,,2020-7-21 19:53:45,,2020-7-21 22:44:51,,,,,1895917.0,,1,1,rest|http|monitoring|splunk|smoke-testing,232,10
658,259114,63028339,Adding custom fields to splunk query,"<p>In my java application, I have added some data to the logs, which then show up in splunk as:</p>
<pre><code>{
    ....
    &quot;duration&quot;:&quot;200&quot;,
    &quot;methodName&quot;:&quot;testMethod&quot;,
    &quot;className&quot;:&quot;com.test.TestClass&quot;,
    ....
}
</code></pre>
<p>Currently, if I have to search for these fields, I need to add something like following to filter query:</p>
<pre><code>log=\*&quot;methodName&quot;:&quot;testMethod&quot;*
</code></pre>
<p>Is there a way to add these custom fields to filter query, so that I can have in my query like:</p>
<pre><code>methodName=&quot;testMethod&quot;
</code></pre>
<p>And then I also need to use this data to generate charts and visualizations.</p>",,1,0,,2020-7-22 06:40:34,,2020-7-22 09:19:22,,,,,2444661.0,,1,0,splunk|splunk-query,153,9
659,259115,63055763,Splunk Indexing and Space Usage,<p>I am new to Splunk and I have download the Enterprise trial.  It permits free usage for 500MB of daily indexed data.  How do I determine my consumption of this limit?</p>,,1,0,,2020-7-23 13:44:56,,2020-7-23 23:53:08,,,,,2279735.0,,1,0,splunk,129,9
660,259116,63059107,Search Splunk API using python,"<p>What I am trying to do is perform a search on Splunk's API using python, I am able to get a session key but thats it. I'm new to both python and splunk so im a bit out-of-depth and any help would be really appreciated.</p>
<p>The error:</p>
<pre><code>Traceback (most recent call last):
      File &quot;splunkAPI.py&quot;, line 31, in &lt;module&gt;
        sid = minidom.parseString(r.text).getElementsByTagName('sid')[0].firstChild.nodeValue
    IndexError: list index out of range
</code></pre>
<p>python:</p>
<pre><code>import time # need for sleep
from xml.dom import minidom

import json, pprint

import requests
from requests.packages.urllib3.exceptions import InsecureRequestWarning
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

base_url = 'https://___________:8089'
username = '______'
password = '______'
search_query = &quot;____________&quot;


#-------------------------get session token------------------------
r = requests.get(base_url+&quot;/servicesNS/admin/search/auth/login&quot;,
        data={'username':username,'password':password}, verify=False)

session_key = minidom.parseString(r.text).getElementsByTagName('sessionKey')[0].firstChild.nodeValue
print (&quot;Session Key:&quot;, session_key)

#-------------------- perform search -------------------------

r = requests.post(base_url + '/services/search/jobs/', data=search_query,
        headers = { 'Authorization': ('Splunk %s' %session_key)},
        verify = False)

sid = minidom.parseString(r.text).getElementsByTagName('sid')[0].firstChild.nodeValue

done = False
while not done:
        r = requests.get(base_url + '/services/search/jobs/' + sid,
                headers = { 'Authorization': ('Splunk %s' %session_key)},
                verify = False)
        response = minidom.parseString(r.text)
        for node in response.getElementsByTagName(&quot;s:key&quot;):
                if node.hasAttribute(&quot;name&quot;) and node.getAttribute(&quot;name&quot;) == &quot;dispatchState&quot;:
                        dispatchState = node.firstChild.nodeValue
                        print (&quot;Search Status: &quot;, dispatchState)
                        if dispatchState == &quot;DONE&quot;:
                                done = True
                        else:
                                time.sleep(1)

r = requests.get(base_url + '/services/search/jobs/' + sid + '/results/',
        headers = { 'Authorization': ('Splunk %s' %session_key)},
        data={'output_mode': 'json'},
        verify = False)

pprint.pprint(json.loads(r.text))
</code></pre>",63065432.0,1,0,,2020-7-23 16:39:20,,2020-7-24 01:23:34,,,,user13820177,,,1,1,python|api|python-requests|splunk,1180,13
661,259117,63099865,Get splunk version and update it with ansible if version is not meet,"<p>I'm new on stackoverflow and I'm very glad to be here.</p>
<p>My question is:</p>
<p>I want to get the Splunk version and then if version is not equal to e.g &quot;8&quot; or something else, then stop Splunk and uninstasll it and install the right version.</p>
<p>Here is my Ansible playbook I write to get the version.</p>
<pre><code>  tasks:
  - name: Read splunk.version file
    slurp:
      src: C:\\Program Files\\SplunkUniversalForwarder\\etc\\splunk.version
    register: result
    ignore_errors: true

  - name: set splunk version
    set_fact:
      splunk_version: &quot;{{ result['content'] | b64decode | regex_findall('(?&lt;=VERSION\\=).*?(?=\\r)') }}&quot;

  - name: message
    debug:
      var: splunk_version

  - name: message
    debug:
      msg: &quot;same version installed&quot;
    when: splunk_version == &quot;8.0.3&quot;
</code></pre>
<p>And here my result</p>
<pre><code>TASK [Read splunk.version file] 
********************
ok: [192.168.*.*]

TASK [set splunk version]
*********************
ok: [192.168.*.*]

TASK [message] 
***********************
ok: [192.168.*.*] =&gt; {
    &quot;splunk_version&quot;: [
        &quot;8.0.3&quot;
    ]
}

TASK [message] 
***********************
skipping: [192.168.*.*]
</code></pre>
<p>As you see despite being in same version, I get wrong result. I know I made a mistake in my comparison conditions but I don't know where!</p>
<p>So I'm very appreciative if anyone could correct my playbook.</p>
<p>Sorry for my mistakes</p>
<p>Thanks in advance</p>",63100545.0,1,0,,2020-7-26 12:03:13,,2020-7-26 13:05:41,,,,,13997747.0,,1,2,ansible|splunk,79,9
662,259118,63115125,Calculating event throughput in splunk,"<p>I want to find throughput for target events. I identify my target events by EVENT_PROCESSED. So my query is:</p>
<pre><code>index=myIndex namespace=myNamespace host=myHost log=\*EVENT_PROCESSED* | bucket _time span=1h | chart count(EVENT_PROCESSED)/3600 as throughput by _time
</code></pre>
<p>But it doesn't work. Error is:</p>
<pre><code>Error in 'chart' command: The data field 'count(EVENT_PROCESSED)/3600' is malformed.
</code></pre>
<p>What mistake am I making here?</p>",63116119.0,1,0,,2020-7-27 12:15:25,,2020-7-27 14:29:40,,,,,2444661.0,,1,0,splunk,115,9
663,259119,63118732,Describing a field (data dictionary not statistics),"<p>Brand new to Splunk and curious whether there is a way to add descriptive text to the pop out window that appears when a user selects (clicks) a field in the search results?  I am building a data dictionary for myself, but I would like to view these descriptions in Splunk.  Is there a way to do this?</p>",,1,0,,2020-7-27 15:41:52,,2020-7-27 21:02:42,,,,,13598463.0,,1,0,splunk,19,5
664,259120,63119004,Splunk get the response time in the log and use it for graph,"<p>I am new to splunk and I am trying to see how to get the values from the log to display on the splunk as a graph.</p>
<p>My search:</p>
<pre><code>index=aws_test sourcetype=aws:ecs source=am/pm-* NOT &quot;healthCheck&quot; target=accounts ResponseTime
</code></pre>
<p>results:</p>
<pre><code>Time           | Event
7/27/20        |
10:52:28.957AM | ssa=|target=am|responseTime=5180ms|type=info
---------------|-----------------------------------------------
7/27/20        |
10:55:38.977AM | ssa=|target=am|responseTime=4180ms|type=info
</code></pre>
<p>what I tried to do is to get responseTime=&quot;*&quot; to the timechart.</p>",,1,1,,2020-7-27 15:58:23,,2020-7-27 21:39:38,,,,,3659052.0,,1,0,splunk,322,11
665,259121,63132733,How to merge two stats by in Splunk?,"<p>I wanted a single graph to show values.
One search is</p>
<pre><code>index=&quot;cumu_open_csv&quot;  Assignee=&quot;ram&quot;
| eval open_field=if(in(Status,&quot;Open&quot;,&quot;Reopened&quot;,&quot;Waiting&quot;,&quot;In Progress&quot;), 1,0)
| stats count(eval(open_field=1)) AS Open, count(eval(open_field=0)) AS closed by CW_Created
</code></pre>
<p>this gives me a table as</p>
<p><a href=""https://i.stack.imgur.com/kLuKa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kLuKa.png"" alt=""OpenCloseTable"" /></a></p>
<p>Similarly I have another search</p>
<pre><code> index=&quot;cumu_open_csv&quot;  Assignee=&quot;ram&quot;
| eval open_field=if(in(Status,&quot;Open&quot;,&quot;Reopened&quot;,&quot;Waiting&quot;,&quot;In Progress&quot;), 1,0)
| stats count(eval(open_field=1)) As DueOpen by CW_DueDate
</code></pre>
<p>which gives me another table as</p>
<p><a href=""https://i.stack.imgur.com/fWNxH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fWNxH.png"" alt=""DUeDatetable"" /></a></p>
<p>I tried to combine these two using appendcols, but the X-axis has only the CW_Created and displays the second table details in wrong CW.</p>
<p>I wanted CW_Created and CW_Duedate to be combined and provide the result in a single table like CW, Open,Close,DueCount wherever DueCount is not for a particular CW fill it with 0, for others display the data like so.</p>
<pre><code>CW      |Open     |Close    |DueCount
CW27    |7        |0        |0
CW28    |2        |0        |0
CW29    |0        |0        |4
CW30    |0        |7        |3
CW31    |0        |0        |1
CW32    |0        |0        |1
</code></pre>",63134059.0,2,0,,2020-7-28 11:08:58,,2020-7-29 11:57:18,2020-7-28 11:51:29,,4420967.0,,13852014.0,,1,1,statistics|splunk|splunk-query,1914,14
666,259122,63136104,How to link the events of a search used as an alert in Splunk,"<p>I have a query that I created that looks like this:</p>
<pre><code>index=&quot;someindex&quot; Level=Error | rex field=_raw &quot;\&quot;Exception\&quot;:\&quot;(?&lt;ExceptionType&gt;.*?):&quot; 
| eval ExceptionType = if(isnull(ExceptionType), &quot;Custom log&quot;,ExceptionType) | search ExceptionType=&quot;Custom log&quot;
</code></pre>
<p>And I saved it as an alert that sends a message to Slack that looks like this:
<a href=""https://i.stack.imgur.com/CHx9A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CHx9A.png"" alt=""enter image description here"" /></a></p>
<p>Here's the problem. When I run this search normally I get the results like so:</p>
<p><a href=""https://i.stack.imgur.com/aVZZB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aVZZB.png"" alt=""enter image description here"" /></a></p>
<p>And I can click on the &quot;events&quot; tab to see the individual events that are aggregated by the &quot;stats&quot; command.
However, when I click the link generated by the alert, I only get the aggregated results. I can't view the individual events. So my question is: is there any way to create a link that will allow to expand the events from which the results are aggregated?</p>",,1,0,,2020-7-28 14:10:56,,2020-7-28 14:31:29,,,,,5285375.0,,1,0,splunk|splunk-query,23,5
667,259123,63137123,Using dedup to find unique hosts. How can I find an average for the selected time frame?,"<p>The goal is to provide percent availability. I would like to check every 15 minutes if the unique count for server1, server2, and server3 is equal to 3 for each interval (indicating the system is fully healthy). From this count I want to check on the average for whatever time period is selected in splunk to output an average and convert to percent.</p>
<pre><code>index=&quot;os&quot; sourcetype=ps host=&quot;server1&quot; OR host=&quot;server2&quot; OR host=&quot;server3&quot;
| search &quot;/logs/temp/random/path&quot; OR &quot;application_listener&quot;
| dedup host
| timechart span=30m count
</code></pre>
<p>The count should be 3 for each interval.</p>",63140409.0,1,0,,2020-7-28 14:59:18,,2021-11-8 21:03:17,2021-11-8 21:03:17,,1127428.0,,1316578.0,,1,0,splunk|splunk-query,195,10
668,259124,63140195,In Splunk how to find the number of words based on the a pipe separator and add a value to it and assign it to a new filed,"<p>Hi Have an event like this shown below</p>
<p><strong>Today's Greeting Messag=Hello|myname|name|is|Alice|myName|is|bob&quot;}</strong></p>
<p>How can i count the number of words between <strong>message=</strong> till <strong>&quot;}</strong>. I have a <strong>|</strong> delimiter that should helps me to get the count of words in between. But for every count i want to add a specific number</p>
<p>example for above log i will get 8 words in between as count based on <strong>|</strong> separator.
But for every count i would like to add some new number like 8+2 and the value to be updated to a new splunk field.</p>
<p>This will help in calculating if any event that is crossing the threshold of that value then i can trigger an alarm.</p>
<p>Some one please help me in getting this.</p>",,1,1,,2020-7-28 18:01:42,,2020-7-28 22:42:38,,,,,4591762.0,,1,0,regex|splunk|splunk-query|splunk-calculation|splunk-formula,284,9
669,259125,63150128,Trigger splunk alert when received values do not change,"<p>I receive exchange rate from an external web service and I log the response received like below (note both line contain data from a single response):</p>
<pre><code>com.test.Currency@366c1a1e[Id=&lt;Null&gt;,Code=&lt;Null&gt;,Feedcode=Gbparslite,Rate=&lt;Null&gt;,Percentaqechangetrigger=&lt;Null&gt;,Bid=93.4269,Offer=93.43987,Mustinvertprice=False], 
com.test.Currency@54acb93a[Id=&lt;Null&gt;,Code=&lt;Null&gt;,Feedcode=Gbphkdlite,Rate=&lt;Null&gt;,Percentaqechangetrigger=&lt;Null&gt;,Bid=10.04629,Offer=10.04763,Mustinvertprice=False],
</code></pre>
<p>I want to set up an alert which triggers when the last x (x=5) values received did not changed.</p>",,1,0,,2020-7-29 09:06:40,,2020-7-29 12:14:44,2020-7-29 12:14:44,,2241033.0,,2241033.0,,1,0,splunk|splunk-query,85,7
670,259126,63151824,"Splunk REST API: How to set ""Send to triggered alerts"" action when creating an alert?","<p>I wish to create a Splunk alert using the REST API. However, I can't find the action &quot;Send to triggered alerts&quot; in the actions list. How can I add that action?</p>",63151825.0,1,0,,2020-7-29 10:43:40,,2021-12-2 12:47:30,,,,,3207874.0,,1,0,rest|splunk,258,10
671,259127,63152205,Splunk REST API - Specify relative time range for alert,"<p>I want to create an alert using Splunk's REST API. I want the alert to get events which happened in the last two minutes. How can I do that?</p>
<p>This is my alert so far:</p>
<pre><code>curl -k -u admin:password https://my.company:8089/servicesNS/admin/search/saved/searches \
  -d name=test7 \
  --data-urlencode output_mode='json' \
  --data-urlencode actions='' \
  --data-urlencode alert.digest_mode='1' \
  --data-urlencode alert.expires='24h' \
  --data-urlencode alert.managedBy='' \
  --data-urlencode alert.severity='3' \
  --data-urlencode alert.suppress='1' \
  --data-urlencode alert.suppress.fields='' \
  --data-urlencode alert.suppress.period='5m' \
  --data-urlencode alert.track='1' \
  --data-urlencode alert_comparator='greater than' \
  --data-urlencode alert_condition='' \
  --data-urlencode alert_threshold='10' \
  --data-urlencode alert_type='number of events' \
  --data-urlencode allow_skew='0' \
  --data-urlencode cron_schedule='*/2 * * * *' \
  --data-urlencode description='' \
  --data-urlencode disabled='0' \
  --data-urlencode displayview='' \
  --data-urlencode is_scheduled='1' \
  --data-urlencode is_visible='1' \
  --data-urlencode max_concurrent='1' \
  --data-urlencode realtime_schedule='1' \
  --data-urlencode restart_on_searchpeer_add='1' \
  --data-urlencode run_n_times='0' \
  --data-urlencode run_on_startup='0' \
  --data-urlencode schedule_priority='default' \
  --data-urlencode schedule_window='0' \
  --data-urlencode search='sourcetype=&quot;auth&quot; failed'
</code></pre>",63152206.0,1,0,,2020-7-29 11:06:10,,2020-7-29 11:06:10,,,,,3207874.0,,1,0,rest|splunk,193,9
672,259128,63152602,Splunk REST API - How to add an extra field to a saved search?,"<p>I wish to create an alert which should have an additional &quot;Selected Field&quot; - <code>uri_path</code>. I don't know how to add the field as a &quot;selected field&quot;. How can I do that?</p>
<p>This is my current code:</p>
<pre><code>curl -k -u admin:password https://splunk.rf:8089/servicesNS/admin/search/saved/searches \
  -d name=http1 \
  --data-urlencode output_mode='json' \
  --data-urlencode actions='' \
  --data-urlencode alert.digest_mode='0' \
  --data-urlencode alert.expires='24h' \
  --data-urlencode alert.managedBy='' \
  --data-urlencode alert.severity='4' \
  --data-urlencode alert.suppress='1' \
  --data-urlencode alert.suppress.fields='uri_path' \
  --data-urlencode alert.suppress.period='5m' \
  --data-urlencode alert.track='1' \
  --data-urlencode alert_comparator='greater than' \
  --data-urlencode alert_condition='' \
  --data-urlencode alert_threshold='10' \
  --data-urlencode alert_type='number of events' \
  --data-urlencode allow_skew='0' \
  --data-urlencode cron_schedule='*/2 * * * *' \
  --data-urlencode description='' \
  --data-urlencode disabled='0' \
  --data-urlencode displayview='' \
  --data-urlencode is_scheduled='1' \
  --data-urlencode is_visible='1' \
  --data-urlencode max_concurrent='1' \
  --data-urlencode realtime_schedule='1' \
  --data-urlencode restart_on_searchpeer_add='1' \
  --data-urlencode run_n_times='0' \
  --data-urlencode run_on_startup='0' \
  --data-urlencode schedule_priority='default' \
  --data-urlencode schedule_window='0' \
  --data-urlencode dispatch.earliest_time='-2m' \
  --data-urlencode dispatch.latest_time='now' \
  --data-urlencode search='sourcetype=&quot;auth&quot; failed'
</code></pre>",63152603.0,1,0,,2020-7-29 11:30:19,,2020-7-29 11:30:19,,,,,3207874.0,,1,0,rest|splunk,113,8
673,259129,63154500,how count and plot several searches at once?,"<p>I am counting the number of hits on my website using <code>splunk</code>. My current search looks for a <code>keywordA</code>  as follows:</p>
<pre><code>index=mydata keywordA |bucket _time span=day |stats count by _time
</code></pre>
<p>However, I would like to add several other searches to the output, say for other keywords (<code>keywordB</code> for instance):</p>
<pre><code>index=mydata keywordB |bucket _time span=day |stats count by _time
</code></pre>
<p>Note: these searches are not necessarily mutually exlusive! So the searches need to be run independently.</p>
<p>I would like to have the <strong>total daily count</strong> for each search at once, so that I avoid running each search separately.</p>
<p>Output should be:</p>
<pre><code>day          keyA  keyB
2020-01-01   423   354
2020-01-02   523   254
</code></pre>
<p>What is the best way to proceed?</p>
<p>Thanks!</p>",63160439.0,1,1,,2020-7-29 13:16:26,,2020-7-29 19:49:05,2020-7-29 14:30:29,,1609428.0,,1609428.0,,1,0,splunk,35,7
674,259130,63162070,Recording earliest login time for each day,"<p>I need to return the earliest login time per day for a single username. However, some returns do not match the login from that date. Query below:</p>
<pre><code>index=app_redacted_int_* sourcetype=&quot;redacted&quot; SessionState=&quot;Active&quot; UserName=ABCDE123

| rex field=UserRealName &quot;(?&lt;IDNUM&gt;\d+$)&quot;

| bucket _time span=1d as day
| eval day=strftime(_time,&quot;%F&quot;)

| stats earliest(SessionStateChangeTime) as SesssionStateChangeTime by day IDNUM UserRealName UserName
</code></pre>
<p>Results:</p>
<pre><code>day             IDNUM               UserRealName             UserName              SessionStateChangeTime
2020-07-23       123                John Smith               ABCDE123              7/22/2020 09:48:52
2020-07-24       123                John Smith               ABCDE123              7/23/2020 12:47:13
2020-07-25       123                John Smith               ABCDE123              7/24/2020 07:23:01
2020-07-27       123                John Smith               ABCDE123              7/27/2020 07:54:34
2020-07-28       123                John Smith               ABCDE123              7/27/2020 07:54:34
2020-07-29       123                John Smith               ABCDE123              7/28/2020 07:32:04
</code></pre>
<p>As you can see, some days are returning their earliest login as a login from the previous day. I need the dates on the left side and the right side to be matching, and I need this all together in one query, I already know how to do it one query at a time. Thanks for taking your time to help! It is greatly appreciated!</p>",,1,0,,2020-7-29 20:39:41,,2020-7-30 12:44:43,2020-7-30 12:44:43,,2776566.0,,14018200.0,,1,1,splunk|splunk-query|splunk-sdk|splunk-formula,36,6
675,259131,63165750,Regex search for UUID based uri in splunk,"<p>I am trying to search for all events which contain a UUID as part of a request url. Here is my query:</p>
<pre><code>.... | regex requestURI=*/employee/[0-9a-f]{8}\b-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-\b[0-9a-f]{12}*
</code></pre>
<p>It gives error as:</p>
<pre><code>Unknown search command '0'
</code></pre>
<p>What's the mistake I am making?</p>",63166056.0,1,0,,2020-7-30 03:27:39,,2020-7-30 04:09:43,,,,,2444661.0,,1,0,regex|splunk,491,11
676,259132,63189850,Splunk query to get field from JSON cell,"<p>The splunk query outputs a table where one of the column has these kind of json
the part of the query that gives this output is <code>details.ANALYSIS</code></p>
<pre><code>{&quot;stepSuccess&quot;:false,&quot;SR&quot;:false,&quot;propertyMap&quot;:{&quot;Url&quot;:&quot;https://example.com&quot;,&quot;ErrCode&quot;:&quot;401&quot;,&quot;transactionId&quot;:&quot;7caf34342524-3d232133da&quot;,&quot;status&quot;:&quot;API failing with error code 401&quot;}}
</code></pre>
<p>I want to edit my splunk query so that instead of this json, I get only <code>Url</code> in this same column.</p>
<p>Here is my splunk query I was using</p>
<pre><code>|dbxquery connection=&quot;AT&quot; query=&quot;select service.req_id, service.out,details.ANALYSIS from servicerequest service,SERVICEREQUEST_D details where service.out like 'XYZ is%'  and service.row_created &gt; sysdate-1 and service.SERVICEREQUEST_ID = details.SERVICEREQUEST_ID and  details.ANALYSIS_CLASS_NAME = 'GetProduction' &quot; shortnames=0 maxrows=100000001
</code></pre>
<p>I tried using details.ANALYSIS.propertyMap.Url but it throws error.</p>",,1,0,,2020-7-31 10:10:39,,2020-7-31 12:58:36,2020-7-31 12:15:16,,4418.0,,2058355.0,,1,1,sql|json|splunk|splunk-query|splunk-dbconnect,167,10
677,259133,63202226,Got malformed error when do replace string manipulation,"<p>I was following string <a href=""https://docs.splunk.com/Documentation/DSP/1.1.0/FunctionReference/Stringmanipulation"" rel=""nofollow noreferrer"">manipulation docs</a> from splunk itself</p>
<blockquote>
<ol>
<li>SPL2 example Returns the &quot;body&quot; field with phone numbers redacted.</li>
</ol>
<p><code>...| eval body=replace(cast(body, &quot;string&quot;), /[0-9]{3}[-.][0-9]{3}[-.][0-9]{4}/, &quot;&lt;redacted&gt;&quot;);</code></p>
</blockquote>
<p>But when I tried to do query</p>
<pre><code>... | eval hostname=replace(cast(hostname, &quot;string&quot;), /cron*/, &quot;&quot;); | ..
</code></pre>
<p>I got error
<code>Error in 'eval' command: The expression is malformed. An unexpected character is reached at '/cron*/, &quot;a&quot;);'.</code></p>
<p>I got confused, what did I do wrong?</p>
<p>Update:
String example:</p>
<ul>
<li><code>pods-name-cron-3829hr832</code></li>
<li><code>pods-name-cron-8923eh32b</code></li>
</ul>
<p>My goal was to remove the <code>cron-&lt;random_id&gt;</code></p>",63203085.0,3,0,,2020-8-1 05:51:21,,2020-8-3 08:25:50,2020-8-3 01:40:59,,4728165.0,,4728165.0,,1,0,replace|splunk,502,12
678,259134,63208179,How to uninstall an app on Splunk from CLI?,"<p>Can someone please guide how to uninstall an app from Splunk using CLI?</p>
<p>I know how to delete the app from the Splunk Etc folder...need guidance using CLI.</p>",,2,2,,2020-8-1 17:33:46,,2021-10-25 17:51:12,2020-8-3 12:17:09,,4418.0,,14034160.0,,1,-3,splunk,665,11
679,259135,63229688,stats latest not showing any value for field,"<p>We have following query -</p>
<pre><code>index=yyy sourcetype=zzz &quot;RAISE_ALERT&quot; logger=&quot;aaa&quot; | table uuid message timestamp | eval state=&quot;alert&quot; | append [SEARCH index=yyy sourcetype=zzz &quot;CLEAR_ALERT&quot; logger=&quot;aaa&quot; | table uuid message timestamp | eval state=&quot;no_alert&quot; ] | stats latest(state) as state by uuid
</code></pre>
<p>But this query is not showing anything for state, it shows only uuid.</p>
<p><a href=""https://i.stack.imgur.com/OrCWK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OrCWK.png"" alt=""enter image description here"" /></a></p>
<p>Query before and without latest works just fine. Here is screenshot of result of everything before stats -</p>
<p><a href=""https://i.stack.imgur.com/egT1K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/egT1K.png"" alt=""enter image description here"" /></a></p>
<p>If we replace <code>stats latest</code> with <code>stats first</code>, we can see uuid and state, its just not the latest observed value of state for that uuid.</p>
<p>Any idea as to why this can happen?</p>",,1,0,,2020-8-3 12:44:26,,2020-8-3 14:49:27,,,,,7155594.0,,1,0,splunk|splunk-query,20,6
680,259136,63254466,How to remove everything before first occurrence of a character in splunk,"<p>I have a string with certain formate</p>
<pre><code>154787878_2582_test.txt.zip
</code></pre>
<p>I need to remove everything before first occurrence of <code>-</code> and remove  154787878_</p>
<p>I have tried</p>
<pre><code>| eval txtFile=replace(mvindex(split(txtFile,&quot;_&quot;),0),&quot;&quot;) 
</code></pre>
<p>Please help</p>",,1,2,,2020-8-4 20:17:46,,2020-8-4 20:46:47,,,,,9340011.0,,1,2,regex|replace|splunk,856,16
681,259137,63261933,Splunk REST API - How to add a webhook action?,"<p>I want to create an alert, and add a webhook action to it. However, looking at the <a href=""https://docs.splunk.com/Documentation/Splunk/latest/RESTREF/RESTsearch#saved.2Fsearches"" rel=""nofollow noreferrer"">Splunk documentation</a>, it doesn't seem to say how to do it.</p>
<p>Here is my current request:</p>
<pre><code>curl -s -k -u admin:password https://splunk.rf:8089/servicesNS/admin/search/saved/searches &gt; /dev/null \
  -d name=bruteforcetest \
  --data-urlencode output_mode='json' \
  --data-urlencode alert.digest_mode='0' \
  --data-urlencode alert.expires='24h' \
  --data-urlencode alert.managedBy='' \
  --data-urlencode alert.severity='3' \
  --data-urlencode alert.suppress='1' \
  --data-urlencode alert.suppress.fields='source_ip' \
  --data-urlencode alert.suppress.period='2m' \
  --data-urlencode alert_comparator='greater than' \
  --data-urlencode alert_condition='' \
  --data-urlencode alert_threshold='20' \
  --data-urlencode alert_type='number of events' \
  --data-urlencode alert.track='1' \
  --data-urlencode cron_schedule='* * * * *' \
  --data-urlencode description='' \
  --data-urlencode disabled='0' \
  --data-urlencode displayview='' \
  --data-urlencode is_scheduled='1' \
  --data-urlencode is_visible='1' \
  --data-urlencode max_concurrent='1' \
  --data-urlencode realtime_schedule='1' \
  --data-urlencode restart_on_searchpeer_add='1' \
  --data-urlencode run_n_times='0' \
  --data-urlencode run_on_startup='0' \
  --data-urlencode schedule_priority='default' \
  --data-urlencode schedule_window='0' \
  --data-urlencode dispatch.earliest_time='rt-2m' \
  --data-urlencode dispatch.latest_time='rt-0m' \
  --data-urlencode display.events.fields='[&quot;host&quot;,&quot;source&quot;,&quot;sourcetype&quot;, &quot;source_ip&quot;]' \
  --data-urlencode search='&quot;error: invalid login credentials for user&quot;'
</code></pre>
<p>How can I modify this request to add a webhook action? The webhook query should be to <code>http://firewall.mycompany/ban</code>.</p>",63261934.0,1,0,,2020-8-5 09:12:08,,2020-8-5 13:11:48,2020-8-5 13:11:48,,4418.0,,3207874.0,,1,1,rest|curl|webhooks|splunk|splunk-api,257,10
682,259138,63266646,Match everything between optional single or double quotation marks,"<p>I'm developing a regex to parse values out of a JSON response but I'm having trouble with one field because it contains human-written text so, as the content can vary and break the regex, I need a regex that will match all of the following potential values:</p>
<pre><code>, 'resolve_comment': &quot;This value's comment contains an apostrophe / single quotation mark so it will be automatically enclosed in double quotation marks&quot;, 
, 'resolve_comment': 'Some comments, like this one, contain a comma. If there is no comment then there will be no quotation marks as you can see below.', 
, 'resolve_comment': None, 
</code></pre>
<p>None of the regexps I've found online have worked for all 3 of these scenarios.</p>
<p>The closest I've gotten was:</p>
<ol>
<li><code>'resolve_comment': (?:(?:&quot;(?P&lt;resolve_comment&gt;[^&quot;]*)&quot;)|(?:'(?P&lt;resolve_comment&gt;[^']*)')|(?P&lt;resolve_comment&gt;None)), </code> but the system doesn't allow duplicated capture group names.</li>
<li><code>'resolve_comment': (?:(?:&quot;([^&quot;]*)&quot;)|(?:'([^']*)')|(None)),</code> but that created 3 capture groups of which only one was populated.</li>
<li><code>'resolve_comment': [&quot;|']?(.*)[&quot;|']?,</code> but that leaves a trailing quotation mark which isn't ideal.</li>
</ol>",,1,10,,2020-8-5 13:50:50,,2020-8-8 10:50:13,2020-8-5 14:24:33,,1327748.0,,1327748.0,,1,0,regex|splunk,64,7
683,259139,63286835,How to use where clause in my search string in Splunk Enterprise,"<p>I have a search string like below:</p>
<p>index=qrp STAGE IN (ORDER_EVENT)
| bucket _time span=1h
| timechart useother=f span=1h sum(TRADES) as &quot;TradeCount&quot; by ODS_SRC_SYSTEM_CODE
| fillnull value=0</p>
<p>And this is currently giving me aggregates of trades for multiple source systems from the stage table Trade event in a tabular format for every hour of the day.</p>
<p>I need to search exactly for the time frame 8am every day, whether the value of sun of trade for all source systems in the table is equal to zero. How to add the condition to check the column value is Zero or not?</p>
<p>Your help is much appreciated.</p>",,1,0,,2020-8-6 15:25:41,,2020-8-9 20:37:08,,,,,13935026.0,,1,0,splunk|splunk-query|splunk-sdk|splunk-formula,310,9
684,259140,63292162,100 result limit from Splunk API call,"<p>I should be getting around 1000 results, but it this is only returning 100 of them into the output file.</p>
<pre><code>g = requests.get(base_url + '/services/search/jobs/' + sid + '/results/',

headers = {'Authorization':('Splunk %s' %session_key)},data={'output_mode': 'json', 'count':'0'},  verify = False)

data = g.json()

names = [item['name'] for item in data['results']]

with open ('sOutput.csv', mode='w') as csv_file:
        csv_writer = csv.writer(csv_file,  delimiter='\n', quotechar='&quot;', quoting=csv.QUOTE_MINIMAL)
        csv_writer.writerow(names)
</code></pre>",63292356.0,1,0,,2020-8-6 21:41:54,,2020-8-6 22:01:09,,,,user13820177,,,1,0,python|api|rest|python-requests|splunk,303,10
685,259141,63292320,Splunk how to exclude a certain vale from the list if exist,"<p>I have a log with payload something like this:</p>
<pre><code>&quot;Stats&quot;:[        { 
           errors: 0
           type: &quot;Disc&quot;
           success: 878
         },
         {
           errors: 21
           type: &quot;cronJob&quot;
           success: 25
         },
         { 
           errors: 0
           type: &quot;File&quot;
           success: 8787
         },
         { 
           errors: 15
           type: &quot;Unknown&quot;
           success: 0
         }]
</code></pre>
<p>I need to get the get rid of the &quot;Unknown&quot; type object and get the sum of the remaining values</p>
<p>I am able to get the sum of all errors but for the events with type Unknown I am not sure how to  do that. Could you please help?</p>
<pre><code>&lt;search&gt;|rename Stats{}.type= as type|eventstats sum(errors)  as ErrorCount 
</code></pre>
<p>This is my current seach without excluding Unknown type. how to I incorporate the logic to exclude Unknown counts</p>",,2,0,,2020-8-6 21:57:14,,2020-8-10 14:28:40,2020-8-6 22:18:45,,9340011.0,,9340011.0,,1,0,filter|splunk|splunk-query|splunk-formula|splunk-sum,455,11
686,259142,63314298,Splunk: how to select not matching data across two sourcetype,"<p>I have following data in splunk in two different sourcetypes</p>
<p><strong>index=&quot;xyz&quot; sourcetype=&quot;assets&quot;</strong></p>
<pre><code>name
--------
SERVER01
SERVER02
SERVER03
</code></pre>
<p><strong>index=&quot;xyz&quot; sourcetype=&quot;computers&quot;</strong></p>
<pre><code>name
--------
SERVER02
SERVER03
SERVER05
</code></pre>
<p>i am trying to fetch data which is not matching in both sourcetypes</p>
<pre><code> name
 --------
 SERVER01
 SERVER05
</code></pre>
<p>i tried doing data selection using outer join as mentioned below but seems its not working</p>
<pre><code>index=&quot;xyz&quot; sourcetype=&quot;assets&quot;
| table name
| join type=outer name
   [| search index=&quot;xyz&quot; sourcetype=&quot;computers&quot;
    | table name]
| table name
</code></pre>
<p>Please suggest</p>",63317436.0,1,0,,2020-8-8 10:21:18,,2020-8-8 15:58:22,,,,,4409879.0,,1,0,outer-join|splunk|splunk-query|splunk-formula,138,10
687,259143,63326610,How to get the sql output using Splunk DB connect for 2 DBs?,"<p>Getting the payment details from 1 DB and using those values in 2nd DB to get the results. But as per SQL limit payment ID IN (&lt;limited to 999 payment ids from 1st DB&gt;). How do we do this using <strong>SPLUNK DB CONNECT</strong> which allows more than 30K payment IDs? I have both DBs connection strings.</p>
<p>1st sql with connection:</p>
<pre><code>| streamstats count as &quot;rc&quot; 
| table payment_id rc 
| eval rc1 = rc%10 
| table payment_id rc1 
| eval payment_id = &quot;'&quot;+payment_id+&quot;',&quot; 
| stats values(payment_id) as payment_id by rc1 
| mvcombine delim=&quot;&quot; payment_id 
| nomv payment_id 
| rex field=payment_id &quot;(?&lt;payment_id&gt;.*).&quot; 
| map search=&quot;|dbxquery connection = &quot;&quot; 
</code></pre>
<p>&lt;2nd sql here where using the above payment_id in where condition as below&gt;</p>
<pre><code>(payment_id IN (&quot;$payment_id$&quot;))&gt;
</code></pre>
<p>Here it will divide the no of payment_id values by 10 and send the streams to map. As MAP only allows 9 values, I have to divide it by 10. Example: if I have 9K payment ids, 9K%10 streams will be formed and send to 2nd sql query. Here the limitation is only for 10K records only the below will work. it won't work for more than 10K payment_ids</p>",,1,4,,2020-8-9 12:57:59,,2020-8-13 18:05:11,2020-8-13 13:08:49,,4418.0,,4631780.0,,1,0,connection-string|splunk|splunk-dbconnect,194,9
688,259144,63356850,Prediction JSON payload Splunk alert,"<p>At the moment I am busy working on something that requires input from a Splunk alert.</p>
<p>However, I am having trouble in discovering what the JSON output for the alert might be. I was wondering if anyone knows a way to predict JSON output accurately.</p>
<p>I know the specifics of the event that gives the alert and, of course, the details for the alert itself.</p>
<p>I am not allowed to share, unfortunately. I have a webhook, but it's not online yet, only reachable from localhost, because security has not been fitted appropriately yet. That's why I can't simply send an alert to test it.</p>
<p>Instead I want to send a curl with 'example' JSON output, but because there needs to be some decryption on that sample output, I would like for it to be quite accurate instead of simply taking a wild guess at it.</p>
<p>Is there anybody who can tell me how to predict the JSON payload?</p>",,1,0,,2020-8-11 11:03:32,,2020-9-8 17:20:09,2020-8-11 19:57:35,,12701949.0,,13912629.0,,1,0,json|splunk|payload,91,7
689,259145,63358250,Splunk queries: filter by _meta fields,"<h2>Context</h2>
<p>I have a bunch of application servers I would like monitor using <strong>Splunk</strong>. Servers on every environment run the same applications. Looking for a way to tag this information in order to easily disentangle <em>stage</em> servers from <em>prod</em> server in my dashboards, I came across this trick while reading forums.</p>
<p><code>inputs.conf</code> of <em>forwarders</em> on <strong>production</strong> machines</p>
<pre><code>[default]
_meta = env::prod
</code></pre>
<p><code>inputs.conf</code> of <em>forwarders</em> on <strong>stage</strong> machines</p>
<pre><code>[default]
_meta = env::stage
</code></pre>
<p>With this trick, I end up with a <code>env</code> field in my parsed data.</p>
<pre class=""lang-sql prettyprint-override""><code>index=* | stats count by env

| env    | count |
|:------:|:-----:|
| stage  |2415686|
| prod   |55677  |
</code></pre>
<h2>Issue</h2>
<p>I can't filter on <code>env</code></p>
<pre class=""lang-sql prettyprint-override""><code>index=* logLevel=&quot;ERROR&quot; projectName != &quot;null&quot; env=&quot;prod&quot; | stats count(_raw) by projectName
</code></pre>
<p>Why is that so?</p>",63358395.0,1,2,,2020-8-11 12:33:46,,2020-8-26 07:53:07,2020-8-26 07:53:07,,7648881.0,,7648881.0,,1,1,splunk|splunk-query,124,9
690,259146,63371149,New CSV file not syncing with index Splunk,"<p>I'm facing problem with Splunk like there is an index having  a folder of some csv file as a data input. when i'm adding another CSV file in that folder for that index, new source data is not showing in index.
I have restarted Splunk many time and delete index and recreate but problem is still there.</p>
<p>I haven't added any configuration for that folder.</p>
<p>Do i need to add any conf for that folder if yes please help me i'm new with splunk.</p>
<p>One more thing If i check the file count in settings&gt; Data inputs for folder it is showing correct but when i search any query with mapped index then there is some problem and showing less file as expected.</p>
<p>Default Inputs.conf file is :</p>
<pre><code>[default]
index = default
_rcvbuf = 1572864
host = $decideOnStartup

[blacklist:$SPLUNK_HOME/etc/auth]

[blacklist:$SPLUNK_HOME/etc/passwd]

[monitor://$SPLUNK_HOME/var/log/splunk]
index = _internal

[monitor://$SPLUNK_HOME/var/log/watchdog/watchdog.log*]
index = _internal

[monitor://$SPLUNK_HOME/var/log/splunk/license_usage_summary.log]
index = _telemetry

[monitor://$SPLUNK_HOME/var/log/splunk/splunk_instrumentation_cloud.log*]
index = _telemetry
sourcetype = splunk_cloud_telemetry

[monitor://$SPLUNK_HOME/etc/splunk.version]
_TCP_ROUTING = *
index = _internal
sourcetype=splunk_version

[batch://$SPLUNK_HOME/var/run/splunk/search_telemetry/*search_telemetry.json]
move_policy = sinkhole
index = _introspection
sourcetype = search_telemetry
crcSalt = &lt;SOURCE&gt;
log_on_completion = 0

[batch://$SPLUNK_HOME/var/spool/splunk]
move_policy = sinkhole
crcSalt = &lt;SOURCE&gt;

[batch://$SPLUNK_HOME/var/spool/splunk/...stash_new]
queue = stashparsing
sourcetype = stash_new
move_policy = sinkhole
crcSalt = &lt;SOURCE&gt;

[fschange:$SPLUNK_HOME/etc]
#poll every 10 minutes
pollPeriod = 600
#generate audit events into the audit index, instead of fschange events
signedaudit=true
recurse=true
followLinks=false
hashMaxSize=-1
fullEvent=false
sendEventMaxSize=-1
filesPerDelay = 10
delayInMills = 100

[udp]
connection_host=ip

[tcp]
acceptFrom=*
connection_host=dns

[splunktcp]
route=has_key:_replicationBucketUUID:replicationQueue;has_key:_dstrx:typingQueue;has_key:_linebreaker:indexQueue;absent_key:_linebreaker:pars$
acceptFrom=*
connection_host=ip

[script]
interval = 60.0
start_by_shell = true

[SSL]
# SSL settings
# The following provides modern TLS configuration that guarantees forward-
# secrecy and efficiency. This configuration drops support for old Splunk
# versions (Splunk 5.x and earlier).
# To add support for Splunk 5.x set sslVersions to tls and add this to the
# end of cipherSuite:
# DHE-RSA-AES256-SHA:AES256-SHA:DHE-RSA-AES128-SHA:AES128-SHA
# and this, in case Diffie Hellman is not configured:
# AES256-SHA:AES128-SHA

sslVersions = tls1.2
cipherSuite = ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA$
ecdhCurves = prime256v1, secp384r1, secp521r1

allowSslRenegotiation = true
sslQuietShutdown = false
</code></pre>",63373132.0,1,0,,2020-8-12 06:39:40,,2020-8-12 08:55:13,,,,,7165942.0,,1,0,indexing|splunk|splunk-query,274,12
691,259147,63379497,send application logs to splunk using udp protocol,"<p>I am working on service written on golang and I need to send log to splunk using udp protocol.
To write log into file and syslog I use github.com/sirupsen/logrus. There is the hook for splunk, github.com/Franco-Poveda/logrus-splunk-hook, but it using http protocol(POST verb) to send logs.
Could you advise me how to send logs to splunk using udp protocol.</p>
<p>Thank you!</p>",,1,2,,2020-8-12 15:11:12,,2020-8-14 07:23:57,2020-8-14 07:23:57,,13418943.0,,13418943.0,,1,0,go|udp|splunk,264,10
692,259148,63384822,splunk exclude results based on json property,"<p>When I use this search operator <code>search &quot;response.header.status&quot;!=200</code>  splunk will only include results for which the <code>response.header.status</code> path exists.</p>
<p>So the search parameter here is implicitly forcing the requirement for the property to exists, regardless of the value</p>
<p>Is there a variant of search that will let me exclude results based on the value of the path, but still include the result if the path does not exists?</p>",63386388.0,1,0,,2020-8-12 21:16:45,,2020-8-13 00:30:40,,,,,867294.0,,1,0,splunk|splunk-query,121,10
693,259149,63385279,What might cause a splunk lookup table to suddenly show no rows?,"<p>One of my new colleagues was working on a lookup in a splunk app and seems to have somehow made a lookup table unavailable.  His task involved creating and uploading a lookup csv, creating a lookup definition and creating an automatic lookup.  No one else has been modifying this Splunk App.</p>
<p>An existing lookup started no longer populating in a dashboard. I looked at the “datasets” and the lookup csv exists and shows a last modified date from way before this issue started, but no rows display when I click on it.</p>
<p>I’m at a loss for what could have caused this or how to fix this.  It seems the data still exists, as the modified date hasn’t changed, but somehow has become inaccessible.</p>",,1,0,,2020-8-12 22:01:51,,2020-8-12 22:25:36,,,,,14095902.0,,1,0,splunk,222,11
694,259150,63395938,Filter out values using mstats,"<p>I am trying to filter out all negative values in my metrics, I would like to know if the filtering within the <code>mstats</code> call itself possible, to add something like <code>AND metrics_name:data.value &gt; 0</code> to the query below?</p>
<pre><code>| mstats avg(_value) WHERE metric_name=&quot;data.value&quot; AND index=&quot;my_metrics&quot; BY data.team
</code></pre>
<p>Currently, I am using the <code>msearch</code> and then filtering out the events, so my query is something like the one below but its too slow as I am pulling all the events:</p>
<pre><code>| msearch index=my_metrics
| fields &quot;metrics_name:data.value&quot;
| where mvcount(mvfilter(tonumber(metrics_name:data.value') &gt; 0)) &gt;= 1 OR isnull('metrics_name:data.value')
</code></pre>",,1,0,,2020-8-13 13:10:16,,2020-8-14 00:17:06,,,,,2753526.0,,1,1,splunk,248,9
695,259151,63407175,Splunk extract all values from array field,"<p>I have log entries containing counts per country in format:</p>
<pre><code>Map(USA -&gt; 1234, CAN -&gt; 5678, GBR -&gt; 9012, FRA -&gt; 3456)
Map(USA -&gt; 1238, CAN -&gt; 5692, GBR -&gt; 9024, FRA -&gt; 3478)
...
</code></pre>
<p>I want to make a timechart in Splunk with one series per country.</p>
<p>This is what I tried:</p>
<pre><code>| rex &quot;Map\((?&lt;countries&gt;([A-Z]+\ -&gt;\ \d+(,\ )?)*)\)&quot; | rex field=countries max_match=50 &quot;(?&lt;countries&gt;[A-Z]+\ -&gt;\ \d+)(,\ )?&quot; | table _time countries
</code></pre>
<p>That works great and gives us two columns:</p>
<ol>
<li>time</li>
<li>counts per country - array of entries in format &quot;USA -&gt; 1234&quot;</li>
</ol>
<p>Then I tried to create a timechart, I replaced:</p>
<pre><code>| table _time countries
</code></pre>
<p>with:</p>
<pre><code>| rex field=countries (?&lt;country&gt;[A-Z]+)\ -&gt;\ (?&lt;count&gt;\d+) | timechart count by country limit=0
</code></pre>
<p>But the result is that all counts are taken from the last country in the list.</p>
<p>How can I extract the counts per country for all items in array?</p>",63409667.0,1,0,,2020-8-14 05:40:48,,2020-8-14 08:57:59,,,,,1878731.0,,1,1,logging|charts|splunk|data-extraction,375,12
696,259152,63413608,Splunk: combine fields from multiple lines,"<h2>Context</h2>
<p>Say I have logs structured this way</p>
<pre class=""lang-sh prettyprint-override""><code>TID: http-incoming-972453 &gt;&gt; POST /token HTTP/1.1 {org.apache.synapse.transport.http.headers} # I want this
TID: http-incoming-972453 &gt;&gt; Accept: application/json {org.apache.synapse.transport.http.headers}
TID: http-incoming-972453 &gt;&gt; Host: some.organization.com {org.apache.synapse.transport.http.headers}
.....
TID: http-outgoing-8816 &gt;&gt; POST /oauth2/token HTTP/1.1 {org.apache.synapse.transport.http.headers}
TID: http-outgoing-8816 &gt;&gt; Content-Type: application/x-www-form-urlencoded {org.apache.synapse.transport.http.headers}
TID: http-outgoing-8816 &gt;&gt; Transfer-Encoding: chunked {org.apache.synapse.transport.http.headers}
TID: http-outgoing-8816 &gt;&gt; Host: some.other.organization.intra:9444 {org.apache.synapse.transport.http.headers}
TID: http-outgoing-8816 &gt;&gt; Connection: Keep-Alive {org.apache.synapse.transport.http.headers}
TID: http-outgoing-8816 &gt;&gt; User-Agent: Synapse-PT-HttpComponents-NIO {org.apache.synapse.transport.http.headers}
TID: http-outgoing-8816 &lt;&lt; HTTP/1.1 200 OK {org.apache.synapse.transport.http.headers}
.....
TID: http-incoming-972453 &lt;&lt; HTTP/1.1 200 OK {org.apache.synapse.transport.http.headers} # with this
TID: http-incoming-972453 &lt;&lt; X-Frame-Options: DENY {org.apache.synapse.transport.http.headers}
.....
</code></pre>
<p>and I have tuned <code>props.conf</code> so that</p>
<pre class=""lang-sh prettyprint-override""><code>TID: http-incoming-972453 &gt;&gt; POST /token HTTP/1.1 {org.apache.synapse.transport.http.headers}
</code></pre>
<p>ends up indexed with the following fields</p>
<ul>
<li><strong>httpRequestId</strong>: <code>972453</code></li>
<li><strong>ressourceName</strong>: <code>/token</code></li>
</ul>
<p>and</p>
<pre class=""lang-sh prettyprint-override""><code>TID: http-incoming-972453 &lt;&lt; HTTP/1.1 200 OK {org.apache.synapse.transport.http.headers}
</code></pre>
<p>with</p>
<ul>
<li><strong>httpRequestId</strong>: <code>972453</code></li>
<li><strong>httpStatus</strong>: <code>200</code></li>
</ul>
<p>I am looking for a way to count requests, aggregated by <em>httpStatus</em> and <em>ressourceName</em> using <em>httpRequestId</em> as a join</p>
<h2>Attempts</h2>
<p>Since information about <code>ressourceName</code> and <code>httpStatus</code> occur on different events, I thought of using a <strong>join</strong>. This does not give any results</p>
<pre class=""lang-sql prettyprint-override""><code>index=* role=&quot;gw&quot; httpAction=&quot;incoming&quot; | join type=outer httpRequestId [fields ressourceName,httpStatus] | stats count by ressourceName,httpStatus
</code></pre>
<p>While reading Splunk documentation, I also came across <code>selfjoin</code>, results of which where only partial</p>
<pre class=""lang-sql prettyprint-override""><code>index=* role=&quot;gw&quot; httpAction=&quot;incoming&quot; | selfjoin httpRequestId | stats count by ressourceName,httpStatus
</code></pre>
<p>How can I combine fields from multiple events to end up with something like</p>
<pre><code>/somewhere           200         30
/somewhere           403         1
/somewhere/else      200         15
</code></pre>",63440002.0,2,0,,2020-8-14 13:21:14,,2020-8-16 17:31:32,,,,,7648881.0,,1,0,splunk|splunk-query,422,12
697,259153,63472594,3D Graph Network Topology Unknown search command fit,"<p>I am new to splunk so I will try to be as clear as possible. I wanted to test the visualization of networkx graphs in Splunk 3D Graph Network Topology App. I was able to load the csv file of the graph successfully and I can see the data and the graph visualization. However, when I run community detection algorithm, it shows me the following error:</p>
<p>Unknown search command: 'fit'</p>
<p>Can somebody help me fix the issue please, thanks.</p>",63474350.0,1,0,,2020-8-18 16:12:12,,2020-8-18 18:11:07,,,,,9069109.0,,1,1,graph|visualization|splunk,120,9
698,259154,63472790,Dataflow template - Splunk not reading data from topic,"<p>I am using the Dataflow template that streams data from pubsub topic to splunk. I have followed the steps below from pubsub side</p>
<ol>
<li>Create a topic</li>
<li>create a subscription</li>
<li>create a sink that exports log to the topic</li>
<li>give the service account permissions of publisher</li>
</ol>
<p>For the Dataflow template</p>
<ol>
<li>give topic subscription name</li>
<li>HEC endpoint</li>
<li>HEC token base64 encoded</li>
<li>create a service account that has the role/worker data, role/pubsub.reader, role/project viewer</li>
<li>Disable SSL -&gt; true</li>
<li>vpc network</li>
<li>subnetwork name</li>
<li>create job</li>
</ol>
<p>The worker machine spins up. The workflow is running but splunk is not pulling data from the topic. Is there anything particular I should check?</p>
<p>Thank you in advance!!</p>",63490313.0,1,4,,2020-8-18 16:24:52,,2020-8-19 15:25:59,,,,,7877828.0,,1,1,google-cloud-platform|google-cloud-pubsub|splunk,90,9
699,259155,63479320,Local Log Analyzer Tool,"<p>At work we have Splunk in Production. That is a powerful tool that generates timecharts, statistics, tables, groupings from logs in real time.
However sometimes I have to analyse logs that are not indexed in splunk, hence I spend a lot of time using grep, awk, sed and excel (to plot graphs).</p>
<p>Do you know any tool that can be easily installed on my local machine and offers features to analyze logs like Splunk?</p>",63482649.0,1,0,,2020-8-19 02:51:49,,2020-8-19 08:08:58,,,,,1626057.0,,1,0,logging|splunk|log-analysis,43,8
700,259156,63487576,How to get splunk report for different time ranges,"<p>I am running a splunk query for a date range. It is working fine. I want to run the same query for different date ranges. Lets say 1day, 7days and a month.
Example query which running for a day:</p>
<pre><code> index=&quot;a&quot; env=&quot;test&quot; MachineIdentifier source=&quot;D:\\Inetpub\\Logs\\app*.log&quot; earliest=-2d latest=-1d 
| top limit=50 MachineIdentifier
| sort MachineIdentifier asc
</code></pre>
<p>Currently I am running this query for different date ranges by modifying &quot;earliest&quot; and &quot;latest&quot; values and exporting it for consolidation.</p>
<p>I want to prepare a single query which gives this data for 1day, 7day etc in a single report. Is it possible?</p>
<p><strong>EDIT:</strong></p>
<p>Figured out this query but I am not able to get percentage details like above query. How to show percentage details in the results.</p>
<pre><code>index=&quot;a&quot; env=&quot;test&quot; MachineIdentifier source=&quot;D:\\Inetpub\\Logs\\app*.log&quot;  earliest=-2d@d latest=-1d@d 
|fields MachineIdentifier | eval marker=&quot;1DayData&quot; 
| append 
[search index=&quot;a&quot; env=&quot;test&quot; MachineIdentifier source=&quot;D:\\Inetpub\\Logs\\app*.log&quot;  earliest=-3d@d latest=-1d@d 
|fields MachineIdentifier | eval marker=&quot;2DaysData&quot;] 
| stats count(eval(marker=&quot;1DayData&quot;)) AS 1DayCount, count(eval(marker=&quot;2DaysData&quot;)) AS 2DaysCount by MachineIdentifier
</code></pre>",,2,0,,2020-8-19 13:02:31,,2020-8-20 05:53:14,2020-8-20 05:53:14,,2060349.0,,2060349.0,,1,0,splunk|splunk-query,625,13
701,259157,63491962,"In Splunk , how to set fixed colour for bars in column chart?","<p>In Splunk, how do I set a fixed colour for bars in column chart?</p>",,1,0,,2020-8-19 17:18:01,,2021-2-23 16:12:13,2021-2-23 16:12:13,,4418.0,,2377082.0,,1,0,splunk|splunk-dashboard,601,13
702,259158,63498623,Splunk - regex extract fields from source,"<p>I am trying to extract the job name , region from Splunk source using regex .</p>
<p>Below is the format of my sample source :</p>
<pre><code>/home/app/abc/logs/20200817/job_DAILY_HR_REPORT_44414_USA_log
</code></pre>
<p>With the below , I am able to extract job name :</p>
<pre><code>(?&lt;logdir&gt;\/[\W\w]+\/[\W\w]+\/)(?&lt;date&gt;[^\/]+)\/job_(?&lt;jobname&gt;.+)_\d+
</code></pre>
<p>Here is the match so far :</p>
<pre><code>Full match  0-53    /home/app/abc/logs/20200817/job_DAILY_HR_REPORT_44414
Group `logdir`  0-19    /home/app/abc/logs/
Group `date`    19-27   20200817
Group `jobname` 32-47   DAILY_HR_REPORT
</code></pre>
<p>I also need USA (region) from the source . Can you please help suggest.
Region will always appear after number field (44414) , which can vary in number of digits.
Ex: 123, 1234, 56789</p>
<p>Thank you in advance.</p>",63499402.0,1,1,,2020-8-20 05:05:10,,2020-8-20 06:31:11,,,,,6808782.0,,1,0,regex|splunk,517,12
703,259159,63504071,Splunk AWS ALB logs not properly parsing,"<p>I'm trying to ingest my <strong>AWS ALB</strong> logs into <strong>Splunk</strong>. After all, I could search my ALB logs in Splunk. But still the events are not properly parsing. Did anyone had similar issue or any suggestion?</p>
<p>Here is my <strong>prop.conf</strong></p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>[aws:alb:accesslogs]
SHOULD_LINEMERGE=false
FIELD_DELIMITER = whitespace
pulldown_type=true
FIELD_NAMES=type,timestamp,elb,client_ip,client_port,target,request_processing_time,target_processing_time,response_processing_time,elb_status_code,target_status_code,received_bytes,sent_bytes,request,user_agent,ssl_cipher,ssl_protocol,target_group_arn,trace_id
EXTRACT-elb = ^\s*(?P&lt;type&gt;[^\s]+)\s+(?P&lt;timestamp&gt;[^\s]+)\s+(?P&lt;elb&gt;[^\s]+)\s+(?P&lt;client_ip&gt;[0-9.]+):(?P&lt;client_port&gt;\d+)\s+(?P&lt;target&gt;[^\s]+)\s+(?P&lt;request_processing_time&gt;[^\s]+)\s+(?P&lt;target_processing_time&gt;[^\s]+)\s+(?P&lt;response_processing_time&gt;[^\s]+)\s+(?P&lt;elb_status_code&gt;[\d-]+)\s+(?P&lt;target_status_code&gt;[\d-]+)\s+(?P&lt;received_bytes&gt;\d+)\s+(?P&lt;sent_bytes&gt;\d+)\s+""(?P&lt;request&gt;.+)""\s+""(?P&lt;user_agent&gt;.+)""\s+(?P&lt;ssl_cipher&gt;[-\w]+)\s*(?P&lt;ssl_protocol&gt;[-\w\.]+)\s+(?P&lt;target_group_arn&gt;[^\s]+)\s+(?P&lt;trace_id&gt;[^\s]+)
EVAL-rtt = request_processing_time + target_processing_time + response_processing_time</code></pre>
</div>
</div>
</p>
<p><strong>Sample data</strong></p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>https 2020-08-20T12:40:00.274478Z app/my-aws-alb/e7538073dd1a6fd8 162.158.26.188:21098 172.0.51.37:80 0.000 0.004 0.000 405 405 974 424 ""POST https://my-aws-alb-domain:443/api/ps/fpx/callback HTTP/1.1"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.2840.91 Safari/537.36"" ECDHE-RSA-AES128-GCM-SHA256 TLSv1.2 arn:aws:elasticloadbalancing:ap-southeast-1:111111111111:targetgroup/my-aws-target-group/41dbd234b301e3d84 ""Root=1-5f3e6f20-3fdasdsfffdsf"" ""api.mydomain.com"" ""arn:aws:acm:ap-southeast-1:11111111111:certificate/be4344424-a40f-416e-8434c-88a8a3b072f5"" 0 2020-08-20T12:40:00.270000Z ""forward"" ""-"" ""-"" ""172.0.51.37:80"" ""405"" ""-"" ""-""</code></pre>
</div>
</div>
</p>",,1,10,,2020-8-20 11:28:50,,2020-8-20 17:37:34,2020-8-20 16:44:16,,6482719.0,,6482719.0,,1,1,amazon-elb|splunk|splunk-query,175,11
704,259160,63536430,Regex separate IP:Port from a log,"<p>I have below simple regex expressions that works pretty well to split the given sample log. This would provides separate groups of object where I could access with $1 $2 $3 ... etc. I'm using this in Splunk.</p>
<p>Eg.</p>
<pre><code>$1 = https
$2 = 2020-08-20T12:40:00.274478Z
$3 = app/my-aws-alb/e7538073dd1a6fd8
</code></pre>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+?)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)(.*?\s+)</code></pre>
</div>
</div>
</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>https 2020-08-20T12:40:00.274478Z app/my-aws-alb/e7538073dd1a6fd8 162.158.26.188:21098 172.0.51.37:80 0.000 0.004 0.000 405 405 974 424 ""POST https://my-aws-alb-domain:443/api/ps/fpx/callback HTTP/1.1"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.2840.91 Safari/537.36"" ECDHE-RSA-AES128-GCM-SHA256 TLSv1.2 arn:aws:elasticloadbalancing:ap-southeast-1:111111111111:targetgroup/my-aws-target-group/41dbd234b301e3d84 ""Root=1-5f3e6f20-3fdasdsfffdsf"" ""api.mydomain.com"" ""arn:aws:acm:ap-southeast-1:11111111111:certificate/be4344424-a40f-416e-8434c-88a8a3b072f5"" 0 2020-08-20T12:40:00.270000Z ""forward"" ""-"" ""-"" ""172.0.51.37:80"" ""405"" ""-"" ""-""</code></pre>
</div>
</div>
</p>
<p>The problem here is, I want to separate IP:Port into separate group. There are multiple places which have the IP:Port. Those I need as a separate group like other object.</p>
<p>Eg.</p>
<pre><code>$4 = 162.158.26.188
$5 = 21098 
$6 = 172.0.51.37
$7 = 80
</code></pre>
<p>Can anyone help on this? Thank you!</p>",63562264.0,1,9,,2020-8-22 12:57:34,,2020-8-24 13:42:37,2020-8-22 15:27:58,,3832970.0,,6482719.0,,1,0,regex|splunk,54,7
705,259161,63561627,How do I multiply a column with different values element wise in splunk,"<p>I am new to splunk.<br/>
I have aggregated a column using 'by' statement now i want to multiply each element in the column with different elements element wise, say first element with 0.05 and rest all with 0.07.<br/>
Please help<br/>
<a href=""https://i.stack.imgur.com/mxu0c.png"" rel=""nofollow noreferrer"">enter image description here</a></p>",63571421.0,1,2,,2020-8-24 13:03:44,,2020-8-25 02:57:29,,,,,12822090.0,,1,0,splunk|splunk-query,37,6
706,259162,63593286,Splunk: Execute the same query on multiple datasources,"<p>i have multiple dabatases (&gt;100) with the identic structure.
For business-monitoring, i have about 80 queries which check information in the database.
Now, i want to execute each of this queries on each of this databases and load the result into splunk.
In splunk, is it possible to define the &gt;100 database-connections once and the 80 queries once and then make some &quot;magic&quot; step to execute each statement on each database?
I don't want to create a new connection for each combination of database and query.</p>",,1,0,,2020-8-26 08:09:20,,2020-8-26 12:44:13,2020-8-26 12:38:32,,4418.0,,9862226.0,,1,0,database|splunk|splunk-query|dbconnect|splunk-hec,307,10
707,259163,63598298,How do we get/extract log data from splunk,"<p>I'm having splunk with holding 3 months of log details getting refreshed after that (no history we can see after that), but my requirement is: I need to store that log details to another folder in splunk, which holds all the log info with history by dumping. Not sure how to extract data from splunk. Can we use any java code? or any API to extract the log data from splunk and store into another?</p>
<p>I'm new to splunk.</p>",,2,0,,2020-8-26 13:03:20,,2020-10-21 19:33:51,2020-10-21 19:33:51,,4420967.0,,13276568.0,,1,0,splunk,103,8
708,259164,63606365,"Splunk Metrics {""text"":""No data"",""code"":5} error","<p>I'm trying to send metrics to Splunk via HEC, here's my curl command:</p>
<pre><code>curl -k https://www.website.com                    \
-H &quot;Authorization: Splunk password&quot;       \
-d '{&quot;index&quot;:&quot;index-name&quot;,&quot;time&quot;: 313423232.000,&quot;event&quot;:&quot;metric&quot;,&quot;source&quot;:&quot;disk&quot;,&quot;sourcetype&quot;:&quot;perflog&quot;,
&quot;host&quot;:&quot;host_1.splunk.com&quot;,&quot;value&quot;:85,&quot;fields&quot;:{&quot;region&quot;:&quot;us-west-1&quot;,&quot;datacenter&quot;:&quot;dc1&quot;,&quot;rack&quot;:&quot;63&quot;,
&quot;os&quot;:&quot;Ubuntu16.10&quot;,&quot;arch&quot;:&quot;x64&quot;,&quot;team&quot;:&quot;LON&quot;,&quot;service&quot;:&quot;6&quot;,&quot;service_version&quot;:&quot;0&quot;,
&quot;service_environment&quot;:&quot;test&quot;,&quot;path&quot;:&quot;/dev/sda1&quot;,&quot;fstype&quot;:&quot;ext3&quot;,&quot;metric_name:cpu.idle&quot;: 13.34}}'
</code></pre>
<p>And in return i'm getting:
{&quot;text&quot;:&quot;No data&quot;,&quot;code&quot;:5}</p>
<p>This is weird because it was working earlier, all I added was &quot;value&quot;:85.
Could the issue be the website i'm sending to?</p>
<p>Some suggestions were mismatched quotes, and the fact that HEC only supports a specific set of known fields, but i don't think that's the issue i'm having here because i tried running the same curl command with only index, source, time, host, sourcetype, and event and got the same error code.</p>",,2,1,,2020-8-26 22:04:13,,2021-12-2 12:45:24,,,,,11677453.0,,1,0,curl|metrics|splunk,870,12
709,259165,63639928,how to create a splunk CSV file?,"<p>build script to</p>
<p>a) check the size of all lookup files in splunk</p>
<p>b) if size exceeds a specific threshold download the csv file</p>
<p>c) delete the lookup file or recreate a dummy lookup file</p>",,2,2,,2020-8-28 19:34:56,,2021-12-3 06:12:17,,,,,14184465.0,,1,-2,csv|splunk,123,7
710,259166,63653536,Splunk cooked format from universal forwarder,"<p>I want to see what is being sent from Splunk universal forwarder using their &quot;cooked&quot; format version 3. Does anybody know how this format is encoded?</p>
<p>This is what I am currently seeing:</p>
<pre><code>&quot;--splunk-cooked-mode-v3--\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000e4a2da812b43\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u00008089\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000@\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0013__s2s_capabilities\u0000\u0000\u0000\u0000\u0014ack=0;compression=0\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0005_raw\u0000&quot;
</code></pre>",63657364.0,1,0,,2020-8-30 02:54:31,,2020-8-30 12:20:23,,,,,13153510.0,,1,0,splunk|splunk-sdk,192,11
711,259167,63659391,To find New error in server logs that was not present in logs in the past one week,"<p>I am looking to trigger an alert in splunk if a new error is there in server logs. New error is an error/s that was not present in server logs in the past one week. I have index for logs index=Serverlogs1.</p>
<p>Please help!</p>",,1,4,,2020-8-30 16:04:29,,2020-8-31 12:59:38,2020-8-31 05:29:53,,14191992.0,,14191992.0,,1,-2,logging|splunk|splunk-query|splunk-formula|splunk-calculation,109,7
712,259168,63665459,Get Previous Session data based on Session filter Splunk,"<p>I'm facing a problem in splunk like if i choose current session(2020) from filter then i should get the data of previous Session(2019).</p>
<p>I wrote a splunk query like :</p>
<pre><code>index=&quot;entab_due&quot; Session=2019 ClassName=&quot;* *&quot;
| eval n=(tonumber(Session)-1)
| where totalBalance &gt; 0 and Session = n
</code></pre>
<p>but i didn't get any result.</p>
<p>Problem  : Get the data of previous session after selecting Session from filter</p>
<p>Please help me to get the solution.</p>",,1,3,,2020-8-31 05:51:23,,2020-8-31 12:37:11,,,,,7165942.0,,1,0,splunk|splunk-query,23,5
713,259169,63672805,Configure fluentd to pick Jenkins job/application logs in a kubernetes cluster,"<p>I have a kubernetes cluster in which I have jenkins and spinnaker pods up and running. I need to implement a logging mechanism which collects and sends logs to splunk server. I chose to do so using fluentd. I have <strong>deployed a daemon set of fluentd</strong> to run on each node and collect logs from each node and send to splunk server.</p>
<p>It is working fine for logs that we see using &quot;kubectl logs&quot; or logs that come to stdout. However, I need to <strong>pick logs from a jenkins job</strong> (Console output of a jenkins job build). These logs are not going to std out of the node, and are stored at <strong>/var/jenkins_home/jobs/XXX/builds/&lt;buildno</strong>&gt; inside the container storage which is <strong>not directly accessible</strong> to fluentd for log collection.</p>
<p>I am open for any kind of solution to this problem. Please suggest.</p>",,2,0,,2020-8-31 14:32:49,,2020-9-15 12:55:54,,,,,4879659.0,,1,0,jenkins|logging|kubernetes|splunk|fluentd,706,11
714,259170,63684952,Python3 : Records not getting pushed to Splunk,"<p>I have created a custom class, which push my logs to splunk, but somehow it is not working. Here is the class.</p>
<pre><code>class Splunk(logging.StreamHandler):
  def __init__(self, url, token):
    super().__init__()
    self.url = url
    self.headers = {f'Authorization': f'Splunk {token}'}
    self.propagate = False

  def emit(self, record):
    mydata = dict()
    mydata['sourcetype'] = 'mysourcetype'
    mydata['event'] = record.__dict__
    response = requests.post(self.url, data=json.dumps(mydata), headers=self.headers)
    return response
</code></pre>
<p>I call the class from my logger class, somehow like this (adding additional handler), so that it can log on console along with send to splunk</p>
<pre><code>if splunk_config is not None:
    splunk_handler = Splunk(splunk_config[&quot;url&quot;], splunk_config[&quot;token&quot;])
    self.default_logger.addHandler(splunk_handler)
</code></pre>
<p>But somehow, I am not able to see any logs in splunk. Though I can see the logs in console.
When I try to run the strip down version of above logic from python3 terminal, it is successful.</p>
<pre><code>import requests
import json 

url = 'myurl'
token = 'mytoken'
headers = {'Authorization': 'Splunk mytoken'}
propagate = False
mydata = dict()
mydata['sourcetype'] = 'mysourcetype'
mydata['event'] = {'name': 'root', 'msg': 'this is a sample message'}
response = requests.post(url, data=json.dumps(mydata), headers=headers)
print(response.text)
</code></pre>
<p>Things I have already tried, making my dictionary data as JSON serializable using below link but it didn't helped.</p>
<p><a href=""https://pynative.com/make-python-class-json-serializable/"" rel=""nofollow noreferrer"">https://pynative.com/make-python-class-json-serializable/</a></p>
<p>Any other things to try ?</p>",,1,1,,2020-9-1 09:26:23,,2020-9-1 15:26:09,,,,,8522104.0,,1,0,python|python-3.x|python-requests|python-3.6|splunk,192,10
715,259171,63689944,Group events by multiple fields in Splunk,"<p>Hi I have some events in splunk which are of this form-</p>
<p><strong>Location</strong>: some value(same value can be there in multiple events)</p>
<p><strong>Client</strong>: some value(same value can be there in multiple events)</p>
<p><strong>TransactionNumber</strong>: some value(Unique for each event)</p>
<p><strong>Transaction Time</strong>: some value(Unique for each event)</p>
<p>Now I want a table in this form -</p>
<p><a href=""https://i.stack.imgur.com/qbmOq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qbmOq.png"" alt=""Table"" /></a></p>
<p>Basically each location can have multiple clients and each client can have different transactions. Transaction number and transaction time are unique and have one to one mapping.</p>
<p>I am using this query in splunk-</p>
<p><strong>| stats list(TransactionNumber) list(TransactionTime) by Location Client</strong></p>
<p>What's happening is I am getting unique combination of location and client but what I want is unique clients to be listed against a particular Location.</p>
<p>This is what i am getting-</p>
<p><a href=""https://i.stack.imgur.com/0X5za.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0X5za.png"" alt=""enter image description here"" /></a></p>
<p>How can the query be modified to achieve the same?</p>",63690486.0,2,0,,2020-9-1 14:42:11,,2020-9-8 20:05:47,,,,,10955333.0,,1,0,splunk|splunk-query|splunk-calculation,3660,16
716,259172,63690853,Splunk dashboard - token in search query,"<p>I'm new to Splunk, and I'm investigating the structure of an XML dashboard. I see the following in the code:</p>
<pre><code>    &lt;input id=&quot;input&quot; type=&quot;dropdown&quot; token=&quot;serve&quot; searchWhenChanged=&quot;false&quot;&gt;
      &lt;label&gt;Serve&lt;/label&gt;
      &lt;fieldForLabel&gt;serve&lt;/fieldForLabel&gt;
      &lt;fieldForValue&gt;name&lt;/fieldForValue&gt;
      &lt;search id=&quot;search_serve&quot;&gt;
        &lt;query&gt;$custom_search$&lt;/query&gt;
        &lt;earliest&gt;&lt;/earliest&gt;
        &lt;latest&gt;&lt;/latest&gt;
      &lt;/search&gt;
    &lt;/input&gt;
</code></pre>
<p>Searching through the complete XML file, I do not see the</p>
<pre><code>$custom_search$
</code></pre>
<p>token listed anywhere. Is there another location where this token can be defined?</p>",,1,0,,2020-9-1 15:31:32,,2020-9-1 21:07:14,,,,,5550142.0,,1,0,splunk,287,9
717,259173,63691642,Readiness Probe for Docker Splunk inside Kubernetes?,"<p>The docker-splunk image has an added layer of complexity because it has the ansible configurator doing the initial setup.  Ansible even restarts the splunk program as part of the setup.</p>
<p>I'm having trouble thinking of an appropriate kubernetes readiness probe.  TCP passes as soon as it gets a valid return.  But the ansible playbooks need at least another 10 minutes before they're finished.</p>
<p>I currently use an initial delay, but I want something smarter.  I'm thinking a command type probe that will look for when the ansible playbooks are complete.  But I don't know where to look.</p>
<p>I guess this means I have to learn ansible now.</p>",,1,0,,2020-9-1 16:20:49,,2020-9-1 18:00:06,,,,,14014251.0,,1,1,docker|kubernetes|ansible|splunk,54,8
718,259174,63696647,Splunk - Extracted number from search result not showing up in the table,"<p>My splunk result looks like this:</p>
<pre><code>9/1/20
5:00:14.487 PM  
2020-09-01 16:00:14.487, 'TOTALITEMS'=&quot;Number of items registered in the last 2 hours &quot;, COUNT(*)=&quot;1339&quot;
</code></pre>
<p>I am trying to table the number that appears in the end in quotes.</p>
<pre><code>index=my_db sourcetype=no_of_items_registered source=P_No_of_items_registered_2hours | rex field=_raw &quot;\&quot;Number of items registered in the last 2 hours \&quot;, COUNT(\*)=\&quot;(?P&lt;itm_ct&gt;\d+)\&quot;$&quot; | table itm_ct
</code></pre>
<p>This displays a blank table without any numbers. The number of rows in the table however matches the the number of events.</p>
<p>Any help much appreciated</p>",,1,0,,2020-9-1 23:16:53,,2020-9-2 11:19:58,,,,,9616763.0,,1,0,splunk|splunk-query,24,6
719,259175,63697046,How to only extract match strings from a multi-value field and display in new column in SPLUNK Query,"<p>i am trying to extract matched strings from the multivalue field and display in another column. I have tried various options to split the field by delimiter and then mvexpand and then user where/search to pull those data. I was trying to find if there is an easier way to do this without all this hassle in SPLUNK query.</p>
<p><strong>Example:</strong> Lets say i have below multi-value column1 field with data separated by delimiter comma</p>
<blockquote>
<p><strong>column1</strong> = abc1,test1,test2,abctest1,mail,send,mail2,sendtest2,new,code,results</p>
</blockquote>
<p>I was splitting this column using delimiter <code>|eval column2=split(column1,&quot;,&quot;)</code> and using regex/where/search to search for data with <code>*test*</code> in this column and return results, where i was able to pull the results but the column1 still shows all the values <code>abc1,test1,test2,abctest1,mail,send,mail2,sendtest2,new,code,results </code>, what i want is either to trim1 <strong>column1</strong> to show only words match with <code>test</code> or show those entries in new <strong>column2</strong> which should only show this words <code>test1,test2,abctest1,sendtest2</code> as they were only matching <code>*test*</code>.</p>
<p>I would appreciate your help, thanks.</p>",,2,0,,2020-9-2 00:22:36,,2020-9-2 01:01:09,,,,,9465349.0,,1,0,splunk|splunk-query|splunk-sdk|splunk-formula,1865,14
720,259176,63709633,Splunk viewing '_value' not using mcatalog,"<p>According to Splunk, <code>| mcatalog values(_value) WHERE index=index-name</code> is not allowed. Is there another way to view <code>_value</code>'s for all the requests sent without using mcatalog?</p>
<p>P.S. i've looked into mstats but it only offers max, min, sum, etc...</p>",,1,0,,2020-9-2 16:22:09,,2020-9-8 17:03:11,,,,,11677453.0,,1,0,splunk,33,7
721,259177,63711236,How to generate report for the volume in splunk?,"<p>I need to generate the report of Heavy forwarder migrations based on volume of load. Run a query in splunk for host= in the below format
indices, volume. HF name</p>
<p>I am using a tstats search query for that getting the HF name and indices but not able to get the volume</p>
<p>My question is how to generate the volume?</p>",,1,0,,2020-9-2 18:04:57,,2020-9-2 20:57:08,,,,,14184465.0,,1,0,report|volume|splunk,24,5
722,259178,63717489,Splunk Query - Search unique exception from logs with counts,"<p>I want to search exceptions along with its occurrences. I would like see results in below format</p>
<pre><code>|Exception Name      |Count|
|NullPointerException|  2  |
|ConnectException    |  6  |
|MailConnectException|  10 |
</code></pre>
<p><strong>Logs looks like this -</strong></p>
<pre><code>- Caused by: java.lang.NullPointerException: null
- Caused by: com.sun.mail.util.MailConnectException: Couldn't connect to host, port: localhost, 25; timeout -1
- Caused by: java.net.ConnectException: Connection refused (Connection refused)
</code></pre>
<p><strong>Written below search query -</strong></p>
<pre><code>index=&quot;*zp0853-a*&quot; container_name=&quot;test-api&quot; &quot;*Caused by*&quot; (Showing all Exceptions list)
index=&quot;*zp0853-a*&quot; container_name=&quot;test-api&quot; &quot;*Caused by*&quot; | stats count (Showing only total counts)
</code></pre>",63724295.0,2,0,,2020-9-3 05:38:17,,2020-9-3 13:05:49,,,,,1881894.0,,1,0,splunk|splunk-query,474,13
723,259179,63727881,Setting up alerts for metrics in Splunk,"<p>I'm sending data to Splunk and everything is working just fine, i can see the data that i'm sending and run a query and get results. Right now I'm only using a test data set, but eventually people will be sending their own fields (as well as the mandatory ones). My question is, since I don't know what kind of data they will be sending, can I still set up alerts for them? Can I create something general?</p>",,1,0,,2020-9-3 16:25:06,,2020-9-3 20:32:21,,,,,11677453.0,,1,0,splunk,44,7
724,259180,63728098,Splunk: How to Compute Incident Duration Records?,"<p>I have the following events in Splunk:</p>
<pre><code>_time                           Agent_Hostname      alarm               status
2020-08-23T03:04:05.000-0700    m50-ups.a_domain    upsAlarmOnBypass    raised
2020-08-23T03:07:16.000-0700    m50-ups.a_domain    upsTrapOnBattery    raised
2020-08-23T03:07:16.000-0700    m50-ups.a_domain    upsAlarmInputBad    raised
2020-08-23T03:07:39.000-0700    m50-ups.a_domain    upsAlarmOnBypass    raised
2020-08-23T03:07:39.000-0700    m50-ups.a_domain    upsAlarmLowBattery  raised
2020-08-23T03:08:17.000-0700    m50-ups.a_domain    upsTrapOnBattery    raised
2020-08-23T03:09:24.000-0700    m50-ups.a_domain    upsTrapOnBattery    raised
2020-08-23T03:10:31.000-0700    m50-ups.a_domain    upsAlarmOnBattery   cleared
2020-08-23T03:10:32.000-0700    m50-ups.a_domain    upsAlarmInputBad    cleared
2020-08-23T03:11:12.000-0700    m50-ups.a_domain    upsAlarmLowBattery  cleared
2020-08-23T03:19:06.000-0700    m50-ups.a_domain    upsAlarmInputBad    raised
2020-08-23T03:19:06.000-0700    m50-ups.a_domain    upsTrapOnBattery    raised
2020-08-23T03:19:13.000-0700    m50-ups.a_domain    upsAlarmLowBattery  raised
2020-08-23T03:20:10.000-0700    m50-ups.a_domain    upsTrapOnBattery    raised
2020-08-23T03:21:16.000-0700    m50-ups.a_domain    upsTrapOnBattery    raised
2020-08-23T03:22:22.000-0700    m50-ups.a_domain    upsTrapOnBattery    raised
2020-08-23T03:23:29.000-0700    m50-ups.a_domain    upsTrapOnBattery    raised
2020-08-23T03:24:28.000-0700    m50-ups.a_domain    upsAlarmInputBad    cleared
2020-08-23T03:24:28.000-0700    m50-ups.a_domain    upsAlarmOnBattery   cleared
2020-08-23T03:25:09.000-0700    m50-ups.a_domain    upsAlarmLowBattery  cleared
2020-08-23T03:25:58.000-0700    m50-ups.a_domain    upsAlarmOnBypass    cleared
</code></pre>
<p>My problem is how to compute records of incidents' duration for each host and each alarm type, for example,
from the above events I'd have the following through algorithm not just hard-codini the values in the particular examlpe:</p>
<pre><code>start                        end                          Agent_Hostname   alarm
2020-08-23T03:04:05.000-0700 2020-08-23T03:25:58.000-0700 m50-ups.a_domain upsAlarmOnBypass
2020-08-23T03:07:16.000-0700                              m50-ups.a_domain upsTrapOnBattery
2020-08-23T03:07:16.000-0700 2020-08-23T03:24:28.000-0700 m50-ups.a_domain upsAlarmInputBad
2020-08-23T03:07:39.000-0700 2020-08-23T03:25:09.000-0700 m50-ups.a_domain upsAlarmLowBattery
</code></pre>
<p>where start is the earliest time when an alarm for a host is first raised, and
end is the time when the same alarm/host is cleared.</p>
<p>My second problem is how to find the biggest span of duration among those enclosed spans, ignoring those without end time.</p>
<p>My question is how I can achieve within the framework of Splunk?</p>",63733438.0,1,0,,2020-9-3 16:38:58,,2020-9-4 00:47:36,2020-9-3 23:48:39,,126164.0,,126164.0,,1,1,splunk|splunk-query|splunk-formula|splunk-calculation,43,7
725,259181,63740213,I need help to shorten or simplify one sed command,"<p>I like to convert <code>(4)wdcp(9)microsoft(3)com(0)</code> to <code>wdcp.microsoft.com</code>  using sed (only) as part of a <code>rex</code> in Splunk</p>
<p>So remove first and last parentheses with number, and replace middle one with a dot.</p>
<p>I do manage to do it using this two long an ugly commands.</p>
<pre><code>echo &quot;(4)wdcp(9)microsoft(3)com(0)&quot; | sed -r 's/(^\([0-9]+\)|\([0-9]+\)$)//g;s/\([0-9]+\)/./g'
wdcp.microsoft.com
</code></pre>
<p>Can this be simplified and hopefully shorten to one command and not two</p>
<p>PS, there may be more than three part url.  eks <a href=""http://www.microsoft.co.uk"" rel=""nofollow noreferrer"">www.microsoft.co.uk</a></p>",,1,5,,2020-9-4 11:22:39,,2020-9-6 04:46:44,2020-9-4 13:17:36,,4418.0,,2341847.0,,1,0,sed|splunk,61,7
726,259182,63754859,Splunk inputlookup and result extraction,"<p>I'm trying to do a domain search and check to see if a domain in the CSV was visited but I'm getting nothing.</p>
<p>What I'm using:</p>
<pre><code>index=&quot;suricata&quot; sourcetype=&quot;suricata:dns&quot; dns.answers{}.rrname=&quot;*&quot;
[ | inputlookup domains.csv 
| rename bad_domain as dest_ip | fields dest_ip ]
</code></pre>",,1,0,,2020-9-5 14:12:58,,2020-9-5 16:20:44,,,,,2690155.0,,1,0,splunk|splunk-query,214,10
727,259183,63757192,Splunk query do not return value for both columns together,"<p>The Splunk query below returns only the calc_string column and returns blank for the index column, if I remove the ln4 and ln5 it returns index. How can I make this query to return both columns Please advise??</p>
<blockquote>
<p>index=vehicle*
sourcetype=info_ssl
splunk_server_group=ALL
| stats values(xx) as XX values(yy) as YY<br />
| eval calc_string=if(isnull(XX), YY, XX)
| table index calc_string<br />
| sort index</p>
</blockquote>",,1,0,,2020-9-5 18:22:25,,2020-9-7 21:55:32,2020-9-7 15:36:32,,9336348.0,,9336348.0,,1,0,splunk-query,93,8
728,259184,63796801,Splunk limits the results returned by stats list() function,"<p>I have a splunk query which returns a <strong>list</strong> of values for a particular field. The number of values can be far more than 100 but the number of results returned are limited to 100 rows and the warning that I get is this-</p>
<p><em><strong>'stats' command: limit for values of field 'FieldX' reached. Some values may have been truncated or ignored.</strong></em></p>
<p>The query in question can be as simple as this -</p>
<p><em><strong>| stats list(FieldX)</strong></em></p>
<p>Please note that I can't use <em><strong>table FieldX</strong></em>  since I want the results to be grouped based on another field. Also I can't use <em><strong>stats values(FieldX)</strong></em> since I am extracting 2 fields from an event and these fields have one to one mapping, if I use <em><strong>stats values()</strong></em>, the order is messed up.</p>
<p>I tried <em><strong>stats list(values) limit=500</strong></em> but it isn't helping. How can I have all the results returned?</p>",63798262.0,3,4,,2020-9-8 15:12:43,,2020-9-8 20:02:54,2020-9-8 19:27:41,,10955333.0,,10955333.0,,1,0,splunk|splunk-query|splunk-sdk,1404,16
729,259185,63817832,SPLUNK HOSTING ON DIFFERENT,<p>How do we check the availability of each host on Splunk</p>,,1,0,,2020-9-9 18:50:07,,2020-9-9 23:07:02,,,,,14250112.0,,1,-6,azure-devops|splunk,33,5
730,259186,63826857,Splunk query to retrieve value from json log event and get it in a table,"<p>I have a log event getting in a json format like this</p>
<pre><code>{
   &quot;level&quot;:&quot;level  name&quot;,
   &quot;exception&quot;:&quot;exception message&quot;,
   &quot;logger&quot;:&quot;com.log&quot;,
   &quot;thread&quot;:&quot;thread name&quot;,
   &quot;message&quot;:&quot;exception message&quot;,
   &quot;properties&quot;:{
      &quot;id&quot;:&quot;1234&quot;,
      &quot;process&quot;:&quot;Process name,
      &quot;host&quot;:&quot;host name&quot;,
      &quot;type&quot;:&quot;type name&quot;
   }
}
</code></pre>
<p>I need a splunk query to get host inside properties as a value to get it in a table. Please help me.</p>",,2,0,,2020-9-10 09:34:50,,2020-9-11 14:01:37,2020-9-10 09:35:55,,8335151.0,,3800421.0,,1,2,json|splunk|splunk-query|splunk-hec,564,12
731,259187,63832075,Looking for a solution to ingest Pega cloud service logs to Splunk using Splunk addons for AWS,"<p>I am looking for a solution to ingest pega cloud service logs to Splunk. I cam across to approaches push and pull. With Push option splunk has blueprint lamdbda which can be used to push events to Splunk HTTP Event Collector (HEC). I am not finding any clear solution for pull approach. Can some one summarize for which scenarios pull will work and for which scenarios push will work.</p>
<p>Splunk and Pega Cloud services are on different vpc , how we can have secure data transfer between them.</p>",,1,0,,2020-9-10 14:39:10,,2020-9-16 20:12:32,,,,,5148938.0,,1,0,splunk|pega,55,6
732,259188,63836452,I don't know how to make query for extract unknown sender in splunk,"<p>There is SMTP and DNS traffic in <code>sourcetype</code>. I want to extract any unknown sender's email in SMPT based on DNS traffic.</p>
<p>So, I'm trying to get the sender email that does not belong to the DNS host, but I don't know how to do it.</p>
<pre><code>index=payload sourcetype=&quot;stream:smtp&quot;
| rex field=sender_mail &quot;\@&lt;?host&gt;.*&quot;
[ search index=payload sourcetype=&quot;stream:dns&quot; NOT host ]
</code></pre>",63838005.0,1,0,,2020-9-10 19:27:40,,2020-9-11 18:18:15,2020-9-11 18:18:15,,10871073.0,,11230064.0,,1,0,splunk,29,6
733,259189,63846135,Docker container monitor with splunk,"<p>I have a docker service for my application running on Google Cloud VM.
On the same VM, i have installed splunk to monitor the created container.</p>
<p>I had gone over some tutorials and was first testing to push container metrics via HTTP Event collector by a hello-world container.
I used the following command:</p>
<pre><code>sudo docker run --log-driver=splunk \
       --log-opt splunk-url=http://34.121.xx.xxx:8088 \
       --log-opt splunk-token=xxx-xxx-xxx-xxx-xxx \
       --log-opt splunk-insecureskipverify=true \
       hello-world
</code></pre>
<p>But it fails to create that container and gives the error:</p>
<pre><code>docker: Error response from daemon: failed to initialize logging driver: Options http://34.121.xx.xxx:8088/services/collector/event/1.0: read tcp 10.128.x.x:39404-&gt;34.121.xx.xxx:8088: read: connection reset by peer.ERRO[0001] error waiting for container: context canceled 
</code></pre>
<p>10.128.x.x is the Internal IP of the VM.</p>
<p>I am not sure why its happening like this. I am new to this. Can anyone help ? AM i missing some config ?</p>",,1,0,,2020-9-11 11:25:57,,2020-9-11 20:12:45,2020-9-11 20:12:45,,8672459.0,,11387022.0,,1,0,docker|containers|splunk,51,6
734,259190,63852649,"How to ingest db logs into splunk? I need only logs to be Warning , Error , Critical , ORA-* logs. any sample prop changes would be appreciated","<p>I have the host details list and log directories. Keywords I need to get log for to be ingested into splunk on the below list
Warning , Error , Critical , ORA-* logs</p>",,1,0,,2020-9-11 18:44:52,,2020-9-12 07:35:05,,,,,14184465.0,,1,-2,oracle|logging|splunk|ingest|dbconnect,115,8
735,259191,63853526,How to get data from _raw in Splunk,"<p>I have this search query:</p>
<pre><code>index=&quot;abc&quot; |search SomeInfo | table _raw
</code></pre>
<p>and it returns table with results in one column in this format:</p>
<pre><code>2020-09-10 15:57:24,479 [the_value_i_need] INFO  java.class.name:52 - SomeInfo|NAME=NAME1 
2020-09-10 16:57:33,479 [the_value_i_need] INFO  java.class.name:52 - SomeInfo|NAME=NAME1 
2020-09-10 17:58:24,479 [the_value_i_need] INFO  java.class.name:52 - SomeInfo|NAME=NAME1 
</code></pre>
<p>Now, how do I get value &quot;the_value_i_need&quot; in square brackets[] and display it in the table instead of showing everything from _raw?</p>",63855576.0,1,0,,2020-9-11 19:55:23,,2020-9-11 23:53:51,,,,,4113623.0,,1,0,splunk|splunk-query,555,12
736,259192,63855344,Where does the Splunk Deployment Server store its own checksum data?,"<p>This is a great mystery to me.</p>
<p>A Splunk Deployment Server tracks the changes it makes to the Deployment Client by comparing checksums.  You can see in splunkd.log on the client side:</p>
<p><code>Checksum mismatch 0 &lt;&gt; 12612942278184057003 for app=myapp</code></p>
<p>If there is a mismatch, the Deployment Server decides to send an updated version of its app to the client.  The checksum is then written to $SPLUNK_HOME/var/run/serverclass.xml for later use.</p>
<p>My question is where does the Deployment Server store its own copy? I was expecting there to be a similar xml style file, but I haven't found it yet.</p>
<p>The only place I have seen data related to this is in splunkd.log on the server side, during the creation time of the actual checksum.  This log entry then gets indexed into _internal.</p>
<p>Does this mean the Splunk DS searches _internal to determine if its client is up to date? If so, what happens when the index gets rolled? Does it just re-deploy its apps after it realizes it forgot which clients had which checksum?</p>
<p>Or does the Deployment Server hold this checksum data in memory only?  This doesn't seem so good in the event of an outage, right?</p>",,1,0,,2020-9-11 23:11:52,,2020-9-13 21:25:17,,,,,14014251.0,,1,0,checksum|splunk,361,10
737,259193,63865799,Filter access logs on Splunk,"<p>SplunkForwarder is used in order to provide the Apache's access log to Splunk (or was told by DevOps so). AFAIK it's not possible to filter out logs based on given regEx -- the <em>ISSUE</em> that I'm trying to solve. Was thinking to add a trigger on Apache that will intercept all requests and send a message to Splunk if the URL pattern of the request is in the whitelist (have found Splunk HTTP Event Collector - never used before - smells like a part of the solution).
Tried to find a proper example of how to use mod_actions module. Unfortunately, didn't find anything that works for me. The Apache's documentation is useless. Not sure whether mod_actions is the only option I have.
Pls, could you tell me how can I execute py/sh/pl or any other script for each request? Likely, DevOps won't allow me to add any exotic (non-standard) module to Apache.
Thanks a lot.</p>",63875781.0,1,0,,2020-9-12 23:31:29,,2020-9-13 23:09:32,2020-9-12 23:36:29,,1148770.0,,1148770.0,,1,0,python|bash|apache|perl|splunk,414,12
738,259194,63872359,Logging Function results from an API call - Javascript,"<p>I've written a function that makes an API call using AWS lambda and then logs the function result to Splunk.  My issue is that for some reason the result is not being logged.  I think it needs to be converted to an async function, but I'm struggling to convert it being a JS newbie.  Thanks in advance for the help.</p>
<pre><code>const detailedNotification = function() {
     https.get(newUrl, options, (res) =&gt; {
    let body = &quot;&quot;;

    res.on(&quot;data&quot;, (chunk) =&gt; {
        body += chunk;
        const result = JSON.parse(body);
        console.log(result);
    });

    res.on(&quot;end&quot;, () =&gt; {
        let json = JSON.parse(body);
        let output = json.items;
        //console.log(output);
        
    });

}).on(&quot;error&quot;, (error) =&gt; {
    console.error(error.message);
});
};

detailedNotification();
</code></pre>",,0,2,,2020-9-13 15:15:59,,2020-9-13 15:15:59,,,,,4738983.0,,1,0,javascript|aws-lambda|splunk,31,5
739,259195,63882771,Find out huge log statementsusing splunk,"<p>I have an existing application where there are multiple application flows in it.</p>
<p>All the flows are of JMS messaging flows - where different system exchanges messages of queue.</p>
<p>I want to find out the huge logger statements from the log - which are like more than 10 lines or so.</p>
<p><strong>What i tried - I tried using patterns tab where splunk tells us what are repetitive patterns.</strong></p>
<p>I am good with repetitive patterns - but i want to find out logger statements which are huge  in size.</p>
<p>So - is it possible to find out such log statements which are longer/bigger</p>
<p>thank you in advance</p>",63885264.0,1,0,,2020-9-14 10:47:13,,2020-9-14 16:50:54,,,,,4911291.0,,1,0,splunk|splunk-query,50,8
740,259196,63887068,What is the recommended way to write to Splunk using Log4J,"<p>What is the recommended way to write to Splunk using Log4J? And is there any official documentation on it such as what the log4j config file should look like?</p>
<p>If it matters, my source is a MuleSoft/Java 8 application.</p>",,2,0,,2020-9-14 15:09:40,,2020-9-15 12:17:28,,,,,5274.0,,1,-2,log4j|splunk,1092,12
741,259197,63905387,Splunk query reference field in joined data,"<p><em>Full disclosure, I am very new Splunk so I may explain my question incorrectly.</em></p>
<p>I have two data sources and was given a query to pull data from them individually. I am trying to join this data together so I can create some type of chart, but I am unsure of this would be a join/search etc.</p>
<p>My initial query is as follows:</p>
<p>This allows me to search through the mail logs by sender address and show all emails with a <code>bcSendAction=1</code>, which is a successful send.</p>
<p><code>index=mail sourcetype=barracuda [search index=mail sourcetype=barracuda bcSender=&quot;someemail@domain.com&quot; | table  bcMsgId] bcSendAction=1</code></p>
<p>The result of this search is as follows:</p>
<p><a href=""https://i.stack.imgur.com/Hm8Ky.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hm8Ky.png"" alt=""enter image description here"" /></a></p>
<p>Now, my other search is a log that shows all of the sender email addresses during a certain time period. I would like to use the result of this (the email value) in the first search so that I don't have to hard-code the <code>bcSender</code>, but rather have it use the results from the other source.</p>
<pre><code>// Returns an email address
index=mail sourcetype=sendmail_syslog *@sfdc.net |  
rex field=from &quot;&lt;(?&lt;from&gt;.*)&gt;&quot; | 
table from | dedup from
</code></pre>
<p>I was able to parse the log and pull out just the email addresses that I want to use to plug into my first search.</p>
<p>I followed a few emails and tutorials, but a lot of the joins I was seeing only used two different sources/datasets and didn't use the <code>search</code> as I did in my first query.</p>
<p>My attempt at this was something like:</p>
<pre><code>index=mail sourcetype=sendmail_syslog *@sfdc.net 
|  rex field=from &quot;&lt;(?&lt;from&gt;.*)&gt;&quot; 
| table from | dedup from 
|  join from 
    [search index=mail sourcetype=barracuda [search index=mail sourcetype=barracuda bcSender=from | table  bcMsgId] bcSendAction=1]
</code></pre>
<p>I don't know that I am referencing the email from the first result set correctly.
Can someone point me in the right direction with how to approach this search?</p>",63909847.0,2,0,,2020-9-15 15:43:19,1.0,2020-9-15 21:12:33,,,,,2628921.0,,1,1,splunk|splunk-query|splunk-formula,685,13
742,259198,63909930,logs to splunk are getting truncated,"<p>I am using fluentd to forward my Kubernetes pod logs to splunk but in splunk I am not able to see full length of pod log as they getting truncated. For example we have a single line log length of 74286 chars, but splunk shows only 16385 chars.
what can I do to overcome this issue ?</p>
<p>This way I have configured in fluentd configmap.</p>
<pre><code>&lt;match **&gt;
      @id splunk
      @type splunk-hec
      @log_level info
      server &quot;#{ENV['FLUENT_SPLUNK_HOST']}&quot;
      protocol https
      verify false
      host &quot;#{ENV['CLUSTER_NAME']}_#{ENV['NODE_NAME']}&quot;
      token &quot;#{ENV['FLUENT_SPLUNK_TOKEN']}&quot;
      index &quot;#{ENV['SPLUNK_INDEX']}&quot;
      buffer_type memory
      buffer_queue_limit 256
      buffer_chunk_limit 8m
      batch_size_limit 8000000
      flush_interval 1s
    &lt;/match&gt;
</code></pre>",,1,0,,2020-9-15 21:13:30,,2020-9-15 23:03:27,2020-9-15 21:37:02,,11606772.0,,11606772.0,,1,0,splunk|fluentd,319,11
743,259199,63928008,Splunk base search on dashboard and post processing the results,"<p>I have a dashboard that is using a base search, along with 4 other panels that reference this and format the results differently depending on the chart I want to use.</p>
<p>When I run the base query by itself, it returns the data as expected.</p>
<p><strong>Base Query:</strong></p>
<pre><code>index=mail sourcetype=barracuda bcProcess=&quot;outbound/smtp&quot; 
    [ search index=mail sourcetype=barracuda 
        [ search index=mail sourcetype=sendmail_syslog msgid=&quot;&lt;*@sfdc.net&gt;&quot; 
           | rex field=from &quot;&lt;(?&lt;bcSender&gt;.*)&gt;&quot; 
           | stats count by bcSender 
           | fields bcSender 
           | format 
        ] 
      | stats count by bcMsgId 
      | fields bcMsgId
    ]
</code></pre>
<p>In one panel, I am showing a single, total number sent as follows:</p>
<pre><code>&lt;search base=&quot;main_results&quot;&gt;
  &lt;query&gt;
   | stats count(bcMsgId) as total
  &lt;/query&gt;
&lt;/search&gt;
        
</code></pre>
<p>Same with another panel that shows it hourly using a line chart:</p>
<pre><code>&lt;search base=&quot;main_results&quot;&gt;
  &lt;query&gt;
   | timechart span=1h count AS &quot;Total Sends&quot;
  &lt;/query&gt;
&lt;/search&gt;
</code></pre>
<p>Both of the above panels work just fine when referencing the base query.</p>
<hr />
<p>The problem I am running into is on a pie-chart.</p>
<pre><code>&lt;panel&gt;
      &lt;chart&gt;
        &lt;title&gt;Send Action Breakdown&lt;/title&gt;
        &lt;search base=&quot;main_results&quot;&gt;
          &lt;query&gt;| rename bcSendAction as &quot;Send Action&quot; 
| chart count as Total by &quot;Send Action&quot; 
| eval &quot;Send Action&quot;=&quot;Send Action&quot;.&quot; (&quot;.Total.&quot;)&quot; 
| replace 1 WITH &quot;Success&quot; , 2 WITH &quot;Block&quot; , 3 WITH &quot;Deferral&quot; IN &quot;Send Action&quot;&lt;/query&gt;
        &lt;/search&gt;
        &lt;option name=&quot;charting.chart&quot;&gt;pie&lt;/option&gt;
        &lt;option name=&quot;charting.drilldown&quot;&gt;none&lt;/option&gt;
        &lt;option name=&quot;height&quot;&gt;460&lt;/option&gt;
        &lt;option name=&quot;refresh.display&quot;&gt;progressbar&lt;/option&gt;
        &lt;option name=&quot;charting.chart.showPercent&quot;&gt;true&lt;/option&gt;
      &lt;/chart&gt;
    &lt;/panel&gt;
</code></pre>
<p>When the dashboard tries to load this panel, it always returns &quot;No results found&quot;.
However, if I copy the base query into a search, and then paste the query from this panel right below it, I get results as expected.</p>
<p><a href=""https://i.stack.imgur.com/8c2Zt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8c2Zt.png"" alt=""enter image description here"" /></a></p>
<p><strong>Question:</strong></p>
<p>Why would this panel using the same base query having issues getting the data when I can manually paste both parts and it runs fine?</p>
<p><strong>Update for Bounty Clarity:</strong>
My dashboard has 4 panels, and 3 of them pretty much use an identical search query which is why I was trying to get the base search set up so they could all reference it.</p>
<p>Here are my 4 separate searches for the 4 panels if it helps with showing how I was trying to split it up for my base to function correctly.</p>
<pre><code>// Total Emails Sent
index=mail sourcetype=barracuda bcProcess=&quot;outbound/smtp&quot; 
            [ search index=mail sourcetype=barracuda 
              [ search index=mail sourcetype=sendmail_syslog msgid=&quot;&lt;*@sfdc.net&gt;&quot; 
                | rex field=from &quot;&lt;(?&lt;bcSender&gt;.*)&gt;&quot; 
                | stats count as Total by bcSender 
                | fields bcSender 
                | format 
              ] 
              | stats count as Total by bcMsgId 
              | fields bcMsgId, bcSendAction 
            ]
            | stats count(bcMsgId) as total


// Emails per hour
index=mail sourcetype=barracuda bcProcess=&quot;outbound/smtp&quot; 
            [ search index=mail sourcetype=barracuda 
              [ search index=mail sourcetype=sendmail_syslog msgid=&quot;&lt;*@sfdc.net&gt;&quot; 
                | rex field=from &quot;&lt;(?&lt;bcSender&gt;.*)&gt;&quot; 
                | stats count as Total by bcSender 
                | fields bcSender 
                | format 
              ] 
              | stats count as Total by bcMsgId 
              | fields bcMsgId, bcSendAction 
            ]
            | bin _time as hour span=1h
| stats count as hourcount by hour
| eval hour=strftime(hour,&quot;%H:%M&quot;)
| chart sum(hourcount) as count by hour



// Top 10 Senders
index=mail sourcetype=sendmail_syslog msgid=&quot;&lt;*@sfdc.net&gt;&quot;         
            | rex field=from &quot;&lt;(?&lt;bcSender&gt;.*)&gt;&quot;          
            | stats count as Total by bcSender
            | rename bcSender as &quot;From Address&quot;
            | sort -Total | head 10



// Action Breakdown
index=mail sourcetype=barracuda bcProcess=&quot;outbound/smtp&quot; 
            [ search index=mail sourcetype=barracuda 
              [ search index=mail sourcetype=sendmail_syslog msgid=&quot;&lt;*@sfdc.net&gt;&quot; 
                | rex field=from &quot;&lt;(?&lt;bcSender&gt;.*)&gt;&quot; 
                | stats count as Total by bcSender 
                | fields bcSender 
                | format 
              ] 
              | stats count as Total by bcMsgId 
              | fields bcMsgId, bcSendAction 
            ]
          | stats count as Total by bcSendAction
          | rename bcSendAction as Action
          | replace 1 WITH &quot;Success&quot; , 2 WITH &quot;Block&quot; , 3 WITH &quot;Deferral&quot; IN Action
          | eval &quot;Action&quot;=Action.&quot; (&quot;.Total.&quot;)&quot;
</code></pre>",,1,0,,2020-9-16 21:14:37,,2021-5-13 23:28:52,2020-9-24 15:15:27,,2628921.0,,2628921.0,,1,2,splunk|splunk-query|splunk-formula,982,12
744,259200,63931816,How to pass Splunk Token Value into Slack block( json Code )when alert trigger in Splunk,"<p>Any one how to Configure Webhook in Splunk by using the default option which is available in Splunk and how to add payload as there is no other options are available as shown below.
<a href=""https://i.stack.imgur.com/AQ5pv.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I am using Splunk App and able to get trigger alert notification inside my Slack Channel but i wanted to modify text format.</p>
<pre><code>Your Testing Alert alert matched 5 Events At 1600318446.014216
Please Click on below link to View Alert: http://Shailesh-Yadav:8000/app/search/alert?s=%2FservicesNS%2Fnobody%2Fsearch%2Fsaved%2Fsearches%2FTesting%2BAlert
Type of Alert : alert
Owner of Alert is : shaileshyadav
Trigger Date: 2020-09-17
Job Level: Testing Alert
Job Run Duration: 0.169
Job Search ID: scheduler__shaileshyadav__search__RMD511208c77f51c333d_at_1600318440_23
Priority:1
</code></pre>
<p>I have Checked and found that there is an option for message formatting.we Can use json payload and by using Curl command we Can send it to Webhook URL.
for example</p>
<pre><code>curl -X POST -H &quot;Content-Type: application/json&quot; -d @/Users/shaileshyadaav/Desktop/splunk.json https://hooks.slack.com/services/YFVGJDSF3784865/XXXXXX/XXXXXXXYYYYYYYYYY
</code></pre>
<p>Problem facing:how i Can pass the Splunk token value inside my json file on real time basis when event trigger(which is i am able to get by using Splunk APP but format is not as expected).</p>",,1,0,,2020-9-17 05:32:29,,2020-9-17 15:00:22,,,,,12487254.0,,1,0,python-3.x|webhooks|slack|slack-api|splunk,247,9
745,259201,63945800,Counting by table with splunk - consolidate like fields,"<p>I have the following <code>| stats count by HOST, USER, COMMAND | table HOST USER COMMAND count</code> and it gives me a list of what I expect, but I can't seem to figure out how to consolidate HOST and USER and just count how many commands there were so it's just one row.</p>
<p>I'm pretty sure I'm supposed to use list in some way but my results still don't seem to consolidate correctly. Any clues?</p>
<p>I'm trying like this:</p>
<pre><code> stats list(HOST) as HOST list(USER) as USER count(COMMAND) list(count) as count by COMMAND
</code></pre>",63958341.0,1,1,,2020-9-17 20:31:10,,2020-9-18 15:08:47,2020-9-17 20:43:39,,2690155.0,,2690155.0,,1,0,splunk|splunk-query,51,6
746,259202,63949580,how to send splunk email using outlook credentials,<p>I have splunk installed and currently I'm using SMTP server local host with port no-25. I have tried using gmail and its default port number. but I want to send email using Outlook credentials instead of gmail or smtp local host.</p>,,1,1,,2020-9-18 04:59:21,,2020-9-18 07:22:25,2020-9-18 06:23:28,,4420967.0,,9690369.0,,1,0,splunk,298,10
747,259203,63958906,statistics chart in splunk using value from log,"<p>I am new to Splunk dashboard. I need some help with this kind of data.</p>
<pre><code>2020-09-22 11:14:33.328+0100 org{abc}  INFO  3492 --- [hTaskExecutor-1] c.j.a.i.p.v.b.l.ReadFileStepListener     : [] read-feed-file-step ended with status exitCode=COMPLETED;exitDescription= with compositeReadCount 1 and other count status as: BatchStatus(readCount=198, multiEntityAccountCount=0, readMultiAccountEntityAdjustment=0, accountFilterSkipCount=7, broadRidgeFilterSkipCount=189, writeCount=2, taskCreationCount=4)
</code></pre>
<p>I wanted to have statistics in a dashboard showing all the integer values in the above log.</p>
<p><strong>Edit 1:</strong></p>
<p>I tried this but not working.</p>
<pre><code>index=abc xyz| rex field=string .*readCount=(?P&lt;readCount&gt;\d+)  | table readCount
</code></pre>",,2,2,,2020-9-18 15:46:13,,2020-9-24 14:35:16,2020-9-22 14:46:16,,7758110.0,,7758110.0,,1,0,splunk|splunk-query,91,7
748,259204,63960125,How to create alerts in Splunk programmatically through a Python script?,<p>I'm trying to write a Python script that takes user input and sends and creates the alerts in Splunk. Is this something that has been done before?</p>,,1,0,,2020-9-18 17:14:18,,2020-9-18 18:59:46,,,,,11677453.0,,1,0,splunk,230,9
749,259205,63982172,Create a static table in Splunk,"<p>I have a tiny table of data that I want to display as a reference on a dashboard - something like:</p>
<pre><code>date | val1 | val2
9/16/2020 | 10 | 12
9/17/2020 | 11 | 14
9/18/2020 | 12 | 13
</code></pre>
<p>that I want to display as a line chart:</p>
<p><a href=""https://i.stack.imgur.com/aoVkL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aoVkL.png"" alt=""enter image description here"" /></a></p>
<p>I found this very convoluted way to construct it:</p>
<pre><code>| makeresults 
| eval testDay=&quot;9/16/2020&quot;
| eval testVal1=10
| eval testVal2=12
| append 
    [| makeresults 
    | eval testDay=&quot;9/17/2020&quot;
    | eval testVal1=11
    | eval testVal2=14
    ]
| append 
    [| makeresults 
    | eval testDay=&quot;9/18/2020&quot;
    | eval testVal1=12
    | eval testVal2=13
    ]
| chart first(testVal1), first(testVal2) over testDay
</code></pre>
<p>is there a simpler way? Perhaps something more like my little tabular syntax in the table at the beginning of the post? Or at least more like:</p>
<pre><code>val1 = [10,11,12]
</code></pre>",63983660.0,1,0,,2020-9-20 18:09:30,,2020-9-20 21:10:09,,,,,284529.0,,1,0,splunk,192,10
750,259206,63983877,Change marker shape in Splunk line chart,"<p>Though it's weirdly not in the UI, I can enable markers on a line chart with:</p>
<pre><code>&lt;option name=&quot;charting.chart.showMarkers&quot;&gt;true&lt;/option&gt;
</code></pre>
<p><a href=""https://i.stack.imgur.com/eocjM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eocjM.png"" alt=""enter image description here"" /></a></p>
<p>However, if there are multiple series, it's nice to have different marker shapes (square, triangle, etc.) on each series. Is this possible to do on a Splunk line chart?</p>",,1,0,,2020-9-20 21:31:34,,2020-9-22 14:31:56,,,,,284529.0,,1,0,splunk,66,7
751,259207,64014808,"Setup server level monitoring and alerting for Splunk Forwarder, Indexer and Search Head","<p>I want to setup server level monitoring and alerting for Splunk Forwarders, Indexers and Search Heads to monitor and alert if there are issues related to vCPU, RAM, Memory etc. from Linux command line.
Also I want to monitor from Linux command the version of Splunk forwarder/Splunk Enterprise along with other details like if log ingestion and log distribution is working properly or if there are any issues with other servers in a distributed environment.
Can someone please help.
Please note: I do not have access to Splunk GUI, I can only access these servers through Linux command line like a Splunk Admin.</p>
<p>Thanks</p>",,1,0,,2020-9-22 17:26:32,,2020-9-22 20:30:33,,,,,14319806.0,,1,-2,linux|monitoring|splunk,120,8
752,259208,64017500,extract filename out of raw data using regex,"<p>This is my raw data:</p>
<pre><code>h24-71-249-14.ca.shawcable.net - - [07/Mar/2004:22:29:13 - 0800] &quot;GET /icons/gnu-head-tiny.jpg HTTP/1.1&quot; 200 3049

h24-71-249-14.ca.shawcable.net - - [07/Mar/2004:22:29:13 - 0800] &quot;GET /icons/gnu-head-tiny HTTP/1.1&quot; 200 3049
</code></pre>
<p>I want to be able to extract a file's name from the URI (if there is any, if there is not - ignore). The file can be any filetype (jpg, png, txt, etc.)</p>
<p>This is what I have so far:</p>
<pre><code>(\&quot;+)(.*?)(\.\w{1,3})
</code></pre>
<p>I know it is probably not a good idea to start my string from <code>&quot;</code>, and it is probably the reason for my problem, so I'd like to get some help to fix my regex.</p>
<p>thank you!</p>",,2,3,,2020-9-22 20:49:47,,2020-9-23 15:17:17,2020-9-23 07:44:38,,14323570.0,,14323570.0,,1,2,regex|splunk,169,9
753,259209,64019431,How to join 2 tables in Splunk based on shared column?,"<p>I'm sending Splunk 2 datasets that look like:</p>
<p>Dataset 1:</p>
<pre><code> - first_name: David
 - middle_name: Foe
 - last_name: Creek
 - job_title: accountant
 - jobs_finished: 10
</code></pre>
<p>and Dataset 2:</p>
<pre><code> - first_name: Alexis
 - middle_name: Stu
 - last_name: Ronald
 - job_title: accountant
 - num_jobs_must_finish: 20
</code></pre>
<p>I'm trying to join the two datasets based on the 'job_title'. I want a the returned result to look like:</p>
<pre><code> - first_name: David
 - middle_name: Foe
 - last_name: Creek
 - job_title: accountant
 - jobs_finished: 10
 - num_jobs_must_finish: 20
</code></pre>
<p>Any ideas for what the search query needs to look like?
I'm doing:</p>
<pre><code>index=&quot;job_index&quot; middle_name=&quot;Foe&quot; | join job_title [search index=&quot;job_index&quot; middle_name=&quot;Stu&quot;]
</code></pre>",,1,1,,2020-9-23 00:38:26,,2020-9-23 19:00:11,,,,,11677453.0,,1,0,join|splunk|splunk-query,236,9
754,259210,64029454,Docker Container is disappeard after restart service,"<p>I stopped my docker container.
and i restart the service in linux (service docker restart)</p>
<p>after that my container is missing.
I typed <code>docker ps -a</code> but I can't find my splunk container.</p>
<p>I can find container directory and volume folder also.
also I have every folder in inspection result.
but I can't see when I type <code>docker ps -a</code></p>
<p>how can I restore it ?</p>
<p>this is my docker inspect result, when container is exist.</p>
<pre><code>    [
    {
        &quot;Id&quot;: &quot;9e12bb7b2f8a926c0b7852aa9f1bf957363f327d6de53778059bf0fc05711039&quot;,
        &quot;Created&quot;: &quot;2018-10-02T04:05:28.013507313Z&quot;,
        &quot;Path&quot;: &quot;/sbin/entrypoint.sh&quot;,
        &quot;Args&quot;: [
            &quot;start-service&quot;
        ],
        &quot;State&quot;: {
            &quot;Status&quot;: &quot;running&quot;,
            &quot;Running&quot;: true,
            &quot;Paused&quot;: false,
            &quot;Restarting&quot;: false,
            &quot;OOMKilled&quot;: false,
            &quot;Dead&quot;: false,
            &quot;Pid&quot;: 11513,
            &quot;ExitCode&quot;: 0,
            &quot;Error&quot;: &quot;&quot;,
            &quot;StartedAt&quot;: &quot;2020-09-22T05:22:09.939497539Z&quot;,
            &quot;FinishedAt&quot;: &quot;2020-09-22T05:20:05.99542747Z&quot;
        },
        &quot;Image&quot;: &quot;sha256:507021d7e77f9bdd337aeb47729a806162579f36e5d73b14f2a508e545adae72&quot;,
        &quot;ResolvConfPath&quot;: &quot;/storage/docker/containers/9e12bb7b2f8a926c0b7852aa9f1bf957363f327d6de53778059bf0fc05711039/resolv.conf&quot;,
        &quot;HostnamePath&quot;: &quot;/storage/docker/containers/9e12bb7b2f8a926c0b7852aa9f1bf957363f327d6de53778059bf0fc05711039/hostname&quot;,
        &quot;HostsPath&quot;: &quot;/storage/docker/containers/9e12bb7b2f8a926c0b7852aa9f1bf957363f327d6de53778059bf0fc05711039/hosts&quot;,
        &quot;LogPath&quot;: &quot;/storage/docker/containers/9e12bb7b2f8a926c0b7852aa9f1bf957363f327d6de53778059bf0fc05711039/9e12bb7b2f8a926c0b7852aa9f1bf957363f327d6de53778059bf0fc05711039-json.log&quot;,
        &quot;Name&quot;: &quot;/splunk&quot;,
        &quot;RestartCount&quot;: 0,
        &quot;Driver&quot;: &quot;overlay&quot;,
        &quot;MountLabel&quot;: &quot;&quot;,
        &quot;ProcessLabel&quot;: &quot;&quot;,
        &quot;AppArmorProfile&quot;: &quot;&quot;,
        &quot;ExecIDs&quot;: null,
        &quot;HostConfig&quot;: {
            &quot;Binds&quot;: null,
            &quot;ContainerIDFile&quot;: &quot;&quot;,
            &quot;LogConfig&quot;: {
                &quot;Type&quot;: &quot;json-file&quot;,
                &quot;Config&quot;: {}
            },
            &quot;NetworkMode&quot;: &quot;default&quot;,
            &quot;PortBindings&quot;: {
                &quot;8000/tcp&quot;: [
                    {
                        &quot;HostIp&quot;: &quot;&quot;,
                        &quot;HostPort&quot;: &quot;8000&quot;
                    }
                ],
                &quot;8089/tcp&quot;: [
                    {
                        &quot;HostIp&quot;: &quot;&quot;,
                        &quot;HostPort&quot;: &quot;8089&quot;
                    }
                ]
            },
            &quot;RestartPolicy&quot;: {
                &quot;Name&quot;: &quot;unless-stopped&quot;,
                &quot;MaximumRetryCount&quot;: 0
            },
            &quot;AutoRemove&quot;: false,
            &quot;VolumeDriver&quot;: &quot;&quot;,
            &quot;VolumesFrom&quot;: null,
            &quot;CapAdd&quot;: null,
            &quot;CapDrop&quot;: null,
            &quot;Dns&quot;: [],
            &quot;DnsOptions&quot;: [],
            &quot;DnsSearch&quot;: [],
            &quot;ExtraHosts&quot;: null,
            &quot;GroupAdd&quot;: null,
            &quot;IpcMode&quot;: &quot;&quot;,
            &quot;Cgroup&quot;: &quot;&quot;,
            &quot;Links&quot;: null,
            &quot;OomScoreAdj&quot;: 0,
            &quot;PidMode&quot;: &quot;&quot;,
            &quot;Privileged&quot;: false,
            &quot;PublishAllPorts&quot;: false,
            &quot;ReadonlyRootfs&quot;: false,
            &quot;SecurityOpt&quot;: null,
            &quot;UTSMode&quot;: &quot;&quot;,
            &quot;UsernsMode&quot;: &quot;&quot;,
            &quot;ShmSize&quot;: 67108864,
            &quot;Runtime&quot;: &quot;runc&quot;,
            &quot;ConsoleSize&quot;: [
                0,
                0
            ],
            &quot;Isolation&quot;: &quot;&quot;,
            &quot;CpuShares&quot;: 0,
            &quot;Memory&quot;: 0,
            &quot;NanoCpus&quot;: 0,
            &quot;CgroupParent&quot;: &quot;&quot;,
            &quot;BlkioWeight&quot;: 0,
            &quot;BlkioWeightDevice&quot;: null,
            &quot;BlkioDeviceReadBps&quot;: null,
            &quot;BlkioDeviceWriteBps&quot;: null,
            &quot;BlkioDeviceReadIOps&quot;: null,
            &quot;BlkioDeviceWriteIOps&quot;: null,
            &quot;CpuPeriod&quot;: 0,
            &quot;CpuQuota&quot;: 0,
            &quot;CpuRealtimePeriod&quot;: 0,
            &quot;CpuRealtimeRuntime&quot;: 0,
            &quot;CpusetCpus&quot;: &quot;&quot;,
            &quot;CpusetMems&quot;: &quot;&quot;,
            &quot;Devices&quot;: [],
            &quot;DeviceCgroupRules&quot;: null,
            &quot;DiskQuota&quot;: 0,
            &quot;KernelMemory&quot;: 0,
            &quot;MemoryReservation&quot;: 0,
            &quot;MemorySwap&quot;: 0,
            &quot;MemorySwappiness&quot;: -1,
            &quot;OomKillDisable&quot;: false,
            &quot;PidsLimit&quot;: 0,
            &quot;Ulimits&quot;: null,
            &quot;CpuCount&quot;: 0,
            &quot;CpuPercent&quot;: 0,
            &quot;IOMaximumIOps&quot;: 0,
            &quot;IOMaximumBandwidth&quot;: 0
        },
        &quot;GraphDriver&quot;: {
            &quot;Data&quot;: {
                &quot;LowerDir&quot;: &quot;/storage/docker/overlay/25444b7c23c5d0bf0a6d4d1563a587831b7c329e6623ee720e609a0a16787ef5/root&quot;,
                &quot;MergedDir&quot;: &quot;/storage/docker/overlay/53d6023089444b76af280ef1220a2851c1b5045167c471f5f60f9faa0389ccd3/merged&quot;,
                &quot;UpperDir&quot;: &quot;/storage/docker/overlay/53d6023089444b76af280ef1220a2851c1b5045167c471f5f60f9faa0389ccd3/upper&quot;,
                &quot;WorkDir&quot;: &quot;/storage/docker/overlay/53d6023089444b76af280ef1220a2851c1b5045167c471f5f60f9faa0389ccd3/work&quot;
            },
            &quot;Name&quot;: &quot;overlay&quot;
        },
        &quot;Mounts&quot;: [
            {
                &quot;Type&quot;: &quot;volume&quot;,
                &quot;Name&quot;: &quot;cb9d0886db87a17304310bfa5a242972bf9c76464eb0c85c24902c7764443714&quot;,
                &quot;Source&quot;: &quot;/storage/docker/volumes/cb9d0886db87a17304310bfa5a242972bf9c76464eb0c85c24902c7764443714/_data&quot;,
                &quot;Destination&quot;: &quot;/opt/splunk/etc&quot;,
                &quot;Driver&quot;: &quot;local&quot;,
                &quot;Mode&quot;: &quot;&quot;,
                &quot;RW&quot;: true,
                &quot;Propagation&quot;: &quot;&quot;
            },
            {
                &quot;Type&quot;: &quot;volume&quot;,
                &quot;Name&quot;: &quot;5d637a70101b4d6a80261abfeac9534b8b77bde4669c6225dedf3a9bcfb4434a&quot;,
                &quot;Source&quot;: &quot;/storage/docker/volumes/5d637a70101b4d6a80261abfeac9534b8b77bde4669c6225dedf3a9bcfb4434a/_data&quot;,
                &quot;Destination&quot;: &quot;/opt/splunk/var&quot;,
                &quot;Driver&quot;: &quot;local&quot;,
                &quot;Mode&quot;: &quot;&quot;,
                &quot;RW&quot;: true,
                &quot;Propagation&quot;: &quot;&quot;
            }
        ],
        &quot;Config&quot;: {
            &quot;Hostname&quot;: &quot;9e12bb7b2f8a&quot;,
            &quot;Domainname&quot;: &quot;&quot;,
            &quot;User&quot;: &quot;&quot;,
            &quot;AttachStdin&quot;: false,
            &quot;AttachStdout&quot;: false,
            &quot;AttachStderr&quot;: false,
            &quot;ExposedPorts&quot;: {
                &quot;1514/tcp&quot;: {},
                &quot;8000/tcp&quot;: {},
                &quot;8088/tcp&quot;: {},
                &quot;8089/tcp&quot;: {},
                &quot;8191/tcp&quot;: {},
                &quot;9997/tcp&quot;: {}
            },
            &quot;Tty&quot;: false,
            &quot;OpenStdin&quot;: false,
            &quot;StdinOnce&quot;: false,
            &quot;Env&quot;: [
                &quot;SPLUNK_START_ARGS=--accept-license&quot;,
                &quot;SPLUNK_USER=root&quot;,
                &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;,
                &quot;SPLUNK_PRODUCT=splunk&quot;,
                &quot;SPLUNK_VERSION=7.0.0&quot;,
                &quot;SPLUNK_BUILD=c8a78efdd40f&quot;,
                &quot;SPLUNK_FILENAME=splunk-7.0.0-c8a78efdd40f-Linux-x86_64.tgz&quot;,
                &quot;SPLUNK_HOME=/opt/splunk&quot;,
                &quot;SPLUNK_APPS=/opt/splunk/etc/apps&quot;,
                &quot;SPLUNK_GROUP=splunk&quot;,
                &quot;SPLUNK_BACKUP_DEFAULT_ETC=/var/opt/splunk&quot;,
                &quot;LANG=en_US.utf8&quot;,
                &quot;JAVA_HOME=/opt/jdk/jdk1.8.0_152/&quot;
            ],
            &quot;Cmd&quot;: [
                &quot;start-service&quot;
            ],
            &quot;ArgsEscaped&quot;: true,
            &quot;Image&quot;: &quot;klustree/splunk:7.0.0&quot;,
            &quot;Volumes&quot;: {
                &quot;/opt/splunk/etc&quot;: {},
                &quot;/opt/splunk/var&quot;: {}
            },
            &quot;WorkingDir&quot;: &quot;/opt/splunk&quot;,
            &quot;Entrypoint&quot;: [
                &quot;/sbin/entrypoint.sh&quot;
            ],
            &quot;OnBuild&quot;: null,
            &quot;Labels&quot;: {}
        },
        &quot;NetworkSettings&quot;: {
            &quot;Bridge&quot;: &quot;&quot;,
            &quot;SandboxID&quot;: &quot;57cf9a7d9ec5d31e685fd5ea9232a3126fab1a90406449cc4d48b43a2f7c4f57&quot;,
            &quot;HairpinMode&quot;: false,
            &quot;LinkLocalIPv6Address&quot;: &quot;&quot;,
            &quot;LinkLocalIPv6PrefixLen&quot;: 0,
            &quot;Ports&quot;: {
                &quot;1514/tcp&quot;: null,
                &quot;8000/tcp&quot;: [
                    {
                        &quot;HostIp&quot;: &quot;0.0.0.0&quot;,
                        &quot;HostPort&quot;: &quot;8000&quot;
                    }
                ],
                &quot;8088/tcp&quot;: null,
                &quot;8089/tcp&quot;: [
                    {
                        &quot;HostIp&quot;: &quot;0.0.0.0&quot;,
                        &quot;HostPort&quot;: &quot;8089&quot;
                    }
                ],
                &quot;8191/tcp&quot;: null,
                &quot;9997/tcp&quot;: null
            },
            &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/57cf9a7d9ec5&quot;,
            &quot;SecondaryIPAddresses&quot;: null,
            &quot;SecondaryIPv6Addresses&quot;: null,
            &quot;EndpointID&quot;: &quot;b4d39d4b6615127cd6aaeda1e0e14e73d910fadf9b596d5b81698d3fc1c53945&quot;,
            &quot;Gateway&quot;: &quot;172.17.1.1&quot;,
            &quot;GlobalIPv6Address&quot;: &quot;&quot;,
            &quot;GlobalIPv6PrefixLen&quot;: 0,
            &quot;IPAddress&quot;: &quot;172.17.1.4&quot;,
            &quot;IPPrefixLen&quot;: 24,
            &quot;IPv6Gateway&quot;: &quot;&quot;,
            &quot;MacAddress&quot;: &quot;02:42:ac:11:01:04&quot;,
            &quot;Networks&quot;: {
                &quot;bridge&quot;: {
                    &quot;IPAMConfig&quot;: null,
                    &quot;Links&quot;: null,
                    &quot;Aliases&quot;: null,
                    &quot;NetworkID&quot;: &quot;c563f5e5d0e2e4bd6d35f5802395f77874679e40a522dabaee59b93d3747619c&quot;,
                    &quot;EndpointID&quot;: &quot;b4d39d4b6615127cd6aaeda1e0e14e73d910fadf9b596d5b81698d3fc1c53945&quot;,
                    &quot;Gateway&quot;: &quot;172.17.1.1&quot;,
                    &quot;IPAddress&quot;: &quot;172.17.1.4&quot;,
                    &quot;IPPrefixLen&quot;: 24,
                    &quot;IPv6Gateway&quot;: &quot;&quot;,
                    &quot;GlobalIPv6Address&quot;: &quot;&quot;,
                    &quot;GlobalIPv6PrefixLen&quot;: 0,
                    &quot;MacAddress&quot;: &quot;02:42:ac:11:01:04&quot;,
                    &quot;DriverOpts&quot;: null
                }
            }
        }
    }
]
</code></pre>
<p>please help...</p>",,1,1,,2020-9-23 13:49:03,,2020-9-24 05:17:12,2020-9-24 05:17:12,,7013263.0,,11416071.0,,1,0,docker|splunk,108,8
755,259211,64036167,how do I parse a JSON file to SPLUNK?,"<p>I want to load a JSON file of evetlogs to be the source_type of SPLUNK.
how do I parse it to something I can perform searches on?
thank you</p>",,1,0,,2020-9-23 21:02:01,,2020-9-24 04:28:48,,,,,14323570.0,,1,0,json|splunk,250,10
756,259212,64037837,Find mismatched records using shared field,"<p>I'm a Splunk newb and am wrestling with building a specific query. Any help would be appreciated.</p>
<p>This is the base search.</p>
<pre><code>index=index (Food=&quot;*meat*&quot;) AND (Food=&quot;*veggie*&quot; OR Food=&quot;*fruit*&quot;)
</code></pre>
<p>From here, I rex the result and sort.</p>
<pre><code>| rex field=_raw &quot;location \((?&lt;Farm&gt;\w+)\).*operator \((?&lt;Farmer&gt;\w+)\).*crops \((?&lt;CropType&gt;\w+)\).*&quot;
| stats list(Farm), list(CropType), count by Farmer
| sort -count
</code></pre>
<p>This is a good start, but it returns way more than needed, and I'm getting way out in the weeds with this.  Here's the details:</p>
<ul>
<li><strong>CropType</strong> contains two possible values, <em>veggie</em> or <em>fruit</em>.</li>
<li>I only want to see records where one veggie and one fruit share the same Farm.  (No fruit/fruit, veggie/veggie.)</li>
</ul>
<p>The end goal here is to present a list of farms and crops per farmer.</p>
<p>Thanks in advance for any assistance/insight.</p>",,1,1,,2020-9-24 00:07:57,,2020-9-24 04:35:08,,,,,14330458.0,,1,1,splunk|splunk-query,25,5
757,259213,64038353,Splunk convert Wed Sep 23 08:00:00 PDT 2020 to _time and epoch time in splunk,<p>Splunk convert Wed Sep 23 08:00:00 PDT 2020 to _time and epoch time in splunk . What is the splunk query to convert java date format to yyyy-MM-dd</p>,,2,0,,2020-9-24 01:35:52,,2020-9-24 13:14:12,,,,,4476314.0,,1,0,datetime|splunk,249,10
758,259214,64041128,Sending out multiple reports as one email in splunk,"<p>I have 2 different independent queries and I want to send out a single email having these 2 reports as PDFs. Is there a way to achieve this in splunk.</p>
<p>Edit- Tried putting the reports to a dashboard and then scheduling a pdf delivery but I don't see that option. It seems like it requires some extra permissions which won't be possible for me to get :(<a href=""https://i.stack.imgur.com/FLwoP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FLwoP.png"" alt=""enter image description here"" /></a></p>",64046925.0,1,0,,2020-9-24 07:03:16,,2020-9-30 16:31:18,2020-9-25 02:57:22,,10955333.0,,10955333.0,,1,1,reporting|splunk|splunk-query,431,12
759,259215,64054857,Splunk get inner Query results with in the time frame provided by outer Query,"<p>Successfully scheduled PushNotification in UserMessageChanelMap LINK_MORE_ACCOUNTS |eval fields=split(<em>raw,&quot;|&quot;) | eval messageKey</em> =mvindex(fields,2) |eval num=mvindex(fields,5) | table messageKey_, num | eval scheduledDate = replace(num, &quot;scheduledDate:&quot;, &quot;&quot;) | eval messageKey = replace(messageKey_,&quot;messageKey:&quot;,&quot;&quot;)  | eval  newTS=strftime(strptime(scheduledDate, &quot;%a %b %d %H:%M:%S %Z %Y&quot;), &quot;%Y-%m-%d %H:%M:%S&quot;) | stats count by newTS,messageKey | stats min(newTS) as fromScheduledDate, max(newTS) as toScheduledDate | appendcols [search  (  (&quot;<em>Could not send PushNotification</em>&quot;)  messageKey:LINK_MORE_ACCOUNTS  NOT (&quot;*|reason:Failed to Deliver|&quot;) | extract pairdelim=&quot;|&quot; kvdelim=&quot;:&quot;   | table userId,userMessageId,messageKey|  stats count  by userId,userMessageId,messageKey | table userId,userMessageId, messageKey | stats count as pushFallOffPoints by messageKey ]</p>
<p>Here I want to run my SubQuery with in the time range of fromScehduledDate  - toScehduledDate. I was trying to pass these dates to earliest and latest but that did not work. Help is appreciated .</p>",,2,1,,2020-9-24 21:45:32,,2020-9-25 17:14:26,,,,,4476314.0,,1,-1,splunk|splunk-query,88,7
760,259216,64067550,how to access labels and values of form multi select inputs in splunk,"<p>i have a multi select input and able to get the values but want to get all the label values.</p>
<pre><code>&lt;fieldset submitButton=&quot;false&quot; autoRun=&quot;true&quot;&gt;
&lt;input type=&quot;multiselect&quot; token=&quot;endpoint&quot; searchWhenChanged=&quot;true&quot;&gt;
  &lt;label&gt;Endpoints&lt;/label&gt;
  &lt;choice value=&quot;/project/api-aaa/v1.0&quot;&gt;api-aaa&lt;/choice&gt;
  &lt;choice value=&quot;/project/api-bbb/v1.0&quot;&gt;api-bbb&lt;/choice&gt;
  &lt;choice value=&quot;/project/api-ccc/v1.0&quot;&gt;api-ccc&lt;/choice&gt;
  &lt;delimiter&gt; OR &lt;/delimiter&gt;
  &lt;valuePrefix&gt;&quot;&lt;/valuePrefix&gt;
  &lt;valueSuffix&gt;&quot;&lt;/valueSuffix&gt;
  &lt;default&gt;/project/api-aaa/v1.0,/project/api-bbb/v1.0,/project/api-ccc/v1.0&lt;/default&gt;
  &lt;change&gt;
  &lt;set token=&quot;dropToken_label&quot;&gt;$label$&lt;/set&gt; // this is working partially and able to get first label which is `api-aaa`
  &lt;/change&gt;
&lt;/input&gt;
</code></pre>
  
<p><code>&lt;set token=&quot;dropToken_label&quot;&gt;$label$&lt;/set&gt;</code> // this is working partially and able to get first label which is <code>api-aaa</code></p>
<p>i need to access all the labels selected ?</p>",64090151.0,1,0,,2020-9-25 15:58:07,,2020-9-27 15:27:37,,,,,8208248.0,,1,0,splunk|splunk-query,275,11
761,259217,64135614,How to Build Splunk Search Query for below Scenario,"<p>I am able to get the multiple events (api's logs) in splunk dashboard like below</p>
<p><strong>event-1:</strong></p>
<pre><code>{ &quot;corrId&quot;:&quot;12345&quot;, &quot;traceId&quot;:&quot;srh-1&quot;, &quot;apiName&quot;:&quot;api1&quot; }
</code></pre>
<p><strong>event-2:</strong></p>
<pre><code>{ &quot;corrId&quot;:&quot;69863&quot;, &quot;traceId&quot;:&quot;srh-2&quot;, &quot;apiName&quot;:&quot;api2&quot; }
</code></pre>
<p><strong>event-3:</strong></p>
<pre><code>{ &quot;corrId&quot;:&quot;12345&quot;, &quot;traceId&quot;:&quot;srh-3&quot;, &quot;apiName&quot;:&quot;api3&quot; }
</code></pre>
<p>I want to retrieve <code>corrId</code> (ex:- &quot;corrId&quot;:&quot;12345&quot;) dynamically from one event (api log)by providing apiName and build splunk search query based on retrieved <code>corrId</code> value that means it will pull all the event logs which contains same <code>corrId</code> (<code>&quot;corrId&quot;:&quot;12345&quot;</code>).</p>
<p><strong>Output</strong></p>
<p>In above scenario expected results would be like below</p>
<p><strong>event-1:</strong></p>
<pre><code>{ &quot;corrId&quot;:&quot;12345&quot;, &quot;traceId&quot;:&quot;srh-1&quot;, &quot;apiName&quot;:&quot;api1&quot; }
</code></pre>
<p><strong>event-3:</strong></p>
<pre><code>{ &quot;corrId&quot;:&quot;12345&quot;, &quot;traceId&quot;:&quot;srh-3&quot;, &quot;apiName&quot;:&quot;api3&quot; }
</code></pre>
<p>I am new to splunk, please help me out here, how to fetch <code>&quot;corrId&quot;:&quot;12345&quot;</code> dynamically by providing other field like <code>apiName</code> and build Splunk search query based on that.</p>
<p>I have tried out like below, but to no luck.</p>
<pre><code>index = &quot;test_srh source=policy.log [ search index = &quot;test_srh source=policy.log | rex field=_raw &quot;apiName&quot;:|s+&quot;(?[^&quot;]+)&quot; | search name=&quot;api1&quot; | table corrId]
</code></pre>
<p>This query gives event-1 log only but we need all other events which contain same <code>corrId</code> (<code>&quot;corrId&quot;:&quot;12345&quot;</code>). Appreciate quick help here.</p>",,1,0,,2020-9-30 10:26:50,1.0,2020-10-6 21:54:28,2020-10-6 21:54:28,,5570138.0,,5720092.0,,1,1,splunk|splunk-query,198,9
762,259218,64141991,Splunk Addon builder alert action to store results in to a custom index,"<p>I am working on an addon to collect event results based on an alert and send it to an API endpoint. Once the response is a success the endpoint returns a success message in a JSON format and I Want to store it in a custom index and sourcetype.</p>
<p>I tried using the below code but the data is written to the Main index instead of my custom index. Is there a way to write the event into a custom index for an alert action build via the Splunk Add-on builder?</p>
<pre><code>helper.addevent(&quot;hello&quot;, sourcetype=&quot;customsource&quot;)
helper.addevent(&quot;world&quot;, sourcetype=&quot;customsource&quot;)
helper.writeevents(index=&quot;mycustomindex&quot;, host=&quot;localhost&quot;, source=&quot;localhost&quot;)
</code></pre>",64209174.0,1,5,,2020-9-30 16:36:27,,2020-10-5 13:03:19,2020-9-30 21:01:48,,4418.0,,5696733.0,,1,0,python|splunk|splunk-sdk|splunk-api,138,8
763,259219,64143963,Splunk command to check if current search is greater than x% of previous search,<p>I want to know how to write search query in Splunk in order to check if the current search is greater than 20% of previous search. I am getting events on a particular count every 10 min. I want to check if my current count (for the last 10 min) is greater than 20% of my previous count(for the last 20 min). I need to use subsearch to make the comparison. But not getting result though. Can anyone help ?</p>,,1,1,,2020-9-30 18:49:50,,2020-10-1 11:43:40,2020-10-1 11:43:40,,11680578.0,,14368666.0,,1,-1,splunk|splunk-query|splunk-formula|splunk-calculation,225,9
764,259220,64149760,How to configure eml data formatting in splunk,"<p>I have a large dataset of email(.eml).
And I don't know how to parse and configure sourcetype in splunk.
Please, anyone let me know.</p>
<p>Thanks in advance.</p>",64156815.0,1,0,,2020-10-1 05:50:17,,2020-10-1 13:51:06,,,,,11230064.0,,1,-2,splunk,67,7
765,259221,64159072,Combining fields in Splunk alert rules,"<p>In VictorOps' Alert Rule Engine it's easy to check whether a single field exists. <br />
E.g. <code>when azure.context.portalLink matches * using Wildcard</code> checks if this field exists <br />
(and allows us to set up an Azure portal link in the annotations).</p>
<p>However, there are three fields that may contain this portal link:
<a href=""https://i.stack.imgur.com/gr0A5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gr0A5.png"" alt=""enter image description here"" /></a></p>
<p>and these have similar implementations. Would it be possible to check something like</p>
<pre><code>When field1|field2|field3 matches * using Wildcard ?
</code></pre>",,0,4,,2020-10-1 16:00:28,,2020-10-27 09:47:59,2020-10-27 09:47:59,,14344821.0,,14344821.0,,1,0,regex|splunk|victorops,49,6
766,259222,64175664,"i have 3 columns , total count , pass count , and fail count , how do i write formula in SQL or SPL if fail count is > 8% its development error","<p>i have 3 columns , total count , pass count , and fail count , how do i write formula in SQL or SPL if fail count is &gt; 8% it is development error?</p>
<p>sample input -<br />
pass count - 3900 , 5500 , 2500<br />
total count - 3920, 5700, 3000<br />
fail count - 20 , 200 , 500<br />
formula for
if fail count is more then 8% its sends dev error.</p>",64176652.0,1,3,,2020-10-2 17:10:24,,2020-10-2 18:34:47,2020-10-2 18:27:01,,7607190.0,,14381077.0,,1,0,sql|splunk-query,40,6
767,259223,64180905,Configure email IDs in config in Splunk,"<p>Is there a way to configure the send to emails in a config file and call them in all Splunk alerts instead of defining emails in each and every alert?</p>
<p>If so, how?</p>",,1,0,,2020-10-3 05:01:35,,2020-10-5 14:09:24,2020-10-5 14:09:24,,4418.0,,14383190.0,,1,1,splunk,31,7
768,259224,64201508,Finding brute force attacks with splunk,"<p>I have a few login failures then a success for Administrator and this is what I have but it doesn't seem to be getting any results:</p>
<pre><code>source=WinEventLog:Security EventCode=4625 OR EventCode=4624 
 | bin _time span=5m as minute 
 | eval username=mvindex(Account_Name, 1)
 | stats count(Keywords) as Attempts,
 count(eval(match(Keywords,&quot;Audit Failure&quot;))) as Failed,
 count(eval(match(Keywords,&quot;Audit Success&quot;))) as Success by minute username
 | where Failed&gt;=2
 | stats dc(username) as Total by minute 
 | where Total&gt;3
</code></pre>
<p>Any ideas on a better way to find failed login attempts for a user and then a successful login?</p>",64211833.0,1,1,,2020-10-5 01:45:33,,2020-10-5 15:38:34,,,,,2690155.0,,1,0,splunk|splunk-query|intrusion-detection|splunk-formula|splunk-calculation,426,11
769,259225,64213812,How to search using a part of string in splunk and group by,"<p>I need to create a report to show the processing time of certain events in splunk and in order to do that I need to get get all the relevant events and group by a id.</p>
<p>My current splunk events are like</p>
<pre><code>{
  &quot;Timestamp&quot;: &quot;Mon Sep 01 18:19:42 CDT 2020&quot;,
  &quot;Id&quot;: &quot;567&quot;,
  &quot;Application&quot;: &quot;TEST&quot;
},
{
  &quot;Timestamp&quot;: &quot;Mon Sep 01 13:19:42 CDT 2020&quot;,
  &quot;Id&quot;: &quot;567-test-00-10&quot;,
  &quot;Application&quot;: &quot;TEST&quot;
},
{
  &quot;Timestamp&quot;: &quot;Mon Sep 01 10:19:42 CDT 2020&quot;,
  &quot;Id&quot;: &quot;567-test-03-10&quot;,
  &quot;Application&quot;: &quot;TEST&quot;
},
{
  &quot;Timestamp&quot;: &quot;Mon Sep 01 15:19:42 CDT 2020&quot;,
  &quot;Id&quot;: &quot;567-test-01-10&quot;,
  &quot;Application&quot;: &quot;TEST&quot;
},
{
  &quot;Timestamp&quot;: &quot;Mon Sep 01 08:19:42 CDT 2020&quot;,
  &quot;Id&quot;: &quot;567-test-02-10&quot;,
  &quot;Application&quot;: &quot;TEST&quot;
}
</code></pre>
<p>I need to get the latest and oldest timestamps to create the report and I am having difficulty grouping them by the id.</p>
<p>My idea is to get the first part of the id and group them together but I not able to achieve this.</p>
<p>I tried <code>basesearch |eval id= mvindex(split(id, &quot;-&quot;),0)  |stats last(Timestamp) as latestTime by id*</code> which isn't working.</p>
<p>I need to show <code>id,late(Timestamp),first(Timestamp)</code> in the report. I would really appreciate any help</p>",,1,1,,2020-10-5 17:55:53,,2021-4-26 01:50:50,2021-4-26 01:50:50,,7941251.0,,9340011.0,,1,0,search|contains|splunk,609,11
770,259226,64220005,How to do compound query with where clause in Splunk?,"<p>I have the following data as example:</p>
<p>I want to find all events whose locations having had temperature above a threshold, say 80F sometimes.</p>
<pre><code>Temperature=82.4, Location=xxx.165.152.17, Time=Wed Sep 16 07:43:01 PDT 2020, Type=UPS
Temperature=84.2, Location=xxx.165.152.48, Time=Wed Sep 16 07:43:01 PDT 2020, Type=UPS
Temperature=82.4, Location=xxx.165.154.21, Time=Wed Sep 16 07:43:01 PDT 2020, Type=UPS
Temperature=82.4, Location=xxx.165.162.22, Time=Wed Sep 16 07:43:01 PDT 2020, Type=UPS
Temperature=77.0, Location=xxx.165.164.17, Time=Wed Sep 16 07:43:01 PDT 2020, Type=UPS
Temperature=75.2, Location=xxx.165.170.17, Time=Wed Sep 16 07:43:01 PDT 2020, Type=UPS
Temperature=77.0, Location=xxx.165.208.12, Time=Wed Sep 16 07:43:01 PDT 2020, Type=UPS
Temperature=73.4, Location=xxx.165.224.20, Time=Wed Sep 16 07:43:01 PDT 2020, Type=UPS
Temperature=75.3, Location=xxx.165.52.13, Time=Wed Sep 16 07:47:01 PDT 2020, Type=TempSensor
Temperature=77.9, Location=xxx.165.52.14, Time=Wed Sep 16 07:47:01 PDT 2020, Type=TempSensor
Temperature=76.3, Location=xxx.165.54.24, Time=Wed Sep 16 07:47:01 PDT 2020, Type=TempSensor
Temperature=83.8, Location=xxx.165.48.20, Time=Wed Sep 16 07:47:01 PDT 2020, Type=TempSensor
Temperature=73.8, Location=xxx.165.36.21, Time=Wed Sep 16 07:47:01 PDT 2020, Type=TempSensor
</code></pre>
<p>I might first find the subsets of locations with whom the temperatures having been above the threshold with the following:</p>
<pre><code>| Temperature &gt; 80
| fields Location
| dedup Location
</code></pre>
<p>I'd call the locations outcome of the query &quot;hot_locations&quot;,</p>
<p>then I'd like to perform my eventual query:</p>
<pre><code>| Location IN hot_locations
</code></pre>
<p>My question is what's the syntax of Splunk query language to express the specification? That is, how to express the embedded query and use its values to perform the eventual query.</p>
<p>In pseudo code, it might be like the following:</p>
<pre><code>| let hot_locations = {| Temperature &gt; 80
| fields Location
| dedup Location}
| Location IN hot_locations
</code></pre>
<p>What's the proper expression for it?</p>
<p>If it helps, I need the logic in dashboard, I'm thinking that I might use input panel's variable to express the value of hot_locations.</p>",,2,0,,2020-10-6 05:44:55,,2020-12-30 17:04:29,2020-12-30 17:04:29,,13302.0,,126164.0,,1,0,splunk|splunk-query,158,9
771,259227,64232440,"Use sub-second precision on ""earliest"" in Splunk query","<p>I have a Splunk search string. If I add <strong>earliest=10/05/2020:23:59:58</strong>, the search string still works. However, if I changed that to earliest=10/05/2020:23:59:58:<strong>01</strong>, I got an error message say <strong>invalid value &quot;10/05/2020:23:59:58:01&quot; for time term 'earliest'</strong>. Does that mean Splunk's <strong>earliest</strong> parameter's precision is to second only? I cannot find the answer in their documents.</p>
<p>Thanks!</p>",64936207.0,1,1,,2020-10-6 19:18:29,,2020-11-20 20:09:56,2020-11-20 20:02:24,,4418.0,,6655562.0,,1,3,splunk|splunk-query|time-precision,167,10
772,259228,64238164,Regex for Masking Data in a JSON Array,"<p>I've got a requirement to mask the incoming JSON request inside an array using regular expression on Splunk indexer. The JSON data looks like this:</p>
<pre><code>{&quot;Name&quot;:[&quot;Jobs&quot;,&quot;Bill&quot;]}
</code></pre>
<p>I'm expected to mask the incoming data so that it looks like this:</p>
<pre><code>{&quot;Name&quot;:[&quot;******&quot;,&quot;******&quot;]}
</code></pre>
<p>And the regex I'm using to mask the data looks something like this:</p>
<pre><code>s/\&quot;Name\&quot;:\&quot;[^&quot;]*\&quot;/&quot;Name&quot;:&quot;******&quot;/g
</code></pre>
<p>But for some reason I'm unable to mask the JSON data. Could any of you good folks please help?</p>",64239727.0,1,2,,2020-10-7 06:10:36,,2020-10-7 08:00:00,,,,,14404564.0,,1,1,arrays|json|regex|splunk|data-masking,187,11
773,259229,64241350,How to write Splunk query to get first and last request time for each sources along with each source counts in a table output,"<p>I have the following Splunk table data from a Splunk query output.</p>
<pre><code>Source        RequestTime
SourceX     10/07/2020 04:03 AM  
SourceX     10/07/2020 07:15 AM 
SourceX     10/07/2020 11:19 AM
SourceY     10/07/2020 09:13 AM
SourceY     10/07/2020 11:09 AM
SourceY     10/07/2020 03:29 PM 
SourceY     10/07/2020 07:08 PM 
SourceZ     10/07/2020 09:43 AM 
SourceZ     10/07/2020 01:44 PM
SourceZ     10/07/2020 07:08 PM
SourceZ     10/07/2020 08:09 PM
</code></pre>
<p>Please help me to get the output as below table format.</p>
<pre><code>Source    SourceCount     StartTime              EndTime
SourceX      3         10/07/2020 04:03 AM    10/07/2020 11:19 AM 
SourceY      4         10/07/2020 09:13 AM    10/07/2020 07:08 PM 
SourceZ      4         10/07/2020 09:43 AM    10/07/2020 08:09 PM
</code></pre>",,1,0,,2020-10-7 09:42:12,,2020-11-20 20:11:11,,,,,2526830.0,,1,0,splunk|splunk-query,82,8
774,259230,64243233,Searching for a particular kind of field in Splunk,"<p>I'm trying to form a query for searching only specific fields, wherein there are numbers after a specific piece of text. To provide an example, I am currently using the following query:</p>
<p><code>host=&quot;xyz-*&quot; apple &quot;retry *&quot;</code></p>
<p>I have to find specific entries that have this in their result: <em>&quot;retry 1&quot;</em> or <em>&quot;retry 2&quot;</em> or <em>&quot;retry 3&quot;</em> etc. up to I don't know how many retries.</p>
<p>But the problem that I'm facing is that the above query is also displaying results which have <em>&quot;retry banana&quot;</em>, <em>&quot;retry mango&quot;</em>, etc. too.</p>
<p>Can someone help me, please?</p>
<p>Thanks</p>",64245369.0,1,0,,2020-10-7 11:39:37,,2020-10-7 13:44:35,,,,,12932163.0,,1,0,splunk|splunk-query,14,6
775,259231,64249711,SPLUNK enterprise i am trying to calculate results where if > 4% of failure is anomaly?,"<p>SPLUNK enterprise i am trying to calculate results where &gt; 4% of failure  is anomaly. is formula correct? to set anomaly ?(failcount and total count fields are numeric)</p>
<p>| inputlookup  sample.csv | eval isananomaly = if('Failcount' / 'Totalcount' * 100 &gt; 4 ,  1 , 0)</p>",64262356.0,2,2,,2020-10-7 17:54:49,,2020-10-8 12:17:55,2020-10-7 18:05:19,,14381077.0,,14381077.0,,1,2,splunk|splunk-query|splunk-formula,33,6
776,259232,64269848,Epoch time conversion to time in Splunk,"<p>I am uploading an XML in which one of the field is dailyTime. This dailyTime is an epoch time and i want to convert it into human readable time.</p>
<pre><code>&lt;globalView id=&quot;108&quot; version=&quot;17&quot; recordClassName=&quot;NormalizedEvent&quot; retention=&quot;0&quot; hourly=&quot;-1&quot; hourlyTime=&quot;1284336038994&quot; daily=&quot;-1&quot; dailyTime=&quot;1284336038994&quot; intervalMilliseconds=&quot;60000&quot; writeUniqueCountersTime=&quot;0&quot;&gt;
    &lt;criteria bop=&quot;AND&quot;&gt;
      &lt;left&gt;
        &lt;expr&gt;
          &lt;interval serialization=&quot;custom&quot;&gt;
            &lt;com.q1labs.ariel.Interval&gt;
              &lt;short&gt;5000&lt;/short&gt;
              &lt;boolean&gt;true&lt;/boolean&gt;
              &lt;short&gt;5000&lt;/short&gt;
              &lt;boolean&gt;true&lt;/boolean&gt;
            &lt;/com.q1labs.ariel.Interval&gt;
          &lt;/interval&gt;
        &lt;/expr&gt;
        &lt;key class
</code></pre>
<p>My props.conf are</p>
<pre><code>[XMLPARSING]
KV_MODE = xml
SHOULD_LINEMERGE = true
BREAK_ONLY_BEFORE = &lt;globalView\s\w*=(&quot;\d\d\d&quot;)
MAX_EVENTS = 600 
EXTRACT-dailyTime = ^(?:[^=\n]*=){8}&quot;(\d+)
TIME_FORMAT=%s%3N
TIME_PREFIX=dailyTime=
Lookahead=13
TRUNCATE = 1000
category = Custom
disabled = false
pulldown_type = true
</code></pre>",,1,0,,2020-10-8 20:03:37,,2020-10-9 14:49:06,,,,,444268.0,,1,0,time|splunk|epoch|splunk-calculation,403,10
777,259233,64275688,Search on Splunk via Python SDK,"<p>I'm trying to run simple search via Python SDK (Python 3.8.5, splunk-sdk 1.6.14). Examples that are presented on dev.splunk.com are clear but something goes wrong when I run search with my own parameters</p>
<p>The code is as simple as this</p>
<pre><code>search_kwargs_params = {
    &quot;exec_mode&quot;: &quot;blocking&quot;,
    &quot;earliest_time&quot;: &quot;2020-09-04T06:57:00.000-00:00&quot;,
    &quot;latest_time&quot;: &quot;2020-11-08T07:00:00.000-00:00&quot;,        
}
search_query = 'search index=qwe1 trace=111-aaa-222 action=Event.OpenCase'
job = self.service.jobs.create(search_query, **search_kwargs_params)
for result in results.ResultsReader(job.results()):
    print(result)
</code></pre>
<p>But search returns no results. When I run same query manually in Splunk web GUI it works fine.</p>
<p>I've also tried to put all parameters in 'search_kwargs_params' dictionary, widened search time period and got some search results but they seem to be inappropriate to what I got in GUI.</p>
<p>Can someone advise?</p>",,1,1,,2020-10-9 07:27:46,,2021-3-23 05:08:54,,,,,14418678.0,,1,1,python|splunk,386,11
778,259234,64277138,Configure log4j' HTTP appender to accept self-signed certificate,"<p>I am trying to send logs to Splunk Cloud' HTTP Event Collector using Log4J' HTTP Appender. However, it seems Splunk uses a self-signed certificate on their HTTP Event Collector, thus causing SSL Validation errors when Log4J tries to connect to it:</p>
<pre><code>ERROR Unable to send HTTP in appender [Splunk] javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
</code></pre>
<p>How can I temporarily disable SSL Certificate verification (similarly to the -k option of CURL) for the HTTP Appender?</p>",,0,3,,2020-10-9 09:16:57,,2020-10-9 09:16:57,,,,,4373869.0,,1,1,java|logging|log4j|splunk,284,9
779,259235,64311163,How to histogram a numeric variable?,"<p>I want to produce a simple histogram of a numeric variable <code>X</code>.</p>
<p>I'm having trouble finding a clear example.</p>
<p>Since it's important that the histogram be meaningful more than beautiful, I would prefer to specify the bin-size rather than letting the tool decide.  See: <a href=""https://medium.com/analytics-vidhya/data-scientists-stop-randomly-binning-histograms-1069d7380c3a"" rel=""nofollow noreferrer"">Data Scientists: STOP Randomly Binning Histograms</a></p>",,1,0,,2020-10-12 03:02:29,,2020-10-12 03:02:29,,,,,86967.0,,1,1,splunk,249,13
780,259236,64334804,Splunk search Regex: to filter timestamp and userId,"<p>Below the text want to extract timestamp align with UserId from the below line and group it</p>
<pre><code>    2020-10-12 12:30:22.540  INFO 1 --- [enerContainer-4] c.t.t.o.s.s.UserPrepaidService       : Validating the user with UserID:1111 systemID:sys111 
</code></pre>
<p>From below whole logs</p>
<pre><code>2020-10-12 12:30:22.538  INFO 1 --- [ener-4] c.t.t.o.s.service.UserService        :    AccountDetails&quot;:[{&quot;snumber&quot;:&quot;2222&quot;,&quot;sdetails&quot;:[{&quot;sId&quot;:&quot;0474889018&quot;,&quot;sType&quot;:&quot;Java&quot;,&quot;plan&quot;:[{&quot;snumber&quot;:&quot;sdds22&quot;}]}]}]}
    2020-10-12 12:30:22.538  INFO 1 --- [ener-4] c.t.t.o.s.service.ReceiverService        : Received userType is:Normal
    2020-10-12 12:30:22.540  INFO 1 --- [enerContainer-4] c.t.t.o.s.s.UserPrepaidService       : Validating the user with UserID:1111 systemID:sys111 
    2020-10-12 12:30:22.540  INFO 1 --- [enerContainer-4] c.t.t.o.s.util.CommonUtil                : The  Code is valid for userId: 1111 systemId: sys111
    2020-10-12 12:30:22.577  INFO 1 --- [enerContainer-4] c.t.t.o.s.r.Dao        : Saving user into dB ..... with User-ID:1111
</code></pre>
<p>....</p>
<p>same repetitive line</p>
<p>Below is my SPL search commands it returns only userid group by from that specific line.</p>
<p>But I want the time stamp as well from that line and group by it with time chart</p>
<pre><code>index=&quot;tis&quot; logGroup=&quot;/ecs/logsmy&quot; &quot;logEvents{}.message&quot;=&quot;*Validating the user with UserID*&quot; | spath output=myfield path=logEvents{}.message | rex field=myfield &quot;(?&lt;=Validating the user with UserID:)(?&lt;userId&gt;[0-9]+)(?= systemID:)&quot; |  table userId | dedup userId | stats count values(userId) by userId
</code></pre>
<p>Basically I tired the below</p>
<pre><code>(^(?&lt;dtime&gt;\d{4}-\d{1,2}-\d{1,2}\s+\d{1,2}:\d{1,2}:\d{1,2}\.\d+) )(?&lt;=Validating the user with UserID:)(?&lt;userId&gt;[0-9]+)(?= systemID:)
</code></pre>
<p>but it gave all the time stamp not specifically the line I mentioned above</p>",64335242.0,1,6,,2020-10-13 11:57:58,,2021-3-1 09:11:39,2020-10-13 12:19:07,,2463570.0,,2463570.0,,1,0,regex|splunk,94,7
781,259237,64336226,Splunk: How to get N-most-recent values for each group?,"<p>I've been struggling with this query for a few hours and it seems that it should be fairly straightforward, but for some reason I'm finding it quite difficult. My customer wants to see the top 2 most recent &quot;key&quot; for each clientType.</p>
<p>(Really it's the top N - but more than &quot;first&quot; or &quot;last&quot;)</p>
<p>While some translation is done before the step I'm asking about, I have the data looking like this as the output of the query (note: due to proprietary reasons I cannot provide the actual steps above this, nor is this real data but I think it should translate OK). I've starred the records that should end up in the output.</p>
<pre><code>out?  clientType  key            _time
----  ----------  -------------  -----------------
 *    Mobile      asfk129458715  2020-10-13 12:10Z
 *    Online      askg259750505  2020-10-12 11:59Z
 *    Email       dh8iwwih33e99  2020-10-12 11:58Z
      Online      schf38hrnf98u  2020-10-12 11:00Z
 *    Online      vn8n34rf9v83j  2020-10-12 11:56Z
 *    Mobile      sjvn98h3idv9d  2020-10-12 11:56Z
 *    Email       92hnfi928rdh9  2020-10-12 11:55Z
 *    Fax         jkcni983iiff4  2020-10-09 06:54Z
</code></pre>
<p>Now, I've been able to get this working on a smaller scale, say 1 day. but some clientTypes are not very frequent and we need to see the most recent of those as well. The output of this is to be used for certain audit purposes, and What I've found is that when I extend the search to multiple days (returning &gt; 10,000 events), the output is erratic, and I see results that are out of order, not the most recent, or otherwise askew.</p>
<p>What I want to see is this - only the top 2 for each client type, sorted by time descending within each group</p>
<pre><code>clientType  key            _time
----------  -------------  --------------
Mobile      asfk129458715  2020-10-13 12:10Z
Mobile      sjvn98h3idv9d  2020-10-12 11:56Z
Online      askg259750505  2020-10-12 11:59Z
Online      vn8n34rf9v83j  2020-10-12 11:56Z
Email       dh8iwwih33e99  2020-10-12 11:58Z
Email       92hnfi928rdh9  2020-10-12 11:55Z
Fax         jkcni983iiff4  2020-10-09 06:54Z
</code></pre>
<p>for some &quot;what I tried&quot;, I've tried using some query code in various orders mostly revolving around <code>stats list(key)</code>, <code>sort 0 -_time</code> etc, with various &quot;by&quot; clauses. The output of this query will also go through some additional translation to be used in our audit system, which takes a list of keys, each wrapped in single quotes and comma-delimited.</p>
<p>I've used <code>stats delim=&quot;','&quot;</code> and <code>mvcombine</code> with some success at this point in the query to get results that finally look like this. I wanted to include this as part of the question so it's clear what the end state needs to look like, in case something needs to change somewhere in the grouping and sorting section to make this easier.</p>
<pre><code>clientType  keys           
----------  -------------
Mobile      'asfk129458715','sjvn98h3idv9d'
Online      'askg259750505','vn8n34rf9v83j'
Email       'dh8iwwih33e99','92hnfi928rdh9'
Fax         'jkcni983iiff4'
</code></pre>",64338169.0,2,0,,2020-10-13 13:34:52,,2020-10-14 14:50:07,2020-10-14 14:38:14,,4418.0,,10946195.0,,1,0,sorting|grouping|splunk,746,12
782,259238,64369825,Splunk conditional distinct count,"<p>I'm running a distinct count syntax, <code>stats dc(src_ip) by</code>, and it returns the number of distinct source IPs but I would like to create a conditional statement (<code>eval</code>?) that it should only return the stats if the count is greater than 50.</p>
<p>Tried something like this, but no joy. Any idea how to make a conditional distinct count where count has to be more than X?</p>
<p><code>stats dc(src_ip) | eval status=if(count&gt;50)</code> =&gt; doesn't work</p>",,1,0,,2020-10-15 10:42:19,,2020-10-15 17:15:19,2020-10-15 17:15:19,,4418.0,,9918949.0,,1,0,splunk|splunk-query,525,11
783,259239,64404379,Read Squid access.log with Splunk,"<p>I'm trying to create a Splunk dashboard with the results of my squid access.log web traffic.
My problem here is, I cant make any search with the results of access.log, which looks for example like this:</p>
<p>2020-10-17 15:41:37  86 192.168.1.41 NONE/200 0 CONNECT twitter.com:443 - HIER_DIRECT/104.244.42.193 - &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:81.0) Gecko/20100101 Firefox/81.0&quot; &quot;SQUID-CS&quot; 209 -</p>
<p>Do you have any suggestions on how I can regex or split the results of these results? I just wanna get the destination URL (twitter.com).</p>
<p>I have a squid index and a squid sourcetype... fyi.</p>
<p>Any tips are appreciated!</p>
<p>Thanks a lot!</p>",64405061.0,1,1,,2020-10-17 15:53:46,,2020-10-17 17:00:59,,,,,13397315.0,,1,0,splunk|squid|splunk-query,127,9
784,259240,64427313,Splunk showing wrong index time,"<p>I have indexed data on splunk but i can see the _time(indexed time) is showing wrong like.</p>
<p><a href=""https://i.stack.imgur.com/vrPs6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vrPs6.jpg"" alt=""enter image description here"" /></a></p>
<p>I had indexed this data on 19th oct but this is showing like it is indexed on 18th oct.</p>
<p>Please suggest what would be the solution or i need to manually overwrite the _time key with current date time.</p>
<p>Thanks</p>",,1,1,,2020-10-19 12:25:39,,2020-10-19 15:20:31,2020-10-19 12:32:04,,7165942.0,,7165942.0,,1,0,splunk|splunk-query,284,10
785,259241,64527243,how to matching URI in splunk,"<p>There are some domain.<br>
Host: random_doamin:8080<br>
Host: random_domain/hello<br>
Host: random_domain<br>
Host: http://random_doamin:8080<br>
Host: https://random_domain/hello<br></p>
<p>And This is query that I used for testing on splunk.</p>
<pre><code>index=notable earliest=-1h | head 5
| eval domain1=&quot;Host: random_doamin:8080&quot;
| eval domain2=&quot;Host: random_domain/hello&quot;
| eval domain3=&quot;Host: random_domain&quot;
| eval domain4=&quot;Host: http://random_doamin:8080&quot;
| eval domain5=&quot;Host: https://random_domain/hello&quot;
| eval isMalicious = mvappend('domain1', 'domain2', 'domain3', 'domain4', 'domain5')
| mvexpand isMalicious
| dedup isMalicious
| rex field=isMalicious &quot;Host: (http://|https://)?(?&lt;random_domain&gt;.*(:|\/)?)&quot; 
| table isMalicious random_domain
</code></pre>
<p>Here is the result for query.</p>
<p><a href=""https://i.stack.imgur.com/aqKeN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aqKeN.png"" alt=""enter image description here"" /></a></p>
<p>What I want is extract only random_domain<br>
If this is impossible then at least I want to extract like below<br>
&quot;random_domain:&quot; or &quot;random_domain/&quot;<br></p>
<p>I need your help.
Thanks in advance for you kind.</p>",64527288.0,1,0,,2020-10-25 18:24:05,,2020-10-25 18:27:40,,,,,11230064.0,,1,1,regex|splunk,36,7
786,259242,64546588,Simple concatenated json line breaker in Splunk,"<p>I know this is probably simple, but for some reason I am able to get a line breaker working in Splunk.  I am fetching a data source from AWS S3, and multiple events in JSON format are concatenated. e.g.</p>
<pre><code>{&quot;key&quot;:&quot;value&quot;, {&quot;composite&quot;:&quot;result&quot;}}{&quot;something&quot;:&quot;else&quot;}
</code></pre>
<p>So LINE_BREAKER should match on <code>}{</code> with the left brace included.</p>
<p>I have <code>SHOULD_LINEMERGE=false</code> and then <code>LINE_BREAKER=(\{.+\})\{</code> but i loose the closing bracket. The <code>}{</code> don't have any characters between them (not even a newline), what is the best way to split these?</p>",64555231.0,1,5,,2020-10-27 00:01:04,,2020-10-27 13:13:21,,,,,1638209.0,,1,0,line-breaks|splunk,347,11
787,259243,64553190,Splunk : Join two indexes based on substring match,"<p>I am struggling with joining two indexes based on substring match.</p>
<p>I have following indexes :</p>
<pre><code>index1 :
having following fields

PROTOCOL,DIRECTION,FILENAME,DIRECTORYNAME



index2:

having following fields
APPID,CUSTOMERID,FILEPATTERN,DIRECTORYNAME
</code></pre>
<p>I want to join above indexes based on following condition</p>
<ol>
<li><p>FILEPATTERN is substring of FILENAME</p>
</li>
<li><p>DIRECTORYNAME in index1 = DIRECTORYNAME in index 2.</p>
</li>
</ol>
<p>and display output with following fields</p>
<pre><code>PROTOCOL,DIRECTION,APPID,CUSTOMERID,FILEPATTERN,DIRECTORYNAME
</code></pre>
<p>Thanks in anticipation</p>
<p>Regards</p>
<p>Nikhil</p>",,1,0,,2020-10-27 11:11:16,,2020-11-2 13:43:17,,,,,1814087.0,,1,0,splunk|splunk-query,381,12
788,259244,64559757,Splunk: How to apply conditionals for multiple rows with same column value?,"<p>I have got a table with columns in the following format where host_names are repeated and a single host can have both Compliant and Non-Compliant values against it. How can i write a query which checks for each host_name and marks it as Non-Compliant if any of its rows has Non-Compliant.</p>
<pre><code>compliance  host_name
Compliant   Host1
Non-Compliant   Host1
Compliant   Host2
Non-Compliant   Host3
Compliant   Host4
</code></pre>
<p>For ex: in the above table, Host1 has both Compliant and Non-Compliant values in two of its rows. Since one of the value is non-compliant, i want to take that host once and create a table in following format.</p>
<pre><code>compliance  host_name
Non-Compliant   Host1
Compliant   Host1
Non-Compliant   Host3
Compliant   Host4
</code></pre>",64560484.0,1,0,,2020-10-27 17:24:52,,2020-10-27 18:10:49,,,,,1945171.0,,1,0,splunk|splunk-query|log-analysis,459,11
789,259245,64561695,How to get rid of SLF4J bindings in springboot project,"<p>Following the other answers I did exclude this, but I started getting <code> ERROR An exception occurred processing Appender http java.lang.IllegalArgumentException:</code></p>
<pre><code>        &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;exclusions&gt;
            &lt;exclusion&gt;
               &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
               &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;
            &lt;/exclusion&gt;
         &lt;/exclusions&gt;
         &lt;/dependency&gt;
</code></pre>
<p>I need to fix the SLF4J bindings as it is preventing my Logs to go to Splunk :</p>
<pre><code>SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/app.jar!/BOOT-INF/lib/logback-classic- 
      1.2.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/app.jar!/BOOT-INF/lib/log4j-slf4j-impl- 
      2.11.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings  for an explanation.
SLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder]
</code></pre>",,1,0,,2020-10-27 19:36:23,,2020-10-28 06:34:39,2020-10-28 06:34:39,,13628029.0,,13628029.0,,1,0,java|logging|log4j|slf4j|splunk,107,8
790,259246,64566784,Join 2 tables with multiple conditions,"<p>Assume if I type <code>index=endpoints</code> and hit enter in the search bar, I will get results that look something like this:</p>
<pre><code>{
  &quot;user&quot;: Jack,
  &quot;os_name&quot;: &quot;Windows&quot;,
  &quot;hostname&quot;: &quot;Windows-JACK-01&quot;,
  &quot;pid&quot;: &quot;30219&quot;,
  &quot;app&quot;: &quot;/usr/bin/curl&quot;,
  &quot;cmdline&quot;: &quot;curl google.com&quot;,
  &quot;epoch&quot;: &quot;1503452096&quot;,
  &quot;type&quot;: &quot;processes&quot;
}

. . .

{
  &quot;hostname&quot;: &quot;Windows-JACK-01&quot;,
  &quot;pid&quot;: &quot;30219&quot;,
  &quot;app&quot;: &quot;/usr/bin/curl&quot;,
  &quot;epoch&quot;: &quot;1503452096&quot;,
  &quot;ip&quot;: &quot;123.123.123.123&quot;,
  &quot;port&quot;: &quot;1234&quot;,
  &quot;type&quot;: &quot;sockets&quot;
}

. . .
</code></pre>
<p>There are two types of data under the same index - sockets and processes. I would like to find a way to combine these two types of data (that are related) so that I could get a richer data that has all the information.</p>
<pre><code>+-------------------------+-----------------+---------+
| hostname | pid | app | osname | ip | port | etc.... |
+-------------------------+-----------------+---------+
| ...      | ... | ... | ...    | x  | y    | ...     |
+-------------------------+-----------------+---------+
</code></pre>
<p>The problem is if I simply do something like:</p>
<pre><code>index=endpoints type=&quot;processes&quot;
| join left=L right=L WHERE L.pid=R.pid [ search index=endpoints type=&quot;sockets&quot; ]
</code></pre>
<p>Most of the time, I will get a wrong mapping between the <code>app</code> and the <code>pid</code> because any applications can get assigned to same <code>pid</code> when it's available.</p>
<p>I'm thinking maybe if I add more conditions, it will reduce the inaccuracy rate. For example, instead of <code>L.pid=R.pid</code> only, maybe I could do <code>L.pid=R.pid AND L.hostname=R.hostname AND ...</code></p>
<p>My naive approach was to add more condition in the WHERE part</p>
<pre><code>index=endpoints type=&quot;processes&quot;
| join left=L right=L WHERE (L.pid=R.pid AND L.x=R.x AND...)
  [ search index=endpoints type=&quot;sockets&quot; ]
</code></pre>
<p>However, it seems like that's not how it works. Any advice?</p>",64573454.0,1,0,,2020-10-28 05:09:56,,2020-10-28 13:04:58,,,,,6945824.0,,1,0,splunk|splunk-query,306,9
791,259247,64580457,Can Kafka Connect consume data from a separate kerberized Kafka instance and then route to Splunk?,"<p>My pipeline is:</p>
<blockquote>
<p><em>Kerberized Kafka</em> --&gt; <em>Logstash (hosted on a different server)</em> --&gt; <em>Splunk</em>.</p>
</blockquote>
<p>Can I replace the <em>Logstash</em> component with <em>Kafka Connect</em>?</p>
<p>Could you point me to a resource/guide where I can use <em>kerberized Kafka</em> as a source for my <em>Kafka connect</em> (which is hosted separately)?</p>
<p>From the documentation, what I understood is that if <em>Kafka Connect</em> is hosted on the same cluster as that of <em>Kafka</em>, that's quite possible. But I don't have that option right now, as our Kafka cluster is multi-tenant and hence not approved for additional processes on the cluster.</p>",,1,0,,2020-10-28 20:02:13,,2020-11-2 13:37:09,2020-11-2 13:37:09,,4418.0,,4395523.0,,1,0,apache-kafka|logstash|kerberos|apache-kafka-connect|splunk,38,7
792,259248,64593739,SPLUNK : Duplicated json fields on searchHead,"<p>I have clustered infrastructure (simplified)</p>
<p>2 SH (cluster) + 2 Indexer (cluster) + Heavy Forwarder (name HF)</p>
<p>On HF I run some script which returns me JSON file, and I forward it from HF to Indexers  (HF -&gt; IndexCluser)</p>
<p>After that, I have to make some searches on SH with that data</p>
<p>When I make a search request, I have correctly parsed JSON, look perfect. BUT when I use <code>table</code> or just expand results each JSON field is duplicated.</p>
<p>I have a custom sourcetype defined on the Heavy Forwarder (although I tried some variations):</p>
<pre><code>[just_json]

INDEXED_EXTRACTIONS = json
KV_MODE = none
AUTO_KV_JSON = false
NO_BINARY_CHECK = true
pulldown_type = true
category = Application
</code></pre>
<p>I assume that it multiplies on two because of:</p>
<p>JSON parsed during indexing (or sending from Heavy?)</p>
<p>JSON parsed additionally on searchHead during the search performed</p>
<p>I have read some similar questions (not sure about cluster case) but haven't succeeded.</p>
<p>Still can't figure out.</p>
<p>Some sample of event:</p>
<p><a href=""https://i.stack.imgur.com/jAO0M.png"" rel=""nofollow noreferrer"">view after query</a></p>
<p><a href=""https://i.stack.imgur.com/sjJlu.png"" rel=""nofollow noreferrer"">show raw</a></p>
<p><a href=""https://i.stack.imgur.com/fYXGd.png"" rel=""nofollow noreferrer"">expanded view</a></p>
<p>Table view:
<a href=""https://i.stack.imgur.com/cVrUK.png"" rel=""nofollow noreferrer"">table query</a></p>",,1,4,,2020-10-29 15:00:15,,2020-11-3 11:56:56,2020-11-3 11:56:56,,14543349.0,,14543349.0,,1,0,json|logging|configuration|splunk,146,8
793,259249,64612622,Splunk lookup script lifecycle,"<p>I have written a python script that Splunk launches with a lookup command.</p>
<p>Like this: <code>source=&quot;*&quot; | lookup my_script</code></p>
<p>This script, as a first thing, creates a client object that uses later on for its internal logic. This client connects to a server and has a caching layer. This operation is time consuming and, in order to avoid repeating it, I store the client in python <code>globals()</code>.</p>
<p>I noticed that when I run the lookup passing the results of a splunk search to it, it gets called one time for each result found by the search. This would not be a big issue if the whole client wasn't recreated each time, because the <code>globals()</code> namespace is empty as the whole python process was invoked separately for each record and destroyed once the record is processed by the script.</p>
<ul>
<li>Does anyone know the details of lookup scripts lifecycle?</li>
<li>Is each record passed to a separate python invocation?</li>
<li>Is there a way to store the client object to make it survive each lookup script invocation?</li>
</ul>",,1,0,,2020-10-30 16:52:38,,2020-10-30 19:37:36,,,,,3369360.0,,1,0,python|lifecycle|splunk,36,7
794,259250,64638577,How to parse information from a log message in splunk,"<p>I have a log message in splunk as follows:
<code>Mismatched issue counts: 5 vs 9</code></p>
<p>Is there a way to parse the 5 and 9 into variables and draw a graph using them?</p>
<p>I looked into <a href=""https://stackoverflow.com/questions/31341915/splunk-custom-log-format-parsing"">Splunk Custom Log format Parsing</a>
and saw there is an option to use json to parse json log message. But how can I log as json and use spath in splunk chart?</p>",64645369.0,1,0,,2020-11-2 01:06:01,,2020-11-2 12:10:55,,,,,3706638.0,,1,0,splunk,466,12
795,259251,64655271,How to extract a value from fields when using stats(),"<p>Query:</p>
<pre><code>index = test
| stats values(*) as * by ip_addr, location
| where location=&quot;USA&quot;
| fields timestamp, user, ip, location, message
</code></pre>
<p>Result:</p>
<pre><code>+--------------------------------------------------------------------+
| timestamp         | user   | ip          | location | message      |
+--------------------------------------------------------------------+
| 08/08/2020 17:00  | thomas | 10.10.10.10 | USA      | Hello, world!|
| 08/08/2020 17:05. | unknown|             |          | I love steak!|
| 08/08/2020 17:10. |        |             |          | I love soda! |
+--------------------------------------------------------------------+
| 08/08/2020 17:00  | jeffry | 10.10.10.20 | USA      | Hello, world!|
| 08/08/2020 17:35  | unknown|             |          | I love pancke|
| 08/08/2020 17:40  |        |             |          | I love waffle|
+--------------------------------------------------------------------+
</code></pre>
<p>I want to:</p>
<ol>
<li>make those multiple timestamps become one single timestamp</li>
<li>remove the &quot;unknown&quot; value in the &quot;user&quot; field</li>
<li>make &quot;message&quot; field to display only the &quot;Hello, world!&quot; - I dont care about the rest.</li>
</ol>
<p>I tried to do:</p>
<pre><code>index = test
| stats values(*) as * by ip_addr
| where location=&quot;USA&quot;
| eval user=replace(user, &quot;unknown&quot;, &quot;&quot;)
| fields timestamp, user, ip, location, message
</code></pre>
<p>But it removes all the values under &quot;user&quot; field. Any advice? My number 2 and number 3 goals look similar. If I could crack either one of them, I think I could solve the other one easily.</p>",64666206.0,1,4,,2020-11-3 00:46:16,,2020-11-3 15:54:28,,,,,6945824.0,,1,3,splunk|splunk-query,364,11
796,259252,64661467,Need table o/p with each FROM_IP its related uid,"<pre><code>index=name conn &quot;connection from&quot;  
    [search index=name 
        [| inputlookup UIDlist.csv 
        |rename UID AS uid
        | fields uid ]
    &quot;BIND&quot;  
   | fields conn ]  
| rex field=_raw &quot;connection from (?&lt;FROM_IP&gt;\d+\.\d+\.\d+\.\d+):&quot;  
| stats count by FROM_IP
</code></pre>
<p><code>tst.csv</code> file has list of UID so that it can give o/p for one user then other and so on ...<br />
I want the table FROM_IP with which uid</p>
<p>O/p of two query used above :</p>
<blockquote>
<p>index=name BIND uid | fields conn</p>
</blockquote>
<p>[10/Nov/2020:06:38:40 +0000] conn=111111 op=4238 msgId=4239 - BIND dn=&quot;uid=uid,ou=xxx,o=xxxx,o=email&quot; method=128 version=3</p>
<blockquote>
<p>index=name  conn &quot;connection from&quot; | rex field=_raw &quot;connection from
(?&lt;FROM_IP&gt;\d+.\d+.\d+.\d+):&quot;  | stats count by FROM_IP</p>
</blockquote>
<p>[09/Nov/2020:22:52:55 -0800] conn=1111111 op=-1 msgId=-1 - fd=115 slot=115 xxxx connection from xx.xx.xx.xx.xx to xx.xx.xx.xx.xx</p>",64771218.0,1,7,,2020-11-3 10:55:31,,2020-11-10 14:51:57,2020-11-10 06:57:48,,11831790.0,,11831790.0,,1,-2,splunk|splunk-query,52,6
797,259253,64681450,Few hourly events are missing in the splunk dashboard,"<p>Need help on the below weird issue.</p>
<p>We have a Splunk query where it is built on the logs. Below is the query used.</p>
<pre><code>index=aws_lle_airflow &quot;INFO - Source count for aws-bda-lle.marketing.bq_mktg_campaign:&quot;
| rex field=_raw &quot;INFO - Source count for aws-bda-lle.marketing.bq_mktg_campaign: (?&lt;bqTableRecordCount&gt;[^\&quot;]+)&quot;
| table _time bqTableRecordCount
| sort _time
</code></pre>
<p>problem is ideally as per the table inserts 10 events should be displayed in the dashboard but only 8 events are shown.</p>
<p>Even though the logs are written not sure why the dashboard is not reflecting. Could someone help me what could be the issue and what needs to be done to resolve.</p>",,0,3,,2020-11-4 13:57:43,,2020-11-10 23:42:07,2020-11-10 23:42:07,,2055998.0,,10410373.0,,1,0,splunk|splunk-query,21,5
798,259254,64696928,Epoch time field extraction in splunk,"<p>How can we write regex or field extraction for below logs? Field is start and end where value is in epoch format and i want to change this in human readable format ..</p>
<pre><code>cs7=45.45 cs7Label=latitude cs8=28.05 cs8Label=longitude Customer=Romania-UPC start=1604484735041 request=scoala.bibliotecapemobil.ro/download.php ref=https://scoala.bibliotecapemobil.ro/893/borgia requestMethod=GET qstr=book_id\=893&amp;user_id\=&amp;download\=pdf cn1=200 app=HTTPS act=REQ_PASSED deviceExternalId=37982221230540227 sip=195.191.47.151 spt=443 in=45 xff=82.208.137.89 cpt=62542 src=82.208.137.89 ver=TLSv1.3 TLS_AES_128_GCM_SHA256 end=1604484735097
</code></pre>",,1,0,,2020-11-5 12:10:26,,2020-11-5 16:38:39,2020-11-5 16:38:39,,13330893.0,,13330893.0,,1,0,splunk|splunk-query|splunk-calculation,330,10
799,259255,64742358,Splunk: How to get two searches in one timechart/graph?,"<p>I have to queries which look like this:</p>
<pre><code>source=&quot;/log/ABCD/cABCDXYZ/xyz.log&quot; doSomeTasks| timechart partial=f span=1h count as &quot;#XYZ doSomeTasks&quot; | fillnull

source=&quot;/log/ABCD/cABCDXYZ/xyz.log&quot; doOtherTasks| timechart partial=f span=1h count as &quot;#XYZ doOtherTasks&quot; | fillnull
</code></pre>
<p>I now want to get this two searches in one graph (I do not want to sum the numbers I get per search up to one value).</p>
<p>I saw that there is the possibility to take <code>appendcols</code> but my trials to use this command were not successful.</p>
<p>I tried this but it did not work:</p>
<pre><code>source=&quot;/log/ABCD/cABCDXYZ/xyz.log&quot; doSomeTasks|timechart partial=f span=1h count as &quot;#XYZ doSomeTasks&quot; appendcols [doOtherTasks| timechart partial=f span=1h count as &quot;#XYZ doOtherTasks&quot; | fillnull]
</code></pre>",64802718.0,1,1,,2020-11-8 19:49:18,,2020-11-12 11:24:22,,,,,11572712.0,,1,1,append|splunk,187,9
800,259256,64763178,Using splunk to plot table of key counts extracted from json string field,"<p>I want to use splunk to plot a graph from my service's payload logs to highlight the type of query parameters. The logs are of this format:</p>
<pre><code>2020-11-10 04:46:57.471  INFO InfoType=&quot;payload&quot; request=&quot;{&quot;queryParams&quot;:{&quot;field1&quot;:&quot;26453451364&quot;}}&quot; 

2020-11-10 04:46:57.471  INFO InfoType=&quot;payload&quot; request=&quot;{&quot;queryParams&quot;:{&quot;field2&quot;:&quot;71362547612531&quot;}}&quot; 

2020-11-10 04:46:57.471  INFO InfoType=&quot;payload&quot; request=&quot;{&quot;queryParams&quot;:{&quot;field3&quot;:&quot;71547612531&quot;, &quot;field4&quot;:&quot;7136254761&quot;}}&quot;
</code></pre>
<p>For the above, I want output to be something like this:</p>
<pre><code>Query Param   | Count
--------------------
field1        |  1
--------------------
field2        |  1
--------------------
field3,field4 |  1
</code></pre>
<p>I tried a lot with spath, but I am not able to get it to work.</p>
<pre><code>| makeresults
| eval _raw=&quot;2020-11-10 04:46:57.471  INFO InfoType=\&quot;payload\&quot; request=\&quot;{\&quot;queryParams\&quot;:{\&quot;field1\&quot;:\&quot;26453451364\&quot;}}\&quot;&quot;
| spath input=request path={}.queryParams
</code></pre>
<p>Any help on how to proceed or what I am doing wrong will be really helpful. Thanks and regards in advance.</p>",,1,0,,2020-11-10 05:05:02,,2020-11-10 19:20:32,,,,,1522454.0,,1,0,logging|splunk,53,6
801,259257,64767508,Looking for a way to automatically send to Splunk Jenkins pipeline duration per stage,"<p>Is there a way to send Jenkins pipeline duration per stage information to <strong>Splunk</strong> for all pipelines automatically.</p>
<p>I have installed the <strong>Splunk</strong> forwarder plug-in in Jenkins. But that only seems to forward duration per job.</p>
<p>(I can alter every pipeline to send the data, but that seems like an unnecessary amount of work)</p>",,0,0,,2020-11-10 10:53:07,,2020-11-10 11:00:03,2020-11-10 11:00:03,,1189800.0,,14606629.0,,1,1,jenkins|jenkins-pipeline|splunk,26,5
802,259258,64770548,How to configure log4js to write to a file only if Splunk is down?,"<p>Currently, our application writes logs to Splunk, and we don't write to a file.</p>
<p>We have a new requirement whereby I need to write logs to a file when, and only when, there are one or more issues in writing directly to Splunk (Splunk is down, Connectivity issue, etc.)</p>
<p>How can I configure log4j2 to write locally if, and only if, the remote Splunk listener is not available?</p>",,1,1,,2020-11-10 14:12:04,,2021-6-9 15:30:42,2020-11-11 14:56:26,,10746140.0,,10746140.0,,1,1,java|logging|log4j|log4j2|splunk,64,7
803,259259,64779781,Splunk -> Get time taken in milliseconds,"<p>I have splunk json log entries that have epochSecond , nanoOfSecond, eventid and message fields.
I want to find average , 95perc etc. of time taken in milliseconds for an event(which is defined by a unique eventid across all log entries) which is the difference between the time of first log entry and last log entry of an event.</p>
<p>How do I go about doing this in Splunk? I can't even get duration of each event in milliseconds to start with</p>
<p>stats range(_time) as duration by eventid -&gt; seems to give time ins seconds, not millisec
stats range(timestamp) as duration by eventid -&gt; Gives nothing
stats range(epochSecond * 1000000000 + nanoOfSecond) as duration by eventid -&gt; gives syntax error</p>",64788467.0,1,0,,2020-11-11 02:51:56,,2020-11-11 14:34:41,,,,,10680850.0,,1,0,splunk,406,10
804,259260,64780745,How to add total and percentage column for splunk timechart command,"<p>Using a simple example:  count the number of events for each host name</p>
<p>... | timechart count BY host</p>
<pre><code>&gt; ... | timechart count BY host
&gt; 
&gt; This search produces this results table:
&gt; 
&gt; _time         host1   host2   host3   
&gt; 
&gt; 2018-07-05    1038    27      7      
&gt; 2018-07-06    4981    111     35      
&gt; 2018-07-07    5123    99      45       
&gt; 2018-07-08    5016    112     22    
</code></pre>
<p>What I want is to add a column to show the total events for that day and the percentage of each http status code.</p>
<p>I tried</p>
<pre><code> ... | timechart count BY host
| eval total= host1 + host2 + host3
| eval host1_percent = host1 / total
| eval host2_percent = host2 / total
| eval host3_percent = host3 / total
| table _time, host1_percent, host2_percent, host3_percent
</code></pre>
<p>This works most of the time, but I found out if for certain day, a host was offline (no record for a particular host), then the search doesn't work (return blank results), I have to remove that particular host from the &quot;total = host1 + host2 + host3&quot; to get it to work.</p>
<p>So my question is: is there a way to get the total number of record for for every day (row) without having to add them together, e.g. replace the &quot;total = host1 + host2 + host3&quot; with a count or sum, I tried couple of thing, none of them work.</p>",64788146.0,1,0,,2020-11-11 05:03:17,,2020-11-11 14:16:33,,,,,3277841.0,,1,0,charts|splunk|stat|splunk-query,651,12
805,259261,64790673,How to visualize splunk events as column chart,"<p>How could I visualize json data from several Splunk events as column chart?</p>
<p>The json I get each time the app logs data to Splunk has the following structure:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;monat&quot;: &quot;2020-06-01&quot;,
    &quot;duration&quot;: 512,
    &quot;herkunft&quot;: &quot;abc&quot;
}
</code></pre>
<p>The column chart should count the equal <code>monat</code> fields and put them in the same column. If there is a new month (<code>monat</code> field) a new column should be created.</p>
<p>I am struggling to define the search term.</p>",64791701.0,1,0,,2020-11-11 16:50:06,,2021-2-23 16:11:50,2021-2-23 16:11:50,,4418.0,,1820936.0,,1,1,json|charts|splunk|splunk-dashboard,70,9
806,259262,64803118,Splunk: Group by certain entry in log file,"<p>I did this query in Splunk:</p>
<pre><code>source=&quot;/log/ABCD/cABCDXYZ/xyz.log&quot; doSomeTasks|timechart partial=f span=1h count as &quot;#XYZ doSomeTasks&quot; |fillnull
</code></pre>
<p>This query works out fine. Now, I would like to group this results by another entry in my log file. This entry is <code>taskType</code>. <code>taskType</code> can be either <code>One</code>, <code>Two</code> or <code>Three</code>. <code>One</code>, <code>Two</code> or <code>Three</code> are also entries after <code>taskType</code>.</p>
<p>How could I do this?</p>",64806445.0,1,0,,2020-11-12 11:32:26,,2020-11-12 15:04:48,,,,,11572712.0,,1,0,group-by|splunk|splunk-query,102,9
807,259263,64821869,Splunk: Split extracted field after specific position,"<p>I extracted a filed which contains phone numbers. The extracted field is named <code>phoneNumbers</code>. I now want to split this field after the fourth number to get the area code.
How could I do this?</p>
<p>Thanks a lot!</p>",64823461.0,1,0,,2020-11-13 13:33:27,,2020-11-13 15:21:25,,,,,11572712.0,,1,0,splunk,96,8
808,259264,64823272,Splunk: count by Id,"<p>I did a query in Splunk which looks like this:</p>
<pre><code>source=&quot;/log/ABCDE/cABCDEFGH/ABCDE.log&quot; doSomeTasks
</code></pre>
<p>I now want to count the entries in the logfile by <code>Id</code> (<code>Id</code> is an extracted field). But I only want to count every <code>Id</code> once and not every time when <code>doSomeTasks</code> is executed. How could I do this?</p>",64823517.0,1,0,,2020-11-13 15:07:55,,2020-11-13 15:25:41,2020-11-13 15:13:22,,11572712.0,,11572712.0,,1,0,splunk,45,8
809,259265,64836759,Log messages in Splunk are shown as ASCII numbers,"<p>Our Spring Boot applications run inside Docker containers. We log using e.g. log4j2.</p>
<p>When I use in my Spring Boot application the Spring log4j2.xml configuration file (see below) then log statements are readable (as plain text) in the Docker logs. When I try to read them in Splunk the message is shown like:</p>
<pre><code>message=['123', '34' '116', ... ]
</code></pre>
<p>When I remove the log4j2.xml file, then all logs are readable again both in the Docker logs as in Splunk.</p>
<p>Why is this happening?
How can I make the messages readable in Splunk?</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;Configuration status=&quot;info&quot;&gt;
    &lt;Appenders&gt;
        &lt;Console name=&quot;Console-Appender&quot; target=&quot;SYSTEM_OUT&quot;&gt;
            &lt;PatternLayout&gt;
                &lt;pattern&gt;
                    [%-5level] %d{MM-dd HH:mm:ss.SSS} [%t] [%c{1} - %msg%n
                &lt;/pattern&gt;
            &lt;/PatternLayout&gt;
        &lt;/Console&gt;
    &lt;/Appenders&gt;
    &lt;Loggers&gt;
        &lt;Logger name=&quot;nl.mycompany.xyz&quot; level=&quot;info&quot; additivity=&quot;false&quot;&gt;
            &lt;AppenderRef ref=&quot;Console-Appender&quot; /&gt;
        &lt;/Logger&gt;
        &lt;Root&gt;
            &lt;AppenderRef ref=&quot;Console-Appender&quot; /&gt;
        &lt;/Root&gt;
    &lt;/Loggers&gt;
&lt;/Configuration&gt;
</code></pre>",64970620.0,1,3,,2020-11-14 17:40:30,,2020-11-23 14:44:25,2020-11-21 16:37:31,,3143823.0,,3143823.0,,1,3,log4j2|splunk,86,9
810,259266,64863122,Splunk: lowercase values before idexing,"<h2>Context</h2>
<p>I created an <a href=""https://docs.splunk.com/Documentation/Splunk/8.1.0/Data/Configureindex-timefieldextraction"" rel=""nofollow noreferrer"">indexed field</a> for two different data sources. Here is the <strong>props.conf</strong></p>
<pre><code>[source::...stdOutErr*.log]
...
TRANSFORMS-projectName = tjs-projectName

[source::...resuming*.log]
...
TRANSFORMS-projectName = tjs-projectName-resuming
</code></pre>
<p>Because the <code>projectName</code> fields appears in logs structured in a very different ways, I am using two different <strong>REGEX</strong>. Here is the <strong>transforms.conf</strong>.</p>
<pre><code>[tjs-projectName]
REGEX = ^\[[A-Z]{4,5}\s?\]:\s(?&lt;projectName&gt;\w+)
FORMAT = projectName::$1
WRITE_META = true

[tjs-projectName-resuming]
REGEX = &quot;\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}.\d{3}&quot;,&quot;[^&quot;]*&quot;,&quot;[^&quot;]*&quot;,&quot;[^&quot;]*&quot;,&quot;[^&quot;]*&quot;,&quot;[^&quot;]*&quot;,&quot;[^&quot;]*&quot;,&quot;(?&lt;projectName&gt;\w+)&quot;
FORMAT = projectName::$1
WRITE_META = true
</code></pre>
<p>Last, this is for indexing
<strong>fields.conf</strong></p>
<pre><code>[tjs-projectName]
INDEXED = true

[tjs-projectName-resuming]
INDEXED = true
</code></pre>
<h2>Issue</h2>
<p>Values for <code>projectName</code> appears in <strong>CAPITAL</strong> in <code>resuming*.log</code> but in <strong>lowercase</strong> in <code>stdOutErr*.log</code>. I would like to <strong>uniform</strong> things <strong>before indexing</strong>. From what I read, I should use <code>SEDCMD</code> in <strong>props.conf</strong> but my attempts where not quite successful, either</p>
<ul>
<li>because I s**k at perl styled regex used by <code>SEDCMD</code></li>
<li>because this seems illogical and should be handled in <strong>transforms.conf</strong></li>
</ul>
<pre><code>[source::...resuming*.log]
SEDCMD-projectName = s/&quot;\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}.\d{3}&quot;,&quot;[^&quot;]*&quot;,&quot;[^&quot;]*&quot;,&quot;[^&quot;]*&quot;,&quot;[^&quot;]*&quot;,&quot;[^&quot;]*&quot;,&quot;[^&quot;]*&quot;,&quot;\(?&lt;projectName&gt;\w+\)&quot;/\L\1\g
TRANSFORMS-projectName = tjs-projectName-resuming
</code></pre>
<p>Is there a more direct approach (something easier I could use in <strong>transforms.conf</strong>) or should I stick to <code>SEDCMD</code>?</p>",,1,0,,2020-11-16 17:50:28,,2020-11-16 23:57:58,,,,,7648881.0,,1,0,indexing|formatting|splunk,109,9
811,259267,64866113,Pub/Sub to Splunk Dataflow Template - The requested URL was not found on this server,"<p>I am using the Dataflow template (i've tried both the <code>latest</code> and <code>2020-11-02-00_RC00</code> of <code>Cloud_PubSub_to_Splunk </code>) that streams data from a pubsub topic to splunk. I have followed all steps from the <a href=""https://cloud.google.com/solutions/exporting-stackdriver-logging-for-splunk"" rel=""nofollow noreferrer"">Documentation</a>.</p>
<p>My job arguments were:</p>
<pre><code>JOB_NAME=pubsub-to-splunk-$USER-`date +&quot;%Y%m%d-%H%M%S%z&quot;`
gcloud dataflow jobs run $JOB_NAME \
    --subnetwork=https://www.googleapis.com/compute/v1/projects/&lt;PROJECT&gt;/regions/us-central1/subnetworks/&lt;NAME&gt; \
    --gcs-location gs://dataflow-templates/2020-11-02-00_RC00/Cloud_PubSub_to_Splunk \
    --max-workers 2 \
    --parameters=inputSubscription=&quot;projects/&lt;PROJECT&gt;/subscriptions/logs-export-subscription&quot;,token=&quot;&lt;TOKEN&gt;&quot;,url=&quot;https://&lt;URL&gt;:8088/services/collector/event&quot;,outputDeadletterTopic=&quot;projects/&lt;PROJECT&gt;/topics/splunk-pubsub-deadletter&quot;,batchCount=&quot;10&quot;,parallelism=&quot;8&quot;,disableCertificateValidation=true
</code></pre>
<p>I can successfully start the Dataflow job and streaming begins and I can see unacked message count from my <code>logs-export-subscription</code> going down, however the job fails when writing to Splunk with the following error:</p>
<p><strong>Error writing to Splunk. StatusCode: 404, content: {&quot;text&quot;:&quot;The requested URL was not found on this server.&quot;,&quot;code&quot;:404}, StatusMessage: Not Found</strong></p>
<p>When troubleshooting, I can successfully send a request to the Splunk endpoint from the same subnetwork that the Dataflow workers are running in.</p>
<pre><code>curl -k https://&lt;URL&gt;:8088/services/collector/event -H &quot;Authorization: Splunk &lt;HEC TOKEN&gt;&quot; -d '{&quot;event&quot;: {&quot;field1&quot;: &quot;hello&quot;, &quot;field2&quot;: &quot;world&quot;}}'

{&quot;text&quot;:&quot;Success&quot;,&quot;code&quot;:0}
</code></pre>
<p>And so, I don't think it is a connection or url issue like the error message suggests.</p>
<p>I can reproduce the failure with curl when I remove <code>-d</code> key and value.</p>
<pre><code>curl -k https://&lt;IP&gt;:8088/services/collector/event -H &quot;Authorization: Splunk &lt;TOKEN&gt;&quot; 

{&quot;text&quot;:&quot;The requested URL was not found on this server.&quot;,&quot;code&quot;:404}
</code></pre>
<p><strong>Any idea what may be causing this issue?</strong></p>",,2,6,,2020-11-16 21:40:41,,2021-2-5 19:31:46,2020-11-16 22:02:15,,6332680.0,,6332680.0,,1,0,google-cloud-dataflow|google-cloud-pubsub|splunk,316,10
812,259268,64878881,Sending a search query to Splunk using Python's request library,"<p>I want to send a search query to Splunk using Python3 and the requests library and would like to receive a SID of search job. Firstly, I am able to get the <code>session_key</code> with:</p>
<pre><code>#!/usr/bin/env python3

import requests
from bs4 import BeautifulSoup
    
username = 'my_username'
password = 'my_password'
base_url = 'https://splunk-search:8089'

r = requests.get(base_url+&quot;/servicesNS/admin/search/auth/login&quot;, data={'username':username,'password':password}, verify=&quot;/etc/pki/tls/cert.pem&quot;)

session_key = BeautifulSoup(r.text, 'lxml').find(&quot;sessionkey&quot;).text

#verify we get the session key as string
print(f&quot;session key is {session_key} and its type is {type(session_key)}&quot;)
</code></pre>
<p>which makes me confident that I am authenticated. Once I have the <code>session_key</code>, I would like to post a search job with:</p>
<pre><code>search_query = &quot;search = search earliest=-5m index=_internal&quot;

r = requests.post(base_url+'/services/search/jobs', data=search_query, headers = {'Authorization': 'Splunk %s' % session_key}, verify=&quot;/etc/pki/tls/cert.pem&quot;)

#view the response, I would hope to see a SID here
print(r.text)
</code></pre>
<p>Despite having access to the Splunk index I query, I get the following response:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;response&gt;
  &lt;messages&gt;
    &lt;msg type=&quot;WARN&quot;&gt;call not properly authenticated&lt;/msg&gt;
  &lt;/messages&gt;
&lt;/response&gt;
</code></pre>
<p>What am I missing here? Intuition tells me it is the <code>request.post</code> method that is malfored but I can't see to find where the error is.</p>",,1,0,,2020-11-17 16:04:59,,2020-11-18 09:31:24,,,,,4855386.0,,1,0,python-3.x|python-requests|splunk,211,10
813,259269,64896369,Splunk field extractions from different events & delimiters,"<p>My Splunk log format of key event timestamps is as below :</p>
<pre><code>[Date=2020-11-18] [Time=12:36:53] [Mode=DEBUG] [Class=PrintUtil] [Line=557] [Message=Hour = 18-nov-2020 11:00:00]
[Date=2020-11-18] [Time=12:36:53] [Mode=DEBUG] [Class=PrintUtil] [Line=557] [Message=Id = 126566]
[Date=2020-11-18] [Time=12:36:53] [Mode=DEBUG] [Class=PrintUtil] [Line=557] [Message=zipBefore = 18-nov-2020 12:27:08.776174]
[Date=2020-11-18] [Time=12:36:53] [Mode=DEBUG] [Class=PrintUtil] [Line=557] [Message=zipAfter = 18-nov-2020 12:36:52.718122]
[Date=2020-11-18] [Time=12:36:53] [Mode=DEBUG] [Class=PrintUtil] [Line=557] [Message=StartTime = 18-nov-2020 12:17:10.603227]
[Date=2020-11-18] [Time=12:36:53] [Mode=DEBUG] [Class=PrintUtil] [Line=557] [Message=EndTime = 18-nov-2020 12:36:53.094513]
[Date=2020-11-18] [Time=12:36:53] [Mode=DEBUG] [Class=PrintUtil] [Line=557] [Message=beginThread = 18-nov-2020 12:17:10.905782]
[Date=2020-11-18] [Time=12:36:53] [Mode=DEBUG] [Class=PrintUtil] [Line=557] [Message=endThread = 18-nov-2020 12:24:22.628907]
[Date=2020-11-18] [Time=12:36:53] [Mode=DEBUG] [Class=PrintUtil] [Line=557] [Message=LogTime = CASE1~6~18-nov-2020 12:17:11.377070~0~18-nov-2020 12:17:12.608526,CASE1~0~18-nov-2020 12:17:11.365409~0~18-nov-2020 12:17:12.654285,CASE3~0~18-nov-2020 12:17:12.644921~11~18-nov-2020 12:17:13.636655,CASE2~5~18-nov-2020 12:17:13.295225~700000~18-nov-2020 12:23:29.370142,CASE2~2~18-nov-2020 12:17:12.815714~700000~18-nov-2020 12:23:31.400500]
</code></pre>
<p>I would like to extract all the key event timestamps into table fields like below so i could do a difference between them etc :</p>
<pre><code>Hour                  Id      StartTime                    EndTime                      beginThread                 endThread                   zipBefore                    zipAfter
18-nov-2020 11:00:00  126566  18-nov-2020 12:17:10.603227  18-nov-2020 12:36:53.094513  18-nov-2020 12:17:10.905782 18-nov-2020 12:24:22.628907 18-nov-2020 12:27:08.776174  18-nov-2020 12:36:52.718122
</code></pre>
<p>Also, my last event in log has difference cases, threads &amp; timestamp which I need to extract separately based on delimiter something like below :</p>
<pre><code>Case Thread StartTime                    Count EndTime
CASE1     6 18-nov-2020 12:17:11.377070      0 18-nov-2020 12:17:12.608526
CASE1     0 18-nov-2020 12:17:11.365409      0 18-nov-2020 12:17:12.654285
CASE2     5 18-nov-2020 12:17:13.295225 700000 18-nov-2020 12:23:29.370142
CASE2     2 18-nov-2020 12:17:12.815714 700000 18-nov-2020 12:23:31.400500
CASE3     0 18-nov-2020 12:17:12.644921     11 18-nov-2020 12:17:13.636655
</code></pre>",64899103.0,1,0,,2020-11-18 15:41:00,,2020-11-18 18:26:40,,,,,6808782.0,,1,0,splunk|data-extraction|splunk-query,33,7
814,259270,64896757,Splunk: How to extract field directly in Search command using regular expressions?,"<p>I have some log files which looks like this one:</p>
<p><em>2020-11-18 00:11:22.333  INFO [ABC_service,[{&quot;method&quot;:&quot;doSomething&quot;,&quot;id&quot;:&quot;123456789&quot;,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;params&quot;:{&quot;taskType&quot;:&quot;certainType&quot;,&quot;clientNotificationInfo&quot;:{&quot;priority&quot;:xy,&quot;expirationDate&quot;:111111111},&quot;priority&quot;:xy,&quot;deviceId&quot;:&quot;000000000000000&quot;,&quot;taskPayload&quot;:{},&quot;timeout&quot;:22222222}}, XYZ]</em></p>
<p>I now would like to extract fields directly in my search and make a table of the extracted values. I would like to extract the taskType, here: <em>certainType</em>. Now, I was wondering about how to do this.</p>
<p>I tried this command:</p>
<pre><code>source=&quot;/log/ABCDE/ABCDE_service.log&quot; doSomething | rex field=_raw &quot;taskType: (?&lt;taskType&gt;.*)&quot;    | table  taskType
</code></pre>
<p>But got an empty table. What is wrong here?</p>
<p>But I got an empty table for both values.</p>",64898471.0,1,0,,2020-11-18 16:03:22,,2020-11-18 17:44:55,,,,,11572712.0,,1,0,extract|splunk|splunk-query,1473,14
815,259271,64900813,Avoid using Transaction in splunk queries,"<p>I am looking for alternate way to write splunk query without using transaction
Example
assuming r is a unique field in both the  searches
(sourcetype=* &quot;search log 1&quot;) OR (sourcetype=* &quot;search log 2&quot;) | transaction r startswith=&quot;X&quot; endsWith=&quot;y&quot; maxspan=4s</p>",64936101.0,1,0,,2020-11-18 20:24:42,,2020-11-20 20:00:45,2020-11-20 19:58:13,,4418.0,,12976099.0,,1,1,splunk|splunk-query,125,8
816,259272,64910387,Splunk: How to extract fields directly in search bar without having to use regular expressions?,"<p>I find it very hard to use regular expressions directly in the search bar to extract fields. Another problem is that I do not have the permission to share my extracted fields (extracted by the field extractor and stated in field extractions) with other people. I am now looking for another way to extract fields directly in the search bar. Is there something like this possible in Splunk?</p>
<p>Thanks!</p>",64912544.0,1,0,,2020-11-19 11:04:38,,2020-11-19 13:19:40,,,,,11572712.0,,1,0,extract|splunk|splunk-query,255,10
817,259273,64912713,How to modify regular expressions so that it extracts same fields of both fields?,"<p>When looking for some logs in one file I got two types of logs (one with white spaces and one without).
I would now like to extract <em>doSomething</em> and <em>doAnotherThing</em> out of these logs with one regular expression.</p>
<p><strong>Logfile 1:</strong>
<em>&quot;taskType&quot;:&quot;doSomething&quot;</em></p>
<p><strong>Logfile 2:</strong>
<em>&quot;taskType&quot; : &quot;doAnotherThing&quot;</em></p>
<p>I coded this regular expression: <code>taskType.....(?&lt;taskType1&gt;\w+)</code></p>
<p>It works good for Logfile 1 but not for Logfile 2, because it cuts the first two characters of the word. Is there a way to eliminate this issue?</p>
<p>Thanks!</p>",64912831.0,1,0,,2020-11-19 13:29:38,,2020-11-19 13:36:24,,,,,11572712.0,,1,0,extract|splunk,46,8
818,259274,64915269,Splunk: How to use multiple regular expressions in one query?,"<p>I have four regular expressions which I would like to use for one query. All the regular expressions are okay for itselves but I did not find out how to use them in pne query together:</p>
<p>These are the regular expressions:</p>
<p>Expression 1:</p>
<pre><code>(?&lt;time&gt;\d{4}.\d{2}.\d{2}\s\d{2}.\d{2}.\d{2}.\d{3})
</code></pre>
<p>Expression 2:</p>
<pre><code>deviceId...(?&lt;deviceId&gt;\d+)
</code></pre>
<p>Expression 3:</p>
<pre><code>error....code...(?&lt;errorCode&gt;\w+)
</code></pre>
<p>Expression 4:</p>
<pre><code>&quot;\&quot;message...(?&lt;errorMessage&gt;.*?)\&quot;
</code></pre>
<p>And I tried this among some other things in Splunk:</p>
<pre><code> source=&quot;xyz.log&quot; |rex field=_raw  &quot;(?&lt;time&gt;\d{4}.\d{2}.\d{2}\s\d{2}.\d{2}.\d{2}.\d{3}) deviceId...(?&lt;deviceId&gt;\d+) error....code...(?&lt;errorCode&gt;\w+) &quot;\&quot;message...(?&lt;errorMessage&gt;.*?)\&quot;&quot; |table time deviceId errorCode errorMessage
</code></pre>
<p>But I got an error.</p>",64919675.0,2,1,,2020-11-19 15:55:50,,2020-11-19 20:43:48,,,,,11572712.0,,1,0,regex|splunk|splunk-query,539,15
819,259275,64918841,Unable to Send Data to Splunk; Keep Getting Exception Occurred 400 Client Error,"<p>I'm trying to send data to Splunk to monitor on their dashboards. I'm getting this response when I try to send it via my python script that parses JSON data, cleans it up, and sends it to Splunk. I'm certain I have the right Host, Index (HEC Token), and Port. I'm also certain that the body of my request is accurate. Any ideas on what caused this?</p>
<p>I've tried using a different Index. I've tried printing the response (didn't add any detail really). I'm just not sure what it could be, and I wish the exception was more specific.</p>
<p>Response:</p>
<pre><code>Exception Occurred 400 Client Error: Bad Request for url: https://splunk.url.i.am.using:8088/services/collector
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://splunk.url.i.am.using:8088/services/collector
</code></pre>",,0,2,,2020-11-19 19:39:37,,2020-11-19 19:39:37,,,,,14671465.0,,1,0,exception|splunk|httpexception,46,6
820,259276,64963690,Is it possible to call a URL in a Splunk dashboard and get the response as a string?,"<p>I have a Splunk dashboard where you have a table with selected encoded identifiers.</p>
<p>You can click on a row and select an identifier as a token which fills additional fields with data. Now on the intranet of my company we have a url where you can enter the encoded identifier and get back the decoded data in a GET request.</p>
<p>Right now I have a single-value field which displays the encoded identifier and when you click on it it makes a call to the decoder and opens the decoded result in a new tab. That's a standard Splunk link.</p>
<p>Is it possible to make Splunk call the URL automatically (do a GET request) when the identifier is selected (the token is set) and retrieve the response data as a string and extract (using regex) and display the decoded data automatically in the single value field?</p>
<p>If not, is it at least possible for Splunk to get the response data as a string instead of opening the result in a new tab when you click on the encoded identifier?</p>",64971025.0,1,0,,2020-11-23 06:50:45,,2020-11-23 15:11:06,,,,,2756793.0,,1,0,splunk,330,11
821,259277,65017372,Splunk Charting Data Based on Type,"<p>I have derived data from Splunk in the following format (Actual Format). But I want to format furthermore it in such a way that I can see which items are present in which categories, and which are missing (Expected Format). I am trying to chart it based on categoryID, but it's not working for me as I do not think max function is appropriate for this. Can anyone please help me know how can I achieve this</p>
<p>Tried using</p>
<pre><code>| chart max(itemId) over itemId by categoryID
</code></pre>
<p>Actual Data</p>
<p><a href=""https://i.stack.imgur.com/Jq8s4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jq8s4.png"" alt=""Actual Format"" /></a></p>
<p>Expected Format Data:</p>
<p><a href=""https://i.stack.imgur.com/KOq3M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KOq3M.png"" alt=""Expected Format"" /></a></p>",,1,0,,2020-11-26 07:02:22,,2020-11-26 11:00:52,2020-11-26 07:38:25,,2833621.0,,2833621.0,,1,0,charts|splunk|splunk-query,21,5
822,259278,65031325,"Is there a way to achieve either before/after events in Splunk, or a real-time stream of events in a report?","<p>ON a *nix command-line, you can see relative before-and-after lines while using the <code>grep</code> command :</p>
<pre><code>  grep &quot;abc&quot; -A 2 -B 3
</code></pre>
<ol>
<li><p>My question: is there any way in Splunk enterprise product to see relative lines when doing a search?</p>
</li>
<li><p>Splunk search supports <a href=""https://docs.splunk.com/Documentation/Splunk/latest/SearchReference/head"" rel=""nofollow noreferrer""><code>head</code></a> and <a href=""https://docs.splunk.com/Documentation/Splunk/latest/SearchReference/tail"" rel=""nofollow noreferrer""><code>tail</code></a>. Is there a way to do continuous stream on a Splunk dashboard similar to the <code>-f</code> flag to <a href=""https://www.man7.org/linux/man-pages/man1/tail.1.html"" rel=""nofollow noreferrer""><code>tail</code></a> on a *nix command-line?.</p>
</li>
</ol>",,1,0,,2020-11-27 02:18:52,,2020-11-27 16:03:02,2020-11-27 16:01:55,,4418.0,,8312606.0,,1,1,grep|splunk|splunk-query,561,12
823,259279,65034409,Splunk forwarder gets blocked if one output destination is down,"<p>I have a Splunk forwarder which sends events to two third-party systems(through TCP) and also index them into a splunk indexer.</p>
<p>The problem I'm facing on is that if any of the two third-pary systems goes down. Splunk stops indexing events and neither sends them to the other system.</p>
<p>The output.conf I have is:</p>
<pre><code>[tcpout]
defaultGroup = default-system1, default-system2
indexAndForward = 0

[tcpout:default-system1]
server = &lt;IP&gt;:&lt;PORT&gt;

[tcpout-server://&lt;IP&gt;:&lt;PORT&gt;]

[tcpout:default-system2]
server = &lt;IP&gt;:&lt;PORT&gt;
sendCookedData = false
</code></pre>
<p>Is there any way to avoid such a dependency? If one of the destinatios servers is down, it doesn't affect the other sending. I've been looking at the documentation and there are some options that could be use.</p>
<p>heartbeatFrequency in combination with sendCookedData.</p>
<pre><code>heartbeatFrequency = &lt;integer&gt;
* How often (in seconds) to send a heartbeat packet to the receiving server.
* This setting is a mechanism for the forwarder to know that the receiver
  (indexer) is alive. If the indexer does not send a return packet to the
  forwarder, the forwarder declares the receiver unreachable and does not
  forward data to it.
* The forwarder only sends heartbeats if the 'sendCookedData' setting
  is set to &quot;true&quot;.
* Default: 30


sendCookedData = &lt;boolean&gt;
* Whether or not to send processed or unprocessed data to the receiving server.
* If set to &quot;true&quot;, events are cooked (have been processed by Splunk software).
* If set to &quot;false&quot;, events are raw and untouched prior to sending.
* Set to &quot;false&quot; if you are sending events to a third-party system.
* Default: true
</code></pre>
<p>But I'm not sure if it's the most correct approach, based on the description of sendCookedData, <strong>&quot;Set to &quot;false&quot; if you are sending events to a third-party system.&quot;</strong></p>",,1,0,,2020-11-27 08:53:25,,2020-11-27 14:12:33,,,,,7159045.0,,1,0,splunk,268,9
824,259280,65041211,How Serilog sent all trace together (to Splunk),"<p>I'm testing Serilog with Splunk, With a vanilla settings (&quot;Serilog.Sinks.Console&quot;, &quot;Serilog.Sinks.Splunk&quot;):</p>
<pre><code>   Log.Logger = new LoggerConfiguration()
                .ReadFrom.Configuration(builtConfig)
                .Enrich.FromLogContext()
                .CreateLogger();

            try
            {
                Host.CreateDefaultBuilder()
                   .UseSerilog() // &lt;-- Add this line
                .ConfigureWebHostDefaults(webBuilder =&gt;
                {
                    webBuilder.UseStartup&lt;Startup&gt;();
                }).Build().Run();
            }
            catch (System.Exception ex)
            {
                Log.Fatal(ex, &quot;WebHost server has terminated unexpectedly!&quot;);
            }
            finally
            {
                Log.CloseAndFlush();
            }
</code></pre>
<p>But having some problems:</p>
<ul>
<li>Per default, Serilog is sending in a Json format (better if is in a text format);</li>
<li>In console log, is printing the events (per line) in total, but in the Splunk is sending per event, not in the end of trace. Is possible to change it? - Example: Send the document with all trace and not sending per line (event).</li>
</ul>",,0,6,,2020-11-27 17:09:36,,2020-11-27 17:50:01,2020-11-27 17:50:01,,9261652.0,,9261652.0,,1,0,asp.net-core|splunk|serilog,384,10
825,259281,65075588,parsing JSON from splunk fetching from s3,"<p>When i fetch JSON file from azure block storage and aws S3 and parse it in splunk it parses it as normal file.</p>
<p>instead if i try to upload JSON file directly in slunk portal then it parse JSON properly and displays results.</p>
<p>how to parse it as JSON and display when its automatically fetched from S3 or Blop storage</p>
<p>i have tried using following <a href=""https://splunkonbigdata.com/2020/02/18/aws-s3-and-splunk-integration/"" rel=""nofollow noreferrer"">link</a>.</p>
<p>i could be able to parse as normal log file but not as JSON</p>",,0,3,,2020-11-30 14:38:47,,2020-12-10 07:58:31,2020-12-10 07:58:31,,10632369.0,,10467915.0,,1,0,json|amazon-s3|blob|splunk,58,7
826,259282,65081792,Splunk - how to parse JSON ingested from Azure blob?,"<p>I have file called &quot;30 Jan 2020.json&quot; that contains 2 records:</p>
<pre><code>[
  {
    &quot;Sender&quot;: &quot;John&quot;,
    &quot;Recipient&quot;: &quot;Alice&quot;,
    &quot;Subject&quot;: &quot;Hello&quot;,
    &quot;MessageDate&quot;: &quot;10 Jan 2020&quot;
  },
  {
    &quot;Sender&quot;: &quot;Jane&quot;,
    &quot;Recipient&quot;: &quot;Bob&quot;,
    &quot;Subject&quot;: &quot;Holiday&quot;
    &quot;MessageDate&quot;: &quot;15 Jan 2020&quot;
  }
]
</code></pre>
<p>My props.conf file is</p>
<pre><code>[_json_for_azure]
INDEXED_EXTRACTIONS = json
KV_MODE = json
LINE_BREAKER = ([\r\n]+)
NO_BINARY_CHECK = true
category = Structured
description = JavaScript Object Notation format. For more information, visit http://json.org/
disabled = false
pulldown_type = 1
</code></pre>
<p>and in inputs.conf I specified</p>
<pre><code>[mscs_storage_blob://mycontainer]
account = mycontainername
blob_mode = append
collection_interval = 60
container_name = mycontainer
sourcetype = mscs:storage:json
</code></pre>
<p>But when this data is ingested, I only get entries called _Time, and when I expand any of them, they show all records json as single string as row. I want the actual data such as each Sender, Recipient, Subject to be ingested as individual entries.  So when I search for individual sender such as John, I want only the single row to be returned, rather than returning whole file contents. This is the behaviour when using Azure Splunk plugin. When I loaded the file directly via &quot;Add data&quot; option from the main menu, the file is parsed correctly with individual entries.</p>",,0,0,,2020-11-30 22:02:25,,2020-12-1 06:16:02,2020-12-1 06:16:02,,14247919.0,,14247919.0,,1,1,splunk|splunk-formula|splunk-calculation,98,7
827,259283,65096655,How to make a dynamic span for a timechart?,"<p>I have a splunk dashboard whose query looks like so:</p>
<pre><code>index=my_index sourcetype=cloudwatch_log responseTime | timechart span=5m avg(responseTime) as responseTime
</code></pre>
<p>The dashboard has a time input. I want the span in the above query to update based on the time input chosen such that the span is always, say, 10% of the time range. As an example, the user chooses 15minutes, the span would be 1.5m. How can I do this?</p>",65099199.0,2,0,,2020-12-1 18:46:38,,2020-12-1 22:02:06,,,,,6509816.0,,1,0,splunk,325,12
828,259284,65101933,Splunk : Record deduplication using an unique field,"<p>We are considering moving out log analytics solution from ElasticSearch/Kibana to Splunk.</p>
<p>We currently use &quot;document id&quot; in ElasticSearch to deduplicate records when indexing :</p>
<p><a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html</a></p>
<p>We generate the id using hash of the content of the each log-record.</p>
<p>In Splunk, I found the internal field &quot;_cd&quot; which is unique to each record in Splunk index: <a href=""https://docs.splunk.com/Documentation/Splunk/8.1.0/Knowledge/Usedefaultfields"" rel=""nofollow noreferrer"">https://docs.splunk.com/Documentation/Splunk/8.1.0/Knowledge/Usedefaultfields</a></p>
<p>However, using HTTP Event Collector to ingest records, I couldn't find any way to embed this &quot;_cd&quot; field in the request :
<a href=""https://docs.splunk.com/Documentation/Splunk/8.1.0/Data/HECExamples"" rel=""nofollow noreferrer"">https://docs.splunk.com/Documentation/Splunk/8.1.0/Data/HECExamples</a></p>
<p>Any tips on how to achieve this in Splunk ?</p>",65109031.0,3,0,,2020-12-2 03:47:51,,2021-8-3 15:54:36,,,,,180904.0,,1,0,splunk|splunk-query,245,13
829,259285,65112730,configuring Splunk for usability,"<p>I'm trying to work with Splunk, and it's uncomfortable for me. Having worked with log files for plenty of years, the Splunk approach is pretty awkward to me. So I'm trying to configure it for me to enable productive working. So far I couldn't find anything concerning these issues:</p>
<ul>
<li>how can I remove line breaks and get a horizontal scroll bar instead?</li>
<li>how can I increase the pagination size to something reasonable like 1k or 10k?</li>
<li>how can I disable the alternating row colors?</li>
<li>is there something like &quot;tail -f&quot;?</li>
</ul>
<p>So far my best approach for the first 3 questions is downloading the query and using a text editor, but it feels like I'm missing something there.</p>",,1,2,,2020-12-2 17:02:51,,2020-12-7 16:48:18,,,,,2948478.0,,1,-2,splunk,52,7
830,259286,65212109,Splunk : How to get alert on 80 % utilization of memory of an application in splunk dashboard,"<p>am new in splunk, I am trying as below, the value, which I am getting is in digits and I want it should be in percentage, please suggest any solution,.....</p>
<pre><code>index=&quot;xxxxx&quot;  
| spath plugin  
| search plugin=memory 
| spath type_instance  
| search type_instance=free  
| spath host  
| search host=&quot;*&quot; 
| eval percentage=round(mem_used/mem,3)*100 
| where percentage &gt; 80
| fields  mem_used, mem, percentage
| eval gegabytes=(((value/1024)/1024)/1024) 
| timechart span=1m avg(gegabytes) by host

index=&quot;xxxx&quot;  
| spath plugin  
| search plugin=memory 
| spath type_instance  
| search type_instance=free  
| spath host  
| search host=&quot;*&quot; 
| eval gegabytes=(((value/1024)/1024)/1024) 
| timechart span=1m avg(gegabytes) by host
</code></pre>",,0,0,,2020-12-9 07:06:52,,2020-12-9 14:28:15,2020-12-9 14:28:15,,4418.0,,11799578.0,,1,0,splunk|splunk-query,107,8
831,259287,65212859,Splunk Cloud search query with variable does not return results,"<p>I have a query that does not return results and shows no errors (the same with where and search command):</p>
<pre><code>&quot;ExtendedProperties.PrCode&quot;=&quot;myProductName&quot; 
| eval myversion=&quot;12.916&quot;|  where &quot;ExtendedProperties.ProductVersion&quot;=myversion
</code></pre>
<p>The query without eval returns results:</p>
<pre><code>&quot;ExtendedProperties.PrCode&quot;=&quot;myProductName&quot; 
|  search &quot;ExtendedProperties.ProductVersion&quot;=&quot;12.916&quot; 
</code></pre>
<p>The product version last three digits are the month (September) and the day (16), my final goal is to extract them from the current date, using the now() function. This will remove the need to update the query every day.
Unfortunately this query is also not returning results:</p>
<pre><code>&quot;ExtendedProperties.PrCode&quot;=&quot;myProductName&quot; 
| eval month = ltrim(tostring(strftime(now(),&quot;%m&quot;)),&quot;0&quot;) 
| eval day = strftime(now(),&quot;%d&quot;) 
| eval version=&quot;12.&quot; + month + day 
| where &quot;ExtendedProperties.ProductVersion&quot;=version
</code></pre>
<p>Here is some sample data:</p>
<pre><code>{&quot;Timestamp&quot;:&quot;2020-12-14T14:37:00.2662745Z&quot;,&quot;Categories&quot;:[&quot;someCategoryString&quot;],&quot;Metadata&quot;:[&quot;someMetadataString&quot;],&quot;ExtendedProperties&quot;:{&quot;MachineId&quot;:&quot;SomeMachineId&quot;,&quot;ProductVersion&quot;:&quot;12.916&quot;,&quot;PrCode&quot;:&quot;MyProductName&quot;,&quot;ProductType&quot;:&quot;1&quot;,&quot;Type&quot;:&quot;ProductUsed&quot;,&quot;Source&quot;:&quot;SomeSourceString&quot;,&quot;SessionId&quot;:&quot;SomeGuid&quot;,&quot;TimeStamp&quot;:&quot;2020-12-14T14:36:56.7086819Z&quot;,&quot;Environment&quot;:&quot;SomeEnvironment&quot;}}
</code></pre>
<p>This returns results:</p>
<pre><code>|makeresults | eval _raw = &quot;{\&quot;Timestamp\&quot;:\&quot;2020-12-14T14:37:00.2662745Z\&quot;,\&quot;Categories\&quot;:[\&quot;someCategoryString\&quot;],\&quot;Metadata\&quot;:[\&quot;someMetadataString\&quot;],\&quot;ExtendedProperties\&quot;:{\&quot;MachineId\&quot;:\&quot;SomeMachineId\&quot;,\&quot;ProductVersion\&quot;:\&quot;12.1219\&quot;,\&quot;PrCode\&quot;:\&quot;MyProductName\&quot;,\&quot;ProductType\&quot;:\&quot;1\&quot;,\&quot;Type\&quot;:\&quot;ProductUsed\&quot;,\&quot;Source\&quot;:\&quot;SomeSourceString\&quot;,\&quot;SessionId\&quot;:\&quot;SomeGuid\&quot;,\&quot;TimeStamp\&quot;:\&quot;2020-12-14T14:36:56.7086819Z\&quot;,\&quot;Environment\&quot;:\&quot;SomeEnvironment\&quot;}}&quot;, month = ltrim(tostring(strftime(now(),&quot;%m&quot;)),&quot;0&quot;), day = strftime(now(),&quot;%d&quot;),version=&quot;12.&quot;+month+day|spath | search &quot;ExtendedProperties.ProductVersion&quot;=&quot;12.1219&quot;
</code></pre>
<p>However, when I replace the string  &quot;12.1219&quot; with the version variable that has the same value (at the end of the search), there are no results found:</p>
<pre><code>|makeresults | eval _raw = &quot;{\&quot;Timestamp\&quot;:\&quot;2020-12-14T14:37:00.2662745Z\&quot;,\&quot;Categories\&quot;:[\&quot;someCategoryString\&quot;],\&quot;Metadata\&quot;:[\&quot;someMetadataString\&quot;],\&quot;ExtendedProperties\&quot;:{\&quot;MachineId\&quot;:\&quot;SomeMachineId\&quot;,\&quot;ProductVersion\&quot;:\&quot;12.1219\&quot;,\&quot;PrCode\&quot;:\&quot;MyProductName\&quot;,\&quot;ProductType\&quot;:\&quot;1\&quot;,\&quot;Type\&quot;:\&quot;ProductUsed\&quot;,\&quot;Source\&quot;:\&quot;SomeSourceString\&quot;,\&quot;SessionId\&quot;:\&quot;SomeGuid\&quot;,\&quot;TimeStamp\&quot;:\&quot;2020-12-14T14:36:56.7086819Z\&quot;,\&quot;Environment\&quot;:\&quot;SomeEnvironment\&quot;}}&quot;, month = ltrim(tostring(strftime(now(),&quot;%m&quot;)),&quot;0&quot;), day = strftime(now(),&quot;%d&quot;),version=&quot;12.&quot;+month+day|spath | search &quot;ExtendedProperties.ProductVersion&quot;=version
</code></pre>
<p>The expected output is one record that contains the expected version (12.1219 for today).</p>",65374683.0,2,0,,2020-12-9 08:08:58,,2020-12-19 21:34:16,2020-12-19 15:55:58,,7624566.0,,7624566.0,,1,2,splunk|splunk-query,237,11
832,259288,65214290,Splunk : How can we get the combine result of cache and memory on splunk dashboard using splunk query,"<p>I am trying to get combine results of cache and memory utilization on splunk dashboard using splunk query , can someone please help me with splunk query</p>",,1,0,,2020-12-9 09:45:37,,2021-1-14 03:09:53,,,,,11799578.0,,1,0,splunk-query|splunk-formula,31,5
833,259289,65226780,How to collect statistics from a dataframe by filtering it out with values from a list of values-Python?,"<p>I have a list of email addresses and a dictionary turned into a dataframe and I want to match my list of email addresses with the email adddresses in my the dataframe. In addition, count how many times the values match with eachother. Also, add a column called 'date_added' with the current timestamp. Finally I want to send those statistics to a new dataframe.</p>
<p><strong>Here is my email list:</strong></p>
<pre><code>senders =  ['aa@aol.com','ab@aol.com','bs@aol.com','qq@gmail.com']
</code></pre>
<p><strong>Here is my dataframe that has been converted to a csv file. In my script the variables name is df2:</strong></p>
<p><a href=""https://i.stack.imgur.com/hvr16.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hvr16.png"" alt=""enter image description here"" /></a></p>
<p>As you can see there are 4 matches of aa@aol.com, 3 matches of qq@gmail.com, and 1 match of bs@aol.com.</p>
<p><strong>Given that I want my new dataframe to look like this:</strong></p>
<pre><code>count|date_added|sender|tag|sourcetype
4,&lt;current_timestamp&gt;,aa@aol.com,email,source1
3,&lt;current_timestamp&gt;,qq@gmail.com,email,source1
1,&lt;current_timestamp&gt;,bs@aol.com,['email','filter'],source1
</code></pre>
<p>As you can see, my new dataframe displays the value that matched from my email list and how many times it matched in df2. Also, added the 'date_added' column as well.</p>
<p><strong>Here is my python so far:</strong></p>
<pre><code># Get the sender list dataframe
senders = ['aa@aol.com','ab@aol.com','bs@aol.com','qq@gmail.com']

# Create Connection
service = client.connect(host=splunk_config['host'], port=splunk_config['port'],
                         username=splunk_config['username'], password=splunk_config['password'])


# Create Parameters
kwargs_oneshot = {'count': 0}

# Create search query
search_query1 = &lt;my_query&gt;

# Create results
oneshotsearch_results = service.jobs.oneshot(search_query1, **kwargs_oneshot)

# Get the results and display them using the ResultsReader7
reader = results.ResultsReader(oneshotsearch_results)

#Get the splunk query results in a datafra,e
df2 = pd.DataFrame.from_dict(reader)

a=(df1.merge(df2, how='left').groupby('sender').sourcetype.count().reset_index(name='count').assign(date_added=pd.Timestamp.now()))
df2.to_csv(r'sender_stats123.csv', header=True, index=False)
</code></pre>
<p>When I run this code, it doesn't give me the desired output. It writes it to my csv file, but it doesn't accurately capture the # od counts (matches).</p>
<p>I know my code isn't much but I am new to python and dataframes so I am completely lost so any ideas or suggestions would help.</p>",,0,1,,2020-12-10 00:16:15,,2020-12-10 00:16:15,,,,,14088822.0,,1,0,python|python-3.x|pandas|dataframe|splunk,35,6
834,259290,65281287,Splunk Free giving error while installing new app,"<p>I am using Splunk Free on my local Ubuntu machine. I have added a datasource to monitor and it's working properly.</p>
<p>Splunk Free Version - 8.1.1
Ubuntu Version - 20.04.1 LTS</p>
<p>The issue is, when I try to install a new app i.e. <strong>Splunk Add-on for Unix and Linux</strong>.
Even for valid credentials its saying <strong>Incorrect Username or Password</strong> for both Splunk Admin and Splunk User.</p>
<p>Bdw, with same credentials I am able to login to Splunk.com</p>
<p><a href=""https://i.stack.imgur.com/eXNnB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eXNnB.png"" alt=""enter image description here"" /></a></p>",65282284.0,2,0,,2020-12-13 22:18:26,,2021-1-3 17:25:48,2020-12-13 22:24:20,,12085985.0,,12085985.0,,1,0,ubuntu|splunk,155,10
835,259291,65313981,Splunk searching event logs to find values exceeding a given threshold,"<p>I want to search the log event</p>
<pre><code>&quot;Closure request counts: startAssets: &quot; 
</code></pre>
<p>and find occurrences where the <code>startAssets</code> are larger than 50.</p>
<p>How would I do that?</p>
<p>Something like:</p>
<pre><code>Closure request counts: startAssets: 51
</code></pre>
<p>would maybe give a search similar to</p>
<pre><code>&quot;Closure request counts: startAssets: {num} AND num &gt;=50&quot;
</code></pre>
<p>perhaps?</p>
<p>What does that look like in SPL?</p>",65315355.0,1,0,,2020-12-15 21:29:59,,2020-12-16 14:47:49,2020-12-16 14:46:43,,4418.0,,14306240.0,,1,1,splunk|splunk-query,36,10
836,259292,65330303,How to make a Splunk Dashboard dynamic?,"<p>I have a query lets call it query1 that looks at sales and I have storenumber=1 in that query  I have query2 that looks at returns with storenumber=1 in the second query</p>
<p>I want to add these queries to my store's dashboard. Now in the dashboard, I want to have a box (text, dropdown, anything) where I can enter a new store number and run both those queries.</p>
<p>If I enter say 2 in that box how does that 2 get replaced in the queries where store number =1 to be storenumber=2 now?</p>
<p>I thought it would be some token like variable? but I'm not sure how to get that to work so that the number entered is populated where the storenumber= is in the queries?</p>
<p>Any help will be appreciated
Thank You</p>",65333121.0,1,0,,2020-12-16 19:57:14,,2021-2-23 16:11:34,2021-2-23 16:11:34,,4418.0,,1919636.0,,1,1,splunk|splunk-dashboard,179,11
837,259293,65331520,Setting up Splunk alerting,"<p>Is there a way to monitor that data is received regularly, and alert when out of compliance?</p>
<p>I would love to setup alerting for our GCP and Azure environmennts.</p>",,1,0,,2020-12-16 21:41:39,,2020-12-17 00:38:55,,,,,13349004.0,,1,0,splunk|splunk-formula,18,6
838,259294,65341023,Blank CSV in splunk report,"<p>in splunk report, can we get at least header(column name)in attached CSV in autogenerated email, if there are no data in CSV/splunk..
eg. if no data then also it should atleast show header name..</p>",,1,0,,2020-12-17 12:46:14,,2020-12-18 15:31:44,,,,,14843828.0,,1,0,splunk|splunk-query|splunk-formula|splunk-calculation,36,6
839,259295,65355904,Splunk equivalent to chained greps for searching within a search,"<p>I want to be able to search for patterns in Splunk and then search for another pattern in the search results. I should be able to repeat it for any number of strings.</p>
<p>This is very simple in Linux:</p>
<p><code>grep pattern_1 &lt;file name&gt; | grep pattern_2 | grep pattern_3 | grep pattern_4</code></p>
<p>How can I do this in Splunk? I would like to make it generic irrespective of the count of sub-searches.</p>",65357400.0,2,0,,2020-12-18 11:04:26,,2020-12-18 14:06:20,2020-12-18 12:52:02,,4418.0,,10057044.0,,1,0,splunk|splunk-query,513,10
840,259296,65359627,Splunk relative times with + are incorrect in dashboard (out by a week),"<p>Here is a test Splunk dashboard (using Splunk 8.0.4.1):</p>
<pre><code>&lt;dashboard&gt;
  &lt;label&gt;test&lt;/label&gt;
  &lt;init&gt;
    &lt;eval token=&quot;argh1&quot;&gt;now()&lt;/eval&gt;
    &lt;eval token=&quot;argh2&quot;&gt;relative_time(now(), &quot;-1s&quot;)&lt;/eval&gt;
    &lt;eval token=&quot;argh3&quot;&gt;relative_time(now(), &quot;+1s&quot;)&lt;/eval&gt;
  &lt;/init&gt;
  &lt;row&gt;
    &lt;panel&gt;
      &lt;html&gt;
        &lt;h2&gt;Dates ((Now $argh1$ -1s $argh2$ +1s $argh3$))&lt;/h2&gt;
      &lt;/html&gt;
    &lt;/panel&gt;
  &lt;/row&gt;
&lt;/dashboard&gt;
</code></pre>
<p>I would expect this to show me three times in Unix epoch format - the current time, then a number 1 less, then a number 1 more than the first number. Here is an actual result:</p>
<pre><code>Dates ((Now 1608304399 -1s 1608304398 +1s 1607699600))
</code></pre>
<p>As you can see, argh2 is indeed 1 less than current time, but argh3 is not 1 more; instead, it's substantially less. In fact it's exactly a week earlier than the 'now' time, plus one second.</p>
<p>This happens for any '+' time specification, +1w or +1d etc.</p>
<p>If I do the same thing inside a Splunk query on the same server:</p>
<pre><code>index=aws | eval argh1=now() | eval argh2=relative_time(now(), &quot;-1s&quot;) | eval argh3=relative_time(now(), &quot;+1s&quot;)
</code></pre>
<p>Then it works fine, I can look inside the results and see argh3 is 1 second after argh1.</p>
<p>I believe it uses the local computer time zone for dashboard pages which might make a difference but my computer time is correct. I tried in Firefox and Chrome on my Windows 10 PC in case the browser made any difference, but with the same results.</p>
<p>Does anyone else know what's going on? (Note: I don't have admin access to this Splunk install but I can probably ask the person who does.)</p>",,1,0,,2020-12-18 15:24:30,,2020-12-21 15:29:58,,,,,14132590.0,,1,0,splunk,121,8
841,259297,65407880,How do I take an hourly sum of one field value in splunk,"<pre><code>2020-12-07 23:57:10,160 INFO  [+] Number of fetched Availability to publish to Gcp PubSub topic. [ClassUnitKey=BU-STO-460] [NumberOfMessages=95] , [bsName=&quot;BsRunBatch&quot;], [userId=&quot;S-OLB-U-ITSEELM&quot;], [userIdRegion=&quot;EU&quot;]
</code></pre>
<p>As the above splunk log message , How to find the sum of [NumberOfMessages=95] field value in a hourly basis. I have written as below</p>
<p>|  timechart span=1h sum(NumberofMessages)</p>
<p>Its not giving the desired result. The below result i got
<a href=""https://i.stack.imgur.com/Lfaf4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Lfaf4.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Lfaf4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Lfaf4.png"" alt=""enter image description here"" /></a></p>",,1,7,,2020-12-22 11:26:33,1.0,2021-1-14 03:12:55,2020-12-22 19:14:01,,6778250.0,,6778250.0,,1,0,splunk|splunk-query,154,8
842,259298,65410259,How do I instrument my code for Splunk metrics?,"<p>I'm brand new to Splunk, having worked exclusively with Prometheus before. The one obvious thing I can't see from looking at the Splunk website is how in my code, I create/expose a metric... if I must provide an HTTP endpoint for consumption, or call into some API to push values, etc. Further, I cannot see which languages Splunk provide libraries for, in order to aid instrumentation - I cannot see where all this low level stuff is documented!</p>
<p>Can anyone help me understand how Splunk works, particularly how it compares to Prometheus?</p>",,1,2,,2020-12-22 14:16:18,,2020-12-22 15:27:03,,,,,197229.0,,1,-1,metrics|splunk,42,7
843,259299,65415006,How to correctly index files with asynchronous csv stream data into Splunk?,"<p>I am putting asynchronous csv stream data from each URL into each file one after another like below.</p>
<pre><code>async with httpx.AsyncClient(headers={&quot;Authorization&quot;: 'Token token=&quot;sometoken&quot;'}) as session:
    for url in some_urls_list:
        download_data(url, session)

@backoff.on_exception(backoff.expo,exception=(httpx.SomeException,),max_tries=7,)
async def download_data(url, session):
    while True:
        async with session.stream(&quot;GET&quot;, url) as csv_stream:
            csv_stream.raise_for_status()
            async with aiofiles.open(&quot;someuniquepath&quot;, &quot;wb&quot;) as f:
                async for data in csv_stream.aiter_bytes():
                    await f.write(data)
        break
</code></pre>
<p>I am ingesting this data into Splunk via <code>inputs.conf</code> and <code>props.conf</code> as below.</p>
<pre><code>[monitor:///my_main_dir_path]
disabled = 0
index = xx
sourcetype = xx:xx
</code></pre>
<pre><code>[xx:xx]
SHOULD_LINEMERGE = false
LINE_BREAKER = ([\r\n]+)
NO_BINARY_CHECK = true
CHARSET = UTF-8
INDEXED_EXTRACTIONS = csv
TIMESTAMP_FIELDS = xx
</code></pre>
<p>I am getting several issues in this as below.</p>
<ul>
<li>Some files are not indexed at all.</li>
<li>From some files only partial rows are indexed.</li>
<li>Some rows are abruptly divided into 2 events on Splunk.</li>
</ul>
<p>What could be done on the Splunk configuration side to solve above issues while taking care that it does not cause any duplicate data indexing issue?</p>
<p><strong>Sample Data:</strong> (First line is the header.)</p>
<pre><code>A,B B,C D,E,F,G H?,I J K,L M?,N/O P,Q R S,T U V (w x),Y Z,AA BB,CC DD,EE FF,GG HH,II JJ KK,some timestamp field,LL,MM,NN-OO,PP?,QQ RR ss TT UU,VV,WW,XX,YY,ZZ,AAA BBB,CCC,DDD-EEE,FFF GGG,HHH,III JJJ,KKK LLL,MMM MMM,NNN OOO,PPP QQQ,RRR SSS 1,TTT UUU 2,VVV WWW 3,XX YYY,ZZZ AAAA,BBBB CCCC
adata@adata.adata,&quot;bbdata, bbdata&quot;,ccdata ccdata,eedata eedata - eedata,ffdata - ffdata - 725 ffdata ffdata,No,,No,,,,,unknown,unknown,unknown,2.0.0,&quot;Sep 26 22:40:18 iidata-iidata-12cb65d081f745a2b iidata/iidata[4783]: iidata: to=&lt;iidata@iidata.iidata&gt;, iidata=iidata.iidata.iidata.iidata[111.111.11.11]:25, iidata=0.35, iidata=0.08/0/0.07/0.2, iidata=2.0.0, iidata=iidata (250 2.0.0 OK  1569537618 iidata.325 - iidata)&quot;,9/26/2019 22:40,,,,,,,wwdata,xxdata,5,&quot;zzdata, zzdata&quot;,aaadata aaadata aaadata,cccdata - cccdata,ddddata - ddddata,fffdata,hhhdata,25/06/2010,6,2010,&quot;nnndata nnndata nnndata, nnndata.&quot;,(pppdata'pppdata) pppdata pppdata,,,,303185,,
</code></pre>
<p><strong>Sample Broken Event:</strong></p>
<pre><code>adata@adata.adata,&quot;bbdata, bbdata&quot;,ccdata ccdata,eedata eedata - eedata,ffdata - ffdata - 725 ffdata ffdata,No,,No,,,,,unknown,un

known,unknown,2.0.0,&quot;Sep 26 22:40:18 iidata-iidata-12cb65d081f745a2b iidata/iidata[4783]: iidata: to=&lt;iidata@iidata.iidata&gt;, iidata=iidata.iidata.iidata.iidata[111.111.11.11]:25, iidata=0.35, iidata=0.08/0/0.07/0.2, iidata=2.0.0, iidata=iidata (250 2.0.0 OK  1569537618 iidata.325 - iidata)&quot;,9/26/2019 22:40,,,,,,,wwdata,xxdata,5,&quot;zzdata, zzdata&quot;,aaadata aaadata aaadata,cccdata - cccdata,ddddata - ddddata,fffdata,hhhdata,25/06/2010,6,2010,&quot;nnndata nnndata nnndata, nnndata.&quot;,(pppdata'pppdata) pppdata pppdata,,,,303185,,
</code></pre>",65416435.0,1,0,,2020-12-22 19:51:53,,2020-12-25 10:56:40,2020-12-25 10:56:40,,10064174.0,,10064174.0,,1,0,python|python-asyncio|splunk|httpx,73,9
844,259300,65485578,Mismatch in Stats UI & Splunk API,"<pre><code>index=&lt;&lt;My_index&gt;&gt; earliest=&quot;12/23/2020:10:00:00&quot; latest=&quot;12/23/2020:11:00:00&quot; &quot;&lt;&lt;url&gt;&gt;&quot; 
| eval MyFeild=replace(MyFeild,&quot;\d{1}\d+&quot;,&quot;&quot;) 
| search MyFeild=*sample_search* 
| stats count by MyFeild
</code></pre>
<p>When I have been using the above search criteria in UI, it gives a table in Statistics tab with <code>MyFeild</code> and its total Count</p>
<p>When I am using the splunkapi for some reason I am not getting the consolidated output and I am trying to remove any numerical data in the URL</p>
<pre><code>https://&lt;&lt;mydomain for my splunkapi&gt;&gt;/services/search/jobs/export?search=search index=&lt;&lt;My_index&gt;&gt;  earliest=&quot;12/23/2020:10:00:00&quot; latest=&quot;12/23/2020:11:00:00&quot; &quot;&lt;&lt;URL&gt;&gt;&quot; 
| eval MyFeild=replace(MyFeild,&quot;\d{1}\d+&quot;,&quot;&quot;) 
| search MyFeild=*sample_search* 
| stats count by MyFeild
</code></pre>
<p>Not sure If I am missing anything here</p>",,0,2,,2020-12-28 23:55:07,,2020-12-29 13:14:49,2020-12-29 13:14:49,,4418.0,,14896804.0,,1,1,splunk,27,5
845,259301,65565346,How can I provide metrics to Splunk via HTTP?,"<p>I have been reading through Splunk Enterprise documentation and it appears I can provide metrics in JSON format over HTTP/HTTPS: <a href=""https://docs.splunk.com/Documentation/Splunk/8.1.1/Metrics/GetMetricsInOther#Get_metrics_in_from_clients_over_HTTP_or_HTTPS"" rel=""nofollow noreferrer"">https://docs.splunk.com/Documentation/Splunk/8.1.1/Metrics/GetMetricsInOther#Get_metrics_in_from_clients_over_HTTP_or_HTTPS</a></p>
<p>However I can't see a reference what exactly this JSON format looks like, beyond one example. I'm also not clear from the docs if Splunk can be configured to poll this endpoint on my process, or if I must push the data <em>to</em> Splunk.</p>",,1,0,,2021-1-4 15:34:26,,2021-1-4 18:05:56,,,,,197229.0,,1,0,splunk,68,8
846,259302,65565490,what is the default password of Splunk enterprises,"<p>I have configured splunk from AWS marketplace, I am unable to login using default password.</p>
<p>user name: admin
password: changeme</p>
<p>also i tried below password</p>
<p>SPLUNK-{i-00014d5f17644a8e0}</p>",,1,0,,2021-1-4 15:45:14,,2021-1-4 17:46:24,,,,,14852192.0,,1,0,splunk,72,8
847,259303,65576076,Splunk query over @Timed annotation by the Prometheus metrics,"<p>As the title says I have @Timed annotation in my microservice written in spring-boot:</p>
<p>@Timed(value = &quot;api.rest.get-account-msgs&quot;,histogram = true,percentiles = {0.5, 0.95, 0.99})</p>
<p>and I'm struggling to find the correct query for the splunk to visualize the results of this annotations, the query should show results of this method on daily basis.</p>",,0,0,,2021-1-5 09:18:54,,2021-1-5 09:18:54,,,,,10288774.0,,1,1,spring-boot|prometheus|splunk|splunk-query|splunk-calculation,86,7
848,259304,65596568,Multifields search in Splunk without knowing field names,"<p>There are some two values <code>V1</code> and <code>V2</code> and I do not know index field names. How should be looked request if I wanna have in selection response the following:</p>
<pre><code>unknown-field-name1 = V1
unknown-field-name2 = V2
</code></pre>
<p>I am a beginner in Splunk world and just tried to use  &quot;V1 AND V2&quot;, but it doesn't work.</p>",65597115.0,1,0,,2021-1-6 13:22:31,,2021-1-6 13:57:50,,,,,2948684.0,,1,1,splunk|splunk-query,102,10
849,259305,65609167,How does Splunk monitor Files/Directories and what is the performance impact?,"<p>I'm wondering what is the implementation of Splunk Files/Directory Monitor feature that ensures the small footprint on the performance of the system running the Data Collector (in terms of CPU, memory and disk I/O)?</p>
<p>I'm asking since we are considering running an Universal Forwarder on a production machine to forward and monitor its log, but we would like to best analyze the performance hit to make sure it doesn't affect the availability of the service in production.</p>",,1,0,,2021-1-7 08:46:26,,2021-1-7 14:09:48,,,,,6616472.0,,1,0,splunk,54,7
850,259306,65613101,Splunk: Schedule alert to run every 10 minutes,"<p>I want to set an alert which runs every 10 minutes and is triggered when a certain ratio is either bigger than 2.5 or smaller than 0.5.</p>
<pre><code>&quot;certainEvent&quot; source=&quot;abc.log&quot; 
| timechart partial=f span=5m count as numbers 
| fillnull 
| streamstats current=f last(numbers) as last_numbers 
| eval ratio = numbers/last_numbers 
| where ratio&gt;2.5 OR ratio &lt; 0.5
</code></pre>
<p>Now I want to schedule the alert search to run every 10 minutes. Therefore, I want to run it on cron schedule and chose <code>*/10 * * * *</code>. Is that correct?</p>
<p>Secondly, I can choose an expiration date and a Time Range in the <strong>Save as Alert</strong>-menu. By default it seems to be set to the last 24 hours (Time Range) and Expiration date as well to the last 24 hours. I am now wondering if these settings do have an effect on the alert search. I do not want my alert search to find every event which fulfills the condition in the query I set upon every time in this time range but only once (and then get informed by mail). I also do not want the alert to expire after 24 hours but let the alert search run until it is stopped by me or someone else. So, I am wondering if I should or have to modify these two parameters in order to get the required functionality?</p>",65614130.0,1,0,,2021-1-7 13:17:30,,2021-1-7 15:59:25,2021-1-7 14:23:22,,4418.0,,11572712.0,,1,1,alert|splunk,607,12
851,259307,65630975,splunk dashboard to include the time rage that will populate into the pdf file,"<p>I have a splunk dashboard that returns results based on a time picker with a start and end time frame
I would like to have that time range in the dashboard so that the PDF generated it shows the time frame.</p>",65631832.0,1,0,,2021-1-8 14:35:50,,2021-2-23 16:11:16,2021-2-23 16:11:16,,4418.0,,1919636.0,,1,1,pdf|splunk|timepicker|splunk-dashboard,706,11
852,259308,65633206,Unable to send metrics to Splunk HEC,"<p>I have a HEC input set up on my Splunk v. 8.1.1 server and I am trying to send metrics to it, ie.:</p>
<pre><code>curl -k https://$SPLUNK_HOST:$HEC_PORT/services/collector/raw -H &quot;Authorization: Splunk $HEC_TOKEN&quot; \
-d &quot;
{'time': 1610123044, 'fields': {'metric_name': 'kernel.all.load', '_value': 2.8499999046325684, 'instance_id': 1, 'instance_name': '1 minute'}}
{'time': 1610123044, 'fields': {'metric_name': 'kernel.all.load', '_value': 3.8299999237060547, 'instance_id': 5, 'instance_name': '5 minute'}}
{'time': 1610123044, 'fields': {'metric_name': 'kernel.all.load', '_value': 3.6700000762939453, 'instance_id': 15, 'instance_name': '15 minute'}}&quot;
</code></pre>
<p>(Note: line breaks within the quotes added for clarity)</p>
<p>I get a positive response from the server every time:</p>
<pre><code>{'text': 'Success', 'code': 0}
</code></pre>
<p>But no data is saved in the index. I have the default index set for the HEC input. If I delete or disable this index I get the message on the main page in Splunk, ie.:</p>
<pre><code>Received event for unconfigured/disabled/deleted index=pcp_hec with source=&quot;source::http:PCP via HEC&quot; host=&quot;host::localhost:32926&quot; sourcetype=&quot;sourcetype::httpevent&quot;. So far received events from 1 missing index(es).
</code></pre>
<p>When I restore the index back the message disappears, but still no data is saved under the index. I cannot figure out what is wrong in my case, because the official documentation is very brief on this subject. I found two threads on this forum (<a href=""https://stackoverflow.com/questions/63606365"">1</a>, <a href=""https://stackoverflow.com/questions/65565346"">2</a>) and a few similar ones elsewhere, but the answers only contained the same example from the documentation. I tried to include the metadata from the examples, but that did not solve the problem. Nowhere does it say what the <code>perflog</code> sourcetype actually is. I also tried <code>log2metrics_json</code> for the sourcetype, but it did not help either.</p>",66325974.0,1,0,,2021-1-8 16:59:13,,2021-7-26 16:14:24,,,,,3442409.0,,1,2,json|metrics|splunk,213,10
853,259309,65633853,Log file size calculated using len(_raw) in Splunk does not match even close to the actual file size on the host?,"<p>I am using a Splunk query to calculate the size of logs files sent to Splunk. This is the Splunk query I have used:</p>
<pre><code>index=&quot;&lt;my_index&gt;&quot; path=&quot;/&lt;my_path&gt;/&lt;my_log_file&gt;&quot; 
| eval raw_len=len(_raw) 
| eval raw_len_kb = raw_len/1024 
| eval raw_len_mb = raw_len/1024/1024 
| eval raw_len_gb = raw_len/1024/1024/1024 
| stats sum(raw_len) as Bytes sum(raw_len_kb) as KB sum(raw_len_mb) as MB sum(raw_len_gb) as GB by source 
| addcoltotals
</code></pre>
<p>Splunk reports the size as 17 GB. On the other hand, when I do this on the Unix host:</p>
<pre><code>ls -l /&lt;my_path&gt;/&lt;my_log_file&gt;
</code></pre>
<p>the value is just a few MB.</p>
<p>Any idea why there is so much difference?</p>",,1,0,,2021-1-8 17:42:44,,2021-1-12 13:57:41,2021-1-12 13:57:41,,4418.0,,10057044.0,,1,0,splunk|splunk-query,378,11
854,259310,65680929,nested condition in splunk,"<p>I am looking for below result.</p>
<ul>
<li><p>india without scanner IP blocked</p>
</li>
<li><p>india without scanner IP nonblocked</p>
</li>
<li><p>india with scanner IP blocked</p>
</li>
<li><p>india with scanner Ip non blocked
where ip1,ip2=&gt;Scannner IP</p>
</li>
</ul>
<p>I have tried the below one ..but it's showing only &quot;india without scanner IP blocked&quot; count</p>
<pre><code>| eval BlockedStatus = case ( src !=&quot;ip1&quot; OR src !=&quot;ip2.*&quot; OR blocked=1,&quot;india without scanner IP blocked&quot;, src !=&quot;ip1&quot; OR src !=&quot;ip2*&quot; OR  blocked=0 ,&quot;india without scanner IP nonblocked&quot; ,src =&quot;ip1&quot; OR src =&quot;ip2&quot; OR blocked=1,&quot;india with scanner IP blocked&quot;, src =&quot;ip1&quot; OR src =&quot;ip2&quot; OR blocked=0 ,&quot; india with scanner Ip non blocked &quot;)
| stats count by eventtype,BlockedStatus 
| rename eventtype as &quot;Local Market&quot;,count as &quot;Total Critical Events&quot;
</code></pre>",,1,0,,2021-1-12 09:12:11,,2021-1-12 13:59:45,2021-1-12 13:59:45,,4418.0,,14843828.0,,1,0,splunk|splunk-query|splunk-formula|splunk-calculation,44,7
855,259311,65684187,Where are variables in splunk defined,"<p>I have an alert setup in splunk which uses below search string.</p>
<pre><code>index=&quot;someapp&quot; sourcetype = &quot;some:app&quot; &quot;some_scheduler&quot; &quot;ERROR&quot; 
| regex source = &quot;(lambda:prod)&quot;
</code></pre>
<p>I am not able to find where <code>(lambda:prod)</code> is defined. This search query works globally so I am assuming it's not something defined at app level or something.</p>",,1,1,,2021-1-12 12:40:55,,2021-1-12 14:49:09,2021-1-12 14:00:23,,4418.0,,2092445.0,,1,0,splunk,25,5
856,259312,65684393,How can Splunk distinguish between output to stdout and stderr in Docker context?,"<p>We have a Java application that can be run in Docker containers. It produces messages to <em>stdout</em> and <em>stderr</em> with a different level of detail for different audiences.</p>
<p>Configuring Splunk as log driver all log lines received by Splunk a marked with source <em>stdout</em> although there must be log lines being logged to <em>stderr</em>.</p>
<p>Splunk log driver configuration in docker-compose:</p>
<pre><code>logging:
    driver: splunk
    options:
        splunk-url: https://splunkhf:8088
        splunk-token: [TOKEN]
        splunk-index: splunk_index
        splunk-insecureskipverify: &quot;true&quot;
        splunk-sourcetype: log4j
        splunk-format: &quot;json&quot;
        tag: &quot;{{.Name}}/{{.ID}}&quot;
</code></pre>
<p>Example log message sent to splunk:</p>
<pre><code>{
   line: 2021-01-12 11:37:49,191;10718;INFO ;[Thread-1];Logger; ;Executed all shutdown events. 
   source: stdout 
   tag: service_95f2bac29286/582385192fde 
}
</code></pre>
<p>How can I configure Docker or Splunk to differentiate correctly between those different streams?`</p>",65822055.0,1,0,,2021-1-12 12:55:12,,2021-1-27 08:32:25,,,,,1251613.0,,1,1,docker|logging|docker-compose|splunk,124,8
857,259313,65686419,Use fluent-plugin-grok-parser with splunk-hec image,"<p>I am trying to create an image that has grok-parser installed based on the fluentd-hec image.</p>
<p>This is the Dockerfile i'm using:</p>
<pre><code>FROM splunk/fluentd-hec:1.2.4
USER root
RUN gem install fluent-plugin-grok-parser
RUN chown -R fluent:fluent /usr/local/share/gems/gems/fluent-plugin-grok-parser-*
USER fluent
</code></pre>
<p>This is the output from that build</p>
<pre><code>
Step 1/5 : FROM splunk/fluentd-hec:1.2.4
 ---&gt; ac49b85acc6a
Step 2/5 : USER root
 ---&gt; Running in 4ee81880e92a
Removing intermediate container 4ee81880e92a
 ---&gt; e3748059e604
Step 3/5 : RUN gem install fluent-plugin-grok-parser
 ---&gt; Running in 2a1debb084ec
Successfully installed bundler-2.2.5
Building native extensions. This could take a while...
Successfully installed msgpack-1.3.3
Building native extensions. This could take a while...
Successfully installed yajl-ruby-1.4.1
Building native extensions. This could take a while...
Successfully installed cool.io-1.7.0
Successfully installed sigdump-0.2.4
Successfully installed serverengine-2.2.2
Building native extensions. This could take a while...
Successfully installed http_parser.rb-0.6.0
Successfully installed concurrent-ruby-1.1.7
Successfully installed tzinfo-2.0.4
Successfully installed tzinfo-data-1.2020.6
Building native extensions. This could take a while...
Successfully installed strptime-0.2.5
Successfully installed fluentd-1.12.0
Successfully installed fluent-plugin-grok-parser-2.6.2
13 gems installed
Removing intermediate container 2a1debb084ec
 ---&gt; c5155932810c
Step 4/5 : RUN chown -R fluent:fluent /usr/local/share/gems/gems/fluent-plugin-grok-parser-*
 ---&gt; Running in 1c2550dcac74
Removing intermediate container 1c2550dcac74
 ---&gt; 7e216a676427
Step 5/5 : USER fluent
 ---&gt; Running in 5ee31ea2e78a
Removing intermediate container 5ee31ea2e78a
 ---&gt; ea8bdee73ee5
Successfully built ea8bdee73ee5
</code></pre>
<p>the snippet of the configmap is:</p>
<pre><code>      @id snow
      @type tail
      @label @SPLUNK
      tag tail.snow.*
      path /opt/snow/data/*.log
      pos_file /var/log/splunk-snow.log.pos
      path_key source
      &lt;parse&gt;
        @type grok
        grok_failure_key grokfailure
        &lt;grok&gt;
          pattern %{TIMESTAMP_ISO8601:time};%{SPACE}%{GREEDYDATA:log}
        &lt;/grok&gt;
      &lt;/parse&gt;
    &lt;/source&gt;
</code></pre>
<p>when I deploy a daemonset using the new image I get the error
<code>config error file=&quot;/fluentd/etc/fluent.conf&quot; error_class=Fluent::ConfigError error=&quot;Unknown parser plugin 'grok'. Run 'gem search -rd fluent-plugin' to find plugins&quot; </code>
I have tried this in EKS v1.18 as well as docker desktop and they both have the same issue.</p>
<p>Is there anything else I need to add to the dockerfile so that I use extra plugins?</p>
<p>I'd appreciate any help at on this!</p>",,1,0,,2021-1-12 14:57:06,,2021-1-14 14:44:42,,,,,7131699.0,,1,1,splunk|fluentd|grok,167,9
858,259314,65703855,splunk case with wild card search for IP Address,"<p>can you please help me with below ..</p>
<pre><code>index=xyz 
| eval BlockedStatus =  
case(Like(src,&quot;14.19.106.%&quot;) AND blocked=1 ,&quot;Q Blocked&quot;, 
            Like(src,&quot;150.29.121.%&quot;) AND blocked=1,&quot;Q Blocked&quot;,
            Like(src,&quot;14.19.106.%&quot;) AND blocked=0,&quot;Q Not Blocked&quot;, 
            Like(src,&quot;150.29.121.%&quot;) AND blocked=0,&quot;Q Not Blocked&quot;,
            NOT Like(src,&quot;14.19.106.%&quot;) AND blocked=1,&quot;Non Q Blocked&quot;, 
            NOT Like(src,&quot;150.29.121.%&quot;) AND blocked=1,&quot;Non Q Blocked&quot;,
            NOT Like(src,&quot;14.19.106.%&quot;) AND blocked=0,&quot;Non Q Not Blocked&quot;, 
            NOT Like(src,&quot;150.29.121.%&quot;) AND blocked=0,&quot;Non Q Not Blocked&quot;)            
| stats count by eventtype BlockedStatus 
| rename eventtype as &quot;Local Market&quot;, count as &quot;Total Critical Events&quot;
</code></pre>
<p>since we have data for where <code>src=150.29.121.23</code> and <code>blocked=1</code> but above query giving me result as</p>
<pre><code>&quot;Non Q Blocked&quot; instead of &quot;Q Blocked&quot; 
</code></pre>
<p>Not sure what went wrong here</p>",,1,0,,2021-1-13 14:24:05,,2021-1-14 14:11:38,2021-1-14 14:11:38,,4418.0,,14843828.0,,1,0,splunk|splunk-query|splunk-calculation,165,8
859,259315,65717013,Splunk search - How to loop on multi values field,"<p>My use case is analysing ticket in order to attribute a state regarding all the status of a specific ticket.</p>
<p>Raw data look like this :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Id</th>
<th style=""text-align: center;"">Version</th>
<th style=""text-align: center;"">Status</th>
<th style=""text-align: center;"">Event Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">0001</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">New</td>
<td style=""text-align: center;"">2021-01-07T09:14:00Z</td>
</tr>
<tr>
<td style=""text-align: center;"">0001</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">Completed - Action Performed</td>
<td style=""text-align: center;"">2021-01-07T09:38:00Z</td>
</tr>
</tbody>
</table>
</div>
<p>Data looks like this after transaction command:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Id</th>
<th style=""text-align: center;"">Version</th>
<th style=""text-align: center;"">Status</th>
<th style=""text-align: center;"">Event Time</th>
<th style=""text-align: center;"">state</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">0001, 0001</td>
<td style=""text-align: center;"">1, 1</td>
<td style=""text-align: center;"">New, Completed - Action Performed</td>
<td style=""text-align: center;"">2021-01-07T09:14:00Z, 2021-01-07T09:38:00Z</td>
<td style=""text-align: center;"">Acknowlegdement, Work</td>
</tr>
</tbody>
</table>
</div>
<p>I'm using transcation command in order to calculate the duration of acknnowlegdement and resolution of the ticket.</p>
<p>I have predefine rule to choose the correct state. This rules compare the n-1 status (New), and the current status (Completed - Action Performed) to choose the state.</p>
<p><strong>Issue</strong></p>
<p>Each ticket has a different number of status. We can not know in advance the max status number. I can not write a static search comparing each value of the Status field.</p>
<p><strong>Expected Solution</strong></p>
<p>I have a field that inform me the number of index on the status (number of status of a ticket) field.</p>
<p>I want to use a loop (Why not a loop for), to iterate on each index of the field Status and compare the value i-1 and i.</p>
<p>I can not find how to do this. Is this possible ?</p>
<p>Thank you</p>",,1,2,,2021-1-14 10:11:51,,2021-1-21 17:03:05,2021-1-15 09:33:02,,6066873.0,,6066873.0,,1,0,splunk|splunk-query,541,11
860,259316,65719854,grouping/Pivot in splunk,"<p>From below query can see we have event count as Q Blocked , Q Not Blocked, Non Q Blocked and Non Q Non blocked ...</p>
<pre><code>index=xyz 
|eval BlockedStatus =  
 case(Like(src,&quot;14.19.106.%&quot;) AND blocked=1 ,&quot;Q Blocked&quot;, 
        Like(src,&quot;150.29.121.%&quot;) AND blocked=1,&quot;Q Blocked&quot;,
        Like(src,&quot;14.19.106.%&quot;) AND blocked=0,&quot;Q Not Blocked&quot;, 
        Like(src,&quot;150.29.121.%&quot;) AND blocked=0,&quot;Q Not Blocked&quot;,
        NOT Like(src,&quot;14.19.106.%&quot;) AND blocked=1,&quot;Non Q Blocked&quot;, 
        NOT Like(src,&quot;150.29.121.%&quot;) AND blocked=1,&quot;Non Q Blocked&quot;,
        NOT Like(src,&quot;14.19.106.%&quot;) AND blocked=0,&quot;Non Q Not Blocked&quot;, 
        NOT Like(src,&quot;150.29.121.%&quot;) AND blocked=0,&quot;Non Q Not Blocked&quot;)         
| top showperc=f BlockedStatus by eventtype 
| stats list(*) as * by BlockedStatus 
| sort 0 - count
</code></pre>
<p>Now I want every BlockedStatus (Q Blocked, Q Not Blocked, Non Q Blocked, and Non Q Non blocked) should give total count in a grouping manner as below:</p>
<pre><code>Q Blocked = 12  Local Market
            11  foo
            10  ES
            11  GR
======================
Total     = 44

Q Not Blocked = 32  Local Market
                10  foo
                20  ES
                15  GR
======================
Total       77  
</code></pre>",,0,2,,2021-1-14 13:24:53,,2021-1-14 13:59:28,2021-1-14 13:59:28,,14843828.0,,14843828.0,,1,0,splunk|splunk-query|splunk-calculation,52,6
861,259317,65730480,How to solve the problem of scanning based on scanned history with Splunk?,"<p>The requirements is to find the event_A and event_B such that</p>
<ol>
<li>There is event A before it, and the event_A’s TEXT field and the
event_B’s TEXT field have the first character identical, and the
second characters satisfy the condition:</li>
</ol>
<ul>
<li>the event_B’s TEXT’s 2nd character in numerical value is equal to the event_A’s corresponding
field’s 2nd character,</li>
<li>or event_B’s is 1 plus, or 1 minus of the event_A’s.</li>
</ul>
<ol start=""2"">
<li></li>
</ol>
<ul>
<li>It is after some event_A satisfying condition 1, with CATEGORY value “ALARM” and not after such event_A with CATEGORY value “CLEARED”,</li>
<li>or It is after some event_A satisfying condition 1, with CATEGORY value “CLEARED”, but the event_B’s _time is within 60 minutes of the _time of event_A (CATEGORY=CLEARED)</li>
</ul>
<p>Here are some sample data:</p>
<pre><code>_time                           CATEGORY    TYPE    TEXT
2020-12-29T05:20:32.710-0800    ADVISORY    event_B K35JB
2020-12-29T05:37:54.462-0800    ADVISORY    event_B A05KM
2020-12-29T05:57:50.164-0800    ADVISORY    event_B K25CD
2020-12-29T05:59:06.004-0800    ALARM       event_A R20-A
2020-12-29T05:59:24.635-0800    ALARM       event_A K35-E
2020-12-29T05:59:37.200-0800    ALARM       event_A C15
2020-12-29T06:00:24.470-0800    CLEARED     event_A R20-A
2020-12-29T06:00:40.415-0800    CLEARED     event_A K35-E
2020-12-29T06:08:09.945-0800    ADVISORY    event_B R65AG
2020-12-29T06:14:24.740-0800    ADVISORY    event_B K35JB
2020-12-29T06:14:43.988-0800    ADVISORY    event_B K45JB
2020-12-29T06:56:44.642-0800    ADVISORY    event_B A77MD
2020-12-29T06:59:42.745-0800    ADVISORY    event_B C87AB
2020-12-29T07:30:39.080-0800    ADVISORY    event_B M97AF
2020-12-29T08:39:26.008-0800    ADVISORY    event_B K25BA
2020-12-29T09:46:48.175-0800    ADVISORY    event_B C25EG
</code></pre>
<p>Here is the illustration with the above sample data (with comment after # )</p>
<pre><code>_time                           CATEGORY    TYPE    TEXT
                                                                # all the event_B without event_A before are eliminated
2020-12-29T05:59:06.004-0800    ALARM       event_A R20-A   # expecting event_B with TEXT with prefix Ri where i = 1, 2, 3
2020-12-29T05:59:24.635-0800    ALARM       event_A K35-E   # expecting event_B with TEXT with prefix Ki where i = 2, 3, 4
2020-12-29T05:59:37.200-0800    ALARM       event_A C15     # expecting event_B with TEXT with prefix Ci where i = 0, 1, 2
2020-12-29T06:00:24.470-0800    CLEARED     event_A R20-A   # only expecting event_B with TEXT with prefix Ri where i = 1, 2, 3 with _time &lt; 2020-12-29T06:00:24.470-0800 + 60 minutes
2020-12-29T06:00:40.415-0800    CLEARED     event_A K35-E   # only expecting event_B with TEXT with prefix Ki where i = 2, 3, 4 with _time &lt; 2020-12-29T06:00:40.415-0800 + 60 minutes
2020-12-29T06:08:09.945-0800    ADVISORY    event_B R65AG   # to be eliminated, not expected, as R6 does not match Ri, i=1, 2, 3
2020-12-29T06:14:24.740-0800    ADVISORY    event_B K35JB   # kept, as K3 matched the expected prefix, and within the time windows
2020-12-29T06:14:43.988-0800    ADVISORY    event_B K45JB   # kept, as K4 matched the expected prefix, and within the time windows
2020-12-29T06:56:44.642-0800    ADVISORY    event_B A77MD   # to be eliminated, not expected, as A7 does not match any of the expected prefix
2020-12-29T06:59:42.745-0800    ADVISORY    event_B C87AB   # to be eliminated, not expected, as C8 does not match Ci, i=0, 1, 2
2020-12-29T07:30:39.080-0800    ADVISORY    event_B M97AF   # to be eliminated, not expected, as M9 does not match any of the expected prefix
2020-12-29T08:39:26.008-0800    ADVISORY    event_B K25BA   # to be eliminated, not expected, as its _time is beyond the expected window
2020-12-29T09:46:48.175-0800    ADVISORY    event_B C25EG   # kept, as C2 matched the expected prefix, and there is no time window limit for the prefx C2
</code></pre>
<p>I cannot wrap my head to figure a solution with Splunk query.</p>
<p>I could only find a solution when there is only one event_A expecting the corresponding event_B, using streamstats to keep of track the only one expecting event_A’s TEXT prefix, and _time to scan for the satisfying event_B, but once there are multiple event_A’s expecting with different TEXT prefixes and _time’s, then I cannot find a way to remember and perform the scan for the multiple event_A’s expectations.</p>
<p>With a conventional programming language, say Python, I’ll keep track of the union of expecatant prefixes, and time windows, and scan the events against such history state.</p>
<p>Could you kindly help me! Thanks in advance!</p>",,0,0,,2021-1-15 04:21:47,,2021-1-15 16:38:41,2021-1-15 16:38:41,,126164.0,,126164.0,,1,1,splunk|splunk-query,15,4
862,259318,65738770,spring boot + splunk,"<p>I have a spring boot app which writes to a log file and uses  a splunk forwarder. Everything works fine and my logs appear on splunk. when i upgrade from spring boot version 2.2.5 to spring boot 2.3.4. My logs do not get pushed to splunk.</p>
<p>I have tried downgrading, and the logs start getting pushed to splunk again.</p>
<p>here is a snippet of my yml which handles logging config</p>
<pre><code>logging:
  file: myLogs.log
  level:
    org:
      springframework:
        web:
          filter:
            CommonsRequestLoggingFilter: DEBUG

</code></pre>",66972904.0,1,1,,2021-1-15 15:22:34,,2021-4-6 16:43:23,,,,,4912471.0,,1,-1,spring|spring-boot|logging|splunk,264,9
863,259319,65793061,Splunk generate a random events,"<p>I'm a rookie in Splunk. I am using it for the first time.
I noticed that if the interval value is 60, it generates 2 events every minute.</p>
<p>This confused me. Is it a known situation?</p>",,1,1,,2021-1-19 14:02:53,,2021-1-24 22:46:24,,,,,15037542.0,,1,-2,splunk|splunk-query|splunk-calculation,125,7
864,259320,65796013,How to change the Splunk Forwarder Formatting of my Logs?,"<p>I am using our Enterprise's Splunk forawarder which seems to be logging events in splunk like this  which makes reading splunk logs a bit difficult.</p>
<pre><code>{&quot;log&quot;:&quot;[https-jsse-nio-8443-exec-5] 19 Jan 2021 15:30:57,237+0000 UTC INFO rdt.damien.services.CaseServiceImpl CaseServiceImpl :: showCase :: Case Created \n&quot;,&quot;stream&quot;:&quot;stdout&quot;,&quot;time&quot;:&quot;2021-01-19T15:30:57.24005568Z&quot;}
</code></pre>
<p>However, there are different Orgs in our Sibling Enterprise who log splunks thus which is far more readable. (No relation between us and them in tech so not able to leverage their tech support to triage this)</p>
<pre><code>[http-nio-8443-exec-7] 15 Jan 2021 21:08:49,511+0000 INFO  DaoOImpl [{applicationSystemCode=dao-app, userId=ANONYMOUS, webAnalyticsCorrelationId=|}]: This is a sample log
</code></pre>
<p>Please note the difference in logs (mine vs other):</p>
<blockquote>
<p>{&quot;log&quot;:&quot;[https-jsse-nio-8443-exec-5]..</p>
</blockquote>
<p>vs</p>
<blockquote>
<p>[http-nio-8443-exec-7]...</p>
</blockquote>
<p>Our Enterprise team is struggling to determine what causes this. I checked my app.log which looks ok (logged using Log4J) and doesn't have the aforementioned <code>{&quot;log&quot; :...}</code> entry.</p>
<blockquote>
<p>[https-jsse-nio-8443-exec-5] 19 Jan 2021 15:30:57,237+0000 UTC INFO
rdt.damien.services.CaseServiceImpl CaseServiceImpl:: showCase :: Case
Created</p>
</blockquote>
<p>Could someone guide me as to where could the problem/configuration lie that is causing the Splunk Forwarder to send the logs with the <code>{&quot;log&quot;:...</code> format to splunk? I thought it was something to do with JSON type vs RAW which I too dont understand if its the cause and if it is - what configs are driving that?</p>",,1,0,,2021-1-19 16:55:52,,2021-1-19 19:05:15,2021-1-19 17:10:27,,5915500.0,,5915500.0,,1,0,splunk,104,9
865,259321,65807404,Splunk: Throttling of alerts,"<p>I have configured an alert which runs every ten minutes and is triggered if the number of events is bigger than zero. I want this alert to be triggered or rather the mail for this alert to be sent only when it appears the first time. And the next email should then be sent if the alert condition is triggered for example one hour after the first mail. So, what I did I checked the throttle box and chose as time frame there to suppress the next trigger for one hour.</p>
<p>What I want to achieve here is this for example:
Alert condition was triggered at 8:00 a.m. Additionally at 8:30 a.m., 8:55 a.m and at 9:05 a.m. Then I would like to receive in total two alerts. One at 8:00 a.m. and the other one at 9:10.</p>
<p>Do I get exactly this by the configurations I described above?</p>",65809667.0,1,0,,2021-1-20 10:13:17,,2021-1-20 12:34:52,,,,,11572712.0,,1,0,alert|splunk,143,10
866,259322,65810232,Splunk: List indexes and sources to which one has access,"<p>Using this search command</p>
<pre><code>| eventcount summarize=false | dedup index | fields index
</code></pre>
<p>I get a list of all indexes I have access to in Splunk. Is it also possible to get another column besides this within which the source for the index is visible too?</p>
<p><em><strong>EDIT:</strong></em> It seems like I found a solution:</p>
<p><code>| tstats count WHERE index=* sourcetype=* source=* by index, sourcetype, source | fields - count</code></p>
<p>This gives back a list with columns for indexes, sourcetypes and sources.</p>",65810855.0,1,0,,2021-1-20 13:12:13,,2021-1-20 13:50:54,2021-1-20 13:43:20,,11572712.0,,11572712.0,,1,2,splunk|splunk-query,776,11
867,259323,65810828,Using azure-ad-b2c with splunk,"<p>I've beed trying to get azure-ad-b2c to be a IDP with localaccounts for Splunk SAML SSO.
with
<a href=""https://docs.microsoft.com/en-us/azure/active-directory-b2c/custom-policy-get-started"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/active-directory-b2c/custom-policy-get-started</a>
<a href=""https://github.com/Azure-Samples/active-directory-b2c-custom-policy-starterpack"" rel=""nofollow noreferrer"">https://github.com/Azure-Samples/active-directory-b2c-custom-policy-starterpack</a>
<a href=""https://docs.microsoft.com/en-us/azure/active-directory-b2c/connect-with-saml-service-providers"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/active-directory-b2c/connect-with-saml-service-providers</a></p>
<p>Presently I've got different error messages like</p>
<ul>
<li>Verification of SAML assertion using the IDP's certificate provided
failed. Unknown signer of SAML response</li>
<li>Verification of SAML
assertion using the IDP's certificate provided failed. Error: failed
to verify signature with cert</li>
</ul>
<p>Also, given I get this to work, I still need to extract security group association and exposed to Splunk.  I think this <a href=""https://mrochon.azurewebsites.net/2019/05/06/using-groups-in-azure-ad-b2c/"" rel=""nofollow noreferrer"">article</a> is relevant....</p>
<p>Anyone have a good writeup to get this going?</p>
<p>Brgds
Kristen</p>",,1,0,,2021-1-20 13:48:45,,2021-1-22 13:55:51,2021-1-20 14:06:57,,15045102.0,,15045102.0,,1,0,azure-ad-b2c|saml|splunk,96,7
868,259324,65843104,Splunk: Why didn't I get another alert email?,"<p>I configured the following query:</p>
<pre><code>someQuery | timechart partial=f span=5m count as numbers | fillnull | streamstats current=f last(numbers) as last_numbers | eval ratio = numbers/last_numbers | where ratio&gt;2.5 OR ratio &lt; 0.5 OR numbers &lt; 51
</code></pre>
<p>For which I programmed an alert in Splunk. The alert conditions were fullfilled from 03.31 a.m. until 05.45 a.m.</p>
<p>I configured the alert as follows:</p>
<p>The alert type is <strong>scheduled</strong>. It runs on <strong>Cron Schedule</strong>. The Time Range are the <strong>last 12 minutes</strong>. Cron Expression is */10 ****. It expires in <strong>24 hours</strong>. The alert is triggered when the <strong>Number of Results is greater than 0</strong>. Trigger is set to <strong>Once</strong>. Throttle is <strong>checked</strong> and <strong>Supresses triggering for 1 hour(s)</strong>.
When triggered an <strong>email</strong> is sent.</p>
<p>What I would have expected from this configuration is to get an alert email at 03:40 a.m., at 04:40 a.m. and at 05:40 a.m.</p>
<p>What I instead received were mails at 03.40 a.m. (which is expected) and at 05.50 a.m. (what is not expected). I do not understand, what I configured wrong. But I think it is either due to the <strong>throttling</strong> or maybe due to the <strong>time range</strong> of 12 minutes?</p>
<p>Could you tell me what I have to change in order to get the alert mails how I would expect to get them?</p>",,0,7,,2021-1-22 10:08:08,,2021-1-22 10:08:08,,,,,11572712.0,,1,0,alert|splunk,43,6
869,259325,65887648,Splunk: Is there a way to get warning lights in Splunk dashboards?,"<p>I found this really interesting German page (link: <a href=""https://www.splunk.com/de_de/blog/tips-and-tricks/gesund-und-wach-bleiben-wie-ihr-mit-splunk-daten-in-bessere-raumluft-verwandelt.html"" rel=""nofollow noreferrer"">https://www.splunk.com/de_de/blog/tips-and-tricks/gesund-und-wach-bleiben-wie-ihr-mit-splunk-daten-in-bessere-raumluft-verwandelt.html</a>) with a dashboard where some warning lights are included:</p>
<p><a href=""https://i.stack.imgur.com/nO5wx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nO5wx.jpg"" alt=""dashboards"" /></a></p>
<p>The warning lights are based on which values are predicted for different queries. I am now wondering how one could get this warning lights (and also the predictions based on the MLTK for certain variables) within a dashboard?</p>",65888944.0,1,0,,2021-1-25 15:27:19,,2021-1-25 16:48:42,,,,,11572712.0,,1,1,splunk,96,11
870,259326,65892385,How do I access an array value inside a case in Splunk?,"<p>I'm new to Splunk and need some help with the following:</p>
<ul>
<li>authIndexValue[] is an array that will hold at least one value</li>
<li>I want to access its value from inside a case in an eval statement but I get
this error: Unknown search command '0'.</li>
<li>I also tried http.request.queryParameters.authIndexValue{} with no luck</li>
</ul>
<p>Below the eval line:</p>
<pre><code>..search
| eval EventType=case(http.request.queryParameters.authIndexValue[0]==Login_FooBar, &quot;LOGIN&quot;) 
</code></pre>
<p>How can I achieve this?</p>
<p>Thanks.</p>",,1,0,,2021-1-25 21:03:26,,2021-1-26 14:28:26,,,,,3247471.0,,1,0,splunk|splunk-query,472,11
871,259327,65904688,Splunk not showing all databases from Snowflake,"<p>I have successfully made connection between splunk and snowflake and able to get test db data from snowflake to splunk. But Splunk is not showing all the databases from snowflake.</p>
<p>Below is the script which I used in Snowflake to create a new user, role and warehouse.</p>
<pre><code>USE ROLE SECURITYADMIN;
CREATE OR REPLACE ROLE first_role;

USE ROLE SYSADMIN;
CREATE OR REPLACE warehouse first_wh
  warehouse_size = 'SMALL' 
  auto_suspend = 5
  auto_resume = true
  initially_suspended = true
  comment = 'SPLUNK ONLY NVIDIA' ;

GRANT USAGE, OPERATE on warehouse first_wh to role first_role;


USE ROLE SECURITYADMIN;
CREATE OR REPLACE USER first_user
   password = 'some password' 
   must_change_password = false
   default_warehouse = first_wh
   default_role = first_role
;

GRANT ALL PRIVILEGES on database NVIDIA_DB to ROLE first_role;
GRANT ROLE first_role TO USER first_user;
</code></pre>
<p>As you can see from the above script that I have created a user (first_user), role (first_role) and a warehouse (first_wh) and have provides ALL privledges.</p>
<p>At splunk I have configured the identities and connections as below.</p>
<p><a href=""https://i.stack.imgur.com/TznGi.png"" rel=""nofollow noreferrer"">Splunk Connection</a>
<a href=""https://i.stack.imgur.com/3L6Yo.png"" rel=""nofollow noreferrer"">Splunk Identity</a></p>
<p>But when I go to splunk db connect Input to fetch data it doesnt show me the NVIDIA_DB. Here is the screenshot. Any idea how to resolve it or what I am doing wrong here? Thanks</p>
<p><a href=""https://i.stack.imgur.com/6F4un.png"" rel=""nofollow noreferrer"">Splunk Input Page</a></p>",,1,11,,2021-1-26 15:56:07,,2021-2-23 16:17:17,2021-2-23 16:17:17,,4418.0,,15084995.0,,1,0,jdbc|snowflake-cloud-data-platform|splunk|splunk-dbconnect,94,7
872,259328,65909510,disabling sampling in splunk pivot tables,"<p>Is it possible to disable sampling in splunk pivot tables?</p>
<p>When I run my query in splunk I get 136k events. When I open a pivot table ( with 1 field selected ) it only processes about 1k of events.</p>
<p>However when I click “Open in search”, the pivot table does actually process all events, but now I can’t modify the pivot table.</p>
<p>Is there a way to disable sampling when I’m building the pivot table?</p>
<p>Note: I'm using a managed instance of splunk</p>",,0,0,,2021-1-26 21:27:15,,2021-1-26 21:27:15,,,,,867294.0,,1,0,splunk|splunk-query,10,4
873,259329,65910359,Splunk: Find events that don't have a certain attribute logged as different log lines,"<p>We have Splunk logs like:</p>
<pre><code>ts=20:10:01 id=1 state=first foo=bar
ts=20:10:05 id=1 state=second foo=bar
ts=20:10:06 id=1 state=third foo=bar

ts=20:10:03 id=2 state=first foo=bar

ts=20:11:01 id=3 state=first foo=bar
ts=20:11:03 id=3 state=second foo=bar
ts=20:11:05 id=3 state=third foo=bar
</code></pre>
<p>I would like to find all <code>id</code> that does not have the other 2 states. In this example all <code>id=2</code> that logged first state but not the other 2. I was reading on JOIN and found that it can only look at events that occur with both events but we can't exclude those events.</p>
<pre><code>index=my-idx foo=bar 
| join id type=outer
  [search index=my-idx foo=bar NOT (state=second OR state=third) | table id]
| table id
</code></pre>
<p>The Query I am thinking of should return a list of ids that don't have the <code>state=second</code> or <code>state=third</code> which in the above example should return <code>id=2</code></p>",65911788.0,2,4,,2021-1-26 22:44:21,,2021-1-27 02:44:30,2021-1-27 00:08:59,,596990.0,,596990.0,,1,1,splunk|splunk-query|splunk-formula|splunk-calculation,62,9
874,259330,65939242,Splunk Query for Kubernetes PODs CPU and memory alerts,"<p>I have kubernetes setup for Zuul service and couple of pods are there in one namespace and I want to setup CPU and memory usage alert in SPlunk Dashboard, could anyone can suggest with splunk queries by that I can check CPU, cache, memory utilization alert</p>
<p>Thanks</p>",,0,1,,2021-1-28 14:35:28,,2021-1-28 14:35:28,,,,,11799578.0,,1,0,kubernetes|splunk|netflix-zuul|splunk-query|splunk-formula,80,7
875,259331,65958906,Splunk :find percentage of top 1000 in splunk,"<p>How can we get percentage of top 1000 values along with some more field .. i have tried below but its not working ..</p>
<p>|eval percent=round(count/total*100,1000) | eventstats count(src) as total | iplocation src| stats count by  src , dest , msg , Server_Group,Country,percent | sort-count | head 1000</p>",,1,2,,2021-1-29 17:13:43,,2021-2-1 20:22:25,,,,,14843828.0,,1,-2,splunk|splunk-query|splunk-calculation,274,9
876,259332,65997187,How to count the number of event based on JSON field structure in Splunk,"<p>I want to count the number of occurrence of a specific JSON structure. For example in my event there is a field called <code>data</code> which its value is JSON . but this field can have a variety of structures. like:</p>
<pre><code>data = {a: &quot;b&quot;}
data= {d: &quot;x&quot;, h: &quot;e&quot;} 
...
</code></pre>
<p>now I want to know how many event has data with each JSON structure and I don't care about values only keys are matter.</p>",,1,2,,2021-2-1 17:18:02,,2021-2-3 22:05:49,,,,,6150195.0,,1,0,splunk|splunk-formula|splunk-calculation,73,7
877,259333,66014686,How do I set TTL for oneshot search in Splunk API using Python?,"<p>I am intermittently getting the following error back from the Splunk API (about 40% of the time search works as expected):</p>
<blockquote>
<p>HTTP 503 Service Unavailable -- Search not executed: This search could
not be dispatched because the role-based disk usage quota of search
artifacts for user &quot;[REDACTED]&quot; has been reached (usage=1067MB,
quota=1000MB). Use the [[/app/search/job_manager|Job Manager]] to
delete some of your search artifacts, or ask your Splunk administrator
to increase the disk quota of search artifacts for your role in
authorize.conf., usage=1067MB, quota=1000MB, user=[REDACTED],
concurrency_category=&quot;historical&quot;,
concurrency_context=&quot;user_instance-wide&quot;</p>
</blockquote>
<p>The default ttl for a search in the splunk api is 10 min (at least for my company). I am told I need to lower the TTL for my searches (which are massive) and I will stop running out of space.  I do not have admin access, so no ability to increase my space or clear space on the fly (as far I know).  I can find code on how to lower TTL using saved searches, but I use oneshot searches.  It is not reasonable for me to switch.</p>
<p>How do I lower ttl for oneshot searches?
Here is what I have now that does not seem to lower TTL:</p>
<pre><code>#setup splunk connection
service = client.connect(
    host=HOST,
    port=PORT,
    username=suser,
    password=spass,
    autologin=True,
    )
#setup arguments
kwargs_oneshot = {&quot;count&quot; :  &quot;0&quot;,
                  &quot;earliest_time&quot;: begin,
                  &quot;latest_time&quot;: end,
                  &quot;set_ttl&quot;:60
                 }
#setup search job    
oneshotsearch_results = service.jobs.oneshot(query, **kwargs_oneshot)
# Get the results and display them using the ResultsReader
reader = results.ResultsReader(oneshotsearch_results)
</code></pre>",66016641.0,1,0,,2021-2-2 17:29:35,,2021-2-2 19:48:38,2021-2-2 18:02:48,,6706824.0,,6706824.0,,1,1,python-3.x|splunk|splunk-sdk,195,10
878,259334,66021918,How to group strings based on similarities in the string,"<p>I have a splunk query that produces a summarises errors by frequency</p>
<pre><code>index=&quot;pc_1&quot; LogLevel=ERROR 
   | eval Message=split(_raw,&quot;|&quot;) 
   | stats count(LogLevel) as Frequency by Message 
   | sort -Frequency
</code></pre>
<p>This produces results in the form</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Message</th>
<th>Frquency</th>
</tr>
</thead>
<tbody>
<tr>
<td>No such user</td>
<td>137</td>
</tr>
<tr>
<td>unable to deliver mail to example@email.com: Unable to reach server</td>
<td>70</td>
</tr>
<tr>
<td>unable to deliver mail to example1@email.com: Unable to reach server</td>
<td>43</td>
</tr>
<tr>
<td>unable to authenticate user 3456</td>
<td>8</td>
</tr>
<tr>
<td>unable to deliver mail to example2@email.com: Unable to reach server</td>
<td>6</td>
</tr>
<tr>
<td>unable to authenticate user 2321</td>
<td>5</td>
</tr>
<tr>
<td>unable to authenticate user 13321</td>
<td>3</td>
</tr>
<tr>
<td>...</td>
<td>.</td>
</tr>
<tr>
<td>...</td>
<td>.</td>
</tr>
<tr>
<td>...</td>
<td>.</td>
</tr>
<tr>
<td>unable to deliver mail to examplen@email.com: Unable to reach server</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>As you can notice in the results produced, some similar errors are  being split based on difference in ids of users emails, and machine ids.
I am looking for a way I can group this based on similarities in strings. Currently what I am using is the replace the strings with a common regexp and then find the frequency</p>
<pre><code>index=&quot;pc_1&quot; LogLevel=ERROR 
   | eval Message=split(_raw,&quot;|&quot;)

   | eval Message=replace(&quot;unable to deliver mail to (.)* Unable to reach server&quot;, &quot;unable to deliver mail to [email]: Unable to reach server&quot;)
   | eval Message=replace(&quot;unable to authenticate user \d+&quot;, &quot;unable to authenticate user [userId]&quot;)

   | stats count(LogLevel) as Frequency by Message 
   | sort -Frequency
</code></pre>
<p>This approach works but is quite cumbersome as there are a number of different types of errors and if this solution is to be implemented then it require going through each error and developing a regular expression for each.</p>
<p>Is there a way this can be improved with a query that can summarize this error more effectively?</p>",66136676.0,1,11,,2021-2-3 05:44:17,,2021-2-10 12:14:03,2021-2-3 05:54:22,,13680115.0,,13680115.0,,1,0,regex|splunk|splunk-formula,101,9
879,259335,66023059,How to reference an eval variable in query,"<p>I am trying to access a variable (in this example; <code>sampleFromDate</code> and <code>sampleToDate</code>) from a sub-query. I have defined the variables with syntax <code>eval variableName = value</code> and would like to access with syntax <code>filterName=$variableName$</code>. See the example below where I am trying to access values using <code>earliest=$sampleFromDate$ latest=$sampleToDate$</code></p>
<pre><code>index=*
earliest=-8d latest=-1d
| eval sampleToDate=now()
| eval sampleFromDate=relative_time(now(), &quot;-1d&quot;)
| appendcols [
    search (index=*)
    earliest=$sampleFromDate$ latest=$sampleToDate$
] 
</code></pre>
<p>This produces the error:</p>
<blockquote>
<p>Invalid value &quot;$sampleFromDate$&quot; for time term 'earliest'</p>
</blockquote>
<p>The value of <code>sampleFromDate</code> is in the format seconds since epoch time, e.g.</p>
<blockquote>
<p>1612251236.000000</p>
</blockquote>
<p>I know I can do <code>earliest=-d latest=now()</code> - but I don't want to do this because I want to reference the variables in several locations and output them at the end.</p>",,1,1,,2021-2-3 07:35:19,,2021-2-3 17:03:54,,,,,5387193.0,,1,0,splunk,134,9
880,259336,66034975,"Transcribing Splunk's ""transaction"" Command into Azure Log Analytics / Azure Data Analytics / Kusto","<p>We're using AKS and have our container logs writing to Log Analytics. We have an application that emits several print statements in the container log per request, and we'd like to group all of those events/log lines into aggregate events, one event per incoming request, so it's easier for us to find lines of interest. So, for example, if the request started with the line &quot;GET /my/app&quot; and then later the application printed something about an access check, we want to be able to search through all the log lines for <em>that request</em> with something like <code>| where LogEntry contains &quot;GET /my/app&quot; and LogEntry contains &quot;access_check&quot;</code>.</p>
<p>I'm used to queries with Splunk. Over there, this type of inquiry would be a cinch to handle with the <a href=""https://docs.splunk.com/Documentation/Splunk/8.1.1/SearchReference/Transaction"" rel=""nofollow noreferrer""><code>transaction</code> command</a>:
<a href=""https://i.stack.imgur.com/ZsFyP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZsFyP.png"" alt=""example of &quot;transaction&quot; from Splunk documentation"" /></a></p>
<p>But, with Log Analytics, it seems like multiple commands are needed to pull this off. Seems like I need to use <code>extend</code> with <code>row_window_session</code> in order to give all the related log lines a common timestamp, then  <code>summarize</code> with <code>make_list</code> to group the lines of log output together into a JSON blob, then finally <code>parse_json</code> and <code>strcat_array</code> to assemble the lines into a newline-separated string.</p>
<p>Something like this:</p>
<pre><code>ContainerLog
| sort by TimeGenerated asc
| extend RequestStarted= row_window_session(TimeGenerated, 30s, 2s, ContainerID != prev(ContainerID))
| summarize logLines = make_list(LogEntry) by RequestStarted
| extend parsedLogLines = strcat_array(parse_json(logLines), &quot;\n&quot;)
| where parsedLogLines contains &quot;GET /my/app&quot; and parsedLogLines contains &quot;access_check&quot;
| project Timestamp=RequestStarted, LogEntry=parsedLogLines
</code></pre>
<p>Is there a better/faster/more straightforward way to be able to group multiple lines for the same request together into one event and then perform a search across the contents of that event?</p>",,1,0,,2021-2-3 20:13:10,,2021-3-1 06:10:53,,,,,4342230.0,,1,0,azure|splunk|azure-log-analytics,72,7
881,259337,66043104,Splunk query for simple bar chart dashboard,"<p>I am getting data from snowflake in to Splunk using Splunk DB Connect. This is just 4 lines of data for a demo purpose. Below is Splunk data ( SELECT * FROM example_database. table)</p>
<pre><code>EMP_ID  EMP_NAME  EMP_SALARY
    1   John    3000
    2    Greg   3200
    3    Peter  1200
    4    Mark   2000
</code></pre>
<p>I want to create a simple bar dashboard in Splunk which display emp_name and emp_salary on x and y axis respectively. I am using following query in Slplunk search and reporting</p>
<pre><code>source=&quot;check&quot; &quot;EMP_NAME&quot; &quot;EMP_SALARY&quot; | top EMP_SALARY
</code></pre>
<p>But its showing me bar with equal hights (Should be of different heights as salaries are different). Any suggestion what I am doing wrong in query? Thanks for the help :)</p>",66047705.0,1,0,,2021-2-4 09:46:38,,2021-2-4 14:32:37,2021-2-4 10:11:30,,15084995.0,,15084995.0,,1,0,data-visualization|splunk|splunk-query,143,8
882,259338,66051558,index extraction in splunk,"<p>I have list of indexes which contain country name as</p>
<pre><code>germany_Abc_def ,
germany_Local_Abc_def, 
germany_Local_Int_Abc_def ,
Italy_Abc_def ,
Italy_Local_Abc_def ,
Italy_Local_Int_Abc_def
</code></pre>
<p>now I have one dashboard where I have a dropdown where I want the country name to be reflected as <code>Italy_Local_Int</code>.</p>
<p>I tried with the below.. it's giving only Germany, Italy instead of <code>germany_Local_Int</code> and <code>germany_Local</code></p>
<pre><code>eventcount summarize=false index=&quot;$country$_Abc_def&quot;  
| where count!=0 
| eval idx=split(index,&quot;_&quot;) 
| eval country=mvindex(idx,0) 
| dedup country
</code></pre>",,0,1,,2021-2-4 18:22:10,,2021-2-5 23:35:21,2021-2-5 23:35:21,,2055998.0,,14843828.0,,1,0,splunk|splunk-query|splunk-calculation,28,5
883,259339,66110589,"Count the number of different value of a field, and get the average per minute","<p>I have some domain like this:</p>
<pre><code>domain |
A      |
B      |
C      |
D      |
...
</code></pre>
<p>One domain can be called in one request, now I want to know what is the average request number per minute for a domain (no matter what domain is). So I split it into three steps:</p>
<ol>
<li>get the total request number per minute</li>
<li>get the number of domains been called per minute</li>
<li>avg = total request number per minute / number of domain per minute</li>
</ol>
<p>I have got the result of the first step by:</p>
<pre><code>index=&quot;whatever&quot; source=&quot;sourceurl&quot;
| bin _time span=1m
| stats count as requestsPerMin by _time
</code></pre>
<p>However, I don't know how to get the number of domains that been called. For example, in a minute, domain A has been called twice, domain B has been called once, so the number of domains that been called should be two. But I don't know which query can get this result.</p>",,1,0,,2021-2-8 22:58:23,,2021-2-9 14:13:04,2021-2-9 14:13:04,,4418.0,,8898054.0,,1,0,splunk|splunk-query,254,9
884,259340,66121824,Splunk event increasing logic witch each SPL query,"<p>I am getting data in Splunk from Snowflake using Splunk DB Connect. This is just simple orders data. At Splunk search &amp; reporting I am running the following query on my table to get visualization.</p>
<pre><code>source=&quot;big_data_table_inner_join&quot; &quot;UNITS_SOLD&quot; | top COUNTRY
</code></pre>
<p>What I am seeing is that each time I run query the events number at splunk increases quite heavily. For eg. After running first time they were <strong>342000</strong> events and when I ran the same query they were <strong>67445</strong> events. Any idea why is this happening?</p>",,0,0,,2021-2-9 15:19:18,,2021-2-9 15:19:18,,,,,15084995.0,,1,1,splunk|splunk-query|splunk-formula|splunk-calculation,13,4
885,259341,66122532,How to import Shell script output (Comma separated File) into Splunk Index,"<p>I have bash script that generates comma separated file. I want to trigger the script from splunk and convert its output into splunk index.</p>
<p>Any help would be appreciated.</p>",,2,0,,2021-2-9 15:57:55,,2021-2-9 19:36:53,,,,,6114709.0,,1,0,bash|splunk|splunk-query,74,7
886,259342,66135434,how can i use dynamic variable for time range token in splunk for drilldown,"<p>I have created a timechart in splunk and applying drilldown to that timechart.
In my time chart i have define span time. So my requirement is when user click on particular value in timechart than it should pick up the timerange from that particular click till to next break point (click.value+span).</p>
<p>for eg: if i click on 183 (as shown in the below image), it should show me only 183 results not 183+211+50.
(But in my case it is showing 183+211+50 results because the latest time token value is &quot;time_token.latest&quot;)</p>
<p><a href=""https://i.stack.imgur.com/pdBiD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pdBiD.png"" alt=""Timechart"" /></a></p>
<p>I am using below tokens. To achieve my requirement what value should I give for latest.token?
<a href=""https://i.stack.imgur.com/AIDfo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AIDfo.png"" alt=""enter image description here"" /></a></p>",,0,0,,2021-2-10 10:48:48,,2021-2-23 16:11:00,2021-2-23 16:11:00,,4418.0,,11471671.0,,1,0,monitoring|splunk|splunk-query|drilldown|splunk-dashboard,151,8
887,259343,66152037,Gitlab logs into Splunk,"<p>we are using self managed Gitlab and Runner and Gitlab has advanced log system where everything is logged - <a href=""https://docs.gitlab.com/ee/administration/logs.html"" rel=""nofollow noreferrer"">https://docs.gitlab.com/ee/administration/logs.html</a>
Example -
<code>production_json.log</code> <code>production.log</code> <code>api_json.log</code> and many more.</p>
<p>We are planning to ingest them into our Splunk but not sure which one's are really useful. Splunk licensing is expensive so we don't want to ingest everything which may or may not be useful.</p>
<p>On Gitlab documentation, I couldn't find anything how we can ingest logs in Splunk.</p>
<p>Anyone came across the similar scenario and may be provide some guidance on this?</p>",,0,1,,2021-2-11 09:31:42,,2021-2-11 09:31:42,,,,,14316989.0,,1,0,gitlab|gitlab-ci|splunk,353,10
888,259344,66163813,Splunk: Assigning variables,"<p>I have some data in splunk im trying to create a dashboard for. I am unsure how to assign a variable name for the 2 pieces of data im looking to extract.
My data looks like</p>
<pre><code>From 5 new registrations, 4 emails were confirmed
</code></pre>
<p>I am able to search this data very easily with the following query</p>
<pre><code>search &quot;From * new registrations, * emails were confirmed&quot;
</code></pre>
<p>How do i assign a variable to each of the asterisks?</p>
<p>Thank you!</p>",,1,0,,2021-2-11 22:20:40,,2021-2-12 00:49:15,,,,,12459287.0,,1,0,splunk|splunk-query,137,10
889,259345,66177114,Combining the results from 2 indexes in splunk query,"<p>I have one index idx1 and other index idx2 and a common column &quot;A&quot; on which matching needs to be done.</p>
<p>I'm facing difficulty in combining the data from both the columns.
I've to combine the data in such a way that if there is duplicate then the data from idx1 must be prioritized over data from idx2; i.e. basically equivalent of set operation [a+(b-a)].</p>
<p>I've tried the following :</p>
<pre><code>| set diff  [ search index=idx2 sourcetype=src | dedup A ] [search index=idx1 sourcetype=src | dedup A ]
| stats count BY index A
| table index A
</code></pre>
<p>Here I get total 10840 statistics with both columns filled.</p>
<p>But when I want to display other columns from both the indexes I get empty columns for those.</p>
<p>Upon executing :</p>
<pre><code>| set diff  [ search index=1idx1 sourcetype=src | dedup A ] [search index=idx2 sourcetype=src | dedup A ]
    | stats count BY index
</code></pre>
<p>I get the output as</p>
<p>index     count
idx1      4791
idx2      6049</p>
<p>Can anyone help me how should I proceed??</p>
<p>I've tried even this but not sure</p>
<pre><code>index=idx1 sourcetype=src
| append [
| set diff  [ search index=idx2 sourcetype=src | dedup A ] [search index=idx1 sourcetype=src | dedup A ]]
    | stats count BY index A
    | table index A
</code></pre>",,2,0,,2021-2-12 18:23:44,,2021-2-13 06:58:13,2021-2-12 18:34:55,,8287126.0,,8287126.0,,1,0,splunk|splunk-query|splunk-formula|splunk-calculation|splunk-sdk,610,11
890,259346,66181874,splunk date time difference,"<p>I am new to Splunk. My goal is to optimize the API call, since that particular API method is taking more than 5 minutes to execute.</p>
<p>In Splunk I searched using context ID, I got all the functions and sub functions call by main API call function for that particular execution. Now I want to figure what which sub function took the maximum time. In Splunk in left side, in the list of fields, I see field name CallStartUtcTime (e.g. &quot;2021-02-12T20:17:42.3308285Z&quot;) and CallEndUtcTime (e.g. &quot;2021-02-12T20:18:02.3702937Z&quot;). In search how can I write a function which will give me difference between these two times. I google and found we can use eval() function but for me its returning null value.</p>
<p>Additional Info:</p>
<p>search: <a href=""https://i.stack.imgur.com/ZHvG3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZHvG3.png"" alt=""context id and eval in search"" /></a></p>
<p>clicked on &quot;create table view&quot; and checked start, end and diff fields in the left side fields list. but all three are coming as null</p>
<p><a href=""https://i.stack.imgur.com/LW9tX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LW9tX.png"" alt=""enter image description here"" /></a></p>
<p>not sure what wrong I am doing. I want to find out the time taken by each function.</p>",,1,0,,2021-2-13 04:42:42,,2021-2-19 21:18:06,2021-2-19 21:18:06,,4418.0,,7964952.0,,1,0,splunk|splunk-query|splunk-formula,622,11
891,259347,66223409,How to Cluster and create a timechart in splunk,"<p>I have a field with <code>LogMsg</code> error messages That I am grouping based on similarities using <em>cluster</em>.</p>
<p>What I am trying to achieve is a display that will show a timeseries with the grouped error</p>
<pre><code>index=&quot;my_index_here&quot; LogLevel=ERROR
  | cluster showcount=t t=0.2 field=Message | eval &quot;Error Count&quot; = cluster_count
  | head 10 | timechart count(&quot;Error Count&quot;) By LogMsg span=60m
</code></pre>
<p>The Idea is this</p>
<ol>
<li>Get all the error Messages <code>LogLevel=ERROR</code></li>
<li>Group the items based on Message field <code>| cluster showcount=t t=0.2 field=Message | eval &quot;Error Count&quot; = cluster_count</code></li>
<li>Get top 10 results <code>| head 10</code></li>
<li>Draw a timechart <code>timechart count(&quot;Error Count&quot;) By LogMsg span=60m</code>. The time chart should have a plot of number different error messages generated from the cluster against time, something like</li>
</ol>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Message</th>
<th style=""text-align: center;"">8.00</th>
<th style=""text-align: center;"">9:00</th>
<th style=""text-align: center;"">10.00</th>
<th style=""text-align: right;"">11:00</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Unable to authenticate</td>
<td style=""text-align: center;"">90</td>
<td style=""text-align: center;"">40</td>
<td style=""text-align: center;"">30</td>
<td style=""text-align: right;"">60</td>
</tr>
<tr>
<td style=""text-align: left;"">Another Error</td>
<td style=""text-align: center;"">80</td>
<td style=""text-align: center;"">40</td>
<td style=""text-align: center;"">30</td>
<td style=""text-align: right;"">60</td>
</tr>
<tr>
<td style=""text-align: left;"">Yet another error</td>
<td style=""text-align: center;"">70</td>
<td style=""text-align: center;"">40</td>
<td style=""text-align: center;"">30</td>
<td style=""text-align: right;"">60</td>
</tr>
<tr>
<td style=""text-align: left;"">---</td>
<td style=""text-align: center;"">---</td>
<td style=""text-align: center;"">---</td>
<td style=""text-align: center;"">---</td>
<td style=""text-align: right;"">---</td>
</tr>
<tr>
<td style=""text-align: left;"">The 10th most frequent error</td>
<td style=""text-align: center;"">50</td>
<td style=""text-align: center;"">40</td>
<td style=""text-align: center;"">30</td>
<td style=""text-align: right;"">60</td>
</tr>
</tbody>
</table>
</div>
<p>My approach above is not working returning a blank plot,</p>",66227235.0,1,0,,2021-2-16 11:21:25,,2021-2-16 15:22:16,,,,,13680115.0,,1,0,splunk|splunk-query,120,10
892,259348,66228012,AKS log format changed,"<p>we recently updated our AKS cluster from 1.17.x to 1.19.x and recognised that the format of our custom application logs in <code>/var/lib/docker/containers</code> changed.</p>
<p>Before the update it looked like this:
<a href=""https://i.stack.imgur.com/0GJio.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0GJio.png"" alt=""old valid json format"" /></a></p>
<p>Afterwards it looks like this:
<a href=""https://i.stack.imgur.com/PW1LN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PW1LN.png"" alt=""new invalid json format"" /></a></p>
<p>I can find some notes in the changelog that kubernetes changed from just text logs to structured logs (for system components) but I don't see how this correlates to how our log format changed.</p>
<p><a href=""https://kubernetes.io/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/#:%7E:text=In%20Kubernetes%201.19%2C%20we%20are,migrated%20to%20the%20structured%20format"" rel=""nofollow noreferrer"">https://kubernetes.io/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/#:~:text=In%20Kubernetes%201.19%2C%20we%20are,migrated%20to%20the%20structured%20format</a></p>
<p><a href=""https://kubernetes.io/docs/concepts/cluster-administration/system-logs/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/cluster-administration/system-logs/</a></p>
<p>Is there a chance to still get valid json logs to <code>/var/lib/docker/containers</code> in AKS &gt; 1.19.x?</p>
<p>Background:
We send our application logs to Splunk and don't use the Azure stack for log analysis. Our Splunk setup cannot parse that new log format as of now.</p>",66229801.0,1,0,,2021-2-16 16:08:29,,2021-2-16 18:04:56,,,,,13037748.0,,1,3,azure|kubernetes|azure-aks|splunk,731,16
893,259349,66238972,How to extract Key Value fields from Json string in Splunk,"<p>I have json format Splunk search results like below :</p>
<pre><code>&quot;{
    &quot;Name&quot;: &quot;RUNQDATA&quot;,
    &quot;RunId&quot;: &quot;2021021701&quot;,
    &quot;Details&quot;: &lt;{
        &quot;RunQID&quot;: &quot;796562&quot;,
        &quot;TQID&quot;: &quot;796562&quot;,
        &quot;Ent&quot;: {
            &quot;NAME&quot;: &quot;Inv&quot;,
            &quot;Store&quot;: {
                &quot;NAME&quot;: &quot;FSW&quot;,
                &quot;TYPE&quot;: &quot;QUEUE&quot;,
                &quot;USERNAME&quot;: &quot;abc&quot;
            }
        },
        &quot;ADD_COUNT&quot;: &quot;5740&quot;,
        &quot;UPDATE_COUNT&quot;: &quot;0&quot;,
        &quot;DELETE_COUNT&quot;: &quot;0&quot;
    }&gt;,
    &quot;status&quot;: &quot;success&quot;,
}&quot; 
</code></pre>
<p>How can I extract the fields like ADD_COUNT or UPDATE_COUNT from this ? I tried spath &amp; other options , however not able to get the required results. Probably because the json contains &lt;&gt;.</p>
<p>Any help here is appreciated.</p>",66243542.0,1,0,,2021-2-17 09:18:45,,2021-2-17 14:06:18,,,,,6808782.0,,1,0,json|splunk|text-extraction,1390,13
894,259350,66240705,Extract some fields from a part json part text log in Splunk,"<p>I am fairly new to splunk and still learning.
I have a splunk event which is a mix of some texts and json in between. (This isn't the complete log)</p>
<pre><code>2021-02-14 00:00:03,596 [[bapm2DQ].bapmprojectFlow.stage1.02] INFO  com.growl.hdt.dmt.DQ.bapm.RetrieveDataFromDQ - Total Application assets -&gt; 1692
2021-02-14 00:00:03,596 [[bapm2DQ].bapmprojectFlow.stage1.02] INFO  com.growl.hdt.dmt.DQ.bapm.CommonUtils - {&quot;Header&quot;:{&quot;AppId&quot;:&quot;DFG686&quot;,&quot;Type&quot;:&quot;Inbound&quot;,&quot;RecId&quot;:&quot;416c627c-41a7-428e-a871-5317c4842fe5&quot;,&quot;StartTS&quot;:&quot;2021-02-14T05:00Z&quot;,&quot;Ver&quot;:&quot;2.0.0&quot;},&quot;Application&quot;:{&quot;APP_OS&quot;:&quot;Linux 3.10.0-1160.11.1.el7.x86_64&quot;,&quot;APP_Runtime&quot;:&quot;Java 1.8.0_282&quot;,&quot;APP_AppName&quot;:&quot;DQ-bapm&quot;,&quot;APP_AppVersion&quot;:&quot;1.0.0&quot;,&quot;Host&quot;:&quot;zebra.cdc.growl.com/10.102.180.53&quot;,&quot;Channel&quot;:&quot;Other&quot;},&quot;Service&quot;:{&quot;Key&quot;:&quot;DQ2bapm&quot;,&quot;URL&quot;:&quot;https://growl-test.DQ.com/rest/2.0/assets?limit=1000&amp;offset=1000&amp;typeId=00000000-0000-0000-0000-000000031302&amp;communityId=595b27d3-ff42-45e4-8dc7-0172f7d82693&amp;domainId=2c8b39ea-0d7f-445f-acc2-a1fb3a9a12db&amp;statusId=00000000-0000-0000-0000-000000005009&quot;,&quot;CallType&quot;:&quot;REST&quot;,&quot;Operation&quot;:&quot;GET&quot;},&quot;Results&quot;:{&quot;Elapsed&quot;:&quot;0&quot;,&quot;TraceLevel&quot;:&quot;DEBUG&quot;},&quot;Security&quot;:{&quot;Vendor&quot;:&quot;growl&quot;}}
2021-02-14 00:00:03,795 [[bapm2DQ].bapmprojectFlow.stage1.02] INFO  com.growl.RetrieveDataFromDQ - Total Application assets -&gt; 1692
2021-02-14 00:00:03,795 [[bapm2DQ].bapmprojectFlow.stage1.02] INFO  com.growl.RetrieveDataFromDQ - Total Application assets in appAssetList-&gt; 1692
2021-02-14 00:00:04,499 [[bapm2DQ].bapmprojectFlow.stage1.02] INFO  com.growl.bapm.ComparebapmDQRecords - List of Applications in DQ to be marked &quot;Obsolete in bapm&quot;: 
[487684fgfg, hfkeh708089, fgdh678, SDF75664, dffg0007643]
2021-02-14 00:00:04,499 [[bapm2DQ].bapmprojectFlow.stage1.02] INFO  com.growl.ComparebapmDQRecords - ## Total Application count from bapm ##1696
2021-02-14 00:00:04,499 [[bapm2DQ].bapmprojectFlow.stage1.02] INFO  com.growl.hdt.dmt.DQ.bapm.ComparebapmDQRecords - ## Total Application Asset in DQ ##1692
2021-02-14 00:00:04,499 [[bapm2DQ].bapmprojectFlow.stage1.02] INFO  com.growl.ComparebapmDQRecords - ## No of Application to Obsolete in DQ ##5
</code></pre>
<p>How can I extract the below :</p>
<pre><code>List of Applications in DQ to be marked &quot;Obsolete in bapm&quot;: 
[487684fgfg, hfkeh708089, fgdh678, SDF75664, dffg0007643]
Total Application count from bapm ##1696
Total Application Asset in DQ ##1692
No of Application to Obsolete in DQ ##5
</code></pre>
<p>I tried something like below which was suggested for one similar case in another post but this didn't work:</p>
<pre><code>index=hdt sourcetype=dq2 |  rex field=_raw &quot;(?msi)(?&lt;Total Application count from BAPM&gt;\{.+\}$)&quot;
| spath input=&quot;Total Application count from BAPM&quot;
</code></pre>",66243768.0,1,0,,2021-2-17 11:06:06,,2021-2-18 15:36:09,2021-2-18 15:36:09,,12569641.0,,12569641.0,,1,0,splunk|splunk-query,202,10
895,259351,66246929,Automatically shut off Splunk HEC when volume gets too high,<p>We have an issue where certain HEC feeds will burst in volume and blow out our daily ingest license. Is there an automated way to shut off the offending HEC when volume reaches a high level?</p>,66250800.0,1,1,,2021-2-17 17:19:48,,2021-2-17 22:23:49,,,,,4455463.0,,1,0,splunk,27,7
896,259352,66270614,Comparing results of two searches in Splunk dashboard,"<p>I have created a dashboard in Splunk with two panels. The first panel runs a search (query below) for time-window-1 and the second panel runs the same search for time-windows-2.  Both the time windows are passed as parameters to the dashboard as shown below.</p>
<pre><code>index=dev sourcetype!=warn component AND errormessage earliest=$field1.earliest$ latest=$field1.latest$ 
| dedup errormessage,component
</code></pre>
<p>Currently, each panel displays the unique results in the respective time window. The first panel takes time windows as parameters and displays unique results for events matching fields errormessage and component</p>
<p>Similarly, the second panel also displays the unique events for a different time window.</p>
<p>I want the dashboard to compare the results of time-window-1 and time-window-2 (results of pane 1 and pane-2) and display :</p>
<ol>
<li><p>The unique results that are present in time-window-1 and NOT in time-window-2</p>
</li>
<li><p>The unique results that are present in time-window-2 and NOT in time-window-1</p>
</li>
</ol>
<p>Below is the dashboard source.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;form&gt;
  &lt;label&gt;test-1&lt;/label&gt;
  &lt;fieldset submitButton=""false""&gt;
    &lt;input type=""time"" token=""field1""&gt;
      &lt;label&gt;&lt;/label&gt;
      &lt;default&gt;
        &lt;earliest&gt;-24h@h&lt;/earliest&gt;
        &lt;latest&gt;now&lt;/latest&gt;
      &lt;/default&gt;
    &lt;/input&gt;
    &lt;input type=""time"" token=""field2""&gt;
      &lt;label&gt;&lt;/label&gt;
      &lt;default&gt;
        &lt;earliest&gt;-24h@h&lt;/earliest&gt;
        &lt;latest&gt;now&lt;/latest&gt;
      &lt;/default&gt;
    &lt;/input&gt;
  &lt;/fieldset&gt;
  &lt;row&gt;
    &lt;panel&gt;
      &lt;title&gt;time-window-1&lt;/title&gt;
      &lt;event&gt;
        &lt;search&gt;
          &lt;query&gt;index=dev sourcetype!=warn component AND errormessage earliest=$field1.earliest$ latest=$field1.latest$ | dedup errormessage,component&lt;/query&gt;
          &lt;sampleRatio&gt;1&lt;/sampleRatio&gt;
        &lt;/search&gt;
        &lt;option name=""count""&gt;20&lt;/option&gt;
        &lt;option name=""list.drilldown""&gt;none&lt;/option&gt;
        &lt;option name=""list.wrap""&gt;1&lt;/option&gt;
        &lt;option name=""maxLines""&gt;5&lt;/option&gt;
        &lt;option name=""raw.drilldown""&gt;full&lt;/option&gt;
        &lt;option name=""rowNumbers""&gt;0&lt;/option&gt;
        &lt;option name=""table.drilldown""&gt;all&lt;/option&gt;
        &lt;option name=""table.sortDirection""&gt;asc&lt;/option&gt;
        &lt;option name=""table.wrap""&gt;1&lt;/option&gt;
        &lt;option name=""type""&gt;list&lt;/option&gt;
      &lt;/event&gt;
    &lt;/panel&gt;
    &lt;panel&gt;
      &lt;title&gt;time-window-2&lt;/title&gt;
      &lt;event&gt;
        &lt;search&gt;
          &lt;query&gt;index=dev sourcetype!=warn component AND errormessage  earliest=$field2.earliest$ latest=$field2.latest$ | dedup errormessage,component&lt;/query&gt;
          &lt;sampleRatio&gt;1&lt;/sampleRatio&gt;
        &lt;/search&gt;
        &lt;option name=""count""&gt;20&lt;/option&gt;
        &lt;option name=""list.drilldown""&gt;none&lt;/option&gt;
        &lt;option name=""list.wrap""&gt;1&lt;/option&gt;
        &lt;option name=""maxLines""&gt;5&lt;/option&gt;
        &lt;option name=""raw.drilldown""&gt;full&lt;/option&gt;
        &lt;option name=""rowNumbers""&gt;0&lt;/option&gt;
        &lt;option name=""table.drilldown""&gt;all&lt;/option&gt;
        &lt;option name=""table.sortDirection""&gt;asc&lt;/option&gt;
        &lt;option name=""table.wrap""&gt;1&lt;/option&gt;
        &lt;option name=""type""&gt;list&lt;/option&gt;
      &lt;/event&gt;
    &lt;/panel&gt;
    &lt;/row&gt;
&lt;/form&gt;</code></pre>
</div>
</div>
</p>",,0,0,,2021-2-19 01:37:03,,2021-2-19 21:25:41,2021-2-19 21:25:41,,4418.0,,11472725.0,,1,0,splunk|splunk-query,94,7
897,259353,66289582,How to decompress snappy encoded file in Splunk,"<p>I am trying to send metrics from Prometheus to Splunk. By default Prometheus compress using snappy.
But Splunk receives this compressed format.
<a href=""https://i.stack.imgur.com/ePudA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ePudA.png"" alt=""enter image description here"" /></a></p>
<p>I tried to configure Splunk inputs.conf, but no luck.</p>
<pre><code>[tcp]
compressed = true

[splunktcp]
compressed = true
</code></pre>
<p>How to decompress in Splunk?
is it possible to change any other config files to read decompressed data?</p>",,0,0,,2021-2-20 08:39:15,,2021-2-20 08:45:39,2021-2-20 08:45:39,,5454647.0,,5454647.0,,1,1,prometheus|splunk|prometheus-node-exporter,27,5
898,259354,66297393,docker-compose kong configuration real_ip_header,"<p>Create a project in docker-compose where I have Kong, Konga (Web Admin), an API with your DB and Splunk to collect the logs.</p>
<p><a href=""https://i.stack.imgur.com/YRLFd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YRLFd.png"" alt=""enter image description here"" /></a></p>
<p>The problem I have is that I cannot acquire the TCP IP addresses of those who consume the API through Kong.</p>
<p>From what I read you should configure these lines, on etc/kong/kong.conf.default. Is that so?</p>
<ul>
<li>trusted_ips</li>
<li>real_ip_header = X-Real-IP</li>
</ul>
<p>Here its my <a href=""https://github.com/safernandez666/SOCLess"" rel=""nofollow noreferrer"">proyect</a></p>
<p>Logically I obtain, as a client, the IP address of my Kong container.</p>
<p><a href=""https://i.stack.imgur.com/Xi15c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Xi15c.png"" alt=""enter image description here"" /></a></p>
<p>How could I modify the docker-compose so that this configuration is by default in the docker-compose up.</p>
<p>Here its my docker-compose.yaml</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>version: '3'

services:
  #######################################
  # Database
  #######################################

  db:
    image: mysql:5.7
    container_name: db
    environment:
      MYSQL_ROOT_PASSWORD: secret
      MYSQL_DATABASE: maradona
      MYSQL_USER: maradona
      MYSQL_PASSWORD: 
    ports:
      - ""3306:3306""
    volumes:
      - dbdata:/var/lib/mysql
      - ./db/maradona.sql:/docker-entrypoint-initdb.d/init.sql
    command: ['mysqld', '--character-set-server=utf8mb4', '--collation-server=utf8mb4_unicode_ci']
    networks:
      - red

  #######################################
  # API Microservice
  ####################################### 
  api:
    build: api/.
    container_name: api
    environment:
      DATABASE: ""db""
    links:
      - db
    restart: always
    ports:
      - ""5000:5000""
    networks:
      - red

  #######################################
  # Postgres: The database used by Kong
  #######################################
  kong-database:
    image: postgres:9.6
    container_name: kong-postgres
    restart: on-failure
    networks:
      - red
    volumes:
      - kong_data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: kong
      POSTGRES_PASSWORD: ${KONG_PG_PASSWORD:-kong}
      POSTGRES_DB: kong
    ports:
      - ""5432:5432""
    healthcheck:
      test: [""CMD"", ""pg_isready"", ""-U"", ""kong""]
      interval: 30s
      timeout: 30s
      retries: 3

  #######################################
  # Kong database migration
  #######################################
  kong-migration:
    image: ${KONG_DOCKER_TAG:-kong:latest}
    command: kong migrations bootstrap
    networks:
      - red
    restart: on-failure
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_DATABASE: kong
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: ${KONG_PG_PASSWORD:-kong}
    depends_on:
      - kong-database

  #######################################
  # Kong: The API Gateway
  #######################################
  kong:
    image: ${KONG_DOCKER_TAG:-kong:latest}
    restart: on-failure
    networks:
      - red
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_DATABASE: kong
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: ${KONG_PG_PASSWORD:-kong}
      KONG_PROXY_LISTEN: 0.0.0.0:8000
      KONG_PROXY_LISTEN_SSL: 0.0.0.0:8443
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
    links:
      - api
    depends_on:
      - kong-database
    healthcheck:
      test: [""CMD"", ""kong"", ""health""]
      interval: 10s
      timeout: 10s
      retries: 10
    ports:
      - ""8000:8000""
      - ""8001:8001""
      - ""8443:8443""
      - ""8444:8444""

  #######################################
  # Konga database prepare
  #######################################
  konga-prepare:
    image: pantsel/konga:latest
    command: ""-c prepare -a postgres -u postgresql://kong:${KONG_PG_PASSWORD:-kong}@kong-database:5432/konga""
    networks:
      - red
    restart: on-failure
    depends_on:
      - kong-database

  #######################################
  # Konga: Kong GUI
  #######################################
  konga:
    image: pantsel/konga:latest
    restart: always
    networks:
        - red   
    environment:
      DB_ADAPTER: postgres
      DB_URI: postgresql://kong:${KONG_PG_PASSWORD:-kong}@kong-database:5432/konga
      NODE_ENV: production
    depends_on:
      - kong-database
    ports:
      - ""1337:1337""

  #######################################
  # Splunk Server
  #######################################
  splunk:
    environment:
      - TZ=America/Buenos_Aires
      - SPLUNK_ENABLE_LISTEN=9997
      - SPLUNK_START_ARGS=--accept-license --no-prompt --answer-yes
      - SPLUNK_PASSWORD=password
    hostname: splunk
    image: splunk/splunk:latest
    ports:
      - ""8888:8000""
      - ""8088:8088""
      - ""9997:9997""
      - ""1514:1514""
    restart: always
    networks:
      - red   

volumes:
  dbdata:
  config:
  persist_volume:
  kong_data: {}
  opt-splunk-etc:
  opt-splunk-var:

networks:
  red:
    driver: bridge</code></pre>
</div>
</div>
</p>
<p>Thanks a lot.</p>",,0,0,,2021-2-20 22:58:11,,2021-2-20 22:58:11,,,,,1846233.0,,1,0,docker-compose|splunk|kong|konga,470,10
899,259355,66312201,fetching data in Splunk using rest api,"<p>I want to import XML data to Splunk using below .py script</p>
<p>My concerns are:</p>
<ol>
<li>Can I directly configure .py script output to  index data in splunk using inputs.conf, or do I need to save output first into a .csv file. If yes can anyone please suggest some approach so that data does not get changed after storing it into a new .csv file.</li>
<li>How can I configure that .py file to fetch data in every 5 min.</li>
</ol>
<pre><code>import requests
import xmltodict
import json
 
url = &quot;https://www.w3schools.com/xml/plant_catalog.xml&quot;
 
response = requests.get(url)
content=xmltodict.parse(response.text)
print(content)
</code></pre>",,1,0,,2021-2-22 08:22:42,1.0,2021-2-22 12:19:15,2021-2-22 09:31:00,,4420967.0,,13561010.0,,1,0,splunk|splunk-query,283,11
900,259356,66330833,Get the exception and Error in splunk query,"<p>I am trying to build a splunk query to get the error summary from a log.
I want to capture all the events where there is some ERROR, Exception or Failure.</p>
<p>Below is the sample data :</p>
<pre><code>ERROR org.mule.component.ComponentException: Failed to invoke ScriptComponent{bapmFlow.component.797791858}. Component that caused exception is: ScriptComponent{bapmFlow.component.797791858}.
host = host1 = /odt/mule_/logs/bapm.logsourcetype = gdt_index
2/7/21
12:00:04.000 AM 
2021-02-07 00:00:04,422 [[Java2python].bapmFlow.stage1.03] ERROR org.mule.exception.CatchMessagingExceptionStrategy - Failed to dispatch message to error queue after it failed to process.  This may cause message loss. Message identification summary here: id=54972f10-6901-11eb-ad2a-0050568f5886 correlationId=&lt;not set&gt;, correlationGroup=-1, correlationSeq=-1
host = host1 = /odt/mule_/logs/bapm.logsourcetype = gdt_index

2021-02-07 00:00:04,407 [[Java2python].bapmFlow.stage1.03] ERROR org.mule.exception.CatchMessagingExceptionStrategy - 
********************************************************************************
Message               : org.mule.module.db.internal.domain.connection.ConnectionCreationException: Cannot get connection for URL jdbc:sqlserver://VLTROUXRPT.us.global.crux.com\PRS:1713;databaseName=DFT;domain=US;integratedSecurity=false;authenticationScheme=JavaKerberos;userName=Jack;password=&lt;&lt;credentials&gt;&gt;;trustServerCertificate=true;encrypt=true; : Login failed for user 'Jack'. ClientConnectionId:34edad77-7de1-4d0f-bc13-0fb7f090f722 (java.sql.SQLException)


2021-02-07 00:00:02,936 [[Java2python].bapmFlow.stage1.03] ERROR org.mule.exception.CatchMessagingExceptionStrategy - 
... 89 lines omitted ...
2021-02-07 00:00:02,951 [[Java2python].bapmFlow.stage1.03] ERROR org.mule.exception.CatchMessagingExceptionStrategy - Failed to dispatch message to error queue after it failed to process.  This may cause message loss. Message identification summary here: id=54970800-6901-11eb-a3d3-0050568f5165 correlationId=&lt;not set&gt;, correlationGroup=-1, correlationSeq=-1
</code></pre>
<p>I noticed the below:
The ERROR keyword before the failures with the exception name.
So I built this basic query like below but it's not giving the desired results:</p>
<pre><code>index=hdt  sourcetype=gdt_index (&quot;ERROR&quot; AND &quot;Exception&quot;) OR &quot;FAILED&quot;
| rex &quot;.*?(?&lt;Exception&gt;(\w+\.)+\w*Exception).*&quot;
| rex &quot;(?&lt;ErrorMessage&gt;\&quot;Message\&quot;:(.*\&quot;,))&quot;
| stats values(ErrorMessage) as ErrorMessage by Exception
</code></pre>",,0,1,,2021-2-23 09:54:16,,2021-2-23 09:54:16,,,,,12569641.0,,1,0,splunk-query,132,8
901,259357,66343907,Set a token per item in a Splunk multiselect,"<p>I would like to set a boolean token (selected/not selected) for every item in a multiselect input on a form. I saw a workaround for the (bizarre?) lack of out-of-the-box ability to do something like this:
<a href=""https://community.splunk.com/t5/Dashboards-Visualizations/Hide-Display-Panels-Using-Multiselect/m-p/336502/highlight/true#M21830"" rel=""nofollow noreferrer"">https://community.splunk.com/t5/Dashboards-Visualizations/Hide-Display-Panels-Using-Multiselect/m-p/336502/highlight/true#M21830</a></p>
<p>I believe a reasonable simplification is:</p>
<pre><code>&lt;fieldset&gt;
  &lt;input type=&quot;multiselect&quot; token=&quot;item_selector&quot; searchWhenChanged=&quot;true&quot;&gt;
    &lt;delimiter&gt;,&lt;/delimiter&gt;
    &lt;choice value=&quot;item1&quot;&gt;item1&lt;/choice&gt;
    &lt;choice value=&quot;item2&quot;&gt;item2&lt;/choice&gt;
  &lt;/input&gt;
&lt;/fieldset&gt;

&lt;search&gt;
  &lt;query&gt;
   eval data=&quot;$item_selector$&quot;
   | eval show1=match(data,&quot;item1&quot;)
   | eval show2=match(data,&quot;item2&quot;)
  &lt;/query&gt;
  &lt;done&gt;
    &lt;condition&gt;
      &lt;set token=&quot;show1_token&quot;&gt;$result.show1$&lt;/set&gt;
      &lt;set token=&quot;show2_token&quot;&gt;$result.show2$&lt;/set&gt;
    &lt;/condition&gt;
   &lt;/done&gt;
 &lt;/search&gt;
</code></pre>
<p>I don't believe this is working, as:</p>
<pre><code>  &lt;row&gt;
    &lt;panel depends=&quot;$my_true$&quot;&gt;
      &lt;title&gt;Panel 1&lt;/title&gt;
      &lt;html&gt;$show1_token$&lt;/html&gt;
      &lt;html&gt;$show1_token$&lt;/html&gt;
    &lt;/panel&gt;
  &lt;/row&gt;
</code></pre>
<p>simply displays the following on the dashboard no matter which items are selected in the multiselect:</p>
<blockquote>
<p>$show1_token$</p>
</blockquote>
<blockquote>
<p>$show2_token$</p>
</blockquote>
<p>whereas I would have expected some combination of &quot;true&quot; or &quot;false&quot; (depending on which items were selected).</p>
<p>They called this a &quot;dummy search&quot; - when would this search be executed?</p>
<p>Is this a reasonable way (if it can be corrected) to set a boolean token per item? Is there an alternative (other than javascript, as I don't have permissions for that on this dashboard)?</p>",66683609.0,1,0,,2021-2-24 02:38:08,,2021-3-18 01:38:54,,,,,284529.0,,1,0,splunk|splunk-dashboard,535,10
902,259358,66362275,How to label a cluster based on the first Message in splunk,"<p>I am trying to achieve below functionality</p>
<p>Generate a <code>timechart</code> showing the number of different errors occurring in a server</p>
<p>This I can achieve using below query</p>
<pre><code>  index = &quot;my_host&quot; LogLevel=ERROR
  | eval Message=mvindex(field1,1) 
  | timechart count(LogLevel) BY Message
</code></pre>
<p>This generates a graph like below</p>
<p><a href=""https://i.stack.imgur.com/57IhE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/57IhE.png"" alt=""Query Without Cluster"" /></a></p>
<p>Which is working as expected, now the issue is when I try to cluster the message</p>
<pre><code>index = &quot;my_host&quot; LogLevel=ERROR
  | eval Message=mvindex(field1,1) 
  | eval Message=mvindex(field1,1) | cluster t=0.2 field=Message showcount=true labelonly=true | timechart count(LogLevel) BY cluster_label
</code></pre>
<p><a href=""https://i.stack.imgur.com/m7FHn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m7FHn.png"" alt=""Query with cluster"" /></a></p>
<p>The graph is exactly as expected, my challenge is now how to label as label <code>[1, 2, 3, 4, ...]</code> isnt user friendly</p>
<p>Is it possible to change this label to the Message field but still group by cluster_label?</p>",66371685.0,1,0,,2021-2-25 04:06:53,,2021-2-25 15:39:48,2021-2-25 07:23:46,,13680115.0,,13680115.0,,1,0,splunk|splunk-query,51,8
903,259359,66382822,Issue with Splunk Query Stats not brining in all values,"<p>I have a log which has below lines in it:</p>
<pre><code> &quot;Results&quot;:{&quot;Elapsed&quot;:&quot;0&quot;,&quot;Message&quot;:&quot;No of Application to Obsolete in Teradata : 4&quot;,&quot;TraceLevel&quot;:&quot;INFO&quot;},&quot;Security&quot;:{&quot;Vendor&quot;:&quot;CRAB&quot;}}
&quot;Results&quot;:{&quot;Elapsed&quot;:&quot;0&quot;,&quot;Message&quot;:&quot;Total Application Asset in Teradata : 1696&quot;,&quot;TraceLevel&quot;:&quot;INFO&quot;},&quot;Security&quot;:{&quot;Vendor&quot;:&quot;CRAB&quot;}}
&quot;Results&quot;:{&quot;Elapsed&quot;:&quot;0&quot;,&quot;Message&quot;:&quot;Total Application count from SPAM : 1694&quot;,&quot;TraceLevel&quot;:&quot;INFO&quot;},&quot;Security&quot;:{&quot;Vendor&quot;:&quot;CRAB&quot;}}
&quot;Results&quot;:{&quot;Elapsed&quot;:&quot;0&quot;,&quot;Message&quot;:&quot; Application/s to Obsolete in Teradata : [PA00007618, PA00007617, PA00007619, PA00007620]&quot;,&quot;TraceLevel&quot;:&quot;INFO&quot;},&quot;Security&quot;:{&quot;Vendor&quot;:&quot;CRAB&quot;}}
</code></pre>
<p>I want the output to have the below fields like a summary and not like in 4 columns.</p>
<pre><code>ExecutionDate   Host          Summary
 02-24-2021   Production     No of Application to Obsolete in Teradata : 4
                            Total Application Asset in Teradata : 1696
                            Total Application count from SPAM : 1694
                            Application/s to Obsolete in Teradata : [PA00007618, 
                            PA00007617, PA00007619, PA00007620]
</code></pre>
<p>I have built below query but it's only giving me one record :</p>
<pre><code>ExecutionDate Host Total Application count from SPAM : 1694


index=hdt  sourcetype=Teradata_SPAM_logs  | fields -_raw
| where match(_raw, &quot;Host_cdc&quot;) and (match(_raw,&quot;Total\sApplication\scount\sfrom\sSPAM\s*&quot;) 
OR match(_raw,&quot;Total\sApplication\sAsset\sin\sTeradata\s*&quot;) 
OR match(_raw,&quot;No\sof\sApplication\sto\sObsolete\sin\sTeradata\s*&quot;) 
OR match(_raw,&quot;List\sof\sApplications\sin\sTeradata\sto\sbe\smarked*&quot;) 
)
| rex &quot;(?&lt;Summary&gt;\&quot;Message\&quot;:(.*\w+)\s:.*)&quot; 
| rex &quot;(?&lt;Host&gt;\&quot;Host\&quot;:(.*\&quot;,))&quot; 
| rex &quot;(?&lt;ExecutionDate&gt;\d{4}\-\d{2}\-\d{2})&quot; 
| rex field=Summary mode=sed &quot;s/\&quot;Message\&quot;:\&quot;/ /&quot;
| rex field=Summary mode=sed &quot;s/\&quot;TraceLevel.*/ /&quot;
| rex field=Summary mode=sed &quot;s/\&quot;.*$//&quot;
| rex field=Host mode=sed &quot;s/\&quot;Channel.*/ /&quot; 
| rex field=Host mode=sed &quot;s/\&quot;Host\&quot;:\&quot;/ /&quot; 
| rex field=Host mode=sed &quot;s/\/.*/ /&quot;
| eval Host = replace(Host,&quot;Host_cdc.cdc.CRAB.com&quot;, &quot;PRODUCTION&quot;) 
| eval Host = replace(Host,&quot;Host_DEV.cdc.CRAB.com&quot;, &quot;PROFILING&quot;) 
| eval Host = replace(Host,&quot;Host_PP.cdc.CRAB.com&quot;, &quot;VALIDATION&quot;) 
| stats  values(Summary) as Summary by ExecutionDate, Host
| where isnotnull(Summary)
</code></pre>
<p>Can anyone tell me where is the problem here?</p>",,2,2,,2021-2-26 08:56:42,,2021-2-26 12:30:51,2021-2-26 09:23:57,,12569641.0,,12569641.0,,1,1,splunk|splunk-query,98,8
904,259360,66392449,How can I add fields from different serches and plot it on the same graph,"<p>I am getting no data for e2e_latency_ms. I want to plot pp_latency_ms,cv_edge_latency_ms, api_latency_ms and total of all these three latency on the same plot. But can't figure out how to do it. Any help will be appriciated. My query is as below.</p>
<pre><code>index=&quot;till_product_recognition_dev&quot; event_type=&quot;MESSAGES_FORWARDING_STATUS&quot; store_id=1 |
eval ref_time=strptime(timestamp,&quot;%Y-%m-%dT%H:%M:%S.%N&quot;) |
eval _time=if(0==1, ref_time, _time) |
rename payload.mean_latency as pp_latency |
eval pp_latency_ms = pp_latency * 1000 |
append [search index=&quot;till_product_recognition_dev&quot; app_id=&quot;cvp-tpr-edge&quot; event_type=&quot;HEALTH_CHECK_STATUS&quot; store_id=1
| rename payload.predictions_latency.batching_latency_ms as bl_ms
| rename payload.predictions_latency.cropping_latency_ms as cl_ms
| rename payload.predictions_latency.inference_latency_ms as il_ms
| eval cv_edge_latency_ms = bl_ms + cl_ms + il_ms] |
append [search index=&quot;till_product_recognition_dev&quot; app_id=&quot;cvp-tpr-edge-api&quot;  event_type=&quot;PREDICTIONS_ENQUEUE_STATUS&quot; store_id=1| rename payload.mean_latency as api_latency_s | eval api_latency_ms=api_latency_s * 1000] | eval e2e_latency_ms= pp_latency_ms + cv_edge_latency_ms + api_latency_ms |
timechart span=1m median(pp_latency_ms) as cv_post_processing_latency median(cv_edge_latency_ms) as cv_edge_latency median(api_latency_ms) as cv_edge_api_latency median(e2e_latency_ms) as end_to_end_latency

</code></pre>",,0,0,,2021-2-26 20:33:36,,2021-2-26 20:33:36,,,,,10225421.0,,1,0,splunk-query|splunk-calculation|splunk-dashboard,9,3
905,259361,66420754,How to use docker logging plugin,"<p>I have a Docker container(.net core application) running in OpenShift. We would like to have our logs pushed to SPLUNK Enterprise which is already there.</p>
<p>We have come across below plugin
<a href=""https://github.com/splunk/docker-logging-plugin"" rel=""nofollow noreferrer"">https://github.com/splunk/docker-logging-plugin</a></p>
<p>My Current Dokerfile builds and contains steps to run my .net applications. Where should I specify the configuration for &quot;docker-logging-plugin&quot; ?</p>
<pre><code># ----------- Base
FROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base
.....
Initial Docker base steps....

//How to have splunk configured here ?

# ----------- Final
FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [&quot;dotnet&quot;, &quot;Employee.WebServices.dll&quot;]
</code></pre>
<p>Should the Docker-logging-plugin configuration be included in my current Dockerfile which contains steps to build and run my .net core app or should it be like a separate container.</p>
<p>If its a saparate container how does it know it has to read logs from my container and push to SPLUNK ?</p>
<pre><code>docker run \
    --log-driver=splunk \
    --log-opt splunk-token=176FCEBF-4CF5-4EDF-91BC-703796522D20 \
    --log-opt splunk-url=https://splunkhost:8088 \
    --log-opt splunk-capath=/path/to/cert/cacert.pem \
    --log-opt splunk-caname=SplunkServerDefaultCert \
    --log-opt tag=&quot;{{.Name}}/{{.FullID}}&quot; \
    --log-opt labels=location \
    --log-opt env=TEST \
    --env &quot;TEST=false&quot; \
    --label location=west \
    your/application   //What should I mention here ?
</code></pre>
<p>In the above command what should I specify for &quot;your/application&quot;</p>",,0,0,,2021-3-1 10:52:57,,2021-3-1 10:52:57,,,,,804401.0,,1,0,docker|splunk,29,5
906,259362,66422836,Print String array of a json payload in splunk,"<p>I need to print a string array along with one field in my json object.</p>
<p>The data:</p>
<pre><code>{ &quot;key1&quot;:&quot;val1&quot;,  &quot;key2&quot;:&quot;value2&quot;,  &quot;codes&quot;:[&quot;apple&quot;,&quot;mango&quot;,&quot;banana&quot;,&quot;orange&quot;], &quot;key3_conditional&quot;:&quot;yes&quot;}
</code></pre>
<p>My Search query:</p>
<pre><code>&lt;My search query&gt;
| rex &quot;\|(?&lt;payload&gt;[^\|]*)$&quot;
| spath input=payload
| rex &quot;\&quot;codes\&quot;:\&quot;(?&lt;codes&gt;[^\&quot;]*)&quot;
| eval is_unknown=if(isnotnull(key3_conditional), key3_conditional, &quot;no&quot;)
| table codes, is_unknown
</code></pre>
<p>Desired result</p>
<pre><code>codes                               | is_unknown
--------------------------------------------------
apple, mango, banana, orange        | yes
</code></pre>
<p>Currently, this only displays the 1st value in codes i.e. <code>apple</code> and I need all values of codes as comma separated. I'm supposing there is some issue with my regex. Please suggest.</p>",66425124.0,1,1,,2021-3-1 13:15:46,,2021-3-2 19:56:37,,,,,6438896.0,,1,0,regex|splunk|splunk-query,320,11
907,259363,66432116,"Configure enterprise Splunk in docker, so services can log to HTTP Event Collector over HTTP","<p>I'm trying to set up and configure enterprise Splunk in docker for local testing. I want to be able to send logs to the HTTP event collector (HEC) via the docker logging provider for splunk - see <a href=""https://docs.docker.com/config/containers/logging/splunk/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I can configure a HEC token by specifying <code>SPLUNK_HEC_TOKEN</code> as an environment variable when I run the splunk container (see docker-compose below), but I want to be able to call the HEC endpoint over HTTP (i.e. without SSL). If SSL is enabled in <code>/opt/splunk/etc/apps/splunk_httpinput/local/inputs.conf</code>, my test service (see docker-compose below) doesn't work - I get the following error:</p>
<blockquote>
<p>Error response from daemon: failed to initialize logging driver:
Options https://localhost:8088/services/collector/event/1.0: x509:
cannot validate certificate for localhost because it doesn't
contain any IP SANs.</p>
</blockquote>
<p>My docker-compose file looks like this:</p>
<pre><code>version: '3.5'

networks:
   skynet:

services:

   splunk:
      image: splunk/splunk:latest
      environment:
         SPLUNK_START_ARGS: &quot;--accept-license&quot;
         SPLUNK_PASSWORD: $SPLUNK_PASSWORD
         SPLUNK_HEC_TOKEN: $SPLUNK_HEC_TOKEN
      ports:
         - 8000:8000
         - 8088:8088
      networks:
         - skynet

   test:
      image: ryans/test-service
      depends_on:
         - splunk
      environment:
         WAIT_FOR_IT: http://localhost:8000
      ports:
         - 5001:5001
      logging:
         driver: splunk
         options:
            splunk-url: https://localhost:8088
            splunk-token: $SPLUNK_HEC_TOKEN
            splunk-insecureskipverify: 'true'
      networks:
         - skynet
</code></pre>
<p>Interestingly, if I comment out my test service and just run docker-compose with only the Splunk container; I can call the Splunk HEC URL over HTTPS using cURL e.g.</p>
<pre><code>curl -k https://localhost:8088/services/collector -H 'Authorization: Splunk abcd1234' -d '{\&quot;event\&quot;: \&quot;Hello from event collector\&quot;}'
</code></pre>
<p>In the Splunk web interface; if I go to <code>settings &gt; Data Inputs &gt; HTTP Event Collector &gt; Global Settings</code> and explicitly disable Enable SSL; I can then send logs to the HEC endpoint over HTTP (using cURL), and I can bring up my test service without error (and logs start coming through to Splunk).</p>
<pre><code>docker run -p 5001:5001 --log-driver=splunk --log-opt splunk-token=abcd1234 --log-opt splunk-url=http://localhost:8088 ryans/test-service
</code></pre>
<p>My question is how can I get the <code>Enable SSL</code> setting to default to disabled/off?</p>
<p>FYI., I tried overriding the <code>splunk/splunk</code> docker image to manually set enableSSL in <code>/opt/splunk/etc/apps/splunk_httpinput/local/inputs.conf</code>, but for some reason (when I exec into the running container) it's reverted back to enabled/on i.e. <code>enableSSL = 1</code>...</p>
<p>Dockerfile:</p>
<pre><code>FROM splunk/splunk:latest
COPY ./inputs.conf /opt/splunk/etc/apps/splunk_httpinput/local/inputs.conf
</code></pre>
<p>inputs.conf:</p>
<pre><code>[http]
disabled = 0
enableSSL = 0

[http://splunk_hec_token]
disabled = 0
token = abcd1234
</code></pre>",66432402.0,1,0,,2021-3-2 01:47:25,,2021-3-2 02:30:17,2021-3-2 01:53:19,,824434.0,,824434.0,,1,1,docker|docker-compose|splunk,327,12
908,259364,66469504,Open Telemetry Collector issues exporting trace data to Splunk and/or Kafka,"<p>I'm trying to capture telemetry information from a .NET Core app to Kafka and then ultimately into Splunk (either via Kafka Connect or via an open-source forwarder like Logstash or Fluentd). Initially, I'm just looking at trace data, but will look at logs and metrics at some stage as well.</p>
<p>I'm keen to test out the Open Telemetry Collector approach, as this is something I'm interested in.</p>
<p>I'm using docker and docker-compose, so I can test out everything locally - see <a href=""https://github.com/yesmarket/splunk-demo/blob/master/docker-compose.otel-agent.yml"" rel=""nofollow noreferrer"">here</a> for docker-compose file.</p>
<p>The main issue I'm facing at the moment is getting the Open Telemetry Collector agent to run in docker. I've tried exporting to both Splunk as well as Kafka, but I'm getting errors for both:</p>
<p>When trying to export trace info directly to Splunk using the <a href=""https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/splunkhecexporter"" rel=""nofollow noreferrer"">splunk_hec exporter</a> as follows:</p>
<pre><code>exporters:
  splunk_hec:
    token: &quot;${SPLUNK_HEC_TOKEN}&quot;
    endpoint: &quot;${SPLUNK_HEC_URL}&quot;
    source: &quot;otel&quot;
    sourcetype: &quot;otel&quot;
    insecure_skip_verify: true

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [splunk_hec]
</code></pre>
<p>I'm getting the following error:</p>
<blockquote>
<p>otel     | Error: cannot load configuration: unknown exporters type &quot;splunk_hec&quot; for splunk_hec</p>
<p>otel     | 2021/03/04 05:47:13 application run finished with error: cannot load configuration: unknown exporters type &quot;splunk_hec&quot; for splunk_hec</p>
</blockquote>
<p>When trying to export trace info to Kafka using the <a href=""https://github.com/open-telemetry/opentelemetry-collector/tree/main/exporter/kafkaexporter"" rel=""nofollow noreferrer"">kafka exporter</a> as follows:</p>
<pre><code>exporters:
  kafka:
    brokers:
      - &quot;${KAFKA_URL}&quot;
    protocol_version: 2.0.0

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [kafka]
</code></pre>
<p>I'm getting the following error - note I'm using the <a href=""https://hub.docker.com/r/landoop/fast-data-dev"" rel=""nofollow noreferrer"">landoop/fast-data-dev</a> image for Kafka (as this was used in the first couple tutorials I found) and I'm pretty new to Kafka:</p>
<blockquote>
<p>otel     | Error: cannot setup pipelines: cannot build builtExporters: error creating kafka exporter: kafka: client has run out of available brokers to talk to (Is your cluster reachable?)</p>
<p>otel     | 2021/03/04 05:32:08 application run finished with error: cannot setup pipelines: cannot build builtExporters: error creating kafka exporter: kafka: client has run out of available brokers to talk to (Is your cluster reachable?)</p>
</blockquote>
<p>My <code>otel-agent-config.yaml</code> (which is a WIP) is <a href=""https://github.com/yesmarket/splunk-demo/blob/master/otel/otel-agent-config.yaml"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Does anyone know what these issues are and how to resolve them...</p>",,0,0,,2021-3-4 06:08:40,,2021-3-4 06:08:40,,,,,824434.0,,1,1,apache-kafka|splunk|opentracing|open-telemetry,297,9
909,259365,66491990,add disclamier note in all splunk report at one shot,"<p>we want to add one disclaimer notes as below  on 50 reports of splunk.. can we do that in one shot?</p>
<p>Disclaimer:- In case you do not find any attachment OR Blank attachment this means no matching events or Blocked events were found during the report scheduled time.</p>",,1,0,,2021-3-5 11:44:13,,2021-3-5 13:11:39,,,,,14843828.0,,1,0,splunk|splunk-query|splunk-dashboard,19,6
910,259366,66498188,Getting the average of the number of attempts using splunk,"<p>Whenever there is an attempt at creating a template, the message &quot;Show Template Creation Dialog&quot; gets logged. I want to get the total number of times that message got logged over a specific period of time. I also want to get the average number of times that message got logged over a period of time. How do I do so using splunk? I want to get this as a single number and not a chart.</p>
<pre><code>index=&quot;projectm-$env$-ue1&quot;  &quot;Show Template Creation Dialog&quot;
| mvexpand &quot;meta.adobeguid&quot; limit=1
| stats count as &quot;Total&quot;, distinct_count(&quot;meta.adobeguid&quot;) AS &quot;Unique Users&quot;
</code></pre>",,0,2,,2021-3-5 19:02:14,,2021-3-5 19:02:14,,,,,851097.0,,1,0,splunk|splunk-query,20,5
911,259367,66533265,Splunk search for a field value inside a value,"<p>am new at Splunk; I have a query like:</p>
<pre><code>org_name=&quot;Something&quot; app_name=MY_APP status_code=401 requesturl=/api/hi
</code></pre>
<p>and return results like:</p>
<pre><code>app_name: MY_APP 
   org_name: something
   space_name: space
   job: router
   message_type: OUT
   msg: my.domain.com - [2021-03-08T15:49:54.049006875Z] &quot;POST /api/hiHTTP/1.1&quot; 401 347 147 &quot;-&quot; &quot;insomnia/7.0.3&quot; &quot;10.158.192.20:46006&quot; &quot;10.158.192.158:61002&quot; x_forwarded_for:&quot;208.127.201.110, 2.20.143.172, 10.158.192.20&quot; x_forwarded_proto:&quot;https&quot; vcap_request_id:&quot;539a9655-3198-4604-775c-306a750ca7e5&quot; response_time:0.008447 gorouter_time:0.000489 app_id:&quot;9ecffb96-0b8b-4a7a-be74-484ed6b6f589&quot; app_index:&quot;4&quot; x_cf_routererror:&quot;-&quot; x_mc_correlation_id:&quot;0.ac8f1402.1615218593.dafa73c&quot; x_correlation_id:&quot;-&quot; correlation_id:&quot;0.ac8f1402.1615218593.dafa73c&quot; x_b3_traceid:&quot;0ca5ab5513f3cb99&quot; x_b3_spanid:&quot;0ca5ab5513f3cb99&quot; x_b3_parentspanid:&quot;-&quot; b3:&quot;0ca5ab5513f3cb99-0ca5ab5513f3cb99&quot;
   origin: gorouter
   source_instance: 7
   source_type: RTR
   timestamp: 1615218594057716500
</code></pre>
<p>I need to search for all the entries with</p>
<blockquote>
<p>correlation_id:&quot;0.ac8f1402.1615218593.dafa73c&quot;</p>
</blockquote>
<p>Key could be correlation_id, XC_correlation_id, MC-REAL_correlation_id and others...</p>",66533674.0,1,0,,2021-3-8 16:11:21,,2021-3-8 16:53:33,,,,,3044117.0,,1,0,splunk|splunk-query,35,6
912,259368,66550659,Geospatial lookup returns wrong results,"<p>I have a problem with identifying if a point is within a polygon or not.</p>
<p>For the same latitude and longitude, sometimes I get right results, but sometimes not. I get wrong results if I search for more IDs (so there are more events) or I change the time range (not necessarily bigger).</p>
<p>The line which causes the issue: | lookup some_lookup_name latitude longitude</p>
<p>I have reproduced the issue with sample data and simple geospatial lookup. All events with id=1 are inside the geofence, events with id=2 are outside. I am getting incorrect results (different when sorting according to time). I have two sample data sources (data_sample_2 has changed latitude in one of the events for id=2, from 51.501835 to 51.501836).</p>
<p><a href=""https://i.stack.imgur.com/VjbXN.png"" rel=""nofollow noreferrer"">When sorting from earliest to latest, all events are inside</a></p>
<p><a href=""https://i.stack.imgur.com/ul5tt.png"" rel=""nofollow noreferrer"">When sorting from latest to earliest, some events for id=1 are outside (which is incorrect)</a></p>
<p><a href=""https://i.stack.imgur.com/00yQT.png"" rel=""nofollow noreferrer"">Removing event with problematic latitude</a></p>
<p>Search:</p>
<pre><code>index=&quot;debug_idx&quot; source=&quot;data_sample_1&quot;
| eval time_epoch=_time
| sort 0 _time
| lookup geo_4 latitude longitude
| table id latitude longitude _time time_epoch featureId featureCollection
| noop search_optimization=false
</code></pre>
<p>Data_sample_1:</p>
<pre><code>{&quot;location&quot;:{&quot;state&quot;:{&quot;position&quot;:{&quot;timestamp&quot;:&quot;2021-01-11T01:14:34.000Z&quot;,&quot;latitude&quot;:51.501835,&quot;longitude&quot;:6.906453}}},&quot;ident&quot;:{&quot;attr&quot;:{&quot;id&quot;:&quot;2&quot;}}}}
{&quot;location&quot;:{&quot;state&quot;:{&quot;position&quot;:{&quot;timestamp&quot;:&quot;2021-01-11T01:14:34.000Z&quot;,&quot;latitude&quot;:51.501835,&quot;longitude&quot;:6.906453}}},&quot;ident&quot;:{&quot;attr&quot;:{&quot;id&quot;:&quot;2&quot;}}}}
{&quot;location&quot;:{&quot;state&quot;:{&quot;position&quot;:{&quot;timestamp&quot;:&quot;2021-01-06T22:23:23.000Z&quot;,&quot;latitude&quot;:53.500091,&quot;longitude&quot;:9.909387}}},&quot;ident&quot;:{&quot;attr&quot;:{&quot;id&quot;:&quot;1&quot;}}}}
{&quot;location&quot;:{&quot;state&quot;:{&quot;position&quot;:{&quot;timestamp&quot;:&quot;2021-01-06T22:16:58.000Z&quot;,&quot;latitude&quot;:53.49732,&quot;longitude&quot;:9.9082}}},&quot;ident&quot;:{&quot;attr&quot;:{&quot;id&quot;:&quot;1&quot;}}}}
{&quot;location&quot;:{&quot;state&quot;:{&quot;position&quot;:{&quot;timestamp&quot;:&quot;2021-01-06T22:13:30.000Z&quot;,&quot;latitude&quot;:53.501923,&quot;longitude&quot;:9.910017}}},&quot;ident&quot;:{&quot;attr&quot;:{&quot;id&quot;:&quot;1&quot;}}}}
{&quot;location&quot;:{&quot;state&quot;:{&quot;position&quot;:{&quot;timestamp&quot;:&quot;2021-01-06T22:13:15.000Z&quot;,&quot;latitude&quot;:53.501923,&quot;longitude&quot;:9.910017}}},&quot;ident&quot;:{&quot;attr&quot;:{&quot;id&quot;:&quot;1&quot;}}}}
</code></pre>
<p>Data_sample_2:</p>
<pre><code>{&quot;location&quot;:{&quot;state&quot;:{&quot;position&quot;:{&quot;timestamp&quot;:&quot;2021-01-11T01:14:34.000Z&quot;,&quot;latitude&quot;:51.501836,&quot;longitude&quot;:6.906453}}},&quot;ident&quot;:{&quot;attr&quot;:{&quot;id&quot;:&quot;2&quot;}}}}
{&quot;location&quot;:{&quot;state&quot;:{&quot;position&quot;:{&quot;timestamp&quot;:&quot;2021-01-11T01:14:34.000Z&quot;,&quot;latitude&quot;:51.501835,&quot;longitude&quot;:6.906453}}},&quot;ident&quot;:{&quot;attr&quot;:{&quot;id&quot;:&quot;2&quot;}}}}
{&quot;location&quot;:{&quot;state&quot;:{&quot;position&quot;:{&quot;timestamp&quot;:&quot;2021-01-06T22:23:23.000Z&quot;,&quot;latitude&quot;:53.500091,&quot;longitude&quot;:9.909387}}},&quot;ident&quot;:{&quot;attr&quot;:{&quot;id&quot;:&quot;1&quot;}}}}
{&quot;location&quot;:{&quot;state&quot;:{&quot;position&quot;:{&quot;timestamp&quot;:&quot;2021-01-06T22:16:58.000Z&quot;,&quot;latitude&quot;:53.49732,&quot;longitude&quot;:9.9082}}},&quot;ident&quot;:{&quot;attr&quot;:{&quot;id&quot;:&quot;1&quot;}}}}
{&quot;location&quot;:{&quot;state&quot;:{&quot;position&quot;:{&quot;timestamp&quot;:&quot;2021-01-06T22:13:30.000Z&quot;,&quot;latitude&quot;:53.501923,&quot;longitude&quot;:9.910017}}},&quot;ident&quot;:{&quot;attr&quot;:{&quot;id&quot;:&quot;1&quot;}}}}
{&quot;location&quot;:{&quot;state&quot;:{&quot;position&quot;:{&quot;timestamp&quot;:&quot;2021-01-06T22:13:15.000Z&quot;,&quot;latitude&quot;:53.501923,&quot;longitude&quot;:9.910017}}},&quot;ident&quot;:{&quot;attr&quot;:{&quot;id&quot;:&quot;1&quot;}}}}
</code></pre>
<p>Geolookup_4.kml:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt;
&lt;kml xmlns=&quot;http://www.opengis.net/kml/2.2&quot;&gt;
&lt;Document id=&quot;root_doc&quot;&gt;
&lt;Schema name=&quot;geo_4&quot; id=&quot;geo_4&quot;&gt;
    &lt;SimpleField name=&quot;Name&quot; type=&quot;string&quot;&gt;&lt;/SimpleField&gt;
&lt;/Schema&gt;
&lt;Folder&gt;
&lt;name&gt;geo_4&lt;/name&gt;
&lt;Placemark&gt;
    &lt;name&gt;{&quot;place&quot;:&quot;Location 1&quot;,&quot;category&quot;:&quot;Locations&quot;,&quot;description&quot;:[&quot;A&quot;],&quot;subdescription&quot;:&quot;01&quot;}&lt;/name&gt;
    &lt;Style&gt;&lt;LineStyle&gt;&lt;color&gt;ff0000ff&lt;/color&gt;&lt;/LineStyle&gt;&lt;PolyStyle&gt;&lt;fill&gt;0&lt;/fill&gt;&lt;/PolyStyle&gt;&lt;/Style&gt;
    &lt;Polygon&gt;&lt;outerBoundaryIs&gt;&lt;LinearRing&gt;&lt;coordinates&gt;9.92195,53.51249 9.92848,53.49588 9.89352,53.49482 9.89561,53.51567 9.92195,53.51249&lt;/coordinates&gt;&lt;/LinearRing&gt;&lt;/outerBoundaryIs&gt;&lt;/Polygon&gt;
&lt;/Placemark&gt;
&lt;/Folder&gt;
&lt;/Document&gt;&lt;/kml&gt;
</code></pre>",,0,0,,2021-3-9 16:15:48,,2021-3-9 16:15:48,,,,,15362279.0,,1,1,geospatial|lookup|splunk,26,5
913,259369,66551039,Splunk Dashboard Formatting issues,<p>need some help regarding Splunk Dashboard Report formatting issues.. whereby the dashboard legend is getting truncated when being either exported or saved as PDF. Need some solutions to get the report format as it does in the dashboard view.</p>,,0,0,,2021-3-9 16:37:15,,2021-3-9 16:37:15,,,,,15362109.0,,1,0,formatting|splunk-dashboard,8,3
914,259370,66556348,"In splunk dashboards, what is the most efficient way to apply user input changes?","<p>I am new to splunk.</p>
<p>I have a dashboard with multiple dropdown inputs.  Let's say:</p>
<ul>
<li>Input 1</li>
<li>Input 2</li>
<li>Input 3</li>
</ul>
<p>When the user chooses Input 1 I want to populated the dropdowns for Input 2.
When the user chooses the value for Input 2 I want to populate the dropdowns for Input 3.</p>
<p>When all Inputs are populated I want to populate the dashboard.</p>
<p>Currently I have defined all inputs with the <code>searchWhenChanged=true</code>.</p>
<p>But I think all the queries are ran each time one of the inputs are changed.</p>
<p>Is there a better way to do this?</p>",,1,0,,2021-3-9 23:43:31,,2021-3-10 13:07:46,,,,,317027.0,,1,0,splunk|splunk-query,157,9
915,259371,66561021,How does splunk dashboard actually display the search query data?,"<p>I am new to Splunk and have been exploring its features. I have uploaded some dummy data in CSV format and tried to create a dashboard out of it.
My query is if some new data is added, how does the dashboard reflect the change in data?</p>
<p>Any understanding of it is highly appreciated.</p>",,1,0,,2021-3-10 08:32:20,0.0,2021-3-10 12:46:07,,,,,15366716.0,,1,1,splunk|splunk-dashboard,32,7
916,259372,66570103,Can I refer to a token dynamically in Splunk?,"<p>New to Splunk...</p>
<p>Not sure if there is a way to do this.</p>
<p>Let's say I have a token</p>
<pre><code>&lt;set token=&quot;token_1&quot;&gt;one&lt;/set&gt;
&lt;set token=&quot;token_2&quot;&gt;two&lt;/set&gt;
</code></pre>
<p>Let's say I have another token:</p>
<pre><code>&lt;set token=&quot;qualifer&quot;&gt;1&lt;/set&gt;
</code></pre>
<p>Can I then do something like</p>
<pre><code>&lt;eval token=&quot;value&quot;&gt; token_{qualifer} &lt;/eval&gt;
</code></pre>
<p>and basically set value to one (the value of token_1)?  Effectively...</p>
<pre><code>&lt;set token=&quot;value&quot;&gt; one &lt;/set&gt;
</code></pre>",,1,0,,2021-3-10 17:42:11,,2021-3-12 16:02:40,2021-3-11 22:57:24,,317027.0,,317027.0,,1,0,splunk|splunk-query|splunk-formula,39,7
917,259373,66571907,Splunk query for http status for AWS for active and inactive elb's,<p>Looking to get a Splunk query that can give HTTP status code for instances on 2 elb's say A has 70% traffic for B has 30% traffic. how to fetch a traffic status only on Farm A.</p>,,0,0,,2021-3-10 19:52:08,,2021-3-10 19:52:08,,,,,15370943.0,,1,0,splunk-query,16,4
918,259374,66591409,How to write a cron expression for a job to run through Mon to Friday at 12:05AM except for if Mon is the second day of the month?,"<p>I currently have for the first part:</p>
<ol>
<li><p>Cron expression for job to run from Mon-Fri at 12:05AM
--&gt; 5 0 * * 1-5</p>
</li>
<li><p>Now I want to add the second part of the question to the expression
(Not to run if Mon was the 2nd day of the month)</p>
</li>
</ol>",,0,2,,2021-3-11 22:20:48,,2021-3-11 22:27:59,,,,,15378909.0,,1,1,cron|splunk,23,5
919,259375,66592818,fetching data from the XML request and response,"<p>I have xml data request and response in the splunk coming as below</p>
<p><strong>request</strong>
3/11/21
8:07:36.967 PM<br />
[3/12/21 1:07:36:967 UTC] 00001e46 LogMessageHan 3   &lt;soapenv:Envelope xmlns:soapenv=&quot;fo&quot;&gt;soapenv:BodyEWS123DDA345DDA1SxxxNrrr000&lt;/soapenv:Body&gt;&lt;/soapenv:Envelope&gt;</p>
<p><strong>response</strong>
[3/11/21 7:13:59:061 UTC] 00001e0d LogMessageHan 3   &lt;SOAP-ENV:Envelope xmlns:SOAP-ENV=&quot;fd&quot;&quot;&gt;SOAP-ENV:Body&lt;NewTransferReply xmlns=fo.xsd&quot;&gt;001Failure  OLAR500P ERROR CODE: 999  FILE IS CLOSED&lt;/SOAP-ENV:Body&gt;&lt;/SOAP-ENV:Envelope&gt;</p>
<p>now my requirement is to fetch all the transactions which are failing with error OLAR500P ERROR CODE: 999  FILE IS CLOSED where rrr</p>",,0,3,,2021-3-12 01:13:44,,2021-3-12 01:13:44,,,,,15316902.0,,1,0,xml|splunk,17,4
920,259376,66634751,Splunk Alert Creation,"<p>I am new to Splunk and need suggestion for creating the below alert in Splunk.</p>
<p>I need to create an alert which will check the log file last updated timestamp and if it is not updated for last ten minutes, then alert should be triggered.</p>
<p>Thanks in Advance. Hope this would be my kick start for the Splunk learning part.</p>",66640300.0,1,0,,2021-3-15 08:40:19,,2021-3-15 14:52:45,,,,,8164380.0,,1,1,splunk|splunk-query,49,8
921,259377,66641473,SIEM plugin for SailPoint and Splunk timestamp discrepancy,"<p>I have a question about how events arrive inside of a tool such as Splunk.</p>
<p>Currently, it appears that the way the plugin works is it will send the records over to the log collection application in question, placing the time stamp of the event in epoch time (located in the attributes of the event).</p>
<p>However, it would appear that within Splunk, it is marking the event time at which point it entered into Splunk.</p>
<p>So, for example, I have an event that happened two years ago, the time stamp on the event shows that and SailPoint shows that (even Splunk will show that if you drill into the event).</p>
<p>However, when I import into Splunk, let's say today (March 15, 2021), Splunk will show the event timestamp of <em>today</em> instead of when the actual event took place.</p>
<p>So when I am doing analytics, I can't actually look for when the event took place based on Splunk's timestamps (because Splunk is showing the day of import, not when the event took place): it won't actually be showing me events that took place two years ago.</p>
<p>Splunk would be showing those events that took place two years ago as events that took place today because that's when the events from SailPoint were imported into Splunk.</p>",66642863.0,1,0,,2021-3-15 16:00:29,,2021-3-16 12:51:06,2021-3-16 12:51:06,,4418.0,,613876.0,,1,2,integration|splunk|sailpoint,69,8
922,259378,66643257,Splunk query not endswith,"<p>I am just into learning of Splunk queries, I'm trying to grab a data from <code>myfile.csv</code> file based on the <code>regex</code> expression.</p>
<p>In particular, I'm looking forward, print only the rows where column <code>fqdn</code> not endswith <code>udc.net</code> and <code>htc.com</code>.</p>
<p>Below is my query which is working but i'm writing it twice.</p>
<pre><code>| inputlookup myfile.csv 
| regex support_group=&quot;^mygroup-Linux$&quot; 
| regex u_sec_dom=&quot;^Normal Secure$&quot; 
| regex fqdn!=&quot;.*?udc.net$&quot;
| regex fqdn!=&quot;.*?htc.com$&quot;
| where match(fqdn,&quot;.&quot;)
</code></pre>
<p>I am trying them to combine with <code>|</code> separeted but not working though...</p>
<pre><code>   | regex fqdn!=&quot;(.*?udc.net | &quot;.*?htc.com)$&quot;
</code></pre>",66655995.0,2,0,,2021-3-15 18:05:02,,2021-3-17 11:27:51,2021-3-17 11:27:51,,4418.0,,5704863.0,,1,0,regex|splunk|splunk-query|splunk-dashboard,105,10
923,259379,66654768,Splunk: indexes and metrics,"<p>Could you please help me with the following question:
When creating a <code>metrics</code> and <code>index</code> in the Splunk, do you have to create a single <code>index</code> per <code>metrix</code> or you can use  many to many connection?</p>",66656780.0,1,0,,2021-3-16 11:55:37,,2021-3-16 14:01:05,,,,,1392154.0,,1,0,metrics|splunk,47,7
924,259380,66654906,Data ingestion with HTTP Event Collector in Splunk Web vs data ingestion using Splunk REST API,"<p>I am new to Splunk and have been exploring it's features. I have tried to ingest some dummy data into Splunk Web using the Http Event Collector(HEC). I wanted to know if there is any other REST API available in splunk for data input. If so, then what is the difference between HEC and the other REST API provided by Splunk. Thanking in advance for any understanding. :)</p>",,1,0,,2021-3-16 12:06:21,,2021-3-17 13:23:31,,,,,15366716.0,,1,0,splunk|splunk-api,78,7
925,259381,66688709,how to apply multiple addition in Splunk,"<p>Hi Have below data from below query ..
(index=abc OR index=def) |rex field=index &quot;(?&lt;Local_Market&gt;[^cita]\w.*?)_&quot; | chart count by blocked , Local_Market</p>
<p>blocked     dub rat mil
0       10  20  21
1       02  03  09
2       9   2   1</p>
<p>Now i want the data as below</p>
<p>total bolocked(sumof 0 and sumof 2)   dub   rat     mil     total found(Sumof 1)
(10+20+21+9+2+1)=63               10    20       21     (02+03+09)=14</p>",,2,1,,2021-3-18 09:57:57,,2021-3-18 14:58:08,2021-3-18 11:51:58,,14843828.0,,14843828.0,,1,0,splunk|splunk-query|splunk-formula,41,7
926,259382,66695006,Splunk search if host is inactive for more than 5 minutes,"<p>I have two specific messages in splunk data that I'm searching for per user.</p>
<ul>
<li>active</li>
<li>inactive
Anyone know how I can search in splunk for a user that is inactive for more than 5 minutes.
I already have the search where it finds the inactive and active messages and a timestamp for each.
What I want to do is only return results if the time between those 2 messages was more than 5 minutes per user.</li>
</ul>
<p>Example:</p>
<pre><code>index=&quot;document&quot; (message=&quot;inactive&quot; OR message=&quot;active&quot;) 
</code></pre>
<p>Not sure how to work out the time between those two messages based on the timestamp per user ?</p>",66696194.0,1,0,,2021-3-18 16:19:52,,2021-3-18 17:30:51,2021-3-18 16:33:21,,1096499.0,,1096499.0,,1,0,splunk|splunk-query,116,8
927,259383,66699698,Splunk search if message is x for more than 5 minutes,"<p>I have two specific messages in splunk data that I'm searching for per user.</p>
<ul>
<li>on-screen</li>
<li>off-screen</li>
</ul>
<p>Anyone know how I can search in splunk for a user that is message=&quot;off-screen&quot; for more than 5 minutes with a query checking every 2 minutes ?</p>
<pre><code>index=&quot;document&quot; (message=&quot;off-screen&quot;)
</code></pre>
<p>My query will be ran every 2 minutes so I want to check for the event with message off-screen. Then next time around check if 5 minutes have elapsed since the on-screen message was fired and that no on-screen event was fired in that time period for that user.
Is this possible ?</p>",67732395.0,1,1,,2021-3-18 21:58:07,,2021-5-28 03:10:32,,,,,1096499.0,,1,0,splunk|splunk-query,66,7
928,259384,66713378,Splunk - Setting up earliest and latest time with relative date and fixed time,"<p>I am trying to define a query where I have to use the earliest time as 2 days ago at 22:20:45 and latest time 1 day ago at 22:20:45</p>
<p>I tried different formats below</p>
<pre><code>earliest=-2d@:22h:20m:45s
latest=-1d@d+20h+30m+45s
</code></pre>
<p>Are these definitions correct?  How should these fields be defined to get the data in the range I am expecting?</p>
<p>I am not sure if these are correct and also how can I check if this translates to the date and time I am trying to set.</p>
<p>Thank you</p>",,0,0,,2021-3-19 17:57:45,,2021-3-19 17:57:45,,,,,913749.0,,1,0,datetime|splunk-query,112,8
929,259385,66717598,Sllunk: How to tag or group events by multiple possible values of a field without repeated value?,"<p>Here is the data for illustration:</p>
<p>(To facilitate experiment, I provide below the query snippet to recreate the data in Splunk query.)</p>
<pre><code>| makeresults
| eval _raw=
&quot;Date   Time    DEVICE  ATTRIBUTE   STATE   TagExpected
2021-03-19  11:56:22.449    K30 SOR_B_STATUS.STATE  SOR_FAILED  1
2021-03-19  12:16:17.564    K30 SOR_A_STATUS.STATE  SOR_FAILED  1
2021-03-19  12:17:55.191    K30 SOR_RESTRICT_STATUS.STATE   SOR_SPEED_RESTRICT_ON   1
2021-03-19  12:21:16.659    K30 SOR_A_STATUS.STATE  SOR_FAILED  2
2021-03-19  12:32:42.247    K30 SOR_B_STATUS.STATE  SOR_FAILED  2
2021-03-19  12:51:21.456    A60 SOR_RESTRICT_STATUS.STATE   SOR_SPEED_RESTRICT_ON   2
2021-03-19  12:51:52.949    A60 SOR_A_STATUS.STATE  SOR_FAILED  1
2021-03-19  12:54:01.077    A60 SOR_B_STATUS.STATE  SOR_FAILED  1
2021-03-19  13:01:26.367    A60 SOR_RESTRICT_STATUS.STATE   SOR_SPEED_RESTRICT_ON   1
2021-03-19  13:01:26.818    K30 SOR_A_STATUS.STATE  SOR_FAILED  3
2021-03-19  13:02:41.142    K30 SOR_B_STATUS.STATE  SOR_FAILED  3
2021-03-19  13:08:19.694    A60 SOR_RESTRICT_STATUS.STATE   SOR_SPEED_RESTRICT_ON   2
2021-03-19  13:09:14.433    K30 SOR_B_STATUS.STATE  SOR_FAILED  4
2021-03-19  13:10:19.149    W34 SOR_RESTRICT_STATUS.STATE   SOR_SPEED_RESTRICT_OFF  1
2021-03-19  13:16:12.847    A60 SOR_B_STATUS.STATE  SOR_FAILED  3
2021-03-19  13:24:59.420    A60 SOR_A_STATUS.STATE  SOR_FAILED  3
2021-03-19  13:24:59.870    A60 SOR_RESTRICT_STATUS.STATE   SOR_SPEED_RESTRICT_ON   3
2021-03-19  13:25:48.068    A60 SOR_RESTRICT_STATUS.STATE   SOR_SPEED_RESTRICT_OFF
2021-03-19  13:35:47.614    A60 SOR_A_STATUS.STATE  SOR_FAILED  4
2021-03-19  13:38:19.632    A90 SOR_B_STATUS.STATE  SOR_FAILED  1
2021-03-19  13:46:10.118    R20 SOR_B_STATUS.STATE  SOR_FAILED  1
2021-03-19  13:50:30.328    R50 SOR_A_STATUS.STATE  SOR_FAILED  1
2021-03-19  13:54:58.831    W20 SOR_RESTRICT_STATUS.STATE   SOR_SPEED_RESTRICT_ON   1
2021-03-19  13:55:30.622    W20 SOR_RESTRICT_STATUS.STATE   SOR_SPEED_RESTRICT_OFF
2021-03-19  13:56:38.060    A60 SOR_A_STATUS.STATE  SOR_FAILED  5
2021-03-19  14:02:19.102    K30 SOR_B_STATUS.STATE  SOR_FAILED  5
2021-03-19  14:08:51.212    R50 SOR_A_STATUS.STATE  SOR_FAILED  2
2021-03-19  14:09:47.657    R20 SOR_B_STATUS.STATE  SOR_FAILED  2
2021-03-19  14:11:10.387    C30 SOR_B_STATUS.STATE  SOR_FAILED  1
2021-03-19  15:01:15.315    C30 SOR_B_STATUS.STATE  SOR_FAILED  2
2021-03-19  15:02:33.670    R65 SOR_A_STATUS.STATE  SOR_FAILED  1
2021-03-19  15:06:56.258    C50 SOR_B_STATUS.STATE  SOR_FAILED  1
2021-03-19  15:09:32.583    R50 SOR_A_STATUS.STATE  SOR_FAILED  3
2021-03-19  15:09:33.484    R50 SOR_RESTRICT_STATUS.STATE   SOR_SPEED_RESTRICT_ON   3
2021-03-19  15:09:40.240    R50 SOR_RESTRICT_STATUS.STATE   SOR_SPEED_RESTRICT_OFF
2021-03-19  15:36:17.104    A90 SOR_B_STATUS.STATE  SOR_FAILED  1&quot; 
| multikv forceheader=1 
| eval combined=Date.&quot; &quot;.Time 
| eval _time=strptime('combined', &quot;%Y-%m-%d %H:%M:%S.%Q&quot;) 
| fields _time DEVICE ATTRIBUTE STATE TagExpected
</code></pre>
<p>In the above records, I’d like to tag or group the events by the following rules: For a device value, e.g. K30, I’d like to tag the events with DEVICE value K30 with first occurrences of SOR_B_STATUS.STATE or SOR_A_STATUS.STATE (their occurrences do not matter) and SOR_RESTRICT_STATUS.STATE after with the same tag value, to be one group.</p>
<p>For the same DEVICE value, next occurrences of SOR_B_STATUS.STATE or SOR_A_STATUS.STATE (their occurrences do not matter) and SOR_RESTRICT_STATUS.STATE after I’d group them to be tag 2, to be a different group, etc.</p>
<p>Subset of such event group will also be considered to be separate group.</p>
<p>The events in a group does not need to be adjacent in time.</p>
<p>I don’t insist on the tag value, any mechanism to group the events would be fine.</p>
<p>I feel that using streamstats in some fashion might solve the problem. But I need help to wrap around my brain to work it help.</p>
<p>Really appreciate that you could give me some help or hint.</p>
<p>Thanks in advance!</p>",,0,0,,2021-3-20 02:04:32,,2021-3-21 01:19:23,2021-3-21 01:19:23,,126164.0,,126164.0,,1,0,splunk-query,18,5
930,259386,66731869,Serving local map tiles in a {z}/{x}/{y}.png for a leaflet based app,"<p>I'm using a Splunk app named <code>Maps+</code> which is based on a leaflet platform. the computer which uses the app is offline, therefor I'm trying to create a local tile server, which should serve the files in a <code>{z}/{x}/{y}.png</code> format (according to the app).
I've looked into <code>TileStache</code>, <code>Geoserver</code> etc. but I couldn't find a solution for the XYZ format.
Does anyone have a solution?</p>
<p>Thanks in advance.</p>",,0,0,,2021-3-21 11:41:53,,2021-3-21 11:41:53,,,,,10254633.0,,1,0,leaflet|geolocation|gis|geospatial|splunk,23,5
931,259387,66738922,Grouping data in a sorted list,"<p>In splunk, I want to group data in order after it is sorted.</p>
<p><strong>sample events</strong></p>
<pre><code>Time: 1:00 server: A .....
Time: 1:01 server: A ......
Time: 1:02 server: B ......
Time: 1:03 server: A ......
Time: 1:04 server: A ......
Time: 1:05 server: C ......
Time: 1:06 server: A ......
</code></pre>
<p><em><strong>I want to see</strong></em></p>
<pre><code>Server: A Events: 2
Server: B Events: 1
Server: A Events: 2
Server: C Events: 1
Server: A Events: 1
</code></pre>
<p>Extra points if I can get start and end times for each set too.</p>",,1,0,,2021-3-22 00:20:07,,2021-3-22 12:49:17,,,,,2977329.0,,1,0,splunk|splunk-query,53,6
932,259388,66748720,Dynamically generated column using tag::eventtype has header null in splunk,"<p>The splunk dashboard uses the EKS cluster to generate the metrics over the dashboard. One of the panel has the below query :</p>
<pre><code>tag::eventtype=&quot;abc&quot;  OR tag::eventtype=&quot;xyz&quot;  | timechart count by tag::eventtype
</code></pre>
<p>The query gives the count as a single column but with null header. I want to give the column showcasing the sum as a dynamic header or something related to tag in eventtype autopopulates as the column header.
Kindly help.</p>",,0,0,,2021-3-22 15:06:07,,2021-3-22 15:06:07,,,,,9186499.0,,1,0,amazon-web-services|dashboard|splunk|amazon-eks,18,5
933,259389,66752948,Summary Index In splunk,"<p>can you please help me with time stamp of summay index..
we having disk space issue and we are clearing the old logs . but we want keep some field data so if will schedule a SI then does it will add the data from last 1 month at one time ..then why we need to schedule it ? have gone through the splunk document but unable to understand the steps and logic  ..</p>",,1,0,,2021-3-22 19:50:59,,2021-3-22 23:33:33,,,,,14843828.0,,1,-1,splunk|splunk-query|splunk-calculation,454,10
934,259390,66758238,how to check if splunk has received the logs from 100 different hosts,"<p>I am new to splunk. Wanted to create a splunk alert to check if logs has been received from all the host or not and if not need to set a alert trigger.</p>
<pre><code>| tstats latest(_time) as latest where index=* earliest=-24h by host
| eval recent = if(latest &gt; relative_time(now(),&quot;-5m&quot;),1,0), realLatest = strftime(latest,&quot;%c&quot;)
| where recent=0

</code></pre>
<p>is the above splunk Query correct?</p>",,1,0,,2021-3-23 06:27:33,,2021-3-23 11:09:08,,,,,12046526.0,,1,0,splunk|splunk-query|splunk-formula,93,8
935,259391,66769038,host=$decideOnStartup not working in splunk configuration on Windows EC2,"<p>As <a href=""https://community.splunk.com/t5/Getting-Data-In/Inputs-conf-decideonstartup/m-p/103954"" rel=""nofollow noreferrer"">someone mentioned in the Splunk forum</a>:</p>
<blockquote>
<p>$decideonstartup just would not work for me.</p>
</blockquote>
<p>The <a href=""https://docs.splunk.com/Documentation/SplunkCloud/latest/Data/SetadefaulthostforaSplunkserver"" rel=""nofollow noreferrer"">splunk daemon is <em>supposed</em></a> to set the value automatically:</p>
<blockquote>
<p>Note: By default, the host attribute is set to the variable $decideOnStartup, which means that it's set to the hostname of the machine splunkd is running on. The splunk daemon re-interprets the value each time it starts up.</p>
</blockquote>
<p>But it just stayed as <code>host=$decideonstartup</code> for me.</p>
<p>What's the best way to set this value when configuring Splunk via an AWS SSM Document?</p>",66769039.0,1,0,,2021-3-23 18:20:55,,2021-3-23 18:37:17,,,,,9754418.0,,1,0,windows|amazon-web-services|amazon-ec2|splunk|aws-ssm,230,9
936,259392,66771920,Difference between outputs of same query as search and subsearch,"<p>Why is it that the following Splunk search:</p>
<pre><code>search sourcetype=srt  | table serialNumber
</code></pre>
<p>will give me a one-column table of serial numbers as expected, while the same query in subsearch brackets</p>
<pre><code>[search sourcetype=srt  | table serialNumber]
</code></pre>
<p>does not return the same table that I expect, but rather returns the full record?</p>
<p>The documentation seems to suggest that both results should be the same.</p>",,1,0,,2021-3-23 22:00:27,,2021-3-24 07:44:56,,,,,1194651.0,,1,1,splunk,88,9
937,259393,66774233,Splunk Understanding Difference with epoch time and adding min,"<p>I have been able to read the blogs and understand somewhat how the calculations are taking place, but need further clarity.</p>
<p><strong>Case</strong>: Ingestion_Time_Logged will output the CreationTime event on the 7th min or the 37th min regardless of what time the actual CreationTime event occurred. Example:</p>
<p>If CreationTime Event occured at &quot;2021-03-06 07:38:59.000&quot; Then the Ingestion_Time_Logged will be the closest 7th min or 37th min. In this case the Ingestion_Time_Logged will be
&quot;2021-03-06 08:07:59.000&quot;. The code works fine. I am just trying to understand how it is calculating it.</p>
<p><strong>What I understand so far:</strong></p>
<p>% is the modulo operator which is a way to determine the remainder of a division operation. Instead of returning the result of the division, the modulo operation returns the whole number remainder.</p>
<p>latestComposed_min%30 will divide the minutes by 30 and result in the reminder. I used it to find if the minute is between 0 - 7 or 30 - 37.  It is the same as below but much easier and efficient.</p>
<p>if(latestComposed_min &lt; 7 OR (latestComposed_min&gt;30 AND latestComposed_min &lt; 37))</p>
<p>Excluded 37 and 7 to keep them as they are since they are already ok, if we do not exclude them, they will be added 37 minutes. Since both are NOT comparisons AND should be used between them otherwise with OR the result will be always true (which is wrong)</p>
<p><strong>The piece I am struggling with:</strong></p>
<pre><code>| eval Ingestion_Time_Logged=strftime(case(latestCreated_min%30 &lt; 7, CreationTime_epoch-CreationTime_epoch%1800+420+latestCreated_sec, latestCreated_min!=37 AND latestCreated_min!=7, CreationTime_epoch-CreationTime_epoch%1800+2220+latestCreated_sec,1=1,CreationTime_epoch),&quot;%Y-%m-%d %H:%M:%S.%6N&quot;)
</code></pre>
<p><strong>Full Code:</strong></p>
<p>| makeresults</p>
<p>| eval CreationTime=&quot;2021-03-06 07:38:59.000&quot;</p>
<p>| eval CreationTime_epoch=strptime(CreationTime, &quot;%Y-%m-%d %H:%M:%S.%6N&quot;)</p>
<p>| eval latestCreated_hour=tonumber(strftime(CreationTime_epoch, &quot;%H&quot;))</p>
<p>| eval latestCreated_min=tonumber(strftime(CreationTime_epoch, &quot;%M&quot;))</p>
<p>| eval latestCreated_sec=round(CreationTime_epoch%60,6)</p>
<p>| eval Ingestion_Time_Logged=strftime(case(latestCreated_min%30 &lt; 7, CreationTime_epoch-CreationTime_epoch%1800+420+latestCreated_sec, latestCreated_min!=37 AND latestCreated_min!=7, CreationTime_epoch-CreationTime_epoch%1800+2220+latestCreated_sec,1=1,CreationTime_epoch),&quot;%Y-%m-%d %H:%M:%S.%6N&quot;)</p>
<p>| table Ingestion_Time_Logged,  CreationTime, CreationTime_epoch, latestCreated_hour, latestCreated_min</p>",66782868.0,1,0,,2021-3-24 03:16:13,,2021-3-24 14:11:11,,,,,7151703.0,,1,0,case|splunk|epoch,81,9
938,259394,66785593,Splunk: regex - No events counted,"<p>I am trying to extract a field after a specific expression using regex and then running a query which counts the events where this condition is met. I did this:</p>
<pre><code>query | rex field=_raw &quot;text: (?&lt;value&gt;\d+)&quot; | timechart partial=f span=5m count as numbers | where value &gt; 3
</code></pre>
<p>There are some log-entries for which value is greater than 3, but nevertheless this events are not counted. What did I do wrong here and why did I not get a result?</p>",66785883.0,1,0,,2021-3-24 16:45:42,,2021-3-24 17:04:10,,,,,11572712.0,,1,1,regex|splunk|splunk-query,44,10
939,259395,66814967,Splunk: How to set-up an alert that informs about usual behaviour?,"<p>I have a data flow which is usually very constant over smaller time intervals and only increases or decreases with a very similar slope in short time intervals. However, sometimes there occurs a hiccup like in the middle of the graph, where the system is e.g. down and almost no data can be processed. After the system is up again, we will see a peak, where the not processed data will be processed. After this peak data flow will get to a &quot;normal&quot; level again.</p>
<p>The situation is now as follows: it is easy to set-up an alert when the system is down or gets up again. I am now wondering if there is a possibility to establish an alert when the situation is &quot;normal&quot; again? That means, that there is neither an abnormal low data flow level nor an abnormal high level.</p>
<p>What would be your suggestion to solve this objective?</p>
<p><a href=""https://i.stack.imgur.com/aWtyX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aWtyX.png"" alt=""Graph"" /></a></p>",,0,0,,2021-3-26 10:17:53,,2021-3-26 10:17:53,,,,,11572712.0,,1,0,alert|splunk,16,4
940,259396,66822630,Splunk search - how to reset stats by group instead of all stats for the search,"<p>I want to create a report on job execution times by job ID.  The same job ID could be executed multiple times.  The logs capture each time the job starts, but does not explicitly log when it finishes.  For the purposes of this report, we will determine the stop time based on the last log captured for the job. To achieve this, I need loop through the results capturing the start time and the latest log time to determine the stop time.</p>
<p>The problem I'm running into is I need to reset any stats captured for a job ID when the search detects another instance of the job has started. I tried the reset_before/reset_after/reset_on_change to achieve the desired results, but those trigger a reset of stats for all job IDs not the one that was re-executed.  Here's a visual of the raw data and an example of the report I'm trying to generate.</p>
<p>Input Data
<a href=""https://i.stack.imgur.com/V2y3w.png"" rel=""nofollow noreferrer"">sample</a></p>
<p>Desired Result
<a href=""https://i.stack.imgur.com/CLzTO.png"" rel=""nofollow noreferrer"">sample report</a></p>
<p>Here is the start of the search...I removed the reset stat attempts to avoid causing any confusion.  This search pulls back the data, but I have not been successful in getting the stats to reset by job ID when a new job starts.</p>
<pre><code>index=jobs message=&quot;*Started*&quot; OR message=&quot;*processing*&quot; 
| rex field=message &quot;@(?&lt;JobID&gt;[^\(]+)&quot;
| stats earliest(_time) as start, latest(_time) as stop by JobID
| eval starttime=strftime(start,&quot;%Y-%m-%d %H:%M&quot;) 
| eval stoptime=strftime(stop,&quot;%Y-%m-%d %H:%M&quot;) 
| eval runtime=round((stop-start)/60/60,2)
| table JobbID, starttime, stoptime, runtime
</code></pre>
<p>Any help is appreciated!</p>",,1,0,,2021-3-26 18:39:48,,2021-3-28 06:53:22,2021-3-26 18:55:08,,15488749.0,,15488749.0,,1,0,splunk|splunk-query,85,7
941,259397,66826035,Splunk Sequence on Following Eval Case Functions,"<p>How does Spunk prioritize conditional case functions? Lets say I have a case function with 2 conditions - they work fine, and results are as expected, but then lets say I flip the conditions. What I see happen when I flip the conditions in the case function the results are not correct. Shouldn't Splunk be able to still check which condition it applies to even though I have flipped the conditions?  Example below:</p>
<p><strong>Case:</strong> TimeSchedule should output the closest 7th min or 37th min - so every half hour past the 7th min or 37th min from the zipTime_epoch.</p>
<p><strong>Works Fine as output TimeSchedule should be 2021-03-06 23:37:59.000000</strong></p>
<pre><code>| makeresults
| eval zipTime=&quot;2021-03-06 23:35:59.000&quot;
| eval zipTime_epoch=strptime(zipTime, &quot;%Y-%m-%d %H:%M:%S.%6N&quot;)
| eval lastunzip_hour=tonumber(strftime(zipTime_epoch, &quot;%H&quot;))
| eval lastunzip_min=tonumber(strftime(zipTime_epoch, &quot;%M&quot;))
| eval lastunzip_sec=round(zipTime_epoch%60,6)
| eval TimeSchedule=strftime(case(lastunzip_min%30 &lt; 7, zipTime_epoch-

zipTime_epoch%1800+420+lastunzip_sec,lastunzip_min!=37 AND lastunzip_min!=7, zipTime_epoch-zipTime_epoch%1800+2220+lastunzip_sec,1=1,zipTime_epoch),&quot;%Y-%m-%d %H:%M:%S.%6N&quot;)
</code></pre>
<p><strong>Does not work fine when case in conditions are flipped- output should be 2021-03-06 23:37:59.000000 instead.</strong></p>
<pre><code>| makeresults
| eval zipTime=&quot;2021-03-06 23:35:59.000&quot;
| eval zipTime_epoch=strptime(zipTime, &quot;%Y-%m-%d %H:%M:%S.%6N&quot;)
| eval lastunzip_hour=tonumber(strftime(zipTime_epoch, &quot;%H&quot;))
| eval lastunzip_min=tonumber(strftime(zipTime_epoch, &quot;%M&quot;))
| eval lastunzip_sec=round(zipTime_epoch%60,6)
| eval TimeSchedule=strftime(case( lastunzip_min!=37 AND lastunzip_min!=7, zipTime_epoch-
zipTime_epoch%1800+2220+lastunzip_sec,lastunzip_min%30 &lt; 7, zipTime_epoch_epoch-  zipTime_epoch_epoch%1800+420+lastunzip_sec,1=1,zipTime_epoch),&quot;%Y-%m-%d %H:%M:%S.%6N&quot;)
    
| table TimeSchedule, zipTime, lastunzip_hour, lastunzip_min, lastunzip_sec, zipTime_epoch
</code></pre>",,1,0,,2021-3-27 00:51:39,,2021-3-27 11:25:59,,,,,7151703.0,,1,0,bigdata|case|eval|splunk|epoch,113,8
942,259398,66866948,Monitoring a JSON or YAML config file and showing the parent of the changed attributes,"<p>I have a case in which I need to monitor JSON and YAML config files for changes, but all of the solutions that I found only showed line-by-line changes.</p>
<p>For example, if I have a JSON as such:</p>
<pre><code>    {
      &quot;server-1&quot;: {
          &quot;name&quot;: &quot;A&quot; ,
          &quot;metadata&quot;:{
             &quot;tags&quot;: [&quot;X&quot;,&quot;Y&quot;,&quot;Z&quot;], 
             &quot;date-created&quot;: &quot;10 March 2021&quot;
          }
      },
      &quot;server-2&quot;: {
          &quot;name&quot;: &quot;B&quot; ,
          &quot;metadata&quot;:{
             &quot;tags&quot;: [&quot;W&quot;,&quot;X&quot;,&quot;Y&quot;], 
             &quot;date-created&quot;: &quot;11 March 2021&quot;
          }
      },
    }
</code></pre>
<p>If server-2's tags were to be changed, I want to get: <strong>search-1, metadata, tags</strong> as its result.</p>
<p>Does anyone have any ready-to-use solutions that I can use for monitoring purposes?</p>
<p>I am looking for a ready-to-use solutions since the JSON and YAML fields are quite dynamic and contains a lots of fields. I am considering ELK stack or Splunk for this, but I am not quite sure it will work or not.</p>",,0,5,,2021-3-30 08:02:21,,2021-3-30 08:02:21,,,,,5856700.0,,1,0,json|yaml|monitoring|elastic-stack|splunk,33,6
943,259399,66868644,API monitoring using splunk,"<p>I have one API : <code>[GET] http:localhost:8080/myservice/fetchdetails</code>.
Now, I want to raise splunk alert whenever this API is down for any reason.</p>
<p>So, I have my search query as <code>|eval ['http:localhost:8080/myservice/fetchdetails'] | search status=20*</code> to monitor the API.</p>
<p>But it is not fetching me any result. What should be the search query such that it makes a get call to the API and then capture the response status?</p>",66909926.0,2,5,,2021-3-30 09:55:12,,2021-4-1 18:40:28,2021-4-1 14:39:45,,3551125.0,,3551125.0,,1,0,splunk|splunk-query,348,12
944,259400,66890601,splunk to find status report of each student,"<p>Index =’sqat’</p>
<p>Time,</p>
<p>Event</p>
<p>{</p>
<p>“Exam -type” : “Annually”</p>
<p>Exams : { “Message” : Failed in Maths exam ,Result:Failed, “Name” : s1, }</p>
<p>SubjecctName: Maths</p>
<p>}</p>
<p>{</p>
<p>“Exam -type” : “Quarterly”</p>
<p>Exams : { “Message” : Failed in Maths exam ,Result:Failed, “Name” : s2 }</p>
<p>SubjecctName: Maths</p>
<p>}</p>
<p>{</p>
<p>“Exam -type” : “semi - Annually”,</p>
<p>Exams : { “Message” : Failed in Maths exam ,Result:Failed, “Name” : s3 }</p>
<p>SubjecctName: Maths</p>
<p>}</p>
<p>{</p>
<p>“Exam -type” : “semi - Annually”</p>
<p>Exams : { “Message” : “Passed in Maths exam” ,”Result”:”Passed”, ’Name’ : “s4”}</p>
<p>SubjecctName: Maths</p>
<p>}</p>
<p>Statusreport of each student in last 30 days, last 1 day ( Count no of exams passed and failed by each student)</p>
<ol>
<li>S1</li>
<li>S2</li>
<li>S3</li>
</ol>
<p>Search | spath=Exams | mvexpand Exams | rename Exams as _raw | stats count by Result,Name</p>
<p>when i am trying to calcuate the output i am getting aggregates for the exams passed</p>",,0,2,,2021-3-31 15:17:50,,2021-3-31 15:17:50,,,,,15509588.0,,1,0,splunk,15,4
945,259401,66896889,how to calculate duration between two events Splunk,"<p>I need to find the duration between two events. I went over the solutions on <a href=""/questions/tagged/splunk"" class=""post-tag"" title=""show questions tagged &#39;splunk&#39;"" rel=""tag"">splunk</a> and Stack Overflow, but still can't get the calculation.</p>
<p>Both <code>sentToSave</code> and <code>SaveDoc</code> have the time stamp already formatted, which is why I used the case function. I am able to see the fields populate with their time stamps, but I am not able to get the <code>Duration</code> field to populate the duration - it simply does not populate at all.</p>
<p>Need some help on how to get the <code>Duration</code> - any advice? Here is my search:</p>
<pre><code>(index=souce1 dept=qvc event=&quot;sentToSave&quot;) OR (index=source dept=save area=saveDoc)
| eval saveDocTime=case(area=&quot;saveDoc&quot;, TimeStamp), sentToSaveTime=case(event=&quot;sentToSave&quot;, TimeStamp)
| eval Duration=saveDocTime-sentToSaveTime
| stats values(Duration) as Duration earliest(sentToSaveTime) as sentToSaveTime latest(saveDocTime) as saveDocTime  by emailRequest
| where isNotNull(sentToSaveTime) AND isNotNull(saveDocTime)
</code></pre>",,2,1,,2021-4-1 00:17:20,,2021-4-5 02:59:20,2021-4-1 13:19:51,,4418.0,,7151703.0,,1,2,splunk|splunk-query|splunk-calculation,272,14
946,259402,66912791,"Splunk rex extract field, I am close but just cant get it matching","<p>Value session_value contains this info:</p>
<p>not found, name: user@mycompany.com more text here</p>
<p>Trying to use this:</p>
<pre><code>rex field=session_value &quot;:\s(?&lt;USERID&gt;)@&quot;
</code></pre>
<p>To extract: user</p>
<p>I think I am close, anyone assist?</p>",,1,0,,2021-4-1 23:39:15,,2021-4-5 15:18:53,2021-4-5 15:18:53,,4418.0,,745402.0,,1,0,regex|splunk,167,10
947,259403,66955702,In splunk is there a way to prepopulate the time input field with values for another token?,"<p>In Splunk, I have a dashboard with init-section.  I use the init-section to set 2 tokens, then I use the token values to set the default value for a time input.</p>
<p>When I run the dashboard, the time input is unpopulated.  If I replace $earliest_time_token$, $latest_time_token$ with their actual values then the time token is pre-populated.</p>
<p>Is there a way to pre-populate the time input field using variables?</p>
<hr />
<p>fyi - I tried -7d@d &amp; &quot;-7d@d&quot; I get the same result</p>
<hr />
<pre><code>&lt;form&gt;
  &lt;init&gt;
    &lt;set token=&quot;earliest_time_token&quot;&gt;&quot;-7d@d&quot;&lt;/set&gt;
    &lt;set token=&quot;latest_time_token&quot;&gt;&quot;now&quot;&lt;/set&gt;
  &lt;/init&gt;
  &lt;label&gt;Time Input&lt;/label&gt;
  &lt;fieldset autoRun=&quot;true&quot; submitButton=&quot;false&quot;&gt;
    &lt;input type=&quot;time&quot; token=&quot;time_token&quot; searchWhenChanged=&quot;true&quot;&gt;
      &lt;label&gt;Time Range&lt;/label&gt;
      &lt;default&gt;
        &lt;earliest&gt;$earliest_time_token$&lt;/earliest&gt;
        &lt;latest&gt;$latest_time_token$&lt;/latest&gt;
      &lt;/default&gt;
    &lt;/input&gt;
    ....
  &lt;/fieldset&gt;
  ....
&lt;/form&gt;
</code></pre>",67808192.0,1,0,,2021-4-5 15:46:51,,2021-6-2 15:32:19,,,,,317027.0,,1,0,splunk|splunk-dashboard,182,10
948,259404,66975395,How to search events for a value from new eval fieldname in Splunk?,"<p>I need to search for events that contains specific value generated from a new field name. This is what I'm trying to accomplish:</p>
<pre><code>index=app sourcetype=source 
| eval uri_t = &quot;uri:type:subtype:123-5678:DATA_REFERENCE:DATA1:999:123-5678:DATA2:DATA_REFERENCE2:123456&quot; 
| eval uri2=replace(uri_t, &quot;\:&quot;, &quot;%3A&quot;) 
| search uri2
</code></pre>
<p>Basically, I'm encoding part of a url using replace and eval function into field name uri2, then i need to search specifically in the result of the encoded value. But it seems using search, will search for &quot;uri2&quot; instead of the entire encoded string.</p>
<p>Note, I had to use replace to encode part of the url because it seems there is no encode function in splunk.</p>
<p>Any assistance will be appreciated.</p>",66977960.0,1,0,,2021-4-6 19:35:00,,2021-4-7 19:45:50,2021-4-7 19:45:50,,4199236.0,,4199236.0,,1,0,search|eval|encode|splunk,400,10
949,259405,67000247,Splunk extract a value from string which begins with a particular value,"<p>Could you help me extract file name in table format.
Here  the below field just before file name is always constant. &quot;Put File /test/abc/test/test/test to /test/test/test/test/test/test/test/test/test/test destFolderPath: /test/test/test/test/test/test/test/abc/def/hij&quot;</p>
<p>This is an event from splunk
2021-04-08T01:03:40.155069+00:00 somedata||someotherdata||..|||Put File /test/abc/test/test/test to /test/test/test/test/test/test/test/test/test/test destFolderPath: /test/test/test/test/test/test/test/abc/def/hij/CHARGEBACK_20210407_060334_customer.csv</p>
<p>Result should be in table format: (font / format doesnt matter)</p>
<h2>File Name</h2>
<p>CHARGEBACK_20210407_060334_customer.csv</p>",,1,0,,2021-4-8 08:36:41,,2021-4-8 12:18:37,,,,,11352888.0,,1,0,splunk,427,13
950,259406,67008441,Splunk sort class not working with subsearch,"<p>With the below query I'm trying to sort dateTime by descending order but the sorting is not working, could someone please help me to identify the issue in the query .</p>
<pre><code>hostname=&quot;alt*&quot; [search &quot;Starting Batch job&quot; AND hostname=&quot;alt*&quot; UUID=* | stats values(UUID) as uuid by UUID | fields UUID] | regex &quot;JOB Execution*&quot; |stats values(@timestamp) as dateTime,values(UUID) as uuid,values(message) as message | sort - dateTime | table dateTime, uuid, message
</code></pre>",,0,0,,2021-4-8 16:46:58,,2021-4-8 16:46:58,,,,,4758229.0,,1,0,splunk-query,15,4
951,259407,67013035,DateTime format search in the splunk search query,"<p>I have &quot;YYYY-MM-DD HH:MM:SS.QQ ERROR&quot; in my splunk logs.
Now I want to search for similar date pattern along with Status like &quot;2021-Apr-08 23:08:23.498 ERROR&quot; in my splunk logs and create alert if the ERROR tag comes next to the date.
These date are changeable and are generated at run time.</p>
<p>Can any one suggest me how to check for Date time format along with Status in splunk query.</p>",67016128.0,1,0,,2021-4-8 23:14:24,,2021-4-9 06:29:38,,,,,9186499.0,,1,0,amazon-web-services|kubernetes|logging|splunk|splunk-query,146,10
952,259408,67049174,Find Pod Count in splunk search,"<p>I have written below query to fetch the pod count for a particular application as below:</p>
<p>index=&quot;stream_data&quot; sourcetype=&quot;kube:container:stream_app&quot; | search pod=&quot;data-app-streams-*&quot; | stats dc(pod) as pod_count</p>
<p>I have set the duration for this alert as 15 min.
However, the pod count shown by this query comes to 2 because it gives the logs for the busy pods only. But the actually running pods are 4.
Here pod is the already available field.</p>
<p>Can anyone suggest another approach for finding the actual pod count</p>",67051205.0,1,2,,2021-4-11 19:12:24,,2021-4-11 23:54:06,,,,,9186499.0,,1,0,amazon-web-services|kubernetes|logging|splunk|splunk-query,136,10
953,259409,67083889,Finding avg response time using splunk query,"<p>I have a log line stating</p>
<p>Received App information from Source and processed in ms: 467</p>
<p>Now I would like to find the avg response time for the app which would be avg values for the time received after ms: Can you please guide me how do I extract the value of time (ms) and then find average response time</p>",,1,0,,2021-4-14 00:13:32,,2021-4-14 06:31:23,,,,,9186499.0,,1,0,amazon-web-services|kubernetes|logging|splunk|splunk-query,259,12
954,259410,67112352,How to filter out events before joining datasets with stats,"<p>I have some events (2 different sourcetype—process_events and socket_events) that look something like this:</p>
<pre><code>{
  &quot;action&quot;: &quot;added&quot;,
  &quot;columns&quot;: {
    &quot;time&quot;: &quot;1527895541&quot;,
    &quot;success&quot;: &quot;1&quot;,
    &quot;action&quot;: &quot;connect&quot;,
    &quot;auid&quot;: &quot;1000&quot;,
    &quot;family&quot;: &quot;2&quot;,
    &quot;local_address&quot;: &quot;&quot;,
    &quot;local_port&quot;: &quot;0&quot;,
    &quot;path&quot;: &quot;/usr/bin/curl&quot;,
    &quot;pid&quot;: &quot;30220&quot;,
    &quot;remote_address&quot;: &quot;127.0.0.2&quot;,
    &quot;remote_port&quot;: &quot;80&quot;
  },
  &quot;unixTime&quot;: 1527895545,
  &quot;hostIdentifier&quot;: &quot;HOST_ONE&quot;,
  &quot;name&quot;: &quot;socket_events&quot;,
  &quot;numerics&quot;: false
}
{
  &quot;action&quot;: &quot;added&quot;,
  &quot;columns&quot;: {
    &quot;time&quot;: &quot;1527895541&quot;,
    &quot;success&quot;: &quot;1&quot;,
    &quot;action&quot;: &quot;connect&quot;,
    &quot;auid&quot;: &quot;1000&quot;,
    &quot;family&quot;: &quot;2&quot;,
    &quot;local_address&quot;: &quot;&quot;,
    &quot;local_port&quot;: &quot;0&quot;,
    &quot;path&quot;: &quot;/usr/bin/curl&quot;,
    &quot;pid&quot;: &quot;30220&quot;,
    &quot;remote_address&quot;: &quot;10.10.10.10&quot;,
    &quot;remote_port&quot;: &quot;12345&quot;
  },
  &quot;unixTime&quot;: 1527895545,
  &quot;hostIdentifier&quot;: &quot;HOST_ONE&quot;,
  &quot;name&quot;: &quot;socket_events&quot;,
  &quot;numerics&quot;: false
}
{
  &quot;action&quot;: &quot;added&quot;,
  &quot;columns&quot;: {
    &quot;uid&quot;: &quot;0&quot;,
    &quot;time&quot;: &quot;1527895541&quot;,
    &quot;pid&quot;: &quot;30220&quot;,
    &quot;path&quot;: &quot;/usr/bin/curl&quot;,
    &quot;auid&quot;: &quot;1000&quot;,
    &quot;cmdline&quot;: &quot;curl google.com&quot;,
    &quot;ctime&quot;: &quot;1503452096&quot;,
    &quot;cwd&quot;: &quot;&quot;,
    &quot;egid&quot;: &quot;0&quot;,
    &quot;euid&quot;: &quot;0&quot;,
    &quot;gid&quot;: &quot;0&quot;,
    &quot;parent&quot;: &quot;&quot;
  },
  &quot;unixTime&quot;: 1527895550,
  &quot;hostIdentifier&quot;: &quot;HOST_ONE&quot;,
  &quot;name&quot;: &quot;process_events&quot;,
  &quot;numerics&quot;: false
}
</code></pre>
<p>Current query:</p>
<pre><code>(name=socket_events OR name=process_Events) columns.path=*bin*
| stats values(*) as * by hostIdentifier, columns.path, columns.pid
</code></pre>
<p>Result</p>
<pre><code>+-------------------------------------------------------------------------------------------+
| hostIdentifier | columns.path  | columns.pid | cmdline         | columns.remote_addr | columns.remote_p
+-------------------------------------------------------------------------------------------+
| HOST_ONE       | /usr/bin/curl | 30220       | curl google.com | 127.0.0.2           | 80
|                |               |             |                 | 10.10.10.10         | 12345
+-------------------------------------------------------------------------------------------+
</code></pre>
<p>Is there a way for me to apply some filter logics like these</p>
<blockquote>
<p>If columns.remote is multivalue AND one of the remote_address!=127.0.0.0/8 AND &gt; remote_port&gt;5000, then pipe it to stats</p>
<p>If columns.remote is not multivalue AND remote_address!=127.0.0.0/8
AND remote_port&gt;5000, then pipe it to stats()</p>
<p>Else, ignore</p>
</blockquote>
<p>I feel like I need to apply the filter before the <code>| stats ...</code> because I need to exclude all the <code>socket_events</code> events that don't satisfy the condition before the JOIN with <code>process_events</code>.</p>
<p>Any help would be awesome!</p>
<p>Also, sample data taken from <a href=""https://osquery.readthedocs.io/en/stable/deployment/process-auditing/"" rel=""nofollow noreferrer"">https://osquery.readthedocs.io/en/stable/deployment/process-auditing/</a></p>",67115994.0,1,0,,2021-4-15 16:27:38,,2021-4-15 21:01:59,,,,,6945824.0,,1,0,splunk|splunk-query,37,9
955,259411,67173540,Splunk Dashboard - difference between eval case and rangemap result,"<p>I'm running a query to bifurcate splunk results into buckets. I want to divide and count files based on sizes they are taking on disk. This can be achieved using <code>rangemap</code> or <code>eval case</code>.</p>
<p>As I read <a href=""https://wiki.splunk.com/Community:Search_Performance:_Use_Eval_Instead_of_Rangemap"" rel=""nofollow noreferrer"">here</a> using <code>eval</code> is faster than <code>rangemap</code>. But I'm getting different results on using both.</p>
<p>This is the query I'm running -</p>
<pre><code>&lt;source&gt; 
| eval size_group = case(SizeInMB &lt; 150, &quot;0-150 MB&quot;, SizeInMB &lt; 200 AND SizeInMB &gt;= 150, &quot;150-200 MB&quot;, SizeInMB &lt; 300 AND SizeInMB &gt;= 200, &quot;200-300 MB&quot;, SizeInMB &lt; 500 AND SizeInMB &gt;= 300, &quot;300-500 MB&quot;, SizeInMB &lt; 1000 AND SizeInMB &gt;= 500, &quot;500-1000 MB&quot;, SizeInMB &gt; 1000, &quot;&gt;1000 MB&quot;) 
| stats count by size_group
</code></pre>
<p>and this is the result I'm getting -</p>
<p><a href=""https://i.stack.imgur.com/cH8ZH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cH8ZH.png"" alt=""enter image description here"" /></a></p>
<p>Whereas using <code>rangemap</code> this is the query -</p>
<pre><code>&lt;source&gt; 
| rangemap field=SizeInMB &quot;0-150MB&quot;=0-150 &quot;151-200MB&quot;=150-200 &quot;201-300MB&quot;=200-300 &quot;301-500MB&quot;=300-500 &quot;501-999MB&quot;=500-1000 default=&quot;1000MB+&quot; 
| stats count by range
</code></pre>
<p>I tried this range too - <code>rangemap field=SizeInMB &quot;0-150MB&quot;=0-150 &quot;150-200MB&quot;=150-200 &quot;200-300MB&quot;=200-300 &quot;300-500MB&quot;=300-500 &quot;500-1000MB&quot;=500-1000 default=&quot;1000MB+&quot;</code> and I get the same result -</p>
<p><a href=""https://i.stack.imgur.com/wvs3G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wvs3G.png"" alt=""enter image description here"" /></a></p>
<p>There is not a huge difference in both the images results, and we can probably live with it - but I see for the range 150-200MB - it is <code>445958 vs 445961</code>, and for 200-300 MB it is <code>3676 vs 3677</code> and for 300-500 MB it is <code>3346 vs 3348</code>. I want to understand why is that difference, and which one should I trust more? Speedwise <code>eval</code> seems better, but datawise is it not so correct?</p>",,1,0,,2021-4-20 06:23:38,,2021-4-20 13:20:15,2021-4-20 13:12:22,,4418.0,,3909198.0,,1,0,splunk|splunk-query|splunk-formula|splunk-dashboard,216,11
956,259412,67174465,How to add multiple queries in one search in Splunk,"<h1>Need your assistance to add below queries in one query ..
First Query:-</h1>
<pre><code>(index=abc OR index=def  AND index!=ghi AND index!=jkl AND index!=mno AND index!=pqr) 
| eval result=case(blocked==&quot;0&quot;,&quot;Total Detection&quot;,blocked==&quot;1&quot;,&quot;Total Blocked&quot;,blocked==&quot;2&quot;,&quot;Would have Dropped&quot;,RuleAction==&quot;Allow&quot;,&quot;Total Detection&quot;,RuleAction==&quot;Block&quot;,&quot;Total Blocked&quot;)  
| stats count by result
</code></pre>
<h1>Second Query :-</h1>
<pre><code>index=abc  AND Category=* AND index!=ghi AND index!=jkl AND index!=mno AND index!=pqr 
    | eval result=case(blocked==&quot;0&quot;,&quot;Allowed&quot;,blocked==&quot;1&quot;,&quot;Blocked&quot;,blocked==&quot;2&quot;,&quot;Would have Dropped&quot;,RuleAction==&quot;Allow&quot;,&quot;Allowed&quot;,RuleAction==&quot;Block&quot;,&quot;Blocked&quot;)
    | chart count by index, result usenull=f | append [search ( index=def  AND index!=ghi AND index!=jkl AND index!=mno AND index!=pqr)  
    | eval result=case(blocked==&quot;0&quot;,&quot;Allowed&quot;,blocked==&quot;1&quot;,&quot;Blocked&quot;,blocked==&quot;2&quot;,&quot;Would have Dropped&quot;,RuleAction==&quot;Allow&quot;,&quot;Allowed&quot;,RuleAction==&quot;Block&quot;,&quot;Blocked&quot;)  
    | chart count by index, result usenull=f]
</code></pre>",,1,0,,2021-4-20 07:38:09,,2021-4-21 12:02:45,2021-4-21 12:02:45,,4418.0,,14843828.0,,1,0,splunk|splunk-query,516,11
957,259413,67196690,Issue with getting data to Splunk App for Hyperledger Fabric,"<p>I am very new to splunk, We are trying to monitor our hyperledger fabric network with the Splunk App for fabric in the Splunk enterprise. We have a hyperledger fabric network with version 2.2.2.</p>
<p>I installed the app and followed the instructions specified in <a href=""https://splunkbase.splunk.com/app/4605/#/details"" rel=""nofollow noreferrer"">https://splunkbase.splunk.com/app/4605/#/details</a>.</p>
<p>I also setup the fabric-logger and I could see the fabric-logger is running and it is able to fetch the blocks and event details from the peer from which it is connected to.</p>
<p>In the Splunk, I see this error: <code>Search peer indexer-0 has the following message: Received event for unconfigured/disabled/deleted index=hyperledger_logs with source=&quot;source::fabriclogger&quot; host=&quot;host::fabric-logger-6b79d77b99-bncwj&quot; sourcetype=&quot;sourcetype::fabric_logger:endorser_transaction&quot;. So far received events from 1 missing index(es).</code></p>
<p>I have the HEC enabled and I also have the index <code>hyperledger_logs</code>.</p>
<p>I don´t see any errors in the logs of fabric-logger or in the indexer.</p>
<p>But I am not seeing any data in Splunk.</p>
<p>Please find the screenshot below</p>
<p><a href=""https://i.stack.imgur.com/y4tAo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y4tAo.png"" alt=""enter image description here"" /></a></p>",,1,0,,2021-4-21 13:14:18,,2021-4-22 13:54:49,2021-4-22 13:54:49,,4418.0,,9196921.0,,1,0,splunk,117,11
958,259414,67205645,How to do cross validation and counts between two search queries using Multisearch,"<p>Hello Everyone I hope everyone is doing well...</p>
<p>It turns out I have to find how many times a custumer that has made a purchase has contacted the corporate line to complain... I can generate a table that shows me the custumers that have made an actual purchase by ID , and also I can make a table of the custumer that have called on the line to make a complain..</p>
<p>first table would look like this:</p>
<pre><code>ID    PRODUCT_BOUGHT
41545    x_98
1428     x_98
4856     x_91
8596     x_91
1254     x_96

</code></pre>
<p>and the second table would look like this..</p>
<pre><code>ID     CASE_NUMBER
41545     001
4856      002
4856      003
41545     004
1254      005
1254      006

</code></pre>
<p>The issue is that I need to count how many times each ID has called on the line and bring also the product bought and the case number recieved on the line... BUT I can only think of a multiseach in order to create the table but I cant seem to find any documentation on how to do the cross validation or even count and I feel like Im hitting my head against a wall...</p>
<p>This is the multisearch that I am using:</p>
<pre><code>| multisearch
[| search index=&quot;auxpik&quot;
 | search status=&quot;PAY.ok&quot;
 | fields ID PRODUCT_BOUGHT]
[|search index=&quot;auxpik&quot;
 | search in_calls=&quot;corp_cx_cases&quot;)
 | fields ID CASE_NUMBER]

</code></pre>
<p>but since I am a python user trying to learn splunk I cant seem to find a way to obtain this table:</p>
<p>desired results:</p>
<pre><code>ID       CALLS_ON_THE_LINE    PRODUCT_AND_CASES
41545         2                x_98-001-004
4856          2                x_91-002-003
1254          2                x_96-005-006
1428          0                   x_98
8596          0                   x_91

</code></pre>
<p>THank you a million to everyone that can help me out with a guidance or documentation on how to achieve this like form the bottom of my hart thank you so much!!!!!! Im sending you a big hug from Texas!</p>",67213721.0,1,0,,2021-4-22 01:46:07,,2021-4-22 12:55:37,,,,,15587184.0,,1,0,splunk|splunk-formula|splunk-calculation,26,8
959,259415,67229154,How to get cloud watch logs from AWS to splunk enterprise?,"<p>I am new to Splunk and AWS. My current project requirement is to create a Splunk dashboard from scratch. There are some log groups in Cloud Watch with stream of logs. Now, there are existing lambda functions pushing logs to Splunk but I don't have any idea as the developer who wrote lambdas left the company. Finally, I would like to know how can I get index, source and sourceType parameters to write Splunk query by finding existing lambda functions which are acting as log aggregators.</p>
<p>How can I achieve this?</p>",,0,0,,2021-4-23 11:29:44,,2021-4-23 11:29:44,,,,,15164922.0,,1,0,amazon-cloudwatch|splunk|amazon-cloudwatchlogs|splunk-query|splunk-dashboard,65,7
960,259416,67254576,Why is splunk license manager showing very high usage?,"<p>I use splunk together with this addon: <a href=""https://splunkbase.splunk.com/app/3110/"" rel=""nofollow noreferrer"">https://splunkbase.splunk.com/app/3110/</a>, which reads text log files from azure blobs. The problem is that license manager shows warnings and eventually search is blocked.
The license manager shows this for my only index:
<a href=""https://i.stack.imgur.com/Mzkoe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Mzkoe.png"" alt=""enter image description here"" /></a></p>
<p>At the same time I see that this particular index size is only  1.29 GB. And log files are only several MB per day. How is this possible?</p>
<p>I'm in trial license, but switching to free license does not change this bechaviour.</p>
<p>UPDATE:
Full picture
<a href=""https://i.stack.imgur.com/8h2Wz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8h2Wz.png"" alt=""enter image description here"" /></a></p>",,1,0,,2021-4-25 14:41:44,,2021-4-26 19:26:26,2021-4-26 19:26:26,,1116499.0,,1116499.0,,1,0,azure|azure-storage|splunk,96,8
961,259417,67259755,"Getting ""Invalid client secret is provided"" error while setting up Splunk with Azure Event Hub","<p>I am getting the following error while configuring Splunk with Azure Event Hub.</p>
<blockquote>
<p>2021-04-23 10:12:17,141 level=WARNING pid=xxxxxxx tid=Thread-2
logger=azure.eventhub._eventprocessor.event_processor
pos=event_processor.py:_load_balancing:281 | EventProcessor instance
'2ea6353e-ee45-4a4e-b173-5f82ae79707c' of eventhub
'insights-activity-logs' consumer group '$Default'. An error occurred
while load-balancing and claiming ownership. The exception is
EventHubError(&quot;Unexpected response '{'error': 'invalid_client',
'error_description': 'AADSTS7000215: Invalid client secret is
provided.\r\nTrace
ID:xxxxxxx-c913-420f-8dfb-5169faed3800\r\nCorrelation ID:
xxxxxxxx-81b2-4436-9d25-13e38ec15d9d\r\nTimestamp: 2021-04-23
02:12:10Z', 'error_codes': [7000215], 'timestamp': '2021-04-23
02:12:10Z', 'trace_id': 'xxxxxxxxx-c913-420f-8dfb-5169faed3800',
'correlation_id': 'xxxxxxxx-81b2-4436-9d25-13e38ec15d9d', 'error_uri':
'https://login.microsoftonline.com/error?code=7000215'}'\nUnexpected
response '{'error': 'invalid_client', 'error_description':
'AADSTS7000215: Invalid client secret is provided.\r\nTrace ID:
xxxxxxx-c913-420f-8dfb-5169faed3800\r\nCorrelation ID:
xxxxxxx-81b2-4436-9d25-13e38ec15d9d\r\nTimestamp: 2021-04-23
02:12:10Z', 'error_codes': [7000215], 'timestamp': '2021-04-23
02:12:10Z', 'trace_id': 'xxxxxxxxx-c913-420f-8dfb-5169faed3800',
'correlation_id': 'xxxxxxxxxx-81b2-4436-9d25-13e38ec15d9d',
'error_uri':
'https://login.microsoftonline.com/error?code=7000215'}'&quot;). Retrying
after 10.408012031827356 seconds</p>
</blockquote>
<p>I am referring to the following tutorials:</p>
<p><a href=""https://www.splunk.com/en_us/blog/tips-and-tricks/splunking-microsoft-azure-monitor-data-part-1-azure-setup.html"" rel=""nofollow noreferrer"">https://www.splunk.com/en_us/blog/tips-and-tricks/splunking-microsoft-azure-monitor-data-part-1-azure-setup.html</a></p>
<p><a href=""https://www.splunk.com/en_us/blog/tips-and-tricks/splunking-microsoft-azure-monitor-data-part-2-splunk-setup.html"" rel=""nofollow noreferrer"">https://www.splunk.com/en_us/blog/tips-and-tricks/splunking-microsoft-azure-monitor-data-part-2-splunk-setup.html</a></p>
<p>From my understanding, it is that we will have to generate a Azure AD application and set its permission for resource management and here, I am making use of it to enable Splunk to access the activity logs to my Event Hub. I have done setting up an AD application and added the role assignment to the AD application, after that, generated a client secret as mentioned in the tutorial. I am subscribing to Azure for Student, will this be the cause of getting this error as I have limited privileges?</p>",,1,0,,2021-4-26 01:21:56,,2021-5-5 20:36:44,,,,,8042425.0,,1,0,splunk|azure-eventhub,297,9
962,259418,67297498,Make statistic from a list in Splunk,"<p>I am trying to create a statistic regarding users_per_country but I am having troubles processing the list that I am recieving.</p>
<p>I get my data from a business event like this:</p>
<pre><code>{
  &quot;timestamp&quot;: &quot;2021-04-27T12:00:00.133159Z&quot;,
  &quot;eventType&quot;: &quot;STATISTICS&quot;,
  &quot;processRelevantFields&quot;: {
    &quot;eventId&quot;: &quot;users_per_country&quot;,
    &quot;eventDetails&quot;: {
      &quot;result&quot;: {
        &quot;users_per_country&quot;: [
          {
            &quot;attribute&quot;: &quot;it&quot;,
            &quot;count&quot;: 33
          },
          {
            &quot;attribute&quot;: &quot;de&quot;,
            &quot;count&quot;: 9
          }
       
        ]
      }
    }
  }
}
</code></pre>
<p>And my query in Splunk used to process this data and get a statistic is:</p>
<pre><code>  ...
    | fields &quot;processRelevantFields.eventDetails.result.users_per_country&quot;     
    | stats count(eval(like('processRelevantFields.eventDetails.result.users_per_country.attribute','%&quot;attribute&quot;:&quot;it%'))) as IT,
count(eval(like('processRelevantFields.eventDetails.result.users_per_country.attribute','%&quot;attribute&quot;:&quot;de%'))) as DE
</code></pre>
<p>But the result I get is DE = 0 and IT = 0.
How can I get the right numbers? DE = 9 and IT = 33.</p>",,0,0,,2021-4-28 09:29:20,,2021-4-28 09:54:05,2021-4-28 09:54:05,,2621706.0,,13624971.0,,1,0,java|splunk|spl,21,5
963,259419,67304621,How Can I Generate A Visualisation with Multiple Data Series In Splunk,"<p>I have been experimenting with Splunk, trying to emulate some basic functionality from the OSISoft PI Time Series database.</p>
<p>I have two data points that I wish to display trends for over time in order to compare fluctuations between them, specifically power network MW analogue tags.</p>
<p>In PI this is very easy to do, however I am having difficulty figuring out how to do it in Splunk.</p>
<p>How do I achieve this given the field values &quot;SubstationA_T1_MW&quot;, &amp; &quot;SubstationA_T2_MW&quot; in the field <code>Tag</code>?</p>
<p>The fields involved are <code>TimeStamp</code>, <code>Tag</code>, <code>Value</code>, and <code>Status</code></p>
<p>Edit:</p>
<p>Sample Input and Output listed below:</p>
<p><a href=""https://i.stack.imgur.com/aAfex.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aAfex.png"" alt=""Sample Input Data"" /></a></p>
<p><a href=""https://i.stack.imgur.com/J3QtY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J3QtY.png"" alt=""Sample Output"" /></a></p>",67317603.0,2,2,,2021-4-28 16:55:44,,2021-4-29 13:11:29,2021-4-29 12:26:15,,4418.0,,2807312.0,,1,0,splunk|splunk-query|splunk-calculation,211,11
964,259420,67306735,JSONLayout endOfLine property not working as expected,"<p>I am using this configuration in my project's log4j2.xml file to get logs in JSON format in a single line:</p>
<pre><code>&lt;JsonLayout compact=&quot;true&quot; eventEOL=&quot;true&quot; endOfLine=&quot;\n&quot; properties=&quot;true&quot;/&gt;
</code></pre>
<p>Each line is by default separated by <code>\r\n</code> when using <code>eventEOL=&quot;true&quot;</code>, however I do not want <code>\r</code> so I am using <code>endOfLine</code> to replace it with <code>\n</code>.</p>
<p>I would expect the logs to emit in new lines, but instead <code>\n</code> is printed as a string.</p>
<p>Expected:</p>
<pre><code>{&quot;message&quot;:&quot;I want this&quot;}
{&quot;message&quot;:&quot;new log in new line&quot;}
</code></pre>
<p>Current behavior:</p>
<pre><code>{&quot;message&quot;:&quot;I want this&quot;}\n{&quot;message&quot;:&quot;new log in new line&quot;}
</code></pre>
<p>Note: The reason I do not want <code>\r</code> is because Splunk is unable to parse the JSON log with <code>\r</code> (#015) in the event text.</p>
<p>Related documentation: <a href=""https://logging.apache.org/log4j/2.x/manual/layouts.html#JSONLayout"" rel=""nofollow noreferrer"">https://logging.apache.org/log4j/2.x/manual/layouts.html#JSONLayout</a></p>
<p>Any suggestion is appreciated.</p>",,0,1,,2021-4-28 19:29:37,,2021-4-29 12:06:57,2021-4-29 12:06:57,,4418.0,,6122617.0,,1,0,java|json|logging|log4j2|splunk,88,7
965,259421,67307247,Spring batch job is not terminating after adding Splunk logging step,"<p>I am running a Spring batch but for logging purposes, I am redirecting all my logs to Splunk. The logs are getting updated on Splunk as expected, but now my job is not terminating. Even after everything gets processed, it's still running.</p>
<p>How can I terminate my job with Splunk logging enabled?</p>
<p><strong>log4j2.xml :</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;Configuration status=&quot;info&quot; name=&quot;cspprovmi&quot;
               packages=&quot;com.prov&quot;&gt;

    &lt;Appenders&gt;
        &lt;SplunkHttp name=&quot;http&quot; url=&quot;${env:SPLUNK_URL}&quot;
                    token=&quot;${env:SPLUNK_TOKEN}&quot;  host=&quot;${env:SPLUNK_NAMESPACE}&quot;
                    index=&quot;cba_facts_cs&quot; source=&quot;${env:SPLUNK_SOURCE}&quot;
                    sourcetype=&quot;facts_cs:http:log&quot; messageFormat=&quot;json&quot;
                    disableCertificateValidation=&quot;true&quot;&gt;
            &lt;PatternLayout pattern=&quot;%msg&quot; /&gt;
        &lt;/SplunkHttp&gt;
        &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt;
            &lt;PatternLayout
                    pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot; /&gt;
        &lt;/Console&gt;
    &lt;/Appenders&gt;
    &lt;Loggers&gt;
        &lt;Root level=&quot;info&quot; additivity=&quot;false&quot;&gt;
            &lt;AppenderRef ref=&quot;http&quot; level=&quot;info&quot; /&gt;
        &lt;/Root&gt;
    &lt;/Loggers&gt;
&lt;/Configuration&gt;
</code></pre>
<p><strong>pom.xml :</strong></p>
<pre><code>&lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-batch&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                    &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
</code></pre>
<p><strong>JobBuilderConfig.java</strong></p>
<pre><code>  //  Job
    @Bean
    public Job job(){
        return jobBuilderFactory.get(&quot;jobCSProvMI4275&quot;)
                .start(step1())
                .next(step2())
                .next(step3())
                .next(step4())
                .next(step5())
                .next(step6())
                .next(step7())
                .incrementer(new RunIdIncrementer())
                .build();
    }
}
</code></pre>
<p>One workaround I am considering is to include <code>jobListener</code> and call <code>system.exit(0)</code> in the code itself. This seems to work fine, as it's terminating job even with Splunk logging enabled, but not sure if there is a better way to achieve this (ie exit the process gracefully).</p>
<pre><code>   @Bean
    public JobExecutionListener jobMi4275ExecutionListener() {
        JobExecutionListener jobExecutionListener = new JobExecutionListener() {
            @Override
            public void beforeJob(JobExecution jobExecution) {
                LOGGER.info(&quot;jobCSP4275 Started executing..&quot;);
            }
            @Override
            public void afterJob(JobExecution jobExecution) {
                LOGGER.info(&quot;jobCSP4275 finished successfully..Exiting job !!&quot;);
              
                System.exit(0);
            }
        };
        return jobExecutionListener;
    }
</code></pre>",,0,5,,2021-4-28 20:10:25,,2021-4-29 12:04:56,2021-4-29 12:04:56,,4418.0,,982747.0,,1,1,spring-batch|log4j2|splunk,70,7
966,259422,67320368,What may cause Jenkins to skip/truncate some lines in console output?,"<p>Our Jenkins instances have 'Splunk App for Jenkins' installed which lets us send all console output from any given job to our Splunk Enterprise app. However, I've noticed that there seem to be more events in Splunk than there are in the Jenkins console output. For example <a href=""https://i.stack.imgur.com/2r7zH.png"" rel=""nofollow noreferrer"">here's</a> part of a result from the Splunk search:</p>
<pre><code>index=jenkins_console sourcetype=&quot;text:jenkins&quot; earliest=-5d@d latest=now source=source_of_job_I_Want
</code></pre>
<p>| stats values(_raw) by _time</p>
<p>And then <a href=""https://i.stack.imgur.com/xmNu9.png"" rel=""nofollow noreferrer"">here's</a> the corresponding console output in Jenkins.</p>
<p>As you can see, the console output is missing a lot of lines that are present in Splunk. It's an issue because we have an alert for the &quot;<code>Cannot contact X: java.lang.InterruptedException</code>&quot; String to help detect agent failures, but it's confusing when that alert goes off but then we look at the console and don't see that message anywhere.</p>
<p>I've checked the logs on our Jenkins server directly with PuTTY and found they match the console output. I've also checked the plugin's configuration in Jenkins and it seems to be the default settings:</p>
<p><a href=""https://i.stack.imgur.com/THigO.jpg"" rel=""nofollow noreferrer"">One</a></p>
<p><a href=""https://i.stack.imgur.com/Cye2M.png"" rel=""nofollow noreferrer"">Two</a></p>
<p><a href=""https://i.stack.imgur.com/epwNW.png"" rel=""nofollow noreferrer"">Three</a></p>
<p>So I suspect this is an issue with Jenkins more than Splunk or the plugin. Like the missing actions did occur but Jenkins is skipping outputting them to console for some reason. I was hoping someone else has come across this and could help me find a cause.</p>",,0,1,,2021-4-29 15:22:15,,2021-5-3 18:09:53,2021-5-3 18:09:53,,4418.0,,15794402.0,,1,0,jenkins|console|jenkins-plugins|console.log|splunk,65,7
967,259423,67330304,"Splunk ""stats"" command considers only a subset of the items","<p>When I do:</p>
<pre><code>index=&quot;...&quot; | search &quot;needle:&quot; 
| rex &quot;expected field: *(?&lt;field1&gt;[A-Z]*) date: *(?&lt;adate&gt;[0-9][0-9]/[0-9][0-9]/[0-9]{4})&quot;
</code></pre>
<p>it correctly shows 10K+ logs, while when I try the aggregation as follows:</p>
<pre><code>index=&quot;...&quot; | search &quot;needle:&quot; 
| rex &quot;expected field: *(?&lt;field1&gt;[A-Z]*) date: *(?&lt;adate&gt;[0-9][0-9]/[0-9][0-9]/[0-9]{4})&quot; 
| stats count(adate) by field1
</code></pre>
<p>it takes only one instance (or a few instances) of the <code>field1</code> items, like if all the other field1 instances don't count in the stats.</p>
<p>Any idea on what I'm doing wrong?</p>",,0,3,,2021-4-30 08:01:44,,2021-5-1 15:40:36,2021-5-1 15:40:36,,2200058.0,,2200058.0,,1,0,logging|splunk,27,5
968,259424,67333989,Is it possible to alter the default weight used for a Line Chart in Splunk?,"<p>I'm a newby to Splunk, and just wondering if it's possible to alter the default line weight used in a line chart via the standard Search and Visualisations tools?</p>",,0,0,,2021-4-30 12:32:56,,2021-5-3 18:07:51,2021-5-3 18:07:51,,4418.0,,14873096.0,,1,0,splunk,11,4
969,259425,67385554,Configure Splunk in Springboot and Docker,"<p>I am a bit confused on the solutions found online. I need to send the application logs already written to a file to a Splunk server hosted in my organisation. How do I do it?</p>
<p>From what I learnt so far - I can use the <strong>Splunk Universal Forwarder</strong> to achieve this. Would it be correct to run a Splunk universal forwarder Docker instance pointing to Splunk hosted server? Please confirm. If yes, how do I pass the host/port to the container? Is it passed using the below env param?</p>
<ul>
<li><em>SPLUNK_STANDALONE_URL</em>=http://splunk.host.com:9100</li>
</ul>
<p>These are just my observations and assumptions. Please correct me if I am wrong OR please suggest if there is a better way to achieve this.</p>
<p>All the examples suggest to run a Splunk and a universal forwarder containers. Nowhere I could find an example on how to connect to an external Splunk server. Hence any examples, snippets would be very helpful.</p>",,1,0,,2021-5-4 13:16:51,,2021-5-5 14:24:05,2021-5-5 14:24:05,,4418.0,,12987334.0,,1,0,spring-boot|docker|logging|splunk,262,9
970,259426,67389257,Splunk results grouping,"<p>i wanted to group results based on project name in splunk and make it as pie chart. Need some help on this?</p>
<p>The results have to be made into a pie chart so it can have only 2 columns!!</p>
<p><img src=""https://i.stack.imgur.com/3eRmW.png"" alt="""" /></p>
<p>The sample of my results</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Job</th>
<th style=""text-align: center;"">Project</th>
<th style=""text-align: center;"">Duration</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Private</td>
<td style=""text-align: center;"">A</td>
<td style=""text-align: center;"">121</td>
</tr>
<tr>
<td style=""text-align: left;"">Continuos</td>
<td style=""text-align: center;"">B</td>
<td style=""text-align: center;"">232</td>
</tr>
<tr>
<td style=""text-align: left;"">Private</td>
<td style=""text-align: center;"">B</td>
<td style=""text-align: center;"">213</td>
</tr>
<tr>
<td style=""text-align: left;"">Private</td>
<td style=""text-align: center;"">B</td>
<td style=""text-align: center;"">323</td>
</tr>
</tbody>
</table>
</div>
<p>I want to make a pie chart with Job and Duration. I want the Jobs to be grouped by project. i.e 3rd and 4th entry should be as 1 section in the pie chart</p>",,0,3,,2021-5-4 17:14:55,,2021-5-6 04:40:17,2021-5-6 04:40:17,,2490883.0,,15426904.0,,1,0,splunk|splunk-query,43,6
971,259427,67394962,Extract custom field in Splunk from specific events,"<p>I want to extract the kind of error and store it in the field <code>error_type</code> for each event.</p>
<p>I have three kinds of errors majorly occurring in my logs within different events.</p>
<p>I want that <code>error_type</code> should populate only the error that particular event has.</p>
<p>I tried extracting the field from the Splunk logs but I am unable to add a regex or regular expression with OR field for the error types.</p>
<p>Also, I want that if the error apart from A, B or C is present in any other event should not populate the <code>error_type</code> field in the event. Is this possible??</p>",,1,3,,2021-5-5 03:50:12,,2021-5-6 12:18:04,2021-5-5 12:56:26,,4418.0,,9186499.0,,1,-1,amazon-web-services|splunk|splunk-query|alerts,213,9
972,259428,67410390,How to pass time token on an eval for Splunk Dashboard,"<p>Need some help with learning how to set a token for time in a dashboard in eval that will populate the date user selects. I have set my time token already and would like to pass my time token to | eval EndingSpiral=strpTime(&quot;$tokEarliest$&quot;, &quot;%Y-%m-%d %H:%M:%S&quot;), but the dates are not populating. Anyone know what I'm doing wrong or not doing? I would appreciate your help.</p>
<pre><code> This is how I set my time token earlier in the xml code:

&lt;fieldset submitButton=&quot;false&quot;&gt;
    &lt;input type=&quot;time&quot; token=&quot;time_range&quot; searchWhenChanged=&quot;true&quot;&gt;
      &lt;label&gt;Select Time Range&lt;/label&gt;
      &lt;default&gt;
        &lt;earliest&gt;-7d@h&lt;/earliest&gt;
        &lt;latest&gt;now&lt;/latest&gt;
      &lt;/default&gt;
      &lt;change&gt;
      &lt;/change&gt;
    &lt;/input&gt;

&lt;query&gt;
| eval EndingSpiral=strpTime(&quot;$tokEarliest$&quot;, &quot;%Y-%m-%d %H:%M:%S&quot;), Spiraling= strpTime(relevantSprialTime, &quot;%Y-%m-%d %H:%M:%S.%q&quot;)
|  eval Start = strptime(SomeTime,&quot;%Y-%m-%d %H:%M:%S.%q&quot;), End=strptime(SomeOtherTime,&quot;%Y-%m-%d %H:%M:%S.%6N&quot;)



&lt;earliest&gt;$time_range.earliest$&lt;/earliest&gt;
          &lt;latest&gt;$time_range.latest$&lt;/latest&gt;&lt;progress&gt;
      &lt;eval token=&quot;tokEarliest&quot;&gt;strptime($time_range.earliest$,&quot;%Y-%m-%d %H:%M:%S&quot;)&lt;/eval&gt;
      &lt;eval token=&quot;tokLatest&quot;&gt;strptime($time_range.latest$,&quot;%Y-%m-%d %H:%M:%S&quot;)&lt;/eval&gt;
    &lt;/progress&gt;
 
</code></pre>",,1,0,,2021-5-6 00:29:34,,2021-5-6 11:33:50,,,,,7151703.0,,1,0,time|token|splunk,187,9
973,259429,67423645,send metrics to splunk from python,"<p>I have a list of dictionaries containing some metrics, let's say</p>
<pre><code>[{&quot;a&quot;: 1, &quot;b&quot;: 2},
 {&quot;a&quot;: 3, &quot;b&quot;: 4},
 {&quot;a&quot;: 1, &quot;b&quot;: 2}]
</code></pre>
<p>And as the end result I need to send to splunk two metric messages with</p>
<pre><code>value: 2, dimensions: {&quot;a&quot;: 1, &quot;b&quot;: 2}  #just amount of elements with same dim values
value: 1, dimensions: {&quot;a&quot;: 3, &quot;b&quot;: 4}
</code></pre>
<p>Is there a way to send them just as original list of dicts so splunk could calculate everything by itself?</p>",,1,0,,2021-5-6 17:59:58,0.0,2021-5-9 10:36:03,,,,,7263843.0,,1,-1,python|splunk,75,7
974,259430,67424702,Get distinct results (filtered results) of Splunk Query based on a results field/string value,"<p>I have a splunk query something like</p>
<pre><code>index=myIndex* source=&quot;source/path/of/logs/*.log&quot; &quot;Elephant&quot;
</code></pre>
<p>Thus, this brings up about 2,000 results which are JSON responses from one of my APIs that include the world <code>&quot;Elephant&quot;</code>. This is kind of what I want - <em><strong>However</strong></em>, some of these results have duplicate <code>carId</code> fields, and I only want Splunk to show me the unique search results</p>
<p>The Results of Splunk looks something like this:</p>
<pre><code>MyApiRequests {&quot;carId&quot;:3454353435,&quot;make&quot;:&quot;toyota&quot;,&quot;year&quot;:&quot;2015&quot;,&quot;model&quot;:&quot;camry&quot;,&quot;value&quot;:25000.00}
</code></pre>
<p><em><strong>NOW</strong></em>, I just want to filter on the <code>carId</code>'s that are unique. I don't want duplicates. Thus, I would expect the original value of 2,000 results to decrease quite a bit.</p>
<p>Can anyone help me formulate my Splunk Query to achieve this?</p>",,2,0,,2021-5-6 19:20:47,,2021-5-6 20:13:57,2021-5-6 20:13:57,,9796935.0,,9796935.0,,1,3,logging|splunk,2267,18
975,259431,67429221,Link two Splunk Web queries based on results value,"<p>I'm using splunk enterprise web (GUI) and I am filtering through some logs of an API. The API has (2) types of logs - one is <code>&quot;My Api Response&quot;</code>, which logs the API JSON response output, and the other is <code>&quot;My Api Request&quot;</code>, which logs the input JSON. The issue I am investigating is some of my API Request logs contain <code>null</code> values for a key/value pair or field in the log called <code>&quot;carId&quot;:null</code>.</p>
<p>The log in Splunk Web for <strong>both</strong> response/request looks something like this. Both have a &quot;carId&quot; field that I wanted to JOIN or do some kind of match on so I can investigate both request and response logs of the API:
<code>{&quot;carId&quot;:123456789,&quot;make&quot;:&quot;toyota&quot;,&quot;year&quot;:&quot;2015&quot;,&quot;model&quot;:&quot;camry&quot;,&quot;value&quot;:25000.00}</code></p>
<p>I tried doing a few Slunk Queries like:
<code>index=MyIndex* source=&quot;my/source/path/to/app.log&quot; &quot;My Api Request&quot; AND &quot;\&quot;carId\&quot;:null&quot; OR &quot;My Api Response&quot;</code></p>
<p>.... but I can never get both logs. It just gives me 1 or the other no matter what.</p>
<p>I was able to filter out the &quot;carId&quot; field with a query like this:</p>
<pre><code>index=rtm* source=&quot;my/source/path/to/app.log&quot; &quot;My Api Request&quot; &quot;\&quot;carId\&quot;:null,&quot; 
| rex field=_raw &quot;(\&quot;carId\&quot;:)(?&lt;carIdField&gt;[0-9]{1,9})&quot;
| table carIdField
</code></pre>
<p>...but I want to use this extracted carId for my subsearch so I can further filter &quot;My Api Responses&quot; that only <strong>contain</strong> this carId</p>
<p>or is there an easier way? I'm reading the Splunk documentation and StackOverflow posts and Splunk posts, but it's kind of confusing.</p>
<p>As I said above,  I basically want to have both these requests (<code>My Api Request</code> and the <code>My Api Response</code> logs) ordered by Timestamp so I can see the API Requests that only had a <code>null</code> value for carId (I don't care about the ones that had real values, since that is expected) and I want to see the corresponding <code>Api Response</code> that correlates to that request (that would have the same <code>carId</code>). I guess another issue is I can have duplicate <code>My Api Request</code> or <code>My Api Response</code> logs, as it will log each time someone clicks/POSTS to the API, so it would be nice to filter those out, but I suppose for starters it would be nice to get a working query.</p>",,0,2,,2021-5-7 04:57:47,,2021-5-10 14:55:27,2021-5-10 14:55:27,,4418.0,,9796935.0,,1,0,logging|splunk,22,5
976,259432,67434261,Splunk field extraction with square brackets [ ],"<p>I'm trying to parse the below sample using Delimiters, could anyone help with the extraction. Delimiters don't seem to work for this. Can someone help with Regex commands?</p>
<p><code>[2021-05-07T20:54:50.6222+10:00] [BDF] [ERROR:32] [BD99999] [security2] [client_id: 10.10.18.236] [host_id: google.com ] [host_addr: 10.10.05.11] [pid: 5397] [tid: 139783720359680] [user: apaapp] [ecid: 005kRh1ly^x8dpK_yTk3yW0001K80002jb] [rid: 0] [VirtualHost: google:4445] [client 0.10.18.236] ModSecurity: Warning. Pattern match &quot;^[\\\\d.:]+$&quot; at REQUEST_HEADERS:Host. [file &quot;/apps/vbgrt/bdf/Google/Middleware/user_projects/domains/bdf_domain/config/fmwconfig/components/BDF/instances/bcp/crs-rules/REQUEST-920-PROTOCOL-ENFORCEMENT.conf&quot;] [line &quot;735&quot;] [id &quot;920350&quot;] [msg &quot;Host header is a numeric IP address&quot;] [data&quot;10.10.05.11:4445&quot;] [severity &quot;WARNING&quot;] [ver &quot;OWASP_PQR/3.3.0&quot;] [tag &quot;application-multi&quot;] [tag &quot;language-multi&quot;] [tag &quot;platform-multi&quot;] [tag &quot;attack-protocol&quot;] [tag &quot;paranoia-level/1&quot;] [tag &quot;OWASP_PQR&quot;] [tag &quot;capec/1000/210/272&quot;] [tag &quot;PCI/6.5.10&quot;] [hostname &quot;google&quot;] [uri &quot;/&quot;] [unique_id &quot;HTjues090uwmX0Cz1kLVwAAAIw&quot;]</code></p>",,1,3,,2021-5-7 11:35:54,,2021-5-10 14:34:57,2021-5-10 14:34:57,,4418.0,,15862764.0,,1,-2,regex|splunk,125,6
977,259433,67460758,Py Script running in AWS lambda [ERROR] NameError: name 'filecontent' is not defined Traceback (most recent call last):,"<p>I have a Python script running as a lambda function to send data in S3 to Splunk. It can read my data, but cannot send data to Splunk. Can anyone please make any changes to the script attached?</p>
<pre><code>from base64 import b64decode
import io
import os
import urllib
import boto3
from botocore.vendored import requests
import sys

# Define Global Variables
splunk_host = os.environ['splunk_host']
splunk_index = os.environ['splunk_index']
region = os.environ['region']
print(splunk_host,splunk_index)


 
# Disable SSL Warnings
#urllib3.disable_warnings()

def get_object(bucket, object):
    
    # Setup connection with S3
    session = boto3.Session()
    s3 = session.client('s3')
    # Download file
    obj = s3.get_object(Bucket=bucket, Key=object)
    return obj

def lambda_handler(event, context):

    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    session = boto3.Session()
    s3 = session.client('s3')
    # Download file
    obj = s3.get_object(Bucket=bucket, Key=key)
    file_content = obj[&quot;Body&quot;].read().decode('utf-8')
    
    print(&quot;Returned Object: {}&quot;.format(obj))
    print(file_content)
    
    
    for value in filecontent.split('\n'):
        data_json = str(file_content)
        payload = {}
        payload.update({&quot;index&quot;: splunk_index})
        payload.update({&quot;source&quot;: &quot;waf&quot;})
        payload.update({&quot;event&quot;: data_json})
       #Send data to splunk
        send_to_splunk(splunk_host, get_secrets('tropos-splunk')['splunk-hec-token-dev'], payload)
        print(raja)
        
        def get_object(bucket, object):
        s3 = boto3.client('s3')
        obj = s3.get_object(Bucket=bucket, Key=object) 

        return obj
 
#Configure SPLUNK Connection
def send_to_splunk(host, token, logdata):
     url = 'https://' + host + ':8088/services/collector'
     auth_header = {'Authorization': 'Splunk ' + token}
     r = requests.post(url, headers=auth_header, json=logdata, verify=False)
     print(r)
     return r      
 
def get_secrets(secret_id):
    client = boto3.client(service_name='secretsmanager',region_name=region)
    get_secret_value_response = client.get_secret_value(SecretId=secret_id)
    return eval(get_secret_value_response['SecretString'])
    
    
    
</code></pre>
<p>The code is getting to <code>printf(file_content)</code>, and I am getting the below error. Please suggest any changes to the script.</p>
<pre><code>[ERROR] NameError: name 'filecontent' is not defined
Traceback (most recent call last):
  File &quot;/var/task/lambda_function.py&quot;, line 43, in lambda_handler
    for value in filecontent.split('\n'):
</code></pre>",67503672.0,1,0,,2021-5-9 17:49:27,,2021-5-12 12:27:15,2021-5-10 14:39:11,,4418.0,,15858447.0,,1,0,python|amazon-web-services|amazon-s3|aws-lambda|splunk,291,9
978,259434,67495862,Splunk Streamlined search for specific fields only,"<p>I've ran out of GoogleFu, so if anyone can point me in the right direction or a better term or two to Google...  I'm trying to figure out Splunk SPL syntax to search 4 different fields for the same value, any match in the four fields wins, with out searching every field for the <code>TERM(&lt;IP&gt;)</code>.</p>
<pre><code>index=&quot;main&quot; packets_out&gt;0 action=&quot;allowed&quot; TERM(192.168.2.1) 
| fields src_ip, dest_ip, dest_translated_ip, src_translated_ip,packets_out 
| head 10
</code></pre>
<p>These will always be constant: <code>index=&quot;main&quot; packets_out&gt;0 action=&quot;allowed&quot;</code></p>
<p>The IP will be the only variable that will change and I'm trying to make it as simple as possible for others to &quot;open search, change 1 IP, click go&quot;.</p>
<p>This works as is, but once I try to search against prod with 2000 devices.. I'm expecting my query time will not be 1 second anymore, even with using &quot;Fast Mode&quot; search.  I've reduced the 4 second query time to 1. Along with the size of data queried with this already, in my home lab, but I don't think this is going to scale very well.</p>
<p>Is there a better way to do this, besides plugging in 10-20 device names into the query like this?  I would rather not have static device names, so if someone &quot;forgets&quot; to update the query; I'll get blamed for the external IP overlap issue.</p>
<pre><code>index=&quot;main&quot; packets_out&gt;0 action=&quot;allowed&quot; TERM(192.168.2.1) dvc_name=&quot;firewall1&quot; OR dvc_name=&quot;firewall2&quot; &lt;*18&gt;
| fields src_ip, dest_ip, dest_translated_ip, src_translated_ip,packets_out 
| head 10
</code></pre>
<p>Raw log if needed:</p>
<pre><code>Apr  7 23:59:55 192.168.2.1 Apr  7 23:59:55 wall 1,2021/04/07 23:59:54,012801092758,TRAFFIC,end,2560,2021/04/07 23:59:54,192.168.2.189,173.194.219.94,10.10.10.2,173.194.219.94,web_access_out-1,,,quic,vsys1,trust,untrust,ethernet1/8,ethernet1/2,splunk,2021/04/07 23:59:54,2004,1,53384,443,59427,443,0x400050,udp,allow,5528,2350,3178,15,2021/04/07 23:57:53,1,any,0,5261883,0x0,192.168.0.0-192.168.255.255,United States,0,6,9,aged-out,0,0,0,0,,wall,from-policy,,,0,,0,,N/A,0,0,0,0,f863e426-7e87-4999-b5cb-bc6dc38d788f,0,0,,,,,,,,0.0.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,2021-04-07T23:59:55.282-04:00,,
</code></pre>
<p>Thanks,</p>",,1,0,,2021-5-12 00:22:53,,2021-5-12 01:19:19,,,,,6471066.0,,1,0,field|splunk|splunk-query|spl,30,6
979,259435,67525334,Splunk data export using API,"<p>I want to export data from Splunk via rest API, I've been wondering whether there is a good &quot;Splunk export&quot; solution that can help me to send my query output/result to a third part application with the help of rest API</p>
<p>I have created below Splunk query, and now I want to export the output of my below query to third party application on regular interval, I have the API details of that application with me</p>
<p><code>index=main| timechart avg(page)</code></p>
<p>For Example API:</p>
<p><a href=""https://webhook.site/66e9b123-ee72-4621-98bb-4ab23a46d1e8"" rel=""nofollow noreferrer"">https://webhook.site/66e9b123-ee72-4621-98bb-4ab23a46d1e8</a></p>
<p><strong>Happy to clarify more details if required.</strong></p>
<p>Also, I checked Splunk official documentation for this where they suggested one solution to use via CURL command, if I go with curl how can I schedule to run on regular interval:</p>
<p>'''curl -k -u admin:changeme <br />
https://localhost:8089/services/search/jobs/ -d search=&quot;search sourcetype=access_* earliest=-7d&quot;'''</p>",67548376.0,1,0,,2021-5-13 19:48:11,,2021-5-15 15:27:25,,,,,15919467.0,,1,1,splunk|splunk-query,520,12
980,259436,67530392,Splunk to take the second queries result(field) into first query for Percentage Memory of Linux host,"<p>I am a newbie to SplunK.</p>
<p>I am trying to pull the <code>Memory %</code> of my Linux hosts which belong to a particular group called <code>Database_hosts</code>.</p>
<p>I am able to get the <code>Memory %</code> of a particular host if I provide that explicitly as <code>host=&quot;host01.example.com&quot;</code> however, I'm looking to run this query against multiple hosts.</p>
<p>Multiple hosts which belong to <code>Database_hosts</code> group I can extract from the <code>inputlookup cmdb_host.csv</code> in Splunk.</p>
<p>Now, I can extract the hosts from <code>inputlookup cmdb_host.csv</code> where it contains the hosts in <code>name</code> field but I am clueless how to put my second query into my first query ie <code>sourcetype=top pctMEM=* host=&quot;host01.example.com&quot;</code></p>
<p>Both the queries working independently though.</p>
<p>My first query:</p>
<pre><code>sourcetype=top pctMEM=* host=&quot;host01&quot; OR host=&quot;host02&quot;
| multikv 
| dedup host
| rex field=pctMEM &quot;(?&lt;usage&gt;\d+)&quot; 
| where usage&gt; 40
| table  host pctMEM
</code></pre>
<p>Result on run:</p>
<p><a href=""https://i.stack.imgur.com/CfTLH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CfTLH.png"" alt=""enter image description here"" /></a></p>
<p>and this is my second query:</p>
<pre><code>| inputlookup cmdb_host.csv
| search support_group=&quot;Database_hosts&quot; NOT (fqdn IN(&quot;ap*&quot;, &quot;aw*&quot;,&quot;&quot;))
| table name
</code></pre>
<p>Result on run:</p>
<p><a href=""https://i.stack.imgur.com/lGNr1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lGNr1.png"" alt=""enter image description here"" /></a></p>
<p>How I can use my second query output field <code>name</code> into first query's <code>host=</code> field?</p>
<p>Any help will be much appreciated.</p>
<p><strong>EDIT</strong>: just tried but no luck:</p>
<pre><code>sourcetype=top pctMEM=* host=&quot;[inputlookup cmdb_host.csv where support_group=&quot;Database_hosts&quot; | table name] 
| multikv 
| dedup name
| rex field=pctMEM &quot;(?&lt;usage&gt;\d+)&quot; 
| where usage&gt;20
| table  name pctMEM
</code></pre>",67535118.0,1,0,,2021-5-14 07:14:15,,2021-10-9 12:52:43,2021-10-9 12:52:43,,13302.0,,10694247.0,,1,0,splunk|splunk-query,59,9
981,259437,67574669,Combining multiples searches into a trellis layout of single value visualizations,"<p>I have a number of networked devices that I am pulling temperature and humidity data from and ingesting into Splunk. Each device is located in a physical location and most, but not all have two sensors.</p>
<p>The perfect panel for my needs is one that uses Single Value visualizations to show the current temperature from all sensors in all locations. I can create two different panels using the following queries, but I'm trying to figure out if it is possible to combine them to create a single dashboard panel.</p>
<pre><code>index=&quot;climate&quot; | timechart latest(s1_temp) as &quot;S1_Temp&quot; by location
</code></pre>
<p>and</p>
<pre><code>index=&quot;climate&quot; | timechart latest(s2_temp) as &quot;S2_Temp&quot; by location
</code></pre>
<pre><code>&lt;panel&gt;
      &lt;single&gt;
        &lt;search&gt;
          &lt;query&gt;index=&quot;climate&quot; | timechart latest(s1_temp) as &quot;S1_Temp&quot; by location&lt;/query&gt;
          &lt;earliest&gt;-24h@h&lt;/earliest&gt;
          &lt;latest&gt;now&lt;/latest&gt;
          &lt;sampleRatio&gt;1&lt;/sampleRatio&gt;
        &lt;/search&gt;
        &lt;option name=&quot;colorBy&quot;&gt;value&lt;/option&gt;
        &lt;option name=&quot;colorMode&quot;&gt;none&lt;/option&gt;
        &lt;option name=&quot;drilldown&quot;&gt;none&lt;/option&gt;
        &lt;option name=&quot;numberPrecision&quot;&gt;0.00&lt;/option&gt;
        &lt;option name=&quot;rangeColors&quot;&gt;[&quot;0xdc4e41&quot;,&quot;0xf8be34&quot;,&quot;0x53a051&quot;,&quot;0xf8be34&quot;,&quot;0xdc4e41&quot;]&lt;/option&gt;
        &lt;option name=&quot;rangeValues&quot;&gt;[67,69,85,87]&lt;/option&gt;
        &lt;option name=&quot;showSparkline&quot;&gt;1&lt;/option&gt;
        &lt;option name=&quot;showTrendIndicator&quot;&gt;1&lt;/option&gt;
        &lt;option name=&quot;trellis.enabled&quot;&gt;1&lt;/option&gt;
        &lt;option name=&quot;trellis.scales.shared&quot;&gt;1&lt;/option&gt;
        &lt;option name=&quot;trellis.size&quot;&gt;medium&lt;/option&gt;
        &lt;option name=&quot;trendColorInterpretation&quot;&gt;standard&lt;/option&gt;
        &lt;option name=&quot;trendDisplayMode&quot;&gt;absolute&lt;/option&gt;
        &lt;option name=&quot;trendInterval&quot;&gt;-24h&lt;/option&gt;
        &lt;option name=&quot;unit&quot;&gt;°&lt;/option&gt;
        &lt;option name=&quot;unitPosition&quot;&gt;after&lt;/option&gt;
        &lt;option name=&quot;useColors&quot;&gt;1&lt;/option&gt;
        &lt;option name=&quot;useThousandSeparators&quot;&gt;1&lt;/option&gt;
      &lt;/single&gt;
    &lt;/panel&gt;
</code></pre>
<p>While I could display the data easily enough using a column chart, the single value with colors and sparkline is perfect for my use case. I've tried a lot of things, but haven't managed to figure it out yet. I figure at this point the answer is either embarrassingly easy or impossible.</p>",,1,0,,2021-5-17 17:48:41,,2021-5-18 11:14:04,,,,,15908926.0,,1,1,splunk|splunk-dashboard,138,8
982,259438,67578091,Splunk query for ignoring results with a regex pattern,"<p><strong>Problem</strong>: I want to ignore all results from search that have <code>message: &lt;4 digits&gt;</code> in them.
For example: <code>{ timestamp: 2021-05-17T22:30:06.299Z, level: error, message: 9173 }</code></p>
<p><strong>Research done</strong>: I have looked into <a href=""https://docs.splunk.com/Documentation/Splunk/8.1.3/Knowledge/AboutSplunkregularexpressions"" rel=""nofollow noreferrer"">Splunk docs</a>
I tried implementing <code>NOT regex &quot;message: \d{4}&quot;</code> and &quot;NOT rex &quot;message: \d{4}&quot; but it did not work.</p>",,0,1,,2021-5-17 23:17:24,,2021-5-19 19:47:03,2021-5-19 19:47:03,,11904363.0,,11904363.0,,1,0,splunk-query,53,6
983,259439,67588434,Extract/filter Splunk Query and for conditional logic,"<p>I use basic Splunk queries mostly, like</p>
<pre><code>index=myIndexHere source=&quot;path/to/logs/app.log&quot; &quot;Keyword to Filter Query On Example&quot;
</code></pre>
<p>My question is, I want to find logs that have a value called <code>&quot;Time taken:&quot;</code>. Ok, that's great - based on what I wrote above, I know how to do that. I get a bunch of search results back in Splunk, that is in a JSON-style (this is logging from my Java Spring Boot application), i.e.</p>
<pre><code>Object state is { &quot;key1&quot;: &quot;value&quot;,
  &quot;key2&quot;: &quot;value&quot;,
  &quot;key3&quot;: &quot;value&quot;
}, Time taken: 500 ms
</code></pre>
<p>So it's some format like that. How can I filter/extract the <code>&quot;Time taken:&quot; VALUE</code> (just the numeric portion) and do a simple logic condition like &quot;<code>&gt; 1000ms</code>&quot; such that I only get search results back that are greater than 1000ms?</p>",67589520.0,1,0,,2021-5-18 14:35:12,,2021-6-7 15:58:36,2021-6-7 15:44:30,,9796935.0,,9796935.0,,1,1,java|spring-boot|splunk,277,10
984,259440,67598264,Listing Splunk saved searches,"<p>This is my first post here. I'm sure this topic is old, but I do not seem to find any solution for my problem.</p>
<p>I'm trying to design a Windows Form application (c#) in which the user can connect to a Splunk instance and perform a few actions, among others getting a list of existing saved searches.</p>
<p>I'm using &quot;Splunk.client&quot; and no issues with connection or executing a search. But no matter what I try, I can't get the list of existing saved searches.</p>
<p>Is this even possible? Any of you guys have any tips.</p>
<p>I appreciate the help.</p>",,2,0,,2021-5-19 06:52:36,,2021-5-19 14:30:18,2021-5-19 10:28:25,,1926806.0,,14536385.0,,1,0,c#|splunk,109,9
985,259441,67602429,Splunk Token for multiple Microservices,"<p>I am researching to integrate splunk with my service running in Docker. In my case, the splunk enterprise runs on a different host.</p>
<p>One way to achieve this is to use docker's built-in splunk logging driver. I see that one of the configuration parameter is <code>&quot;splunk-token&quot;: &quot;&quot;</code> which is the splunk Http Event Collector token that needs to be created in the Splunk enterprise.</p>
<p>My question is - Would I be required to create separate HEC tokens for each of the microservice projects. Let's say, if we have 10 microservices projects running which need to integrate with Splunk. Does that mean I would have to create 10 different tokens in Splunk enterprise?</p>",,1,0,,2021-5-19 11:34:02,,2021-5-19 13:13:07,,,,,12987334.0,,1,0,java|spring|splunk,21,6
986,259442,67606081,Serilog Splunk is not logging messages,"<p>I have a simple console app with the following code, to log into Splunk which is not working. I dont see any logs in the Splunk dashboard. I have masked the url and token.</p>
<pre><code>var url = &quot;https://http-inputs-*****.splunkcloud.com/services/collector&quot;;
var log = new LoggerConfiguration()
    .WriteTo.EventCollector(url, &quot;49A89C45-****-47D2-8A4C-EF89DA*****&quot;, index:&quot;app_crius_services&quot;)
    .CreateLogger();
    

log.Error(JsonConvert.SerializeObject(new ExceptionMessage
    {Index = &quot;app_crius_services&quot;, Event = new Event()}));
log.Debug(JsonConvert.SerializeObject(new ExceptionMessage
    {Index = &quot;app_crius_services&quot;, Event = new Event()}));
log.Information(JsonConvert.SerializeObject(new ExceptionMessage
    {Index = &quot;app_crius_services&quot;, Event = new Event()}));
</code></pre>
<p>I am using the same URL and token to post similar kinds of messages using Postman, and those arrive successfully (I can see them in my Splunk dashboard).</p>
<p>Is there anything that I am doing wrong, or is my logger configuration is incorrect?</p>",,1,0,,2021-5-19 15:18:01,,2021-5-20 19:27:43,2021-5-20 13:03:45,,4418.0,,1505521.0,,1,0,.net-core|splunk|serilog|.net-core-logging,333,10
987,259443,67607159,Forward syslog stream using Splunk Forwarder,"<p>How can I forward syslog stream using Splunk Universal Forwarder?</p>
<p>I have a centos7 system, and I want to forward the stream eg. localx without having to write it to disk.
Currently, I am only able to forward if I write the stream to disk and configure the forwarder to consume the file.</p>",67610100.0,1,1,,2021-5-19 16:25:42,,2021-5-19 20:00:50,,,,,8410648.0,,1,0,splunk,66,9
988,259444,67633979,Splunk search query times out,"<p>Not sure if this has been answered, but I don't seem to find any solution for it.</p>
<p>I’m using the C# Splunk client to connect to a Splunk server and get the result of some searches.</p>
<p>I can get the result of simple searches. That is if I limit the number of &quot;events&quot; using “|head” in my search. If too many  ebtries are sent back, I get a time out in C#.</p>
<p>Also if the result is too complex, it times out.</p>
<p>The job inspector in Splunk itself shows only 9.232 seconds for 30,018 events. I have limited the number of events to be returned too 100 &quot;|head 100&quot;, So do not understand why C# is complaining .Is there any way to suppress the timeout in C#?</p>
<p>I appreciate the help.</p>
<pre><code>Job job = await service.Jobs.CreateAsync(search);
</code></pre>
<p>SearchResultStream stream;
stream = await job.GetSearchResultsAsync(); &lt;&lt; timeout occurs here</p>
<p>System.Threading.Tasks.TaskCanceledException: A task was canceled.
at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task)</p>
<pre><code>(index=index1 OR index=index2) (source=source1 OR source=source2)  (field1=vale OR field2=value)
|eval XXXX
|eval XXX=  case (XXXXX)
|eval helper2 = host + &quot; &quot; + user
|eval helper= host + &quot; &quot; + name
|eval username = case (xxx)
|eval helper4 = if(name=xxxxxx, name,  null)
|eval session_status = case(xxxxxx)
|eval status = case(xxxxxx()
|eval host_status = xxxxt + &quot; &quot; + status 
|head 100
|stats c(yyyy) as xxx by _time xxx
</code></pre>",,0,3,,2021-5-21 09:15:41,,2021-5-21 09:15:41,,,,,14536385.0,,1,0,c#|search|splunk|splunk-sdk,108,8
989,259445,67634409,Splunk bucket name conversion to epoch to human script,"<p>I'm facing the following issue: I have some frozen bucket in a Splunk enviroment that are saved in epoch format. More specifically the template is:</p>
<pre><code>db_1181756465_1162600547_1001
</code></pre>
<p>that, if converted, return to me the end date, which is in the first number, and the start one, that is in the second one. So, it means that, base on my example:</p>
<pre><code>1181756465 = Wednesday 13 June 2007 17:41:05
1162600547 = Saturday 4 November 2006 00:35:47
</code></pre>
<p>Now, ho to convert in human is clear for me, also because if not i coudn't put the translation here. My problem is that I have file full of bucket name that must be converted, with hunderds of entry; so, I'm asking if there is a script or other way to authomatize this conversion and print the output in a file. The idea is to have the final oputput with somehting like that:</p>
<pre><code>db_1181756465_1162600547_1001 = Wednesday 13 June 2007 17:41:05 - Saturday 4 November 2006 00:35:47
</code></pre>",67658794.0,1,1,,2021-5-21 09:40:49,,2021-8-26 12:47:09,2021-8-26 12:47:09,,372239.0,,7188480.0,,1,0,splunk|epoch,56,6
990,259446,67648877,Splunk simulate show source button,"<p>I'm trying to write a script which gets a payload from my splunk enterprise account. I'm using BeautifulSoup for this (splunk-sdk not working for me, maybe due to not having admin access). I'm following the code from here (<a href=""http://www.emalis.com/2018/11/python-script-that-uses-splunk-web-log-to-authenticate-then-searches-splunk-logs-non-admin-user-using-requests/"" rel=""nofollow noreferrer"">http://www.emalis.com/2018/11/python-script-that-uses-splunk-web-log-to-authenticate-then-searches-splunk-logs-non-admin-user-using-requests/</a>).</p>
<p>I tried various search terms, one of those worked for me: <code>post_search_body = {'search': &quot;search &lt;request&gt; request received earliest=-30hours&quot;}</code>. However, I want to go into one of these results' show source button, and get all the results from there, and select some result from there.
Endpoint looks something like this:</p>
<pre><code>https://splunk.splunkcompany.com/en-US/app/search/show_source?sid=&lt;sid&gt;&amp;offset=1&amp;lates
</code></pre>
<p>Can anyone suggest how do we go about this?</p>",67651982.0,1,0,,2021-5-22 11:18:10,,2021-5-22 16:47:35,,,,,9905924.0,,1,0,python|beautifulsoup|splunk,29,7
991,259447,67695517,Splunk query to return list when a process' first step is logged but its last step is not,"<p>Splunk count and grouping question...</p>
<p>I have records for various points in a repeated process. At each point four fields are logged: _time, uniqueId, sessionId, and message.</p>
<p>The uniqueId and sessionId are different from each other, but are consistent each time through the process.</p>
<p>The message has several possible values. But I only care about when it is &quot;firstMessage&quot; or &quot;lastMessage&quot;.</p>
<p>The logs may look something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">_time</th>
<th style=""text-align: left;"">uniqueId</th>
<th style=""text-align: left;"">message</th>
<th style=""text-align: left;"">sessionId</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">a</td>
<td style=""text-align: left;"">12-AB</td>
<td style=""text-align: left;"">firstMessage</td>
<td style=""text-align: left;"">aaa-3</td>
</tr>
<tr>
<td style=""text-align: left;"">b</td>
<td style=""text-align: left;"">12-AB</td>
<td style=""text-align: left;"">otherMessage</td>
<td style=""text-align: left;"">aaa-3</td>
</tr>
<tr>
<td style=""text-align: left;"">c</td>
<td style=""text-align: left;"">12-AB</td>
<td style=""text-align: left;"">lastMessage</td>
<td style=""text-align: left;"">aaa-3</td>
</tr>
<tr>
<td style=""text-align: left;"">d</td>
<td style=""text-align: left;"">28-EA</td>
<td style=""text-align: left;"">firstMessage</td>
<td style=""text-align: left;"">bbb-5</td>
</tr>
<tr>
<td style=""text-align: left;"">e</td>
<td style=""text-align: left;"">28-EA</td>
<td style=""text-align: left;"">otherMessage</td>
<td style=""text-align: left;"">bbb-5</td>
</tr>
<tr>
<td style=""text-align: left;"">f</td>
<td style=""text-align: left;"">33-A7</td>
<td style=""text-align: left;"">firstMessage</td>
<td style=""text-align: left;"">jjj-9</td>
</tr>
<tr>
<td style=""text-align: left;"">g</td>
<td style=""text-align: left;"">33-A7</td>
<td style=""text-align: left;"">otherMessage</td>
<td style=""text-align: left;"">jjj-9</td>
</tr>
<tr>
<td style=""text-align: left;"">h</td>
<td style=""text-align: left;"">71-3C</td>
<td style=""text-align: left;"">firstMessage</td>
<td style=""text-align: left;"">uuu-3</td>
</tr>
<tr>
<td style=""text-align: left;"">i</td>
<td style=""text-align: left;"">71-3C</td>
<td style=""text-align: left;"">otherMessage</td>
<td style=""text-align: left;"">uuu-3</td>
</tr>
<tr>
<td style=""text-align: left;"">j</td>
<td style=""text-align: left;"">71-3C</td>
<td style=""text-align: left;"">lastMessage</td>
<td style=""text-align: left;"">uuu-3</td>
</tr>
</tbody>
</table>
</div>
<p>I want a table showing all fields when the firstMessage for a uniqueId is recorded, but the lastMessage for that uniqueId is not. That is, the process starts but does not end successfully.</p>
<p>The output I would like, given the above, would be:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">_time</th>
<th style=""text-align: left;"">uniqueId</th>
<th style=""text-align: left;"">message</th>
<th style=""text-align: left;"">sessionId</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">d</td>
<td style=""text-align: left;"">28-EA</td>
<td style=""text-align: left;"">firstMessage</td>
<td style=""text-align: left;"">bbb-5</td>
</tr>
<tr>
<td style=""text-align: left;"">f</td>
<td style=""text-align: left;"">33-A7</td>
<td style=""text-align: left;"">firstMessage</td>
<td style=""text-align: left;"">jjj-9</td>
</tr>
</tbody>
</table>
</div>
<p>I tried:</p>
<pre><code>...start of search... message=&quot;firstMessage&quot; OR msg=&quot;lastMessage&quot;
| stats count values(message) by uniqueId
| where count &lt; 2
| fields _time uniqueId message sessionId
</code></pre>
<p>But the time and sessionId columns are blank.</p>
<p>Any and all help is appreciated.</p>
<p><em>Yes, _time is supposed to be your basic Splunk Date/Time. I just wasn't typing those out.</em></p>",67696109.0,1,1,,2021-5-25 20:57:25,,2021-5-25 23:09:56,,,,,16030655.0,,1,0,splunk|splunk-query,34,7
992,259448,67696183,Data Analysis in Splunk,"<p>How do you perform data analysis for Splunk?
I'm told there are no traditional DBs in Splunk - do you install non-relational DBs like mongoDB and configure it to connect to your Splunk instances/environments?</p>",,1,1,,2021-5-25 22:10:43,,2021-5-26 13:00:23,,,,,9969942.0,,1,0,splunk,31,6
993,259449,67704864,Moving Application logs deployed in OpenShift to Splunk,"<p>We have a DotNet Core Application deployed in OpenShift. We have containerized it using Docker and deployed in OpenShift.</p>
<p>From the OpenShift Cluster level a Splunk Connector is setup. There will be specific indexes for object events and logs. The index is set on the connector config, and the Splunk information will be sent to Index specifically for OpenShift related logs.</p>
<p>We are planning to have Custom Index for our Container/Pod
<strong>Question:</strong>
Is there any best practice on how to configure the Log Stream from our application to a specific index that is different from the OpenShift Cluster itself ?</p>
<p>I am guessing there would be some SideCar kind of pattern that will help us, but unable to find a good resource on how to do it.</p>",,0,1,,2021-5-26 12:26:55,1.0,2021-6-1 13:12:33,,,,,804401.0,,1,2,docker|openshift|splunk,41,6
994,259450,67706149,Is it possible to forward raw Security Onion data/logs to splunk (stand-alone) server for visualization?,<p>I am trying to forward raw data collected by security onion to Splunk server installed in stand-alone mode</p>,,1,0,,2021-5-26 13:39:15,,2021-5-26 15:35:10,,,,,16038241.0,,1,-1,splunk|splunk-query|splunk-formula|splunk-dashboard,43,6
995,259451,67706574,Correlating logs with a pattern in Splunk,"<p>I have a wrapper script calling actual script &amp; have 2 log files corresponding to it &amp; i need a way to correlate them and show them in a single splunk search for failed run.</p>
<p>For example, I have a log called wrapper__22238.log inside which i have Status message along with a runID which will be the keyword to correlate with child log : worker.log</p>
<p>Sample wrapper_JOB1_22238.log :</p>
<pre><code>2021-05-25 05:19:59.817 INFO : Got response: Job: JOB1, runID: 1001-751b81bf-9d79-4283-b700-74e0c10c472a, Timeout: 7199, Request Time: 2021-05-25 04:19:03.333, Start Time: 2021-05-25 04:19:04.726, Status: RUNTIME_ERROR, Total Time: 3645869 ms
</code></pre>
<p>worker.log :</p>
<pre><code>[Date=25/May/2021 04:19:06] [THREAD=61] [runID=1001-751b81bf-9d79-4283-b700-74e0c10c472a] [STEP=Write to Target] [status=ERROR] [Error copying the file to the destination /app/logis/internal/: /app/logis/internal/carrier_1456.out.gz (No such file or directory)]
</code></pre>
<p>Is there someway to retrieve both logs by just searching based on Job Name(JOB1 above) ?
Assuming I will always have <code>runID:</code> in the wrapper status call and worker log will have that tagged in every transaction.</p>
<p>Thank you for any assistance.</p>",67715586.0,2,0,,2021-5-26 14:03:16,,2021-5-27 04:15:20,,,,,6808782.0,,1,0,splunk|splunk-query,30,6
996,259452,67763565,"If I run through Oozie (downloading data from splunk) getting Main class [org.apache.oozie.action.hadoop.ShellMain], exit code [1]","<p>I am running a curl command through a shell script and Oozie
The curl command download data from splunk. Locally working fine , but not working through Oozie shell action</p>
<p>Getting below error on oozie console
Main class [org.apache.oozie.action.hadoop.ShellMain], exit code [1]</p>
<p>llap_data_usage.sh
curl --silent -k -u 'api_user_analytics:password' <a href=""https://splunk-operations.ocset.net:8089/servicesNS/api_user_analytics/SHC_app_Analytics/search/jobs/export?output_mode=csv"" rel=""nofollow noreferrer"">https://splunk-operations.ocset.net:8089/servicesNS/api_user_analytics/SHC_app_Analytics/search/jobs/export?output_mode=csv</a>  -d search=&quot;savedsearch alation_llap_use&quot; &gt; llap_usage_data_from_splunk.csv</p>
<p>job.properties</p>
<p>user.name=svc-tdi-prod</p>
<p>UserPrincipal=svc-tdi-prod@XXX</p>
<p>hiveMetastore=thrift://xxx:9083,thrift://xxx:9083</p>
<p>jobtracker=yarn-cluster</p>
<p>namenode=hdfs://nnproxies</p>
<p>yarnQueueName=apps</p>
<p>oozie.wf.application.path=hdfs://nnproxies/user/svc-tdi-prod/LLAP_Usage_Workflow.xml</p>
<p>oozie.use.system.libpath=true</p>
<p>spark_submit_script=llap_usage_data.sh</p>",,0,0,,2021-5-30 16:40:22,,2021-5-30 16:40:22,,,,,14869353.0,,1,0,hadoop|bigdata|oozie|splunk,12,4
997,259453,67830791,How to display table of top 5 URL with their status and percentage on splunk,"<p>Need a table to show the top 5 URL as given below in Splunk. Is this possible in Splunk? I tried many ways but I can't get all status of a URL as a single row.</p>
<pre><code>API                         200        204  400 401 499 500

/wodetails/ACP              895(50%)    -    -   -   -   1
</code></pre>",67857968.0,1,0,,2021-6-4 02:34:57,,2021-6-6 11:14:49,2021-6-5 07:56:57,,10794031.0,,15902340.0,,1,0,splunk|splunk-query|splunk-formula,98,8
998,259454,67836835,"upgrading from ossec to wazuh - ""local/standalone"" mode?","<p>I am currently running ossec 3.6 in local mode and forwarding data to Splunk.  I cannot seem to find something similar in wazuh - am I missing something?  We really don't want to have a manager as all our data goes to Splunk anyway.  We'd like to continue outputting ossec/wazuh data in Splunk format and send straight to Splunk.  I've Googled and read the wazuh docs, but cannot find anything that addresses this. Is this possible?</p>",67867349.0,1,0,,2021-6-4 11:57:13,,2021-6-7 07:19:04,,,,,1309220.0,,1,0,splunk|ossec|wazuh,56,6
999,259455,67844372,How to use splunk-logging nodeJs module with typeScript (node v14),"<p>im trying to make splunk logging tpo worth with my typescript code (nodeJS v14). I found the <a href=""https://github.com/splunk/splunk-javascript-logging"" rel=""nofollow noreferrer"">documentation</a> that mention this example:</p>
<pre><code>var SplunkLogger = require(&quot;splunk-logging&quot;).Logger;

var config = {
    token: &quot;your-token-here&quot;,
    url: &quot;https://splunk.local:8088&quot;
};

var Logger = new SplunkLogger(config);

var payload = {
    // Message can be anything; doesn't have to be an object
    message: {
        temperature: &quot;70F&quot;,
        chickenCount: 500
    }
};

console.log(&quot;Sending payload&quot;, payload);
Logger.send(payload, function(err, resp, body) {
    // If successful, body will be { text: 'Success', code: 0 }
    console.log(&quot;Response from Splunk&quot;, body);
});
</code></pre>
<p>The problem I'm having is that 1) I'm new to JS and Typescript and 2 I can &quot;translate&quot; this to type script where I usually use <code>import {XXXXX} from &quot;XXXXXX&quot;</code> and the use <code>let</code> and <code>const</code>, etc...</p>
<p>Any idea or guide will be most appreciated.</p>",69079053.0,1,0,,2021-6-4 21:51:52,,2021-9-6 18:43:30,,,,,1269916.0,,1,0,javascript|node.js|typescript|splunk,208,9
1000,259456,67868525,How to simplify Splunk search with duplicated query statements,"<p>I wrote the following Splunk search that I plan to use for an alert:</p>
<pre><code> index=* earliest=-1h source=&quot;*video-bic*.log&quot; &quot;Get video&quot; |
 dedup videoid | stats list(videoid) as l_videoid | 
 appendcols [search index=* earliest=-1h source=&quot;*video-bic*.log&quot; &quot;Get video&quot; |
 dedup videoid | stats count(videoid) as num_msgs] | 
 table num_msgs, l_videoid
</code></pre>
<p>It works but I would like to simplify it, so that I don't have to repeat this part of the query twice:</p>
<pre><code> index=* earliest=-1h source=&quot;*video-bic*.log&quot; &quot;Get video&quot;
</code></pre>
<p>Any ideas?</p>",,1,0,,2021-6-7 08:45:53,,2021-6-7 11:11:38,2021-6-7 09:47:45,,1883212.0,,1883212.0,,1,0,splunk,17,5
1001,259457,67874897,"Subsearch produced 221180 results, truncating to maxout 10000","<p>i have 221180 ips in csv(deattackerv1.csv)  with only one field &quot;ip&quot; .. where i want to check if we have any hit in splunk for that ip's in given one index .. How can we achieve that ..</p>
<p>below is the query .. but i am getting error as &quot;Subsearch produced 221180 results, truncating to maxout 10000.&quot;</p>
<p>index=*_abc  | search [| inputlookup deattackerv1.csv | table ip | rename ip as src_ip] | stats count by src_ip,index</p>
<p>Note:- src_ip is one column in index=*_abc .</p>
<p>I have tried below as well ..</p>
<p>index=*_abc
| stats count as eventcount by src_ip,index
| append [| inputlookup deattackerv1.csv | table ip | rename ip as src_ip]
| eventstats values(index) as indexes
| eval index=if(isnull(index),indexes,index)
| table eventcount src_ip index
| mvexpand index
| stats count as sourcecount values(eventcount) as eventcount by src_ip index
| where sourcecount &gt; 1
| table src_ip eventcount index</p>
<p>but getting error as
INFO MESSAGES:
[subsearch]: Subsearch produced 221180 results, truncating to maxout 50000.</p>",,1,0,,2021-6-7 15:56:07,,2021-6-7 21:02:39,2021-6-7 18:12:38,,14843828.0,,14843828.0,,1,0,splunk|splunk-query|splunk-calculation,78,7
1002,259458,67875469,Set difference of a table field in Splunk,"<p>From a search I composed a table, let's call it <strong>T1</strong>, formed by two columns <code>table name, sourcetype</code></p>
<p>Now I need to create a static, code generated table, call it <strong>T2</strong>, that contains all the expected values for the above mentioned table <strong>T1</strong>, hardcoded.
1st question: How could I?</p>
<p>2nd question:
As a result, I need to generate a table <strong>T3</strong> equal to: <code>T2 - T1</code>, basically a logical set difference of the first field, which answer the business question &quot;I want to know which records are missing in T1 based on T2&quot;</p>
<p>I am a newbie of Splunk and its query language and I tried to play a bit with <code>set diff</code> and <code>eval</code> to create static data but I did not manage to create the logic I want at all.</p>
<p>Could you point me to the correct logical implementation of this task?</p>
<p>I do script fluently in both SQL and Python, is there any kind of concept I could reuse to become more familiar with this query language?</p>
<p>Stupid graphical example:</p>
<h2>T1</h2>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>sourcetype</th>
</tr>
</thead>
<tbody>
<tr>
<td>service_1</td>
<td>acpt</td>
</tr>
</tbody>
</table>
</div><h2>T2</h2>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>sourcetype</th>
</tr>
</thead>
<tbody>
<tr>
<td>service_1</td>
<td>acpt</td>
</tr>
<tr>
<td>service_2</td>
<td>acpt</td>
</tr>
</tbody>
</table>
</div><h2>T3</h2>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>sourcetype</th>
</tr>
</thead>
<tbody>
<tr>
<td>service_2</td>
<td>acpt</td>
</tr>
</tbody>
</table>
</div>",67886412.0,2,5,,2021-6-7 16:33:05,,2021-6-9 04:24:19,2021-6-7 18:12:44,,6421394.0,,6421394.0,,1,0,splunk|splunk-query,337,12
1003,259459,67890218,Accessing Splunk Cloud Rest API,"<p>Has anyone had any experience accessing custom rest APIs in Splunk Cloud?</p>
<p>I have created a custom API with a python handler but have not been able to test this in Splunk Cloud as the free development environment does not allow this functionality.</p>
<p>Locally (in Splunk Enterprise) the endpoint is available at https://localhost:8089/services/[endpointName], should this be the case when uploading to splunk cloud with localhost replaced with the domain that we are currently using?</p>",,1,0,,2021-6-8 15:37:53,,2021-6-8 18:33:13,,,,,16166273.0,,1,0,splunk,83,7
1004,259460,67901839,How can I ingest into Kafka text files that were created for splunk?,"<p>I'm evaluating the use of apache-kafka to ingest existing text files and after reading articles, connectors documentation, etc, I still don't know if there is an easy way to ingest the data or if it would require transformation or custom programming.</p>
<p>The background:</p>
<p>We have a legacy java application (website/ecommerce). In the past, there was a splunk server to do several analytics.</p>
<p>The splunk server is gone, but we still generate the log files used to ingest the data into splunk.</p>
<p>The data was ingested to Splunk using splunk-forwarders; the forwarders read log files with the following format:</p>
<pre><code>date=&quot;long date/time format&quot; type=&quot;[:digit:]&quot; data1=&quot;value 1&quot; data2=&quot;value 2&quot; ...
</code></pre>
<p>Each event is a single line. The key &quot;type&quot; defines the event type and the remaining key=value pairs vary with the event type.</p>
<p>Question:</p>
<p>What are my options to use these same files to send data to Apache Kafka?</p>
<p>Edit (2021-06-10): Found out this log format is called Logfmt (<a href=""https://brandur.org/logfmt"" rel=""nofollow noreferrer"">https://brandur.org/logfmt</a>).</p>",67902924.0,1,0,,2021-6-9 09:58:56,,2021-6-10 13:27:00,2021-6-10 13:27:00,,16174029.0,,16174029.0,,1,0,apache-kafka|splunk,40,6
1005,259461,67906599,splunk regex issue,"<p>How can we write regex for below?</p>
<pre><code>CEF:0|Incapsula|SIEMintegration|1|1|Normal|0| fileId=465000430130063349 
</code></pre>
<p>Here I want to extract only <code>0</code> placed between <code>||</code> just before <code>fileId</code>.</p>",,1,0,,2021-6-9 14:50:51,,2021-6-9 15:04:10,2021-6-9 14:55:12,,14419.0,,14843828.0,,1,2,splunk|splunk-calculation,37,8
1006,259462,67910789,curl splunk strftime give Unparsable URI-encoded request data,"<p>When I use eval time=strftime(_time, &quot;%Y-%m-%d&quot;) in curl, I get Unparsable URI-encoded request data, could you please help?</p>",,1,2,,2021-6-9 19:46:33,,2021-6-11 15:06:42,,,,,3033207.0,,1,0,curl|splunk,62,7
1007,259463,67913798,SplunkHttp Appender Error: ERROR Error processing element SplunkHttp ([Appenders: null]): CLASS_NOT_FOUND,"<p>Suddenly from 31/May/2021- we are getting this error</p>
<blockquote>
<p>ERROR Error processing element SplunkHttp ([Appenders: null]): CLASS_NOT_FOUND</p>
</blockquote>
<p>We are using log4j2 configuration file for sending logs to Splunk.</p>
<p>Full file below: Intentionally hiding host, port, token, env details.</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;Configuration status=&quot;INFO&quot; name=&quot;cloudhub&quot;
    packages=&quot;com.mulesoft.ch.logging.appender,com.splunk.logging,org.apache.logging.log4j,com.mule.support&quot;&gt;
        &lt;Appenders&gt;
                &lt;SplunkHttp name=&quot;SPLUNK-ONPREM&quot; source=&quot;app-dev&quot;
                        url=&quot;host:port&quot; host=&quot;dummy-host&quot;
                        token=&quot;token -value&quot; index=&quot;main&quot;
                        disableCertificateValidation=&quot;true&quot;&gt;
                        &lt;PatternLayout pattern=&quot;%-5p %d [%t] [event: %X{correlationId}] %c: %m%n&quot;&gt;&lt;/PatternLayout&gt;
        &lt;/SplunkHttp&gt;
        &lt;/Appenders&gt;
        &lt;Loggers&gt;
                &lt;AsyncLogger
                        name=&quot;org.mule.runtime.core.internal.processor.LoggerMessageProcessor&quot;
                        level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;com.mulesoft.agent&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncRoot level=&quot;INFO&quot;&gt;
                        &lt;AppenderRef ref=&quot;SPLUNK-ONPREM&quot; /&gt;
                &lt;/AsyncRoot&gt;
                &lt;AsyncLogger name=&quot;com.gigaspaces&quot; level=&quot;ERROR&quot; /&gt;
                &lt;AsyncLogger name=&quot;com.j_spaces&quot; level=&quot;ERROR&quot; /&gt;
                &lt;AsyncLogger name=&quot;com.sun.jini&quot; level=&quot;ERROR&quot; /&gt;
                &lt;AsyncLogger name=&quot;net.jini&quot; level=&quot;ERROR&quot; /&gt;
                &lt;AsyncLogger name=&quot;org.apache&quot; level=&quot;WARN&quot; /&gt;
                &lt;AsyncLogger name=&quot;org.apache.cxf&quot; level=&quot;WARN&quot; /&gt;
                &lt;AsyncLogger name=&quot;org.springframework.beans.factory&quot;
                        level=&quot;WARN&quot; /&gt;

                &lt;AsyncLogger name=&quot;org.mule&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;com.mulesoft&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;org.jetel&quot; level=&quot;WARN&quot; /&gt;
                &lt;AsyncLogger name=&quot;Tracking&quot; level=&quot;WARN&quot; /&gt;
                &lt;AsyncLogger name=&quot;org.mule&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;com.mulesoft&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;org.mule.extensions.jms&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;org.mule.service.http.impl.service.HttpMessageLogger&quot;
                        level=&quot;INFO&quot; /&gt;

                &lt;AsyncLogger name=&quot;org.mule.extension.salesforce&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;org.mule.extension.ftp&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;org.mule.extension.sftp&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;com.mulesoft.extension.ftps&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;org.mule.modules.sap&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;com.mulesoft.extension.mq&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;com.mulesoft.mq&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;org.mule.extension.db&quot; level=&quot;INFO&quot; /&gt;
                &lt;AsyncLogger name=&quot;httpclient.wire&quot; level=&quot;DEBUG&quot; /&gt;
                &lt;AsyncLogger name=&quot;org.mule.transport.email&quot; level=&quot;DEBUG&quot; /&gt;

        &lt;/Loggers&gt;
&lt;/Configuration&gt;
</code></pre>",,0,4,,2021-6-10 02:22:17,,2021-6-10 07:12:38,2021-6-10 07:12:38,,13447.0,,3069970.0,,1,0,splunk|mulesoft|appender,136,8
1008,259464,67914742,How to count text that are replaced by rex commands as one in Splunk,"<p>I have a Splunk Query to fetch top 5 API based on error percent. Below is the query for it</p>
<pre><code>index=myaccount sourcetype=myaccountweb-master Response status=* url=* |  
 chart count over url by status | addtotals 
 | foreach * [ 
 | eval &lt;&lt;FIELD&gt;&gt; = if('&lt;&lt;FIELD&gt;&gt;'==0,&quot;-&quot;,'&lt;&lt;FIELD&gt;&gt;') 
 | eval p_&lt;&lt;MATCHSTR&gt;&gt; = 
 if(isnull(tonumber('&lt;&lt;FIELD&gt;&gt;')),'&lt;&lt;FIELD&gt;&gt;',round(('&lt;&lt;FIELD&gt;&gt;'/Total)*100,2)) 
| eval p_&lt;&lt;MATCHSTR&gt;&gt; = if('p_&lt;&lt;MATCHSTR&gt;&gt;'&lt;1, &quot;&lt; 1&quot;,'p_&lt;&lt;MATCHSTR&gt;&gt;') 
| eval &lt;&lt;FIELD&gt;&gt; = if(&quot;&lt;&lt;FIELD&gt;&gt;&quot;==&quot;Total&quot;,'&lt;&lt;FIELD&gt;&gt;', case('&lt;&lt;FIELD&gt;&gt;'==&quot;-&quot;,&quot;- 
&quot;,tonumber('&lt;&lt;FIELD&gt;&gt;')&gt;1,'&lt;&lt;FIELD&gt;&gt;'.&quot; (&quot;.p_&lt;&lt;MATCHSTR&gt;&gt;.&quot;%)&quot;,1=1,'&lt;&lt;FIELD&gt;&gt;')) ] 
| fields - p_* | eval url=lower(url) | rex mode=sed field=url 
&quot;s/account\/(\d+)\//account\/me\//&quot; | rex mode=sed field=url 
&quot;s/\d+account.\w+|\d+fm|\d+fs\d+/*/g&quot; | rex mode=sed field=url &quot;s/..:..:..:..:..:../*/&quot; | rex 
mode=sed field=url &quot;s/accounts\?ip=.*/accounts?ip=__/&quot;| rex mode=sed field=url &quot;s/[^\/] 
{30,}/*/g&quot; | rex mode=sed field=url &quot;s/(\d|\.){8,}/*/g&quot;
| rex field=&quot;500&quot; &quot;\d+\s\((?&lt;perc&gt;.*)%\)&quot; | sort - perc | where perc&gt;10 | head 5
</code></pre>
<p>I have URL's where userID comes in between and to replace those userID with * I have used rex commands and it works replacing the userID as *</p>
<p>But the issue is it counts them separately since userID differs for each hit made on the URL. Because of this my top5 API hits output differs.</p>
<p>Eg URL:/account/user/JHWERTYQMNVSJAIP/email where JHWERTYQMNVSJAIP is userID and its replaced by *</p>
<p>I am getting below output for the query</p>
<pre><code>url                     200 201 204 400 401 500
/account/user/*/email   -   -   -   -   -   5 (100.00%)
/account/user/*/email   -   -   -   -   -   4 (100.00%)
/account/user/*/email   -   -   -   -   -   4 (100.00%)
</code></pre>
<p>Whereas all these URLs are actually one and the expected output should be like adding 5+4+4 and displaying once like this</p>
<pre><code>url                     200 201 204 400 401 500
/account/user/*/email   -   -   -   -   -   13 (100.00%)
</code></pre>
<p>Since userID differs for each one, it take count separately. Any help on this would be appreciated. Thanks in advance</p>",67922464.0,1,0,,2021-6-10 04:28:41,,2021-6-11 01:03:42,2021-6-11 01:03:42,,15902340.0,,15902340.0,,1,0,splunk|splunk-query|splunk-formula|splunk-calculation|splunk-dashboard,61,7
1009,259465,67966820,When ever a particular search query matches in splunk i would like to send the search result to a rest api,"<p>I am new to Splunk and its apps.</p>
<p><strong>My Requirement is When Splunk got the particular logging like &quot;Login Success&quot; i would like to send that whole message to a rest API</strong>.</p>
<p>I can use a scheduler to call Splunk-Api and get the result, but i don't want to use any schedulers.</p>
<p>How we can implement this in Splunk itself.</p>",,1,0,,2021-6-14 08:07:29,,2021-6-14 12:28:35,,,,,4469261.0,,1,0,splunk|splunk-query|splunk-calculation|splunk-sdk|splunk-api,32,7
1010,259466,68004444,Remove one column name in Splunk chart,"<p>I'm receiving data involving the status of various prefixed servers, something like the following:</p>
<pre><code>twserv1: UP
twserv2: UP
poserv1: DOWN
poserv2: UNKNOWN
</code></pre>
<p>I want to display this data in a table separated by the prefix and server name like the following:</p>
<pre><code>              TW        PO
serv1         UP        DOWN
serv2         UP        UNKNOWN
</code></pre>
<p>I've got this mostly working with the following command courtesy of <a href=""https://community.splunk.com/t5/Splunk-Search/How-to-place-and-sort-data-in-a-table-for-a-single-event-based/m-p/205757"" rel=""nofollow noreferrer"">this Q&amp;A</a>:</p>
<pre><code>| inputlookup serverStatus
| table twserv* poserv*
| eval temp=1
| untable temp key value
| rex field=key &quot;(?&lt;x&gt;tw|po)(?&lt;y&gt;\w+\d+)&quot;
| chart values(value) over y by x
| rename tw as TW po as PO
</code></pre>
<p>This yields something like the following:</p>
<pre><code>y             TW        PO
serv1         UP        DOWN
serv2         UP        UNKNOWN
</code></pre>
<p>Is there any way to remove just that first column title? I've tried setting it to whitespace, but to no avail.</p>",68005365.0,1,0,,2021-6-16 14:13:55,,2021-6-16 15:06:31,,,,,901083.0,,1,1,data-visualization|splunk,73,8
1011,259467,68022987,IDX directory for splunk,<p>Where to find IDX directory for app while deployment in SPLUNK . I tried to find it in opt/splunk/etc/shcluster/apps</p>,,1,4,,2021-6-17 16:12:34,,2021-6-19 17:00:37,,,,,16134665.0,,1,0,deployment|splunk|idx,25,5
1012,259468,68033443,Splunk automation using Jenkins,"<p>I am working on a project where I have to integrate Splunk, git, and Jenkins together.</p>
<p>What I want to achieve is whenever I make any changes in the dashboard.xml file (exported from Splunk dashboard) and commit the file on git, then the changes should be automatically reflected on Splunk dashboard.</p>
<p>I don't know how to go about it.</p>",,0,0,,2021-6-18 10:20:20,,2021-6-18 12:22:28,2021-6-18 12:22:28,,4418.0,,14331067.0,,1,0,git|jenkins|automation|splunk,22,5
1013,259469,68062185,Url monitoring splunk,<p>I was monitoring 30 urls in splunk through data inputs . Out of which 1 url showing 500 error rest of the urls are working fine . When asked to user they replied it is working fine and URL is also correct what do I need to do</p>,,1,0,,2021-6-21 04:07:07,,2021-6-21 14:19:12,2021-6-21 14:19:12,,4418.0,,16134665.0,,1,-1,monitoring|splunk,79,8
1014,259470,68064044,Data not received using HEC token splunk,"<p>We gave a dev HEC Token to user 3 months back.</p>
<p>It was configured and worked fine sending data.</p>
<p>When the same user was trying to send data now in JSON format by typing it and send it through HEC, they are getting a Success response code, but in Splunk we can't find where the HEC token directs the data.</p>
<p>When we tried to send data manually through my machine I can see data in the index.</p>
<p>What can be done on the user side to troubleshoot and resolve?</p>",,0,0,,2021-6-21 07:35:56,,2021-6-21 14:17:08,2021-6-21 14:17:08,,4418.0,,16134665.0,,1,0,splunk|http-token-authentication,33,6
1015,259471,68074074,dispatch.earliest_time in savedsearches.conf file,"<p>What does <code>dispatch.earliest_time = -15m@m</code> mean in savedsearches.conf file?
I'm confusing what's the exact time for <strong>-15m@m</strong>?
Thanks.</p>",,1,0,,2021-6-21 20:05:48,,2021-6-22 13:10:37,2021-6-22 13:10:37,,4418.0,,15788148.0,,1,1,splunk|splunk-query,169,10
1016,259472,68120826,How can I send the content of the file to HTTP Event Collector in Splunk?,"<p>I am using a script that gives me some data in json format, I want to send this data to splunk.
I can store the output of the script in a file but how can I send it to HTTP Event Collector?</p>
<p>Couple of things I tried but did not work:</p>
<hr />
<pre><code>FILE=&quot;output.json&quot;
file1=&quot;cat answer.txt&quot;
curl -k &quot;https://prd-pxxx.splunkcloud.com:8088/services/collector&quot;  -H &quot;Authorization: Splunk XXXXX&quot;  -d  '{&quot;event&quot;: &quot;$file1&quot;, &quot;sourcetype&quot;: &quot;manual&quot;}'

-----------------------------------------------------------

</code></pre>
<p>curl -k &quot;https://prd-pxxx.splunkcloud.com:8088/services/collector&quot;  -H &quot;Authorization: Splunk XXXXX&quot;  -d  '{&quot;event&quot;: &quot;@output.json&quot;, &quot;sourcetype&quot;: &quot;manual&quot;}'</p>
<hr />
<pre><code>curl -k &quot;https://prd-p-w0gjo.splunkcloud.com:8088/services/collector&quot;  -H &quot;Authorization: Splunk d70b305e-01ef-490d-a6d8-b875d98e689b&quot;   -d '{&quot;sourcetype&quot;:&quot;_json&quot;, &quot;event&quot;: &quot;@output.json&quot;, &quot;source&quot;: &quot;output.json}

-----------------------------------------------------------------

After trying this I understand that it literally sends everything specified in the event section. Is there a way I can send the content of the file or use a variable? 

Thanks in advance!
</code></pre>",,1,0,,2021-6-24 18:13:37,,2021-6-24 20:44:12,,,,,16309222.0,,1,0,http|events|splunk|collectors,42,6
1017,259473,68125305,Splunk API works on Postman but not on PHP,"<p>I have a CakePHP application where I'm calling Splunk API to make a search and obtain its results. It works as a charm when I try it on Postman, but when run by my server it doesn't return results every time. In fact, when I call the API constantly inside a while loop, it does tend to bring results, although sometimes it could take more 2 minutes to return something.</p>
<p>I'm calling Splunk this way:</p>
<pre><code>$querying = array(
                &quot;search&quot; =&gt; &quot;search index=api_sandbox OR index=api_prod token=&quot; .$token,
                &quot;output_mode&quot; =&gt; &quot;raw&quot;
            );
$querying = http_build_query($data);
 try{
        $curl = curl_init();

            curl_setopt_array($curl, array(
            CURLOPT_URL =&gt; 'https://mysplunk:8089/servicesNS/user/environment/search/jobs/export?' . $querying,
            CURLOPT_RETURNTRANSFER =&gt; true,
            CURLOPT_ENCODING =&gt; '',
            CURLOPT_MAXREDIRS =&gt; 10,
            CURLOPT_TIMEOUT =&gt; 0,
            CURLOPT_FOLLOWLOCATION =&gt; true,
            CURLOPT_SSL_VERIFYPEER, false,
            CURLOPT_USERPWD =&gt; 'Basic ' . base64_encode(&quot;user:password&quot;),
            CURLOPT_HTTPHEADER =&gt; array(
                'Authorization: Basic ' . base64_encode(&quot;user:password&quot;)
              ),
            CURLOPT_HTTP_VERSION =&gt; CURL_HTTP_VERSION_1_1,
            CURLOPT_CUSTOMREQUEST =&gt; 'GET',
            ));

            curl_setopt($curl, CURLOPT_SSL_VERIFYPEER, false);
            curl_setopt($curl, CURLOPT_SSL_VERIFYHOST, FALSE);

            $response = curl_exec($curl);


            curl_close($curl);

            return $response;
        }catch (\Throwable $th) {
            Log::error(&quot;callApi ERR: &quot; . $th-&gt;getMessage());
            debug(&quot;Error: &quot; . $th-&gt;getMessage());
            return false;
        }
</code></pre>
<p>The API call is run inside a CakePHP Shell Task, and for the record when executed the Shell Task through console command (php bin/cake.php Queue.Queue runworker) the API does return results without empty results.</p>
<p>Why this strange behavior could be occurring?</p>",,0,0,,2021-6-25 04:06:59,,2021-6-25 04:06:59,,,,,11941232.0,,1,2,php|postman|splunk,61,7
1018,259474,68134704,Log4j Version 1 Custom Throwable (stacktrace) Renderer for removing new line characters,"<p>We are trying to customize Log4j version 1 appenders which generate the output in Json format that there are no new line characters including Stacktrace, i.e., we read these logs in Splunk and need all the data in one line so that Splunk can parse the Json properly. If we end up generating multiline Json, Splunk fails to understand the Json correctly, Splunk dashboards won't work correctly.</p>
<p>Added a ThrowableRender to strip the new line characters inside the stacktrace</p>
<pre><code>class Log4JThrowableRenderer extends ThrowableRender {
 override def doRender(throwable: Throwable) : Array[String] = {
  
   // Kept it simple for now, could have optimized the replace statements
   val defaultRepresentation: Array[String] = new DefaultThrowableRenderer().doRender(throwable)
   val newRepresentation: String = defaultRepresentation.map( l =&gt; {
      l.replace('\n','|').replace('\r', '|')
   }).mkString(&quot;|)
      
   Array(newRepresentation)
}
</code></pre>
<p>Updated the log4j appenders</p>
<pre><code>log4j.throwableRenderer=com.foo.Log4JThrowableRenderer
log4j.appender.fooAppender.layout.ConversionPattern={&quot;timestamp&quot;:&quot;%d{yyyy-MM-dd}&quot;, &quot;stackTrace&quot;:&quot;%throwable&quot; } // simplified
</code></pre>
<p>And we are able to remove the new line characters from Stacktrace in Json output, except that the ending &quot;}&quot; bracket ends up in new line as in</p>
<pre><code>    {&quot;timestamp&quot;: &quot;2020-01-01&quot;, &quot;stackTrace&quot;: &quot;..| ... |... |...| 
&quot;}
</code></pre>
<p>Not sure how to get all in one line, any ideas?</p>",,0,1,,2021-6-25 16:59:07,,2021-6-28 13:19:25,2021-6-28 13:19:25,,185907.0,,185907.0,,1,1,java|scala|log4j|splunk,76,7
1019,259475,68142455,"Exception in thread ""OkHttp Dispatcher"" java.lang.NoSuchMethodError: com.google.gson.JsonParser.parseString(Ljava/lang/String;)","<p>I am using splunk java logging library and application is able to send logs to splunk server. But when we are starting application so we see below logs and those are very annoying because it gets printed again and again.</p>
<pre><code>Exception in thread &quot;OkHttp Dispatcher&quot; java.lang.NoSuchMethodError: com.google.gson.JsonParser.parseString(Ljava/lang/String;)Lcom/google/gson/JsonElement;
    at com.splunk.logging.HttpEventCollectorErrorHandler$ServerErrorException.&lt;init&gt;(HttpEventCollectorErrorHandler.java:56)
    at com.splunk.logging.HttpEventCollectorSender$3.completed(HttpEventCollectorSender.java:308)
    at com.splunk.logging.HttpEventCollectorSender$4.onResponse(HttpEventCollectorSender.java:356)
    at okhttp3.RealCall$AsyncCall.execute(RealCall.java:141)
    at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Exception in thread &quot;OkHttp Dispatcher&quot; java.lang.NoSuchMethodError: com.google.gson.JsonParser.parseString(Ljava/lang/String;)Lcom/google/gson/JsonElement;
    at com.splunk.logging.HttpEventCollectorErrorHandler$ServerErrorException.&lt;init&gt;(HttpEventCollectorErrorHandler.java:56)
    at com.splunk.logging.HttpEventCollectorSender$3.completed(HttpEventCollectorSender.java:308)
    at com.splunk.logging.HttpEventCollectorSender$4.onResponse(HttpEventCollectorSender.java:356)
    at okhttp3.RealCall$AsyncCall.execute(RealCall.java:141)
    at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)to
</code></pre>",,1,3,,2021-6-26 12:39:47,,2021-7-10 06:34:18,2021-6-28 21:28:16,,4418.0,,3846187.0,,1,-1,spring-boot|splunk,224,9
1020,259476,68169998,Splunk - Split a field into multiple fields based on delimiters,"<p>I have the following value in a field which needs to be split into multiple fields,</p>
<p>Classname:</p>
<pre><code>abc.TestAutomation.NNNN.Specs.Prod/NDisableTransactionalAccessUsers.#()::TestAssembly:abc.TestAutomation
</code></pre>
<p>Required output:</p>
<pre><code>Productname : abc.TestAutomation.NNNN.Specs.Prod

Feature name : NDisableTransactionalAccessUsers

Project : TestAssembly:abc.TestAutomation
</code></pre>
<p>I have been trying to extract the values into my fields using REX command, but I am failing.</p>
<pre><code>source=&quot;Reports.csv&quot;  index=&quot;prod_reports_data&quot; sourcetype=&quot;ReportsData&quot;  
| rex &quot;classname(?&lt;Productname&gt;/*)\.(?&lt;Featurename&gt;#*)\.(?&lt;Project&gt;.*)&quot; 
| table classname Productname Featurename Project
</code></pre>
<p>While I execute this command, there are no results.  I am very new to Splunk, can someone guide.</p>
<p>Thanks.</p>",68170164.0,1,0,,2021-6-28 21:01:44,,2021-6-28 21:24:01,2021-6-28 21:14:22,,4418.0,,16120973.0,,1,0,regex|splunk|splunk-query,361,10
1021,259477,68208237,Splunk - round up time or work with it as a string,"<p>I have logs with the following structure (I put sensitive data in &lt;&gt;):</p>
<pre><code>[28/06/2021 09:27:33.955] - &lt;username&gt; - &lt;ip&gt; - 17 - Transaction 2452 Received the applet response 62369 ms

&lt;some other logs here with data I fetch and &lt;projected&gt;&gt;

[28/06/2021 09:27:35.820] - &lt;username&gt; - &lt;ip&gt; - 17 - The project &lt;projectID&gt; has been properly created in &lt;name&gt;.
[28/06/2021 09:27:35.823] - &lt;username&gt; - &lt;ip&gt; - 17 - Transaction 2452 done
[28/06/2021 09:27:35.824] - &lt;username&gt; - &lt;ip&gt; - 17 - Transaction 2452 COMPLETED 2710 ms wait time 62479 ms
</code></pre>
<p>I have to make a dashboard for logs of project creation where I list:</p>
<ol>
<li>the data from 2nd line which I can easily fetch by searching for 'has been properly created', extracting projectId and username</li>
<li>Transaction completion time.</li>
</ol>
<p>The 2) is problematic because:</p>
<ul>
<li>the only common factor is username which is not handy as the user makes many different transaction</li>
<li>logs with similar structure (&quot;Transaction sth sth COMPLETED&quot;) are the same for other transactions</li>
</ul>
<p>So I came to a conclusion that the best way is to find COMPLETED log by the time similarity with previous lines but I have no idea how to do it (have to round up time as times differ by less than a second) and splunk documentation is not helpful on that matter.</p>
<p>This is the search I have right now. It works how I wanted to until I hit the completion time. It seems not to round time whatsoever.</p>
<pre><code>index=&quot;&lt;indexName&gt;&quot; 
| search &quot;has been properly created&quot; 
| rex &quot;project\s(?&lt;projectId&gt;\w+)\s&quot; 
| dedup projected 
| rex &quot;] -\s(?&lt;userName&gt;\w+) -\s&quot; 
| eval timeNew=strptime(_time, &quot;%m/%d/%y&quot;)
| join type=inner userName, timeNew 
    [| search &quot;o_json_prj_result&quot; 
    | rex &quot;] -\s(?&lt;userName&gt;\w+) -\s&quot; 
    | rex &quot;PLW_SYSTEM\&quot;:\&quot;(?&lt;systemName&gt;\w+)\&quot;&quot; 
    | eval timeNew=strptime(_time,&quot;%m/%d/%y&quot;)
    | join type=inner userName, timeNew 
        [| search &quot;*.*.*.*-*-*COMPLETED&quot; 
        | rex &quot;] -\s(?&lt;userName&gt;\w+) -\s&quot; 
        | rex &quot;COMPLETED\s(?&lt;howLong&gt;\d+)\s&quot;] 
    ] 
| table howLong, projectId, userName, _time, timeNew
</code></pre>
<p>I'd be glad for any tips. I described the whole process as I don't know if my idea for time rounding up is the best.</p>",,0,0,,2021-7-1 10:06:44,,2021-7-1 17:04:35,2021-7-1 17:04:35,,4418.0,,10718901.0,,1,0,splunk|splunk-query,41,6
1022,259478,68269132,converting json splunk log to table query,"<p>I have splunk request, which i have to convert into table.</p>
<pre><code>{
    &quot;level&quot;: &quot;INFO&quot;,
    &quot;payload&quot;: {
      &quot;list&quot;: [
        {
          &quot;Code&quot;: &quot;1&quot;,
          &quot;Name&quot;: &quot;TEST&quot;,
          &quot;Status&quot;: &quot;ACTIVE&quot;
        },
        {
          &quot;Code&quot;: &quot;2&quot;,
          &quot;Name&quot;: &quot;TEST2&quot;,
          &quot;Status&quot;: &quot;ACTIVE&quot;
        },
        {
          &quot;Code&quot;: &quot;3&quot;,
          &quot;Name&quot;: &quot;TEST4&quot;,
          &quot;Status&quot;: &quot;INACTIVE&quot;
        }
      ]
    }
}
</code></pre>
<p>I am seeing repetitive records in response. Can someone help me with the code.</p>
<pre><code>Code,Name,Status
1,TEST,ACTIVE
2,TEST1,ACTIVE
3,TEST4,INACTIVE
</code></pre>",,0,0,,2021-7-6 10:46:41,,2021-7-6 19:21:00,2021-7-6 19:21:00,,5866580.0,,5123343.0,,1,0,splunk-query,23,5
1023,259479,68295688,Splunk dashboard multiple drilldowns,"<p>I have 3 panels in a dashboard - area chart ( populatind data from server logs) , bar chart ( populating data from access logs) and a panel deisplaying the search results when we click on the data points in chart. When I click on data points in area chart, the search works fine and updates the panel named DataPanel. However, when I click on data points on bar chart, it messes up all the data ans displays same data multiple times. Below is the xml that I am using currently.</p>
<p>please provide any suggestions to fix the search.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;form theme=""light""&gt;
  &lt;label Dash&lt;/label&gt;
  &lt;fieldset submitButton=""false"" autoRun=""false""&gt;
    &lt;input type=""time"" token=""tkn_search"" searchWhenChanged=""true""&gt;
      &lt;label&gt;Select Time Range&lt;/label&gt;
      &lt;default&gt;
        &lt;earliest&gt;-12h@h&lt;/earliest&gt;
        &lt;latest&gt;now&lt;/latest&gt;
      &lt;/default&gt;
    &lt;/input&gt;
    &lt;input type=""dropdown"" token=""tkn_dom""&gt;
      &lt;label&gt;Domain&lt;/label&gt;
      &lt;fieldForLabel&gt;index&lt;/fieldForLabel&gt;
      &lt;fieldForValue&gt;index&lt;/fieldForValue&gt;
      &lt;search&gt;
        &lt;query&gt;|eventcount summarize=false index=""*_dom"" | dedup index | fields index&lt;/query&gt;
        &lt;earliest&gt;-30d@d&lt;/earliest&gt;
        &lt;latest&gt;now&lt;/latest&gt;
      &lt;/search&gt;
    &lt;/input&gt;
    &lt;input type=""dropdown"" token=""tkn_sourcetype"" searchWhenChanged=""true""&gt;
      &lt;label&gt;Log Source Type&lt;/label&gt;
      &lt;search&gt;
        &lt;query&gt;|metadata type=sourcetypes index=$tkn_dom$ | fields sourcetype&lt;/query&gt;
        &lt;earliest&gt;-24h@h&lt;/earliest&gt;
        &lt;latest&gt;now&lt;/latest&gt;
      &lt;/search&gt;
      &lt;fieldForLabel&gt;sourcetype&lt;/fieldForLabel&gt;
      &lt;fieldForValue&gt;sourcetype&lt;/fieldForValue&gt;
      &lt;default&gt;access_log&lt;/default&gt;
      &lt;initialValue&gt;access_log&lt;/initialValue&gt;
    &lt;/input&gt;
    &lt;input type=""dropdown"" token=""tkn_host"" searchWhenChanged=""true""&gt;
      &lt;label&gt;Host&lt;/label&gt;
      &lt;fieldForLabel&gt;host&lt;/fieldForLabel&gt;
      &lt;fieldForValue&gt;host&lt;/fieldForValue&gt;
      &lt;search&gt;
        &lt;query&gt;| metadata type=hosts index=$tkn_dom$ | fields host&lt;/query&gt;
        &lt;earliest&gt;-30d@d&lt;/earliest&gt;
        &lt;latest&gt;now&lt;/latest&gt;
      &lt;/search&gt;
    &lt;/input&gt;
    &lt;input type=""text"" token=""tkn_search_txt"" searchWhenChanged=""true""&gt;
      &lt;label&gt;Search Text&lt;/label&gt;
      &lt;default&gt;ERROR&lt;/default&gt;
    &lt;/input&gt;
  &lt;/fieldset&gt;
  &lt;row&gt;
    &lt;panel id=""DataChartPanel""&gt;
      &lt;title&gt;Logs TIme Series&lt;/title&gt;
      &lt;chart&gt;
        &lt;search&gt;
          &lt;query&gt;index=$tkn_dom$ host=$tkn_host$ sourcetype=$tkn_sourcetype$ ($tkn_search_txt$) | timechart count&lt;/query&gt;
          &lt;earliest&gt;$tkn_search.earliest$&lt;/earliest&gt;
          &lt;latest&gt;$tkn_search.latest$&lt;/latest&gt;
          &lt;sampleRatio&gt;1&lt;/sampleRatio&gt;
        &lt;/search&gt;
        &lt;option name=""charting.axisLabelsX.majorLabelStyle.overflowMode""&gt;ellipsisNone&lt;/option&gt;
        &lt;option name=""charting.axisLabelsX.majorLabelStyle.rotation""&gt;0&lt;/option&gt;
        &lt;option name=""charting.axisTitleX.text""&gt;Time&lt;/option&gt;
        &lt;option name=""charting.axisTitleX.visibility""&gt;visible&lt;/option&gt;
        &lt;option name=""charting.axisTitleY.text""&gt;Number of Errors&lt;/option&gt;
        &lt;option name=""charting.chart""&gt;area&lt;/option&gt;
        &lt;option name=""charting.drilldown""&gt;all&lt;/option&gt;
        &lt;option name=""refresh.display""&gt;progressbar&lt;/option&gt;
        &lt;option name=""charting.backgroundColor""&gt;#FFFFFF&lt;/option&gt;
        &lt;option name=""charting.fontColor""&gt;#000000&lt;/option&gt;
        &lt;option name=""charting.fieldColors""&gt;{""ERROR"" : 0xF70B0B}&lt;/option&gt;
        &lt;drilldown&gt;
          &lt;eval token=""drilldown.earliest""&gt;$earliest$&lt;/eval&gt;
          &lt;eval token=""drilldown.latest""&gt;$latest$&lt;/eval&gt;
        &lt;/drilldown&gt;
      &lt;/chart&gt;
    &lt;/panel&gt;
    &lt;panel id=""DataChartPanel1""&gt;
      &lt;title&gt;Access Logs&lt;/title&gt;
      &lt;chart&gt;
        &lt;search&gt;
          &lt;query&gt;index=$tkn_dom$ host=$tkn_host$ sourcetype=access | chart count by StatusCode&lt;/query&gt;
          &lt;earliest&gt;$tkn_search.earliest$&lt;/earliest&gt;
          &lt;latest&gt;$tkn_search.latest$&lt;/latest&gt;
          &lt;sampleRatio&gt;1&lt;/sampleRatio&gt;
        &lt;/search&gt;
        &lt;option name=""charting.axisTitleX.visibility""&gt;visible&lt;/option&gt;
        &lt;option name=""charting.axisTitleY.visibility""&gt;visible&lt;/option&gt;
        &lt;option name=""charting.axisTitleY2.visibility""&gt;visible&lt;/option&gt;
        &lt;option name=""charting.chart""&gt;bar&lt;/option&gt;
        &lt;option name=""charting.drilldown""&gt;all&lt;/option&gt;
        &lt;option name=""charting.legend.placement""&gt;right&lt;/option&gt;
        &lt;option name=""height""&gt;175&lt;/option&gt;
        &lt;option name=""refresh.display""&gt;progressbar&lt;/option&gt;
        &lt;drilldown&gt;
          &lt;set token=""tkn_sourcetype""&gt;access&lt;/set&gt;
          &lt;eval token=""drilldown.earliest""&gt;$earliest$&lt;/eval&gt;
          &lt;eval token=""drilldown.latest""&gt;$latest$&lt;/eval&gt;
          &lt;eval token=""tkn_search_txt""&gt;$click.value$&lt;/eval&gt;
        &lt;/drilldown&gt;
      &lt;/chart&gt;
    &lt;/panel&gt;
  &lt;/row&gt;
  &lt;row&gt;
    &lt;panel id=""DataPanel""&gt;
      &lt;event&gt;
        &lt;title&gt;Errors&lt;/title&gt;
        &lt;search&gt;
          &lt;query&gt;index=$tkn_dom$ host=$tkn_host$ sourcetype=$tkn_sourcetype$ ($tkn_search_txt$)&lt;/query&gt;
          &lt;earliest&gt;$drilldown.earliest$&lt;/earliest&gt;
          &lt;latest&gt;$drilldown.latest$&lt;/latest&gt;
        &lt;/search&gt;
        &lt;option name=""list.drilldown""&gt;none&lt;/option&gt;
        &lt;option name=""refresh.display""&gt;progressbar&lt;/option&gt;
        &lt;option name=""rowNumbers""&gt;1&lt;/option&gt;
      &lt;/event&gt;
    &lt;/panel&gt;
  &lt;/row&gt;
&lt;/form&gt;</code></pre>
</div>
</div>
</p>",,0,0,,2021-7-8 04:33:31,,2021-7-8 04:33:31,,,,,10306461.0,,1,0,splunk|splunk-query|splunk-dashboard,25,5
1024,259480,68296624,How to find duplicate log events in Splunk,"<p>I'm trying to query my Splunk logs to find duplicate data, but am unable to find the right query.</p>
<p>Example logs:</p>
<pre><code>{&quot;time&quot;:&quot;2021-07-08 02:16:17.9232&quot;,&quot;level&quot;:&quot;debug&quot;,&quot;message&quot;:&quot;update&quot;,&quot;parameters&quot;:{&quot;id&quot;:[&quot;1&quot;], other params...}}
{&quot;time&quot;:&quot;2021-07-08 02:17:17.9232&quot;,&quot;level&quot;:&quot;debug&quot;,&quot;message&quot;:&quot;update&quot;,&quot;parameters&quot;:{&quot;id&quot;:[&quot;1&quot;], other params...}}
</code></pre>
<p>The duplicate log events would have the same id parameter, but a different timestamp.</p>",68302497.0,1,0,,2021-7-8 06:30:20,0.0,2021-7-8 13:23:21,,,,,2450507.0,,1,0,logging|splunk|splunk-query,259,10
1025,259481,68311945,Splunk custom RUM data with angularjs,"<p>I am trying to send custom RUM data to Splunk. I am referring <a href=""https://github.com/signalfx/splunk-otel-js-web/blob/main/docs/ManualInstrumentation.md"" rel=""nofollow noreferrer"">ManualInstrumentation</a>. As I have front-end in <em>Angularjs</em> so not able to import <em>trace</em> from <em>opentelemetry/api</em>. Is there any <em>.js</em> file for <em>opentelemetry/api</em> like <a href=""https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js"" rel=""nofollow noreferrer"">splunk-otel-web.js</a> so that I can use <em>trace</em> directly.</p>",,1,2,,2021-7-9 06:00:05,,2021-7-29 06:41:44,,,,,4779803.0,,1,-1,angularjs|splunk|open-telemetry,48,6
1026,259482,68312281,Required assistance how to add specifics source file with same string (date) in file name in splunk query,"<p>I am having one requirement where we are getting files every day with the respective date mentioned in the files:</p>
<p>for example the file names are:</p>
<pre><code>test_dev_08_07_2021.json
test_dev_09_07_2021.json
test_prod_08_07_2021.json
test_prod_09_07_2021.json
</code></pre>
<p>Now the requirement we have here is to add the files content which have same dates. The splunk query we are using is below :</p>
<pre><code>eventtype=&quot;metric:sample:example&quot; source=&quot;test_dev_.json&quot; OR source=&quot;test_prod_.json&quot; | stats sum(number_of_car) as &quot;# Total_Car  &quot;, 
sum(Parked_cars) as &quot;# Stopped_Cars&quot;, sum(Buses) as &quot;# Total_Bus&quot;, sum(Parked_buses) as &quot;# Stopped_Buses &quot; by source | addcoltotals
</code></pre>
<p>but there it's getting combined result of all the four file:</p>
<pre><code>source  # Total Car # Stopped Cars  # Total Bus # Stopped  Buses
test_dev_08_07_2021.json    23  21  295 124
test_dev_09_07_2021.json    22  22  297 123
test_prod_08_07_2021.json   2   3   429 66
test_prod_09_07_2021.json   2   3   427 66
                                                                             
                           49   49  1448    379
</code></pre>
<p>What we are trying to achieve only content of file with same date should get added. For example if the date is mentioned 08_07_2021 in test_dev and test_prod then only these two file content should get added and it should show the result and same for the files with date 09_07_2021  as well. We should be getting separate result after the addition of the addition.</p>
<p><strong>Please Note:</strong> Also we will be getting these files every day. hence the date and month range will varies in each file and no way we can't change the file name now</p>
<p>Is there any way can we achieve this task or if someone can help us with the respective splunk query will much help.</p>
<p>Please assist.</p>",,1,0,,2021-7-9 06:37:16,,2021-7-9 13:57:28,2021-7-9 07:09:43,,7511122.0,,7511122.0,,1,0,splunk|splunk-query|splunk-formula|splunk-calculation|splunk-dashboard,20,5
1027,259483,68318007,Connecting to Splunk using python failing with error ConnectionResetError: [Errno 104] Connection reset by peer,"<p>I am trying to connect to Splunk using below python code . But failing with error ConnectionResetError: [Errno 104] Connection reset by peer</p>
<pre><code>import urllib
import httplib2
username = '*********'
password = '******'
baseurl = 'https://xyz.splunkcloud.com:8089'
myhttp = httplib2.Http(disable_ssl_certificate_validation=True)
myhttp.add_credentials(username, password)
servercontent = myhttp.request(baseurl + '/services/auth/login', 'POST', headers={}, body=urllib.parse.urlencode({'username':username, 'password':password}))[1]
</code></pre>
<p>Error message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;splunktest.py&quot;, line 8, in &lt;module&gt;
    servercontent = myhttp.request(baseurl + '/services/auth/login', 'POST', headers={}, body=urllib.parse.urlencode({'username':username, 'password':password}))[1]
  File &quot;/usr/lib/python3/dist-packages/httplib2/__init__.py&quot;, line 1316, in request
    (response, content) = self._request(conn, authority, uri, request_uri, method, body, headers, redirections, cachekey)
  File &quot;/usr/lib/python3/dist-packages/httplib2/__init__.py&quot;, line 1066, in _request
    (response, content) = self._conn_request(conn, request_uri, method, body, headers)
  File &quot;/usr/lib/python3/dist-packages/httplib2/__init__.py&quot;, line 1019, in _conn_request
    response = conn.getresponse()
  File &quot;/usr/lib/python3.8/http/client.py&quot;, line 1322, in getresponse
    response.begin()
  File &quot;/usr/lib/python3.8/http/client.py&quot;, line 303, in begin
    version, status, reason = self._read_status()
  File &quot;/usr/lib/python3.8/http/client.py&quot;, line 264, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), &quot;iso-8859-1&quot;)
  File &quot;/usr/lib/python3.8/socket.py&quot;, line 669, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer
</code></pre>
<p>But I am able to connect to splunk using Curl command below</p>
<pre><code>curl -k -u ********:****** https://xyz.splunkcloud.com:8089/services/messages
</code></pre>
<p>Please let me know fix for this issue</p>",,0,0,,2021-7-9 13:56:27,,2021-7-12 15:44:36,2021-7-12 15:44:36,,4418.0,,16374210.0,,1,2,python-3.x|splunk|splunk-api|splunk-cloud,74,7
1028,259484,68322677,Splunk Alert - exclude IP address from time range only,"<p>I'm trying to to build a Splunk Alert whose aim is to detect if a user account has been used. In this alert, I want to exclude a time range (between 5 to 6 a.m.) every Tuesday and Thursday. During this time, the account is supposed to be used legitimately. During this time frame I also want to exclude the IP address of the server that is using it.</p>
<p>To clarify, let's say that we have a server with the IP <code>10.10.10.5/32</code>. This server uses the account <code>useraccount</code> each Tuesday and Thursday between 05:00 and 06:00. I need the Splunk Alert to search for any usage of the account, even on <code>10.10.10.5</code>, but for the time period above, exclude <code>10.10.10.5</code> from the alert search, if that makes sense.</p>
<p>This is what I have so far, and I haven't been able to figure out a way to also exclude the server's IP address during this time frame.</p>
<pre><code>index=firewall  [search sourcetype=&quot;pan_panorama&quot; AND &quot;useraccount&quot;]
| where NOT ( (date_wday==&quot;tuesday&quot; OR date_wday==&quot;thursday&quot;)  AND NOT (date_hour &gt;= 5 AND date_hour &lt; 6) )
</code></pre>
<p>If I try with this:</p>
<pre><code>index=firewall  [search sourcetype=&quot;pan_panorama&quot; AND &quot;useraccount&quot;]
| where NOT ( (date_wday==&quot;tuesday&quot; OR date_wday==&quot;thursday&quot;)  AND NOT (date_hour &gt;= 5 AND date_hour &lt; 6) AND NOT (&quot;From: 10.10.10.5&quot;) )
</code></pre>
<p>I receive <code>Error in 'where' command: Typechecking failed. 'XOR' only takes boolean arguments.</code></p>
<p>I'm not sure how to proceed from here. How can I build a Splunk Alert search that excludes a time period and an IP address only during that time period?</p>",68323795.0,1,0,,2021-7-9 21:02:46,,2021-7-10 00:31:49,,,,,5872381.0,,1,0,splunk|splunk-query,146,10
1029,259485,68328142,how to set up splunk stand alone environment,"<p>As I want to create stand-alone where heavyforwarder,indexer,search head in single server.
mostly stand-alone used for staging environment, I want to use stand alone for testing few apps.
How to set-up stand-alone for splunk?</p>",,2,1,,2021-7-10 13:25:17,,2021-7-18 08:00:29,2021-7-10 16:58:22,,16257740.0,,16257740.0,,1,0,admin|splunk|splunk-query,78,8
1030,259486,68351471,How to pattern match a list of APIs calls to their API Pattern (or group)?,"<p>I need to do an <strong>analysis on API calls</strong> using logs, like <strong>avg, min, max response time</strong>.</p>
<p><strong>Background</strong></p>
<p>I am using <strong>splunk</strong>, so these API calls get logged in my ILB logs which then I am able to capture and export to a <strong>CSV</strong> using Splunk. So, for an API <code>/data/user/{id}</code>, I might have multiple calls along with their response times, Ex :</p>
<pre><code>/data/users/1443 | 0.5 sec
/data/users/2232 | 0.2 sec
</code></pre>
<p><strong>Challenge :</strong></p>
<p>What would be an easy and optimal way, if let's say the sheet which has these API calls hits data has 10 million records, and I have a list of API patterns, let's say 3000 API Patterns (Ex: <code>/data/user/{id}/address</code>, <code>/data/user/{id}/xyz/{another_id}</code>)</p>
<p><em>Note: <strong>{id}</strong> could be alphanumeric in some APIs.</em></p>
<p><strong>Current Approach</strong></p>
<p>I load this CSV sheet into a table (call it the <code>raw data table</code>, with details of every single API hit on ILB along with it's response time) in a <strong>PostgresSQL db</strong>, and then have a <code>master table</code> with fixed list of APIs and a column in which I replace the path variables with regex patterns, and then I join the master table with the raw data table using a regex based join, and calculate avg, min, max response_time etc.</p>
<p>However, due to using regex join, <em><strong>the query is taking like 5-6 hrs to run</strong></em>.</p>
<p>Could I do something better?</p>
<p><em>Note: <strong>Sample CSV</strong> with few rows of raw data : <a href=""https://pastebin.com/Nx0Hnc9u"" rel=""nofollow noreferrer"">https://pastebin.com/Nx0Hnc9u</a></em></p>
<pre><code>proxy                              method   request_time
/data/users/{id}                    POST    0.046
/server/healthcheck/check/up        GET     0.001
/data/commons/people/multi_upsert   POST    0.141
/store/org/manufacturing/multi_read POST    0.363
/data/users/{id}/homepage/{name}    POST    0.084
/data/view/{name}/pagecount         PUT     0.043

Category 1 (path variable only at the end) :
/data/users/{id}                    POST    0.046

Category 2 (1 or more path variables only in the middle) :
/data/view/{name}/pagecount                         PUT     0.043
/data/view/{name}/details/{type}/pagecount          PUT     0.043

Category 3 (1 or more path variables only in the middle and also at the end) :
/data/users/{id}/homepage/{name}    POST    0.084
/data/users/{id}/homepage/{type}/details/{name} POST    0.084
</code></pre>",,1,22,,2021-7-12 17:15:44,,2021-7-15 23:15:52,2021-7-14 17:47:09,,2533630.0,,2533630.0,,1,1,regex|postgresql|splunk,138,9
1031,259487,68363284,How to use TypeScript based libraries from node_modules into AngularJs-v1.2.20,"<p>I am trying to import <code>trace</code> from <code>@opentelemetry/api</code> into AngularJs controller but the library is TypeScript based so not able to import it into code.</p>
<p>For example, if I have to use <code>validator</code> library (<code>npm i validator</code>) then I can import it using the following code snippent:</p>
<pre><code>&lt;script type=&quot;text/javascript&quot; src=&quot;validator.min.js&quot;&gt;&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  validator.isemail('foo@bar.com');
&lt;/script&gt;
</code></pre>
<p><strong>How can I use <code>trace</code> from <code>@opentelemetry/api</code> in AngularJs-v1.2.20 controller?</strong></p>",,0,2,,2021-7-13 13:23:28,1.0,2021-7-14 13:16:47,2021-7-14 13:16:47,,14046997.0,,14046997.0,,1,0,javascript|angularjs|typescript|splunk|open-telemetry,37,6
1032,259488,68366277,Splunk fields extraction limit for specific Index,<p>In Limits.conf fields is limited to 200 we want 500 fields extraction for specific index  . Tried giving limits to sourcetype of the index but not working . If we give it in limits.conf it will effect every index leading to high storage use . Is there a way to change field extraction for specific index</p>,,1,5,,2021-7-13 16:36:15,,2021-7-14 15:52:36,2021-7-14 14:32:24,,4418.0,,16134665.0,,1,0,indexing|field|limit|splunk-cloud,278,9
1033,259489,68384064,How to extract a field from a Splunk search result and do stats on the value of that field,"<p>I have following search results</p>
<pre><code>2021-07-14 17:12:55,525 INFO [NiFi logging handler] returned 202: response_time:0.029 retry_count:2
</code></pre>
<p>Out of this I would like to extract &quot;response_time&quot; values like this so I can find the average, max, min, etc.</p>
<pre><code>response_time:0.029
</code></pre>
<p>How can I accomplish this?</p>",68384542.0,1,0,,2021-7-14 19:29:21,,2021-7-14 20:14:44,2021-7-14 19:40:54,,4420967.0,,432053.0,,1,1,splunk|splunk-query,225,12
1034,259490,68461022,Regex to pull last 2 segments from FQDN,"<p>Working on trying to figure out some regex to pull out the last 2 segments of an FQDN.</p>
<p><code>^.*\shostname=[\w-]+\.(?P&lt;myfield&gt;[^\t]+)</code></p>
<p>This RegEx works and takes out the first segment of an FQDN.</p>
<p><code>www.aaa.bbb.someurl.net</code> --&gt; <code>aaa.bbb.someurl.net</code></p>
<p>But… I only want to keep the last 2 segments of any FQDN.</p>
<p>I need it to be --&gt; <code>someurl.net</code></p>
<p><em>Other restrictions</em>:<br/>
The hostname field will always be at least 3 segments - don't know the max.</p>
<p>This is for Splunk so I can't use a script. I need it to be PCRE compatible regex.</p>
<p>Here is an example of data:</p>
<pre><code>2021-07-20 18:19:14 reason=Not allowed to browse this category event_id=12345 protocol=HTTP action=Blocked transactionsize=16051 responsesize=789 requestsize=456 urlcategory=Blocked serverip=1.2.4.5 clienttranstime=0 requestmethod=GET refererURL=None useragent=Microsoft-Delivery location=Internal ClientIP=5.6.7.8 status=403 user=John url=dl.delivery.mp.microsoft.com/filestreamingservice/files/abcd-efgh-ijkl/pieceshash vendor=Zscaler hostname=dl.delivery.mp.microsoft.com
</code></pre>
<p>From this I data I need the field “myfield” to be: <code>microsoft.com</code>.</p>",68461091.0,3,1,,2021-7-20 20:37:35,,2021-7-21 14:18:14,2021-7-20 20:45:28,,3832970.0,,9620222.0,,1,4,regex|splunk,61,11
1035,259491,68468069,It is possible to modify already existing report within Splunk dashboard?,"<p>I have already created report in Splunk: &quot;splunk_report&quot;.
I want to use it in Splunk dashboard.</p>
<p>I have already add new panel -&gt; new from report, but now it shows me a table (as it was defined in saved report)
I would like to get the total number of rows in this table. It is possible to make count of current report?
eg. &quot;splunk_report&quot; | stats count(system)</p>
<p>I do not want to copy whole search query, because when search in report change in the future, I have to change it in 2 places.</p>",,2,0,,2021-7-21 10:38:06,,2021-7-22 18:10:48,,,,,6451990.0,,1,0,dashboard|splunk,35,6
1036,259492,68479024,Need to write a regex to extract path for first 5 slashes or up to a number for Splunk,"<p>Hi I need to write a regex to extract path from the first 5 slashes from the path or up to a number
Example:</p>
<pre><code>https://example.com/first/second/third/fourth/fifth/sixth 
https://example.com/first-1/second-1/third-1/
https://example.com/first-1/second-1/third-1
https://example.com/first/12345
</code></pre>
<p>Result:</p>
<pre><code>/first/second/third/fourth/fifth
/first-1/second-1/third-1
/first-1/second-1/third-1
/first
</code></pre>
<p>I am able to strip off the domain by using the regex</p>
<pre><code>http(s)*\:\/\/([^\/]+)\/(?&lt;uri&gt;[^\?\s]+)
</code></pre>
<p>However, I am unable to get the first 5 or up to a numeric value is reached.</p>",,2,0,,2021-7-22 04:01:20,,2021-7-28 07:59:09,2021-7-28 07:59:09,,2747661.0,,5030551.0,,1,1,regex|splunk|splunk-query,62,9
1037,259493,68484585,How to write a Splunk query to count response codes of multiple endpoints,"<p>I'm trying to monitor performance/metrics of my application as an external system is going through a heavy data ingest. Currently, I can easily watch one endpoint using the following</p>
<pre><code>index=my_index environment=prod service=myservice api/myApi1 USER=user1 earliest=07/19/2021:12:00:00 | stats count by RESPONSECODE
</code></pre>
<p>How can I adjust this query to include the additional endpoints I'd like to monitor? Ultimately I'd like a pie chart showing the total numbers of successes and failures across this API for the user.</p>
<p>Thanks all!</p>
<p>Edit: In the above query, api/myApi1 is the field I'm referring to. How can I include additional api/myApi# endpoints properly?</p>",,1,0,,2021-7-22 12:09:32,,2021-7-23 18:07:08,2021-7-23 18:07:08,,5837344.0,,5837344.0,,1,0,api|splunk|splunk-query,33,7
1038,259494,68490905,Matching the left and rights side side of a string using a regex,"<p>In any regex language (PCRE preferred though) is there a way to negate matching a portion of a string in the middle? I've looked at both negative/positive lookahead/behind and those seem to be able to match either the left or right sides but not both at the same time (likely I'm misunderstanding something).</p>
<p>Some example inputs:</p>
<pre><code>a &quot;b&quot; c
d &quot;e&quot; f
</code></pre>
<p>Expected matches:</p>
<pre><code>a  c
d  f
</code></pre>
<p>The strings either the left or right can be any character/symbol, I essentially want to discard anything inside the <code>&quot;&quot;</code>, including the quotes itself.</p>
<hr />
<p>To make this more concrete, I'm trying to parse some error logs we have in Splunk where the names of Kubernetes pods are contains within the <code>&quot;&quot;</code>. Removing the unique pod names would allow me to make a distinct list of error messages affecting the system I'm trying to triage.</p>",68492825.0,2,1,,2021-7-22 20:03:48,,2021-7-23 00:35:05,2021-7-22 20:21:11,,344480.0,,724251.0,,1,1,regex|splunk,39,9
1039,259495,68491826,Figuring out an outage based on splunk logs,"<p>I'm trying to figure out a way where, when there are n number of failures of a particular service based on the splunk logs, I'd like to send in the response of that service going forward for all the calls made that there is an outage.</p>
<p>For example, xyz.com/support is a service that is failing for more than 300 times in an hour, splunk will send me an email saying there are so many failures for this service, now based on this I'd like to modify my response to something like below response until the outage is resolved. This should be automated.</p>
<p>{&quot;status&quot;:&quot;failure&quot;,&quot;level&quot;:&quot;&quot;,&quot;message&quot;:&quot;There is an outage currently&quot;}</p>",,1,0,,2021-7-22 21:43:57,,2021-7-23 02:00:25,,,,,12882317.0,,1,1,java|rest|spring-mvc|error-handling|splunk,23,5
1040,259496,68498208,Remove property from json event Splunk,"<p>I have events JSON events in splunk, but one of the key pair/property I would like to remove.</p>
<p>E.g.:</p>
<p>From below JSON I want to remove &quot;country&quot;: &quot;Algeria&quot;, from every event which will come. It is possible? I have tried something like this in my props.conf, but no success.</p>
<pre><code>[k8s]
INDEXED_EXTRACTIONS=JSON
TRUNCATE = 200000
SEDCMD-remove=/&quot;country&quot;: &quot;.*/g
</code></pre>
<pre><code>
       {
            &quot;random&quot;: 23,
            &quot;random float&quot;: 28.173,
            &quot;bool&quot;: false,
            &quot;date&quot;: &quot;1990-08-31&quot;,
            &quot;regEx&quot;: &quot;helloooooooooooooooooooooooooooooooooooooooooooooooooo world&quot;,
            &quot;enum&quot;: &quot;generator&quot;,
            &quot;firstname&quot;: &quot;Latisha&quot;,
            &quot;lastname&quot;: &quot;Alexandr&quot;,
            &quot;city&quot;: &quot;Tiraspol&quot;,
            &quot;country&quot;: &quot;Algeria&quot;,
            &quot;countryCode&quot;: &quot;MC&quot;,
            &quot;email uses current data&quot;: &quot;Latisha.Alexandr@gmail.com&quot;,
            &quot;email from expression&quot;: &quot;Latisha.Alexandr@yopmail.com&quot;,
            &quot;array&quot;: [
                &quot;Dyann&quot;,
                &quot;Christal&quot;,
                &quot;Renie&quot;,
                &quot;Tilly&quot;,
                &quot;Margette&quot;
            ],
            &quot;array of objects&quot;: [
                {
                    &quot;index&quot;: 0,
                    &quot;index start at 5&quot;: 5
                },
                {
                    &quot;index&quot;: 1,
                    &quot;index start at 5&quot;: 6
                },
                {
                    &quot;index&quot;: 2,
                    &quot;index start at 5&quot;: 7
                }
            ],
            &quot;Raquela&quot;: {
                &quot;age&quot;: 50
            }
        }

</code></pre>",68499743.0,1,0,,2021-7-23 11:07:31,,2021-8-26 03:40:25,2021-8-26 03:40:25,,1161484.0,,10956229.0,,1,2,json|events|filter|splunk,56,8
1041,259497,68542206,Use eval variable for earliest/latest in append search splunk,"<p>I need your help please.</p>
<p>I would to compare 2 graphs in different day with this query :</p>
<pre><code>`mc_stomv2_jboss(&quot;pr&quot;,*,*,&quot;app&quot;)` environnement=&quot;rrr&quot; espace=&quot;toto&quot; slot=&quot;*&quot; canal=&quot;2&quot; message.technicalChannel=api  earliest=-4h@m latest=now
| eval periode_earliest=&quot;-4h@m&quot;
| eval periode_latest=&quot;now&quot;
| eval earliesttime_f=if(like('periode_earliest',&quot;%@%&quot;),relative_time(now(), &quot;-7d@s&quot;.'periode_earliest'), 'periode_earliest'-86400*7)
| eval latesttime_f=if(like('periode_latest',&quot;now&quot;),relative_time(now(), &quot;-7d@s&quot;), 'periode_latest'-86400*7)
| eval key=&quot;Nb_utilsateurs&quot;+&quot;-&quot;+'kubernetes.namespace_name'.&quot;-&quot;.'slot'.&quot;-&quot;.&quot;1&quot;
| append
[
search `mc_stomv2_jboss(&quot;pr&quot;,*,*,&quot;app&quot;)` environnement=&quot;rrr&quot; espace=&quot;toto&quot; slot=&quot;*&quot; canal=&quot;2&quot; message.technicalChannel=api   fr.laposte.disf.fwmc.tech.trace.impl.DefaultPerformanceTrace earliest=$earliesttime_f$ latest=$latesttime_f$
| eval key=&quot;Nb_utilsateurs&quot;+&quot;-&quot;+'kubernetes.namespace_name'.&quot;-&quot;.'slot'.&quot;-&quot;.&quot;2&quot;
| eval _time=_time+86400*7
]
| timechart span=$fieldspan$ dc(user) by key
</code></pre>
<p>How to use values eval (latesttime_f and earliesttime_f) to first search in append search for earliest and latest?</p>
<p>In a dashboard splunk, I use this :</p>
<pre><code>| eval periode_earliest=&quot;$periode.earliest$&quot;
| eval periode_latest=&quot;$periode.latest$&quot;
| eval earliesttime=if(like('periode_earliest',&quot;%@%&quot;),relative_time(now(),&quot;-7d@s&quot;.'periode_earliest'),'periode_earliest'-86400*7)
| eval latesttime=if(like('periode_latest',&quot;now&quot;),relative_time(now(), &quot;-7d@s&quot;),'periode_latest'-86400*7)
</code></pre>
<p>Thanks you for your help.</p>",,0,0,,2021-7-27 09:18:46,,2021-7-27 09:18:46,,,,,16535954.0,,1,0,splunk-query|splunk-dashboard,114,8
1042,259498,68545795,kubernetes audit log filtering with fluentd and forwarding to Splunk,"<p>After some struggling I got fluentd to forward Openshift audit log files to Splunk. However this resulted in a huge number of events, so I applied a filter to exclude &quot;get&quot; and &quot;watch&quot;. I would  like to include the get secrets.</p>
<p>My question, how to change the filter to exclude &quot;get&quot; but include &quot;get secret&quot;?</p>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: splunk-kubernetes-audit
  namespace: splunk-logging
  labels:
    app: splunk-kubernetes-audit
data:
  fluent.conf: |-
    &lt;system&gt;
      log_level info
    &lt;/system&gt;
    @include source.audit.conf
    @include output.conf
  output.conf: |-
    &lt;label @SPLUNK&gt;
      # = filters for non-container log files =
      # extract sourcetype
       &lt;filter tail.file.**&gt;
          @type grep
          &lt;exclude&gt;
             key verb
             pattern /watch/
          &lt;/exclude&gt;
          &lt;and&gt;
            &lt;exclude&gt;
               key verb
               pattern /get/
            &lt;/exclude&gt;
          &lt;/and&gt;
  &lt;/filter&gt;
      &lt;filter tail.file.**&gt;
        @type jq_transformer
        jq '.record.sourcetype = (.tag | ltrimstr(&quot;tail.file.&quot;)) | .record.cluster_name = &quot;opcdev&quot; | .record.splunk_index = &quot;openshift_audit_n&quot; | .record'
      &lt;/filter&gt;
      # = custom filters specified by users =

      # = output =
      &lt;match **&gt;
        @type splunk_hec
        protocol https
        hec_host &quot;splunk-heavyforwarder.linux.rabobank.nl&quot;
        hec_port 8088
        hec_token &quot;#{ENV['SPLUNK_HEC_TOKEN']}&quot;
        index_key splunk_index
        insecure_ssl false
        ca_file /fluentd/etc/splunk/hec_ca_file
        host &quot;#{ENV['K8S_NODE_NAME']}&quot;
        source_key source
        sourcetype_key sourcetype
        &lt;fields&gt;
          # currently CRI does not produce log paths with all the necessary
          # metadata to parse out pod, namespace, container_name, container_id.
          # this may be resolved in the future by this issue: https://github.com/kubernetes/kubernetes/issues/58638#issuecomment-385126031
          pod
          namespace
          container_name
          cluster_name
          container_id
        &lt;/fields&gt;
        app_name splunk-kubernetes-audit
        app_version 1.4.7
        &lt;buffer&gt;
          @type memory
          chunk_limit_records 100000
          chunk_limit_size 10m
          flush_interval 10s
          flush_thread_count 1
          overflow_action block
          retry_max_times 5
          retry_type exponential_backoff
          retry_wait 2
          retry_max_interval 300
          total_limit_size 600m
        &lt;/buffer&gt;
        &lt;format&gt;
          @type &quot;json&quot;
        &lt;/format&gt;
      &lt;/match&gt;
    &lt;/label&gt;
  source.audit.conf: |-
    # This fluentd conf file contains sources for log files other than container logs.
    &lt;source&gt;
      @id tail.file.kube-api-audit
      @type tail
      @label @SPLUNK
      tag tail.file.kube-api-audit
      path /var/log/kube-apiserver/audit.log
      pos_file /var/log/splunk-fluentd-audit-kube-api-audit.pos
      read_from_head true
      path_key source
      &lt;parse&gt;
        @type json
      &lt;/parse&gt;
    &lt;/source&gt;
    &lt;source&gt;
      @id tail.file.oauth-api-audit
      @type tail
      @label @SPLUNK
      tag tail.file.oauth-api-audit
      path /var/log/oauth-apiserver/audit.log
      pos_file /var/log/splunk-fluentd-audit-oauth-api-audit.pos
      read_from_head true
      path_key source
      &lt;parse&gt;
        @type json
      &lt;/parse&gt;
    &lt;/source&gt;
    &lt;source&gt;
      @id tail.file.openshift-api-audit
      @type tail
      @label @SPLUNK
      tag tail.file.openshift-api-audit
      path /var/log/openshift-apiserver/audit.log
      pos_file /var/log/splunk-fluentd-audit-openshift-api-audit.pos
      read_from_head true
      path_key source
      &lt;parse&gt;
        @type json
      &lt;/parse&gt;
    &lt;/source&gt;
</code></pre>
<p>the secrets</p>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  labels:
    app: splunk-kubernetes-audit
  name: splunk-kubernetes-audit
  namespace: splunk-logging
type: Opaque
data:
  hec_ca_file: {{ base64 encoded CA certificate }}
  splunk_hec_token: {{ base64 encoded Get_token_for_index }}
</code></pre>
<p>and the daemonset</p>
<pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  annotations:
    configmap.update: &quot;1&quot;
    deprecated.daemonset.template.generation: &quot;34&quot;
  generation: 34
  labels:
    app: splunk-kubernetes-audit
    engine: fluentd
  name: splunk-kubernetes-audit
  namespace: splunk-logging
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: splunk-kubernetes-audit
      release: rabo-splunk
  template:
    metadata:
      annotations:
        checksum/config: 0574cfe32baa34dcb02d7e3293f7c5ac0379ffb45cf4b7e455eb6975e6102320
        configmap.update.trigger: &quot;1&quot;
        prometheus.io/port: &quot;24231&quot;
        prometheus.io/scrape: &quot;true&quot;
      creationTimestamp: null
      labels:
        app: splunk-kubernetes-audit
        release: rabo-splunk
    spec:
      containers:
      - env:
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: MY_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: SPLUNK_HEC_TOKEN
          valueFrom:
            secretKeyRef:
              key: splunk_hec_token
              name: splunk-kubernetes-audit
        - name: SSL_CERT_FILE
          value: /fluentd/etc/splunk/hec_ca_file
        image: docker.io/splunk/fluentd-hec:1.2.6
        imagePullPolicy: Always
        name: splunk-fluentd-k8s-audit
        ports:
        - containerPort: 24231
          name: metrics
          protocol: TCP
        resources:
          requests:
            cpu: 500m
            memory: 600Mi
        securityContext:
          privileged: true
          runAsUser: 0
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/log
          name: varlog
        - mountPath: /var/log/kube-apiserver
          name: varlogkube
          readOnly: true
        - mountPath: /var/log/oauth-apiserver
          name: varlogoauth
          readOnly: true
        - mountPath: /var/log/openshift-apiserver
          name: varlogopenshift
          readOnly: true
        - mountPath: /fluentd/etc
          name: conf-configmap
        - mountPath: /fluentd/etc/splunk
          name: secrets
          readOnly: true
      dnsPolicy: ClusterFirst
      imagePullSecrets:
      - name: acr-secret
      nodeSelector:
        node-role.kubernetes.io/master: ''
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: splunk-logging
      serviceAccountName: splunk-logging
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      volumes:
      - hostPath:
          path: /var/log
          type: &quot;&quot;
        name: varlog
      - hostPath:
          path: /var/log/kube-apiserver
          type: &quot;&quot;
        name: varlogkube
      - hostPath:
          path: /var/log/oauth-apiserver
          type: &quot;&quot;
        name: varlogoauth
      - hostPath:
          path: /var/log/openshift-apiserver
          type: &quot;&quot;
        name: varlogopenshift
      - configMap:
          defaultMode: 420
          name: splunk-kubernetes-audit
        name: conf-configmap
      - name: secrets
        secret:
          defaultMode: 420
          secretName: splunk-kubernetes-audit
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
</code></pre>",,1,0,,2021-7-27 13:29:30,,2021-8-17 07:00:01,2021-7-30 08:33:40,,4636652.0,,4636652.0,,1,1,kubernetes|openshift|splunk|fluentd,208,10
1043,259499,68550799,How do I pipe the output of a Splunk custom reporting command into another Splunk command?,"<p>I am having an issue with piping the output of a custom reporting command, as documented <a href=""https://dev.splunk.com/enterprise/docs/devtools/customsearchcommands/"" rel=""nofollow noreferrer"">here</a>, into another SPL command. I can get the basic reporting command example to work. It's called &quot;sum&quot; and is provided within the &quot;searchcommands_app&quot; directory of the Splunk Python SDK hosted on <a href=""https://github.com/splunk/splunk-sdk-python/blob/master/examples/searchcommands_app/package/bin/sum.py"" rel=""nofollow noreferrer"">GitHub</a>. However, once I get statistics output from the &quot;sum&quot; command, I cannot pipe those results into another command.</p>
<p>This first query works fine:</p>
<pre><code>index = _internal | head 200 | sum total=lines linecount
</code></pre>
<p>However, this query does not work:</p>
<pre><code>index = _internal | head 400 | sum total=lines linecount | stats count
</code></pre>
<p>When I try to pipe the output of the &quot;sum&quot; command into the &quot;stats&quot; command, I get the following error:</p>
<pre><code>KeyError at &quot;/opt/splunk/etc/apps/t-digest-custom-command/bin/sum.py&quot;, line 63 : 'linecount'
</code></pre>
<p>Am I getting this error due to a bug in the custom search command API, or am I missing something?</p>
<p>Follow up question: why don't reporting commands reduce to a single value for sufficiently large numbers of input events? For example, this query yields a single statistic value as I expect:</p>
<pre><code>index = _internal | head 50 | sum total=lines linecount
</code></pre>
<p>However, this query yields multiple statistic values, even when I only want one value:</p>
<pre><code>index = _internal | head 400 | sum total=lines linecount
</code></pre>",,0,0,,2021-7-27 19:31:29,,2021-7-27 19:31:29,,,,,9588940.0,,1,0,python|splunk,22,5
1044,259500,68559629,I need to authenticate to Splunk ONLY with apache2,"<p>Good afternoon everyone and first of all thank you very much for giving me a few minutes.
It turns out that we are using Splunk Free in a laboratory, which does not allow any type of authentication, which, even though it is a laboratory is not something that we approve and we are calm about it, I decided to implement Apache2 authentication for Splunk ...
First I want to perform the tests in a simple way, but then I would like to perform the authentication with LDAP, but for now I am stuck simply with the way to configure this, I would very much like if you could help me, I have done different tests but nothing, even trying to modify in ports.conf so that it listens for the port that the connection to Splunk is made, for the 8000 but of course, being assigned to Splunk it does not allow me to restart the service.
Greetings and thanks in advance to all.!</p>",,0,3,,2021-7-28 11:33:39,,2021-7-28 11:33:39,,,,,16545635.0,,1,0,ldap|apache2|splunk,10,4
1045,259501,68564943,Why is my Python Lambda function timing out while trying a POST request?,"<p>I have a python script that makes a couple of HTTP requests, working 100% when I run it locally.
I need to implement this script in a lambda function in AWS.</p>
<p>Since I'm using <em><strong>requests</strong></em> to make the script work, I learned that the requests library had to be imported into the lambda function's repository, and I already did that.
In fact I have another lambda working similarly with a python script, making a GET request, working without any issues. <strong>The problem is that this new script tries to make a POST request and for some reason it gets stuck in it, timing out every time I try to test it</strong>.</p>
<p>I have increased the preset timeout time all the way to 60 seconds, and it still times out.</p>
<p>This is the part of the function where the timeout is happening:</p>
<pre class=""lang-py prettyprint-override""><code>def getFilePaths(fileList):
    filePaths = []
    if fileList:
        print(&quot;fileList has values!&quot;)
        print(fileList)
        for file in fileList: # for each file name in the input list
            print(&quot;started with: &quot; + file)
            filePath = ''
            searchURL = '{}/servicesNS/{}/search/search/jobs'.format(baseURL,un)
            print(searchURL)
            resp = requests.post(url=searchURL, data={'search': 'search ' + file}, verify=False, auth=(un, pw)) # create search job for current file
            print(&quot;search job requested!&quot;)
            if resp.ok:
                ...
</code></pre>
<p>When I run a test, I check the function logs and everything works fine, until it gets to the line where the post request is done.</p>
<p><code>resp = requests.post(url=searchURL, data={'search': 'search ' + file}, verify=False, auth=(un, pw))</code></p>
<p>I'm fairly new to AWS and I've found some articles mentioning that this may be caused by the lambda function being in a VPC, but this is not the case.
I'm not sure if the execution role that I'm using for it has something to do, but I have no access to create a new one. I've been just using one that was available (and it has worked fine with GET requests).</p>
<p>Another point to highlight is that this POST request is trying to run a search in a <strong>Splunk server</strong>.
I don't have access to viewing or modifying its settings, but if this may be caused by any configuration with it, I can try asking the administrator to adjust it. Although I personally believe the problem is somewhere else, since as I mentioned, running the exact same script locally works correctly.</p>
<p><em><strong>Does anyone know what could be causing my script to get stuck at this specific line, and only when I try to test it in AWS Lambda?</strong></em></p>
<p><em><strong>Thanks in advance for your help!</strong></em></p>",,0,6,,2021-7-28 17:30:08,,2021-7-28 23:20:51,2021-7-28 23:20:51,,174777.0,,11331427.0,,1,1,python|amazon-web-services|aws-lambda|splunk|python-requests-html,88,7
1046,259502,68581002,Splunk String Search,"<p>I have a powershell command value for a process field that I am trying to match in my splunk search but I am getting
<em>&quot;Error in 'SearchParser': Subsearches are only valid as arguments to commands. Error at position '3271' of search query 'litsearch (index=notable ((sourcetype=&quot;WMI:LocalPr...{snipped} {errorcontext = andLine&quot;=[[Decoded Ba}'.&quot;</em>:</p>
<pre><code>`macro` rule_name=&quot;rule-name&quot; process=&quot;[[Decoded Base64]] -LogName Microsoft-Windows-Diagnostics-Performance/Operational 
| Where-Object {$_.Id -eq 100 -and $_.TimeCreated -ge ((Get-Date) - (New-TimeSpan -Day 90))} 
| Format-list ;&lt;!LF!&gt;$ImpersonatedUser.ImpersonationContext.Undo()&lt;!LF!&gt;&quot;
</code></pre>
<p>I need a solution for grabbing every instance that this value occurs</p>",,0,3,,2021-7-29 18:09:59,,2021-7-29 21:06:17,2021-7-29 21:06:17,,4418.0,,16556223.0,,1,0,powershell|splunk,40,6
1047,259503,68618688,Access Webpage only if it has been authenticated before in Apache2,"<p>Good morning, I have a question that I don't know how to solve:</p>
<p>I have a Splunk Free server, to authenticate I use an Apache server through port 443, and after authentication it sends me to the Splunk domain name and its port 8000, the fact is that you can still access without authentication, you just have to put, for example: splunk.local: 8000 and you automatically sign in.</p>
<p>I would like to block that entry, that this URL cannot be accessed if it is not redirecting the authentication of apache, splunk.local: 443, could someone give me a hand? Thanks in advance!</p>",,0,0,,2021-8-2 08:41:04,,2021-8-2 08:41:04,,,,,16545635.0,,1,0,linux|apache2|splunk,8,3
1048,259504,68636429,Why is Splunk not showing miliseconds for JSON?,"<p>hello,</p>
<p>Splunk is not showing up miliseconds for JSON logs. I have find some Questions and Answers here in splunk community, but without success.</p>
<p>Description:</p>
<p>I have HFs, indexer cluster and search head cluster.</p>
<p>HF props.conf</p>
<pre><code>    [k8s:dev]
#temporary removed to fix 123123
#INDEXED_EXTRACTIONS = JSON
TIME_PREFIX = {\\&quot;@timestamp\\&quot;:\\&quot;
TIME_FORMAT = %Y-%m-%dT%H:%M:%S.%6N
TRUNCATE = 200000
TRANSFORMS-discard_events = setnull_whitespace_indented,setnull_debug_logging
SEDCMD-RemoveLogProp = s/(&quot;log&quot;:)(.*)(?=&quot;stream&quot;:)//
</code></pre>
<p>HF transforms.conf</p>
<pre><code>[setnull_java_stacktrace_starttab]
SOURCE_KEY = field:log
REGEX = ^\tat\s.*
DEST_KEY = queue
FORMAT = nullQueue

[setnull_whitespace_indented]
SOURCE_KEY = field:log
REGEX = ^\s+.*
DEST_KEY = queue
FORMAT = nullQueue

[setnull_debug_logging]
SOURCE_KEY = field:log
REGEX = .*?\sDEBUG\s
DEST_KEY = queue
FORMAT = nullQueue
</code></pre>
<p>Search props.conf</p>
<pre><code>#workaround, see 123123


[k8s:dev]
KV_MODE = json

</code></pre>
<p>Everything looks fine in web ADD DATA in HF and SEARCH too.
<a href=""https://i.stack.imgur.com/PNjE9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PNjE9.png"" alt=""enter image description here"" /></a></p>
<p>But not when I search it.
<a href=""https://i.stack.imgur.com/RbbqM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RbbqM.png"" alt=""enter image description here"" /></a></p>
<p>I can insert only part of the JSON.</p>
<pre><code>{&quot;log&quot;:&quot;{\&quot;@timestamp\&quot;:\&quot;2021-08-03T09:00:57.539+02:00\&quot;,\&quot;@version\&quot;:\&quot;1\&quot;,\&quot;message\&quot;:
</code></pre>
<p>Question is:</p>
<p>1.what am I doing wrong?</p>
<ol start=""2"">
<li><p>Is it possible to configure TIME_PREFIX and TIME_FORMAT for KV_MODE on search? Because as I know they are used in HF during parsing.</p>
</li>
<li><p>Is it possible to configure KV_MODE?</p>
</li>
</ol>
<p>Thank you very much for your suggestions.</p>",,1,0,,2021-8-3 12:52:55,,2021-8-3 19:07:06,,,,,10956229.0,,1,0,json|timestamp|config|splunk|forwarding,28,5
1049,259505,68642413,Regular Expression for different api url and renaming the resultant events in splunk,"<p>I am trying to get the event count for below api in splunk for that I am trying to write regular expression for api but its not selecting hypen not sure how to write the regular expression to extract field out of it</p>
<pre><code>&quot;GET /v1/resource-plans/store-manager-view?type=
&quot;GET /v1/resource-plans/trend ? xyz=
&quot;GET /v1/resource-plans/store-director-view ? location =
&quot;POST /v1/resource-plans
</code></pre>
<p>I have tried below expression its selecting store-manager and store-director into one but i need count for all api different row</p>
<pre><code>(?&lt;TYPE&gt;\/v1\/resource-plans\/\w+)
</code></pre>
<p><a href=""https://i.stack.imgur.com/bDjjF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bDjjF.png"" alt=""enter image description here"" /></a></p>
<p>And also i want to <strong>rename the resultant events</strong> defined in field column TYPE how to do this ? Below is my Splunk query</p>
<pre><code>  index=msc AND app=xyz AND source=resouce NOT message=&quot;*/_status&quot; | rex field= message &quot;(?&lt;TYPE&gt;\/v1\/resource-plans\/\w+)
    &quot; | stats count by TYPE
</code></pre>",,1,5,,2021-8-3 20:26:38,,2021-8-4 16:16:33,,,,,1884937.0,,1,0,regex|splunk|splunk-query,62,7
1050,259506,68645825,How do I Query on Splunk Dashboard information,"<p>We are currently using ADO (pipeline artifacts) to Splunk for Build step reviews. I am working on a project to migrate Splunk Dashboards to powerbi. I need a query to list down the Dashboards and reports from Splunk on the below criteria so we would identify which ones to be created in powerbi</p>
<ol>
<li>List of Dashboards / reports (possibly with Author details)</li>
<li>frequency of usage - like how many times the Dashboard / reports was viewed in the last 30 days</li>
</ol>
<p>I tried few queries from mysplunk but it did not give the result. Thanks for any inputs / suggestions. Thanks.</p>",68652230.0,2,0,,2021-8-4 05:28:59,,2021-8-5 17:18:47,,,,,16539658.0,,1,0,splunk|splunk-query|splunk-dashboard,221,9
1051,259507,68651931,Can we index only one field of json log file in splunk,"<p>When running an application in EKS, the container logs are coming in the format of:</p>
<p><code>{ log: 2021-08-04 12:28:24,803INFO hostname com.application.RequestListener Sent response with request_id=1  stream: stdout time: 2021-08-04T12:28:24.803533339Z }</code></p>
<p>As I am only interested in the value relating to log, if there a way I can extract only:</p>
<p><code>2021-08-04 12:28:24,803INFO hostname com.application.RequestListener Sent response with request_id=1  </code></p>
<p>I have tried setting source to log in outputs.conf, however, this does not seem to work and I still get the json format present in the splunk search head</p>",,0,2,,2021-8-4 13:15:08,,2021-8-4 13:15:08,,,,,1114569.0,,1,0,splunk,18,5
1052,259508,68659326,How to do a unique search in Splunk,"<p>I'm trying to do a search in Splunk in which I'm trying to narrow it down to a unique substring.</p>
<p>An example of my query so far would be:</p>
<pre><code>host=node-1 AND &quot;userCache:&quot;
</code></pre>
<p>Which returns something like this:</p>
<pre><code>Time                Event
06/04/2021-blah     Cache miss: userCache:  tjohnson
                    host=node-1
06/04/2021-blah     Cache miss: userCache:  sbaca
                    host=node-1
06/04/2021-blah     Cache miss: userCache:  tjohnson
                    host=node-1
</code></pre>
<p>What I want to do, though, is to return only one unique value based on what comes after userCache:</p>
<p>In the above example, only two results would be returned - one for tjohnson and one for sbaca.  The additional tjohnson would be stripped since there is already a tjohnson in the results.</p>
<p>Any suggestions?</p>
<p>Thanks much</p>",68665503.0,1,0,,2021-8-5 00:29:36,,2021-8-5 11:12:18,,,,,13894850.0,,1,0,splunk|splunk-query,55,6
1053,259509,68663778,Find all newer events logged by application after a certain date in splunk,"<p><strong>Question:</strong> How can we find diff between log statements before and after a given date.</p>
<p><strong>Applicability:</strong>  Let's say we release a new application code and I want to be able to see all new events that application has started logging.</p>
<p>Now definition of new is very vague here but any suggestion would help.  Idea is that splunk should be able to compare type of events were being logged earlier and only show new events that were not present before.</p>
<p>That would help finding any new Exceptions Errors or Warning that are being logged and not yet surfaced as a failed customer interaction.</p>
<p><strong>Example:</strong>  After a new code release,  our application started logging an WARN event regarding &quot;open file handlers&quot; that kept building up over the time and ultimately reached a stage where no more unix file handlers were available to process any new request.</p>",,1,0,,2021-8-5 09:15:45,,2021-8-5 17:45:36,,,,,1402699.0,,1,0,detection|splunk|anomaly-detection,13,4
1054,259510,68674172,Splunk search the key in json,"<p>Could anyone help me with the below Splunk query?</p>
<p>I want to get the <code>count</code> of records by <code>message.type</code>. The <code>message.type</code> can take value either 'typeA' or 'typeB'.</p>
<p>I tried the below query but it lists and doesn't give the <code>count</code> in the result. That is, separate <code>count</code> for typeA and typeB.</p>
<p>The messages are below.</p>
<pre><code>message: name=app1,version=1, type=typeA,task=queryapp
message: name=app2,version=1, type=typeB,task=testapp
message: name=app1,version=1, type=typeB,task=issuefix
</code></pre>
<pre><code>index=myapp message=&quot;name=app1&quot; 
| stats count by message.type
</code></pre>",,1,1,,2021-8-5 22:30:33,,2021-8-19 05:30:12,2021-8-8 16:47:02,,2055998.0,,9192593.0,,1,0,splunk,43,7
1055,259511,68678466,Splunk Avg Query,"<p>I am consuming some data using an API, I want to calculate avg time it took for all my customer, after each ingestion (data consumed for a particular customer), I print a time matrix for that customer.</p>
<p><code>timechart span=24h avg(total_time)</code></p>
<p>Now to calculate average I cannot simply extract the time field and do avg(total_time), because if customerA completes ingestion in 1 hour, and customerB takes 24 hours, customer A will be logged 24 times and B will be logged once, giving me inaccurate results and bringing down the average.</p>
<p>How do I create a filter let's say time duration is 7 days, so I get only those log lines for a particular customer which has the maximum total_time over a period of 7 days. i.e one log line per customer that has max total_time over a period of 7 days for that particular customer.</p>",68683738.0,1,1,,2021-8-6 08:23:47,,2021-8-6 15:01:45,,,,,15592487.0,,1,0,splunk|splunk-query|splunk-formula|splunk-calculation|splunk-dashboard,33,6
1056,259512,68699105,Generate Splunk report with only extracted fields,"<p>First and foremost, maybe what I am looking for isn’t possible or I am going down the wrong path.   Please suggest.</p>
<p>Consider, I’ve raw data which has n number of parameters each separated by ‘&amp;’.</p>
<pre><code>Id=1234&amp;ACC=bc3gds5&amp;X=TESTX&amp;Y=456567&amp;Z=4457656&amp;M=TESTM&amp;N=TESTN&amp;P=5ec3a 
</code></pre>
<p>Using SPL, I’ve filtered only a few fields(ACC, X, Y) which I’m interested in.  Now, I would like to generate the report only with the filtered fields in a tabular format, not the whole raw data.</p>",68700829.0,1,0,,2021-8-8 08:36:24,,2021-8-8 14:20:48,,,,,5529420.0,,1,0,report|splunk|splunk-query|splunk-sdk|splunk-dashboard,26,7
1057,259513,68708432,Splunk Query to list the usage of Reports,<p>I need a query to list down the frequency of usage of reports in Splunk. I tried few queries from mysplunk but it did not give the result. Thanks for any inputs.</p>,,1,3,,2021-8-9 07:40:47,,2021-8-11 13:38:34,,,,,16539658.0,,1,-2,report|splunk,35,5
1058,259514,68729899,How to set up Splunk with multiple IIQ SailPoint environments with Splunk TA configuration using: SailPoint Adaptive Response,"<p>I noticed that the Splunk documentation on this site says that this should support multiple environments (s) - looking at the code in the python scripts though it looks like it doesn't?</p>
<p>SailPoint IIQ version: 8.1p3
Splunk version: 8.0.9
TA version: 2.0.5</p>
<p>After reviewing the Splunk Plugin code (the Python code which Splunk uses to read data from SailPoint), I noticed the following bits of information:</p>
<p>Splunk/etc/apps/Splunk_TA_sailpoint is the plugin directory where the plugin derives its files.
Splunk/etc/apps/Splunk_TA_sailpoint/bin/input_module_sailpoint_identityiq_auditevents.py – this is the file in question that caught my attention.
The way the code appears to operate is that it defines a SINGLE file for where it stores the epoch date following the logic outlined below:
 </p>
<p>1.  Initially, there is a file check (audit_events_checkpoint.txt) to see if the file exist<br />
2.  If Python doesn’t find it attempts to create it<br />
3.  If it fails again, it creates the folder structure and then adds the file<br />
4.  After the first three steps, Python opens the file.<br />
5.  Python then reads the files and pull the first value in (the Unix/Epoch timestamp)<br />
6.  It then uses this as part of its query outbound.
 </p>
<pre><code>#Read the timestamp from the checkpoint file, and create the checkpoint file if necessary
    #The checkpoint file contains the epoch datetime of the 'created' date of the last event seen in the previous execution of the script. 
    checkpoint_file = os.path.join(os.environ['SPLUNK_HOME'], 'etc', 'apps', 'Splunk_TA_sailpoint', 'tmp', &quot;audit_events_checkpoint.txt&quot;)
    try:
        file = open(checkpoint_file, 'r')
    except IOError:
        try:
            file = open(checkpoint_file, 'w')
        except IOError:
            os.makedirs(os.path.dirname(checkpoint_file))
            file = open(checkpoint_file, 'w')
            
    with open(checkpoint_file, 'r') as f:
         checkpoint_time = f.readlines()
     
    #current epoch time in milliseconds 
    # new_checkpoint_time = int((datetime.datetime.utcnow() - datetime.datetime(1970, 1, 1)).total_seconds() *1000)
    
    if len(checkpoint_time) == 1:
        checkpoint_time =int(checkpoint_time[0])
    else:
        checkpoint_time = 1562055000000
        helper.log_info(&quot;No checkpoint time available. Setting it to default value.&quot;)
    
    #Standard query params, checkpoint_time value is set from what was saved in the checkpoint file
    queryparams= {
         &quot;startTime&quot; : checkpoint_time,
         &quot;startIndex&quot; : 1,
         &quot;count&quot; : 100
    }
</code></pre>
<p> 
1.  Jumping down to the next references, we find that the JSON object pulled in is used to create the new timestamp that the system will use on the next request.<br />
2. It then takes this value and writes it to file, which it will reuse the next time a call is made.
 </p>
<pre><code>#Iterate the audit events array and create Splunk events for each one
    invalid_response = isListEmpty(auditEvents)
    if not invalid_response:
        for auditEvent in auditEvents:
 
            data = json.dumps(auditEvent)
            event = helper.new_event(data=data, time=None, host=None, index=helper.get_output_index(), source=helper.get_input_type(), sourcetype=helper.get_sourcetype(), done=True, unbroken=True)
            ew.write_event(event)
 
        #Get the created date of the last audit event in the run and save it as a checkpoint key in the checkpoint file
        list_of_created_date = extract_element_from_json(results, [&quot;auditEvents&quot;, &quot;created&quot;])
 
        new_checkpoint_created_date = list_of_created_date[-1]
        helper.log_info(&quot;DEBUG New checkpoint date \n{}&quot;.format(new_checkpoint_created_date))
 
    #Write new checkpoint key to the checkpoint file
        with open(checkpoint_file, 'r+') as f:
            f.seek(0)
            f.write(str(new_checkpoint_created_date))
            f.truncate()
</code></pre>
<p>So here is my thought as to what is happening: When we enter information into Splunk for the connectors to EACH environment (we have a total of 6), the checkpoint_file is being over-written. I would also assume that each connected env calls the same timestamp because they all appear to be pulling that information from the same file. Did we miss a configuration, or is this a coding gap?</p>",,0,2,,2021-8-10 15:48:56,,2021-8-10 16:02:56,2021-8-10 16:02:56,,613876.0,,613876.0,,1,0,python|json|splunk|audit-logging|sailpoint,32,6
1059,259515,68765482,Splunk panel showing graph for a specific time range,"<p>I am working with splunk. I want to pull logs for an api call for a specific range of time 9.30pm to 12:00 am on daily basis. Also, the average time taken for the call duraing that specific duration.</p>
<pre><code>index=&quot;index_a&quot; sourcetype=&quot;pqr&quot; source=&quot;prq_source&quot; &quot;Success in api response&quot;
</code></pre>
<p>Can someone guide me how to handle this how we can fetch for that particular duration for atleast 7 days.</p>",,1,0,,2021-8-13 00:20:00,,2021-8-13 12:53:29,2021-8-13 12:53:29,,4418.0,,9186499.0,,1,0,monitoring|splunk|splunk-query,31,6
1060,259516,68821885,Splunk dashboard and reports source backup for versioning,"<p>We are trying to take a backup of Splunk dashboards and reports source code for versioning. we are on an enterprise implementation where our rest calls are restricted. we can create and access dashboards and reports via Slunk UI, but would like know if we can automatically take backup of them and store in our versioning system.</p>",,2,0,,2021-8-17 17:45:32,,2021-9-4 03:51:53,,,,,6922289.0,,1,1,splunk|splunk-dashboard|splunk-api,55,9
1061,259517,68834286,"Splunk - How to generate a ""identity column"" id 1,2,3,","<p>We're logging info/error logs in Splunk/db. We're using .net and nlog.</p>
<p>In db, we're getting it in the right order when sorting because of identity column.</p>
<p>In Splunk, it's coming out of order if many log entries have the same date.</p>
<p>Is there a chance to tell Splunk to create a &quot;identity column&quot; for everything that is piped into it?
We're piping the logs into Splunk using HTTP Event Collector.</p>
<p>We're using Splunk Enterprise.</p>",,1,6,,2021-8-18 14:26:12,,2021-9-1 18:59:01,2021-8-18 19:15:37,,14419.0,,3401331.0,,1,0,splunk|splunk-query,46,7
1062,259518,68838013,splunk example query to alert when no data is found,"<p>Following is the scenario
Example: query <code>index=XXXX name=somefile  | stats count(msg) as MESSAGES</code></p>
<hr />
<p>This should always return some count for messages</p>
<hr />
<pre><code>the way i am looking is 
Checking for last 15 min and check if two of three 5 min interval the count goes to 0 (here 0 is when there is not data coming in) then do some action```


</code></pre>",,0,0,,2021-8-18 19:03:46,,2021-8-18 19:03:46,,,,,12976099.0,,1,0,splunk-query,13,4
1063,259519,68838020,Splunk HTTP Event Collector - Why does my Time field have 000 milliseconds?,"<p>I'm getting data into Splunk by using: HTTP Event Collector
I'm using Splunk Enterprise 8.0.1</p>
<p>My Time field is missing milliseconds:
18/08/2021 18:36:37.000</p>
<p>My Splunk record:
Time _time 2021-08-18T18:36:37.000+00:00</p>",,1,0,,2021-8-18 19:04:44,,2021-8-18 19:36:32,2021-8-18 19:15:11,,14419.0,,3401331.0,,1,0,splunk|splunk-query,34,7
1064,259520,68866445,Splunk - Assigning custom time,"<p>I want my time to be the &quot;Date&quot; property in the following json:</p>
<pre><code>{ &quot;Level&quot;: &quot;ERROR&quot;, &quot;Date&quot;: &quot;2021-08-20 17:21:53.6355&quot;, &quot;Logger&quot;:.... }
</code></pre>
<p>I created a props.conf here: ...\Splunk\etc\system\local
with:</p>
<pre><code>TIME_PREFIX=\&quot;Date\&quot;:
TIME_FORMAT=%Y-%m-%d %H:%M:%S.%4N
</code></pre>
<p>I then restarted splunk...but it's not working. Any idea what I'm missing?</p>",,1,0,,2021-8-20 18:06:19,,2021-8-20 20:06:32,2021-8-20 18:40:29,,14419.0,,3401331.0,,1,2,splunk|splunk-query,25,6
1065,259521,68886885,Splunk Count Specific String in a Field,"<p>In Splunk, I need to get the count of events from the below msg field value which matches factType=COMMERCIAL and has filters.</p>
<p>Using the basic Splunk query with wildcard does not work efficiently. Could you please assist</p>
<pre><code>app_name=&quot;ABC&quot; cf_space_name=prod  msg=&quot;*/facts?factType=COMMERCIAL&amp;sourceSystem=ADMIN&amp;sourceOwner=ABC&amp;filters*&quot; 
</code></pre>
<blockquote>
<p>msg: abc.asia - [2021-08-23T00:27:08.152+0000] &quot;GET
/facts?factType=COMMERCIAL&amp;sourceSystem=ADMIN&amp;sourceOwner=ABC&amp;filters=%257B%2522stringMatchFilters%2522:%255B%257B%2522key%2522:%2522BFEESCE((json_data-%253E%253E'isNotSearchable')::boolean,%2520false)%2522,%2522value%2522:%2522false%2522,%2522operator%2522:%2522EQ%2522%257D%255D,%2522multiStringMatchFilters%2522:%255B%257B%2522key%2522:%2522json_data-%253E%253E'id'%2522,%2522values%2522:%255B%25224970111%2522%255D%257D%255D,%2522containmentFilters%2522:%255B%255D,%2522nestedMultiStringMatchFilter%2522:%255B%255D,%2522nestedStringMatchFilters%2522:%255B%255D%257D&amp;sorts=%257B%2522sortOrders%2522:%255B%257B%2522key%2522:%2522id%2522,%2522order%2522:%2522DESC%2522%257D%255D%257D&amp;pagination=null</p>
</blockquote>",,0,0,,2021-8-23 03:08:51,,2021-8-23 05:57:09,2021-8-23 05:57:09,,6918179.0,,6918179.0,,1,0,splunk-query,37,6
1066,259522,68893731,Splunk UF not sending data to indexer,"<p>I have Splunk UF and Splunk Enterprise Server, both v8.2.1, running in docker containers but I am unable to see any data on the Enterprise Server with regards to the new index I created, 'mytest':</p>
<p><a href=""https://i.stack.imgur.com/Dp2Sq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dp2Sq.png"" alt=""enter image description here"" /></a></p>
<p>The Enterprise Server has default port 9997 active as a receiver port:</p>
<p><a href=""https://i.stack.imgur.com/qvzEh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qvzEh.png"" alt=""enter image description here"" /></a></p>
<p>Both of the containers are connected to 'splunk' network I created:</p>
<pre><code>        &quot;Containers&quot;: {
        &quot;0f9e44620ce9fba16df21af6d2253c4b02b9714cb3ea126a616f10d06f836eb9&quot;: {
            &quot;Name&quot;: &quot;dspinelli-uf&quot;,
            &quot;EndpointID&quot;: &quot;0e1dd065ee3d815c943a8b52e6107e53a4b57d9e3103b17d1461611543769869&quot;,
            &quot;MacAddress&quot;: &quot;02:42:ac:12:00:03&quot;,
            &quot;IPv4Address&quot;: &quot;172.18.0.3/16&quot;,
            &quot;IPv6Address&quot;: &quot;&quot;
        },
        &quot;3a1a084561eda8013baa8847f4ca30fd68eb74468ff666195bf1c15e0f8a280f&quot;: {
            &quot;Name&quot;: &quot;dspinelli-ent&quot;,
            &quot;EndpointID&quot;: &quot;7159b1a41840f9dfae04b50bb61386f8c3ac2233aee334026b9f1d685cfcf571&quot;,
            &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;,
            &quot;IPv4Address&quot;: &quot;172.18.0.2/16&quot;,
            &quot;IPv6Address&quot;: &quot;&quot;
        }
</code></pre>
<p>Inputs.conf on the UF:</p>
<pre><code>[splunktcp://9997]
disabled = 0

[http://hec-uf]
description = UF HTTP Event Collector
disabled = 0
token = 4022d42f-9132-442a-8a79-5d3eea1ad40d
index = mytest
indexes = mytest
outputgroup = tcpout
</code></pre>
<p>Outputs.conf on UF:</p>
<pre><code>[indexAndForward]
index = false

[tcpout]
defaultGroup = default-autolb-group

[tcpout:default-autolb-group]
server = dspinelli-ent:9997

[tcpout-server://dspinelli-ent:9997]
</code></pre>
<p>Communication between the UF and Enterprise Server is established:</p>
<pre><code>netstat -an | grep 9997
tcp        0      0 0.0.0.0:9997            0.0.0.0:*               LISTEN
tcp        0      0 172.18.0.3:44420        172.18.0.2:9997         ESTABLISHED

./bin/splunk list forward-server
Active forwards:
        dspinelli-ent:9997
Configured but inactive forwards:
        None
</code></pre>
<p>Attempt to curl the UF with some test data shows success:</p>
<pre><code>curl -k https://x.x.x.x:8087/services/collector \
&gt; -H 'Authorization: Splunk 4022d42f-9132-442a-8a79-5d3eea1ad40d' \
&gt; -d '{&quot;sourcetype&quot;: &quot;demo&quot;, &quot;event&quot;:&quot;Hello, I was sent from UF&quot;}'
{&quot;text&quot;:&quot;Success&quot;,&quot;code&quot;:0}
</code></pre>
<p>However, no data is ever displayed on the index in Enterprise Server:</p>
<p><a href=""https://i.stack.imgur.com/Gdp0H.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Gdp0H.png"" alt=""enter image description here"" /></a></p>
<p>Does anyone know what could possibly be wrong here or what the next steps would be?</p>",68897149.0,1,0,,2021-8-23 13:40:42,,2021-8-23 18:05:47,,,,,8189616.0,,1,0,splunk,39,6
1067,259523,68909243,Setting a token based on condition match in splunk dashboard studio,"<p>I am using Dashboard studio feature to create dashboard in splunk. I have created the dashboard using the grid layout. I have a dropdown and want to implement something similar to below in dashboard studio</p>
<p>Below is the code from HTML based dashboards. Where its matching the condition for host_token and assigning a new token TPS_ON_ALL_PANELLS with some value.</p>
<pre><code> &lt;input type=&quot;dropdown&quot; token=&quot;host_token&quot; searchWhenChanged=&quot;true&quot; rejects=&quot;$SHOW_HISTORICAL_ENVIRONMENTS$&quot;&gt;
      &lt;label&gt;Data Source:&lt;/label&gt;
      &lt;search&gt;
        &lt;query&gt;| metadata type=hosts index=mlc_live | fields host&lt;/query&gt;
        &lt;earliest&gt;-1months&lt;/earliest&gt;
        &lt;latest&gt;now&lt;/latest&gt;
      &lt;/search&gt;
      &lt;default&gt;-&lt;/default&gt;    
      &lt;change&gt;

    &lt;condition match=&quot;$host_token$ == &amp;quot;*&amp;quot;&quot;&gt;
        &lt;set token=&quot;TPS_ON_ALL_PANELLS&quot;&gt;true&lt;/set&gt;
      &lt;/condition&gt;
      &lt;/change&gt;
</code></pre>
<p>I have already added the dropdown but not sure how to implement this condition match thing required for setting a token.</p>",,0,5,,2021-8-24 14:37:19,,2021-8-24 14:37:19,,,,,9186499.0,,1,0,amazon-web-services|monitoring|amazon-eks|splunk|splunk-query,369,10
1068,259524,68920229,Apache Spark to query directly a Splunk search result,"<p>Small question regarding an integration between Apache Spark and Splunk please.</p>
<p>Currently, I am doing a search query in Splunk. The result is quite big. Then, I need to perform some kind of data analytics from it.</p>
<p>Therefore, each time I get the result, I dump the search result as CSV from the Splunk UI.</p>
<p>Then, I manually download the CSV.</p>
<p>What I have tried:
I have ready a Spark job that will parse the CSV, the one from previous step, the Splunk search result. The Spark job will then use Spark SQL to load the CSV as table, and perform computation, very happy.</p>
<p>Basically, Splunk search result, very happy. Spark loading and doing map reduce on the csv, very happy.</p>
<p>However, I feel like this is very &quot;stupid&quot; as I am downloading manually each and every time a CSV.</p>
<p>May I ask, how can I connect Spark directly to Splunk search result please?</p>
<p>Thank you</p>",,0,2,,2021-8-25 09:23:28,,2021-8-30 20:14:08,2021-8-30 20:14:08,,10461625.0,,10461625.0,,1,0,csv|apache-spark|splunk|splunk-query,19,5
1069,259525,68929886,Splunk panel for calculating grater then operation,"<p>Sharing the application log:<br></p>
<pre><code>2021-08-25T20:45:17.382Z level=info module=xyz pid=45 message=&quot;queryAPI, Execution Time(ms):,617.195517, pId:45&quot; 
2021-08-25T20:45:17.382Z level=info module=xyz pid=45 message=&quot;queryAPI, Execution Time(ms):,231.195517, pId:45&quot;
</code></pre>
<p>Question: Find the total number of API's which took more then 500 ms in splunk dashboard?</p>
<p>Please share the splunk query to find out below data.</p>
<p>Expected output display in table of two column :<br>
Delayd API-Name:   queryAPI <br>
Total occurences:     1</p>",,1,3,,2021-8-25 21:07:05,,2021-8-26 14:31:54,2021-8-26 00:41:53,,13888271.0,,13888271.0,,1,0,splunk|splunk-query|splunk-formula|splunk-dashboard,35,6
1070,259526,68929990,Timechart with distinct_count per day,"<p>I have a query which shows me the number of hosts for which a given event is logged more than three times within a single day:</p>
<pre><code>index=desktopevents &quot;target&quot; 
| stats count by host | dedup host 
| where count &gt; 3 | stats dc(host)
</code></pre>
<p>What I can't figure out is how to use this with <code>timechart</code> so I can get the distinct count <em>per day</em> over some period of time. The naive timechart outputs cumulative <code>dc</code> values, not per day (and obviously it lacks my more-than-three clause):</p>
<pre><code>index=desktopevents &quot;target&quot; 
| timechart span=1d dc(host)
</code></pre>
<p>I thought this might work but the chart is blank:</p>
<pre><code>index=desktopevents &quot;target&quot; 
| stats count by host | dedup host 
| where count &gt; 3 | timechart span=1d dc(host)
</code></pre>",68930540.0,1,2,,2021-8-25 21:18:40,,2021-8-26 14:23:22,,,,,152997.0,,1,0,splunk,120,10
1071,259527,68938841,Changing panel time for splunk dashboard studio irrespective of the global timing,"<p>Working over grid layout in splunk studio to created a dashboard, could not look for a way where we can customize the panel timing irrespective of the global duration of the dashboard.</p>
<p>For instance, if I want a panel to be showing data for last 15min but the global time remain fixed at 24 hours is it possible?</p>",68942819.0,1,0,,2021-8-26 12:50:21,,2021-8-26 17:14:07,,,,,9186499.0,,1,0,amazon-web-services|amazon-eks|splunk|splunk-query|splunk-dashboard,20,6
1072,259528,68944375,Possible to output Docker syslog-ng container log to stdout & stderr while using Splunk logging driver?,"<p>I have a container running Syslog-ng and I'm trying to feed syslog messages to Splunk.</p>
<p>Within the Syslog-ng container, there is a file /var/log/messages that stores any syslog messages that the container receives.  Is it possible for me to forward the container's /var/log/messages to stdout and stderr so that the syslog messages will be forwarded to my Splunk instance?</p>
<p>Here is an example of my docker-compose.yml:</p>
<pre><code>version: '3.9'
services:
  syslog-ng:
    image: balabit/syslog-ng:latest
    ports:
      - &quot;514:514/udp&quot;
      - &quot;601:601&quot;
    logging:
      driver: &quot;splunk&quot;
      options:
        splunk-token: &quot;XXXX&quot;
        splunk-url: &quot;https://XXXX.splunkcloud.com&quot;
        splunk-insecureskipverify: &quot;true&quot;
        tag: &quot;{{.Name}}/{{.FullID}}&quot;
        labels: type,location
        env: &quot;os&quot;
    labels:
      type: syslog
      location: home
    environment:
      - os=debian
    restart: unless-stopped
</code></pre>
<p>I can then exec into my container and run the command:</p>
<pre><code>ln -sf /proc/self/fd/1 /var/log/messages
</code></pre>
<p>or</p>
<pre><code>ln -sf /dev/stdout /var/log/messages
</code></pre>
<p>If I then send syslog messages to the container, I can view them in /var/log/messages.
They will not show up in stdout.</p>
<p>Running the following command will not show logs from /var/log/messages:</p>
<pre><code>docker logs (containerid)
</code></pre>
<p>I can go to Splunk and see logs from when the container was created via stdout or stderr.  But Splunk will not show any further logs from the container receiving syslog messages.</p>",,0,0,,2021-8-26 19:30:36,,2021-8-26 19:30:36,,,,,8644283.0,,1,0,docker|splunk,117,8
1073,259529,68945743,JSON Extraction from Splunk log,"<p>I have a requirement where i need to extract part of JSON code from splunk log and assign that field to spath for further results</p>
<p>My regex is working in regex101 but not in splunk</p>
<p>below is log snippet --looking to grab the JSON code starting from {&quot;unique_appcodes to end of line..i have shown the expected output below in the post</p>
<pre><code> cwmessage: 2021-08-26 17:14:10 araeapp INFO MRC: Unique AppCodes Report requested.
2021-08-26 17:14:10 araeapp INFO MRC_ARAE_I_042: (local) requesting uniq_appcodes report for KKA
2021-08-26 17:14:10 araeapp INFO {&quot;unique_appcodes&quot;: [{&quot;count&quot;: 2, &quot;app_code&quot;: &quot;XYZ&quot;, &quot;group&quot;: &quot;&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 2, &quot;app_code&quot;: &quot;QQQ&quot;, &quot;group&quot;: &quot;TSR05441&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 1, &quot;app_code&quot;: &quot;QQQ&quot;, &quot;group&quot;: &quot;&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 192, &quot;app_code&quot;: &quot;PPP&quot;, &quot;group&quot;: &quot;TSR05560&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 12, &quot;app_code&quot;: &quot;PPP&quot;, &quot;group&quot;: &quot;&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 12, &quot;app_code&quot;: &quot;GM9&quot;, &quot;group&quot;: &quot;TSR06083&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 139, &quot;app_code&quot;: &quot;ZZZ&quot;, &quot;group&quot;: &quot;TSR06103&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 6, &quot;app_code&quot;: &quot;GNA&quot;, &quot;group&quot;: &quot;TSR06085&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 803, &quot;app_code&quot;: &quot;SSS&quot;, &quot;group&quot;: &quot;MXXX0718&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 3, &quot;app_code&quot;: &quot;SSS&quot;, &quot;group&quot;: &quot;&quot;, &quot;instance&quot;: &quot;KKA&quot;}]}
 
</code></pre>
<p>Rex using:</p>
<pre><code>| rex field=_raw (?msi)(?&lt;json_field&gt;\{\&quot;unique_appcodes\&quot;.+\}$)
 
</code></pre>
<p>and this perfectly working in regex101.com which is extracting  the below required part but when i use this in SPlunk its not giving any results im thinking its the spaces between the JSON attributes</p>
<p>Please let me know your thoughts</p>
<pre><code>{&quot;unique_appcodes&quot;: [{&quot;count&quot;: 2, &quot;app_code&quot;: &quot;XYZ&quot;, &quot;group&quot;: &quot;&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 2, &quot;app_code&quot;: &quot;QQQ&quot;, &quot;group&quot;: &quot;TSR05441&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 1, &quot;app_code&quot;: &quot;QQQ&quot;, &quot;group&quot;: &quot;&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 192, &quot;app_code&quot;: &quot;PPP&quot;, &quot;group&quot;: &quot;TSR05560&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 12, &quot;app_code&quot;: &quot;PPP&quot;, &quot;group&quot;: &quot;&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 12, &quot;app_code&quot;: &quot;GM9&quot;, &quot;group&quot;: &quot;TSR06083&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 139, &quot;app_code&quot;: &quot;ZZZ&quot;, &quot;group&quot;: &quot;TSR06103&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 6, &quot;app_code&quot;: &quot;GNA&quot;, &quot;group&quot;: &quot;TSR06085&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 803, &quot;app_code&quot;: &quot;SSS&quot;, &quot;group&quot;: &quot;MXXX0718&quot;, &quot;instance&quot;: &quot;KKA&quot;}, {&quot;count&quot;: 3, &quot;app_code&quot;: &quot;SSS&quot;, &quot;group&quot;: &quot;&quot;, &quot;instance&quot;: &quot;KKA&quot;}]}
 
</code></pre>",,1,2,,2021-8-26 21:55:18,,2021-8-26 23:46:20,,,,,5001308.0,,1,0,json|regex|splunk|splunk-query|rex,39,6
1074,259530,68955230,splunk query based on log stdout,"<p>I cannot think of how to query a splunk. I have log &quot;Waiting for changelog lock..&quot; and now I need to select all occurrences if in 10 minutes time period, starting from after this is printed, there is no log saying &quot;Successfully acquired change log lock&quot;. This does not work, as it checks if same log contains it: <code>index=&quot;[there goes my index]&quot; | spath log | search NOT log=&quot;*Successfully acquired change log lock*&quot; AND log='Waiting for changelog lock..' earliest=-10m@m latest=now</code></p>",68957245.0,1,0,,2021-8-27 14:36:22,,2021-8-27 17:20:01,,,,,1444413.0,,1,0,select|logging|splunk,20,6
1075,259531,68967207,splunk. find count of occurs in each event,"<p>How to make a query to find the number of occurrences of a string in each event, that is, if a tag occurs more than once in an event, the search should show the number of such tags in each individual event</p>",,1,1,,2021-8-28 18:34:08,,2021-8-29 01:31:38,,,,,16775623.0,,1,0,splunk,62,7
1076,259532,68978891,nested if loop in splunk,"<p>I would like to write in splunk a nested if loop:
<strong>What I want to achieve</strong></p>
<blockquote>
<pre><code>if buyer_from_France: 
   do eval percentage_fruits
   if percentage_fruits&gt; 10:
        do summation
        if summation&gt;20:
                   total_price
                   if total_price&gt;$50:
                              do(trigger bonus coupon)
</code></pre>
</blockquote>
<p>My current code (that works):</p>
<pre><code>&gt; | eventstats sum(buyers_fruits) AS total_buyers_fruits by location
&gt; | stats sum(fruits) as buyers_fruits by location buyers 
&gt; | eval percentage_fruits=fruits_bought/fruits_sold 
&gt; | table fruits_bought fruits_sold buyers
&gt; | where percentage_fruits &gt; 10
&gt; | sort - percentage_fruits
</code></pre>
<p>How do I complete the syntax/expression for the 2nd (summation) and consequently, 3rd (total price), 4th if-loop (trigger)?</p>",69005277.0,1,4,,2021-8-30 04:18:42,,2021-8-31 21:03:18,2021-8-30 07:21:47,,3706016.0,,16783738.0,,1,0,if-statement|nested-loops|splunk|splunk-query,56,6
1077,259533,68999123,Sum of numeric values in all events in given time period,"<p>I have following events that are logged periodically (every minute):</p>
<pre><code>14:58 index=prod_service service.error error.count=&quot;3&quot;
14:59 index=prod_service service.error error.count=&quot;4&quot;
15:00 index=prod_service service.error error.count=&quot;0&quot;
15:01 index=prod_service service.error error.count=&quot;10&quot;
</code></pre>
<p>I've set up an alert to alert me when we have 10 Events in an hour that have more than &quot;0&quot; error.counts, however I would like to change it to alerting me when the count over all events is greater than 10 in an hour. So how can I sum the error.count over all events (which would be 17)</p>
<p>My current query only counts the number of events that have more than 0 errors...:</p>
<pre><code>index=prod-service service.count | where sum('error.count') &gt; 0
</code></pre>",69012703.0,2,0,,2021-8-31 12:58:39,,2021-9-1 11:16:05,,,,,8920381.0,,1,1,splunk|splunk-query,36,7
1078,259534,69002527,Combine Splunk Rex Queries,"<p>I am looking to combine and manipulate two extracted fields from separate logging instances. I am using the <code>rex</code> command to do the extraction.</p>
<p>However, from reading documentation it appears it's not possible to combine to separate rex commands that will match different lines.</p>
<p>For example:</p>
<p>Say I have this line logged: <code>Timestamp Caller foo bar &lt;DATA&gt; blah blah</code>. Using rex I can grab the <code>&lt;DATA&gt;</code> field as simple as <code>rex field=log &quot;foo bar (?&lt;DATA&gt;\d{1,6}).*</code>. (Note it is numeric data).</p>
<p>Now I have another line logged as <code>Timestamp Caller baz qux &lt;DATA2&gt; blah blah</code>. Again, I can grab <code>&lt;DATA2&gt;</code> as <code>rex field=log &quot;baz qux (?&lt;DATA2&gt;\d{1,6}).*</code>.</p>
<p>The problem is there is a relationship between <code>&lt;DATA&gt;</code> and <code>&lt;DATA2&gt;</code> that I'd like to quantify.</p>
<p>How can I run a query that will capture both data points that appear in separate logging lines?</p>
<p>Note: the related <code>How can I combine 2 queries in Splunk</code> questions aren't relevant as the data in those questions either appears in the same line or they aren't using <code>rex</code> explicitly.</p>",,1,1,,2021-8-31 16:47:40,,2021-8-31 18:21:30,,,,,9573099.0,,1,0,splunk|splunk-query|splunk-dashboard,104,9
1079,259535,69020852,Sorting the splunk timechart table with the values in descending order based on a row's values in the timechart table,"<p>This is my splunk query</p>
<pre><code>index=xxxxx &quot;searchTerm&quot;)|rex &quot;someterm(?&lt;errortype&gt;)&quot; | timechart count by
errortype span =&quot;1w&quot; | addcoltotals labelfield=total | fillnullvalue=TOTAL|fileds - abc,def,total
</code></pre>
<p>I am adding the total count of the errors over a week  in another column named TOTAL as depicted in table below.Here A... B... are error names in alphabetical order, the values are total number of errors that occured on that day for that errortype</p>
<pre><code>_time       A....   A....   C....   D....   E....

2021-08-25  11      22      05      23      89  
2021-08-26  15      45      45      13      39  
2021-08-27  34      05      55      33      85
2021-08-28  56      08      65      53      09
2021-08-29  01      06      95      36      01
TOTAL       117     86      265     158     223
</code></pre>
<p>I want these sorted by value in TOTAL row in descending order like</p>
<pre><code>265 223 158 117 86 
</code></pre>
<p>But i am always getting this in alphabetical order of the errortype like</p>
<pre><code>A... A... B...
</code></pre>
<p>how can i improve this query to get the sorted result like i want?</p>",69028849.0,1,0,,2021-9-1 21:40:23,,2021-9-2 10:52:49,,,,,14387037.0,,1,0,splunk|splunk-query|splunk-dashboard,279,11
1080,259536,69066747,Renaming regex returned token values and passing old token value(before they were renamed) to a drilldown search query in Splunk,"<p>I've a splunk query that finds top errors in the log using regular expression. I then display it as a bar chart. the regex returns 10 values for error.</p>
<pre><code>someSearchQuery
| rex &quot;someTerm(?&lt;error&gt;)
| stats count by error
| sort - count 
| head 10
</code></pre>
<p>I want to use the values returned by the query in a drilldown such that clicking on a barchart will display results for that value</p>
<p>The drilldown xml I used for setting token is this</p>
<pre><code>&lt;drilldown&gt; 
    &lt;set token=&quot;show_panel&quot;&gt;true&lt;/set&gt;
    &lt;set token=&quot;selected_value&quot;&gt;$click.value$&lt;/set&gt;
&lt;/drilldown&gt;
</code></pre>
<p>and then I use this token in the drilldown query as such</p>
<pre><code>someSearchQuery
| rex &quot;someTerm(?&lt;error&gt;)
| search error=$selected_value$
| timechart count by errorType span=&quot;1m&quot;
| addcoltotals
| rename NULL as count
</code></pre>
<p>These error names are too technical and I want to change them to something general in the main panel and drilldown both.</p>
<p>for example, if regex returned error &quot;ID not found&quot;, I want to replace it with &quot;Data_error&quot;</p>
<p>Also I want my title to change with the general name</p>
<pre><code>&lt;title&gt;$selected_value$&lt;/title&gt;
</code></pre>
<p>But the problem is when I change the name using <code>eval</code>, the drilldown query doesn't get the actual error name and search fails because there is no such error as &quot;Data_error&quot;. The query needs &quot;ID not found&quot; to function.</p>
<p>Is there any way this can be achieved? Can I change the name of my searchTerm and at the same time use the old searchTerm in drilldown query as well?</p>",,1,0,,2021-9-5 19:55:20,,2021-9-10 20:59:42,2021-9-9 12:25:12,,4418.0,,14387037.0,,1,0,regex|splunk|splunk-query|splunk-calculation|splunk-dashboard,40,6
1081,259537,69122499,Splunk: docker-compose starts but web GUI is not available,"<p>I have a docker-compose.yml to run <strong>splunk</strong> and <strong>splunkforwarder</strong> containers.</p>
<p>This docker-compose start OK, but when I try to access http://localhost:8000, a got error: This site can’t be reached.</p>
<p>My current system is <strong>Linux Ubuntu</strong>.</p>
<p>BTW: I have anothers docker containers working perfect in my system.</p>
<p>Please see my docker-compose below:</p>
<pre><code>version: &quot;3.3&quot;
services:
  splunk:
    image: splunk/splunk
    container_name: splunk
    environment:
      - SPLUNK_START_ARGS=--accept-license
      - SPLUNK_USER=root
      - SPLUNK_PASSWORD=Te0k324ja#
      - SPLUNK_ENABLE_LISTEN=9997
    ports:
      - 8000:8000

  splunkforwarder:
    image: splunk/universalforwarder:latest
    container_name: splunkforwarder
    environment:
      - SPLUNK_START_ARGS=--accept-license --answer-yes
      - SPLUNK_FORWARD_SERVER=splunk:9997
      - SPLUNK_USER=root
      - SPLUNK_PASSWORD=Te0k324ja#
      - SPLUNK_ADD=monitor /logs
    restart: always
    depends_on:
      - splunk
    volumes:
      - log_volume:/logs

volumes:
  log_volume:
</code></pre>
<hr />
<p>EDIT:
After around 10 minutes the splunks begins to listen at port 8000. Could someone test my docker-compose file, too see if works at another system?</p>
<p>In the splunk GUI I got the following RED alert:</p>
<p><a href=""https://i.stack.imgur.com/nYQfE.png"" rel=""nofollow noreferrer"">Screenshot of the splunk alert</a>IOWait
Root Cause(s):
System iowait reached red threshold of 3
Maximum per-cpu iowait reached red threshold of 10
Sum of 3 highest per-cpu iowaits reached red threshold of 15</p>
<p>Thanks in advance!</p>",,2,1,,2021-9-9 17:31:16,2.0,2021-11-21 01:21:27,2021-9-13 19:04:12,,14708449.0,,14708449.0,,1,1,docker|docker-compose|splunk,133,8
1082,259538,69157721,Regex - replacing part of a matched group possible?,"<p>Is it possible to replace half or x characters of a matched group?</p>
<p>I have had a request for a partial email capturing, so something like <code>example123@abcdef.com</code> becomes <code>***mple123@***def.com</code></p>
<p>I can do this if the characters before and after the @ are 3 characters long,</p>
<p><code>([^|@]{0,3})([^]{0,3})</code></p>
<p>This captures <code>123@abc.com</code> perfectly and I can substitute for <code>***@***.com</code> but if it's over 3 characters long on each end, for instance <code>example123@abcdef.com</code> it becomes <code>******e123@abcdef.com</code></p>
<p>The other way I can see is capture everything until the <code>@</code> and everything to the <code>.</code> but then this won't be a partial capture. Is this possible?</p>",69158591.0,1,9,,2021-9-13 05:34:22,,2021-9-13 07:04:34,,,,,16395273.0,,1,3,sed|splunk,63,8
1083,259539,69159662,Gridlines splunk statistical table,<p>I have splunk statistical table dashboard where I need gridlines for that table which is not available in splunk like in excel</p>,,1,2,,2021-9-13 08:37:36,,2021-9-15 17:47:29,,,,,16134665.0,,1,0,dashboard|splunk|gridlines,34,6
1084,259540,69168619,Splunk Kubernetes example,"<p>I found this entry
<a href=""https://stackoverflow.com/questions/61129598/send-kubernetes-pods-logs-to-splunk"">Send Kubernetes pod&#39;s logs to Splunk</a></p>
<p>I was wondering if anyone had a front-to-back example (blog, video, etc.).</p>
<p>I'm used to Kubernetes, but not used to Splunk. I want to deploy something like the <a href=""https://fluentbit.io/"" rel=""nofollow noreferrer"">fluentbit</a> with <a href=""https://www.fluentd.org/"" rel=""nofollow noreferrer"">fluentd</a> solution or <a href=""https://github.com/splunk/splunk-connect-for-kubernetes"" rel=""nofollow noreferrer"">Splunk Connect for Kubnernetes</a>. I've done this with other solutions pretty easily (they usually have something simple to deploy into K8s), but working with Splunk is giving me problems.</p>
<p>I'm covering three scenarios here:</p>
<ol>
<li>Some things log only to stdout, which fluentbit/fluentd should handle</li>
<li>Some things log only to file</li>
<li>I'd like to collect all the Kubernetes events</li>
</ol>
<p>Right now I'm mostly worried about sceario 1, but I have no control over the pods/containers, the configuration, etc. I have to find a way to do that without changing the Helm charts that are being used for deployment. I put in change requests for #2 to at least get the data into stdout, but that's another battle.</p>
<p>I've tried a bunch of things (links below) but it's just not behaving for me. I was hoping to follow a tutorial step-by-step to get a POC done and then get in an customize it further.</p>
<p>Some stuff I've been reading/watching:</p>
<ul>
<li><strong>Most useful one so far:</strong> <a href=""https://computingforgeeks.com/send-logs-to-splunk-using-splunk-forwarder/"" rel=""nofollow noreferrer"">Send Logs to Splunk on Kubernetes using Splunk Forwarder</a> - gets me to the point where I understand I can use the <a href=""https://hub.docker.com/r/splunk/universalforwarder"" rel=""nofollow noreferrer"">splunk/universalforwarder:latest</a> image to get logs flowing to my Splunk instance</li>
<li><strong>Deploy Splunk Enterprise on Kubernetes</strong>: <a href=""https://www.splunk.com/en_us/blog/platform/deploy-splunk-enterprise-on-kubernetes-splunk-connect-for-kubernetes-and-splunk-insights-for-containers-beta-part-1.html"" rel=""nofollow noreferrer"">1</a>/<a href=""https://www.splunk.com/en_us/blog/platform/deploy-splunk-enterprise-on-kubernetes-splunk-connect-for-kubernetes-and-splunk-insights-for-containers-beta-part-2.html"" rel=""nofollow noreferrer"">2</a>/<a href=""https://www.splunk.com/en_us/blog/it/deploy-splunk-enterprise-on-kubernetes-splunk-connect-for-kubernetes-and-splunk-insights-for-containers-beta-part-3.html"" rel=""nofollow noreferrer"">3</a> - which seemed to get me running my own host and local data just fine, but not what I need</li>
<li><strong>Videos from Splunk User Groups</strong>: <a href=""https://www.youtube.com/watch?v=BZgGXk2a5d4"" rel=""nofollow noreferrer"">1</a>/<a href=""https://www.youtube.com/watch?v=V2NH1K8Lh8s"" rel=""nofollow noreferrer"">2</a> - again, got Splunk running but doesn't give insight on forwarding data from Kuberenetes</li>
<li><strong>Splunk Forum Posts</strong>: <a href=""https://community.splunk.com/t5/Getting-Data-In/How-to-index-Kubernetes-STDOUT-data-in-Splunk/m-p/222592"" rel=""nofollow noreferrer"">1</a> - led me to the next one</li>
<li><strong>Splunk Official Git Hub Projects:</strong> <a href=""https://github.com/splunk/splunk-connect-for-kubernetes"" rel=""nofollow noreferrer"">Splunk Connect for Kubnernetes</a> - which I'm just not following for some reason ... but this seems like what I really should be using here</li>
</ul>
<p>I really think I just need something with more verbose, step-by-step instructions written so a toddler can follow them.</p>
<p>I am going to try <a href=""https://github.com/splunk/splunk-connect-for-kubernetes"" rel=""nofollow noreferrer"">Splunk Connect for Kubnernetes</a> from scratch again to see if I can figure it out; however - any links to tutorials would be very helpful.</p>
<p>Unfortunately I don't have time to learn Splunk inside-and-out, nor budget to get someone else in to do this.</p>",,0,1,,2021-9-13 20:13:53,,2021-9-13 20:13:53,,,,,2956829.0,,1,0,kubernetes|splunk|fluentd|fluent-bit,32,6
1085,259541,69198174,how to query splunk when we need to get the output base do three 5 min interval,"<p>I am looking to set up an alert. following is a sample query
Sample query: <code>Sourcetype=XXX index=XXX ERROR_Code=&quot;9032&quot;  | bucket _time span=5m  | stats dc(user) as Devices by _time   | eval sev=case('Devices'&gt;9999, &quot;sev1&quot;, 'Devices'&gt;2999 and 'Devices'&lt;=9999, &quot;sev2&quot;, 'Devices'&gt;500 and 'Devices'&lt;=2999, &quot;sev3&quot;)  | eventstats count as VIOLATIONS by sev  | fields _time Devices sev</code></p>
<hr />
<p>Now what i looking to convert the above into an alert format
in such a way that it should run for last 15 min for 5 min interval and if each 5 min interval is &gt; 500(as a value) then it should alert
Expected output is
500,700, 900 &gt;&gt; sev3</p>",,0,0,,2021-9-15 18:26:28,,2021-9-15 18:26:28,,,,,12976099.0,,1,0,splunk-query,6,3
1086,259542,69219964,Can regular expressions be used in Splunk generating commands?,"<p>I created a regular expression that filters results based on a field &quot;id&quot; value:</p>
<p><code>index=some_index | regex id=&quot;222[1-3]{2}00&quot;</code></p>
<p>Unfortunally, this search is executed very long because it first generates huge data volume and then filters it.
Can you tell me whether it is possible to use a regular expression inside generating command to decrease execution time?</p>",69222777.0,1,0,,2021-9-17 08:11:50,,2021-9-20 18:38:30,,,,,13128766.0,,1,0,splunk,20,7
1087,259543,69251613,relative_time expression meaning,"<p>I have a set of data with timestamps: eg, 12.50pm, 1pm, 1.30pm, 1.50pm, 2pm, 2.20pm, 3pm, 3.30pm</p>
<p>At the start of the code, I have split them into timespan: ie</p>
<blockquote>
<p>bucket _time span = 1h</p>
</blockquote>
<p>I have been trying to understand this expression so that I can determine which sign to use in the where clause later, :</p>
<blockquote>
<p>relative_time(max(_time),&quot;@h&quot;)</p>
</blockquote>
<p>What do they mean and how do they differ?</p>
<pre><code>|where _time &lt; relative_time(max(_time),&quot;@h&quot;)
|where _time = relative_time(max(_time),&quot;@h&quot;)
|where _time &gt; relative_time(max(_time),&quot;@h&quot;)
</code></pre>",,1,0,,2021-9-20 08:54:52,,2021-9-20 13:16:45,,,,,16783738.0,,1,0,splunk|splunk-query,33,7
1088,259544,69275756,Questions related to splunk builtin macros in correlation search,"<p>I am not sure if this is the appropriate forum to ask this question, but really need help and I am stuck. So here goes : I am exploring splunk enterprise security and was specifically looking into analytic stories and correlation searches.
For example :
Analytic story : Trickbot
<br>Correlation search : Attempt to stop security service</p>
<pre><code>| tstats `security_content_summariesonly` values(Processes.process) as process min(_time) as firstTime max(_time) as lastTime from datamodel=Endpoint.Processes where (Processes.process_name = net.exe OR  Processes.process_name = sc.exe) Processes.process=&quot;* stop *&quot; by Processes.dest Processes.user Processes.parent_process Processes.process_name Processes.process Processes.process_id Processes.parent_process_id 
| `drop_dm_object_name(Processes)` 
| `security_content_ctime(firstTime)` 
| `security_content_ctime(lastTime)` 
|lookup security_services_lookup service as process OUTPUTNEW category, description 
| search category=security 
| `attempt_to_stop_security_service_filter`
</code></pre>
<p>I am trying to understand what exactly this code is doing, but stuck at these macros like <code>security_content_summariesonly, drop_dm_object_name, security_content_ctime, attempt_to_stop_security_service_filter</code>. I can't find definitions for these macros anywhere. I have tried to look into -&gt; settings -&gt; advance search -&gt; macros, but these are not listed there.
<br>Can somebody help ?</p>",69287600.0,2,0,,2021-9-21 21:29:01,,2021-9-22 17:27:38,,,,,1337338.0,,1,0,splunk|splunk-query,27,7
1089,259545,69280696,SplunkJS error using sankey diagram from google charts,"<p>I'm trying to include in a splunkjs file the sankey chart visualization provided by google charts. I already achieve a similar result with the org chart (another google charts visualization).
I download the loader.js file and insert it at the head of the dashboard xml together with my splunkjs javascript file.</p>
<p>Everything works fine with orgchart:</p>
<p><a href=""https://i.stack.imgur.com/cLFyd.png"" rel=""nofollow noreferrer"">org chart</a></p>
<p>but when i do the same with sankey chart i got this error:
d3.sankey is not a function</p>
<p><a href=""https://i.stack.imgur.com/LE5es.png"" rel=""nofollow noreferrer"">sankeychart error</a></p>
<p>This is what i have in dashboard tag</p>
<pre><code>&lt;dashboard script=&quot;sankey_diagram_v_001.js, loader.js&quot;&gt;
  &lt;label&gt;Sankey Diagram&lt;/label&gt;
  &lt;description&gt;&lt;/description&gt;
  ...
&lt;/dashboard&gt;
</code></pre>
<p>both js files are in myapp/appserver/static folder.
and that is the require section of my splunkjs file</p>
<pre><code>require([
       'underscore',
       'jquery',
       'splunkjs/mvc',
       'splunkjs/mvc/tableview',
       'splunkjs/mvc/searchmanager',
       'splunkjs/mvc/postprocessmanager',
       'splunkjs/mvc/simplexml/ready!'
   ],function(_, $, mvc, TableView,SearchManager,PostProcessManager) {

...

});
</code></pre>
<p>How can i render properly the sankey diagram without that error?</p>",,0,0,,2021-9-22 08:29:01,,2021-9-22 08:29:01,,,,,8610043.0,,1,0,google-visualization|splunk|splunk-dashboard,34,6
1090,259546,69288426,Field colors splunk,"<p>how to make charting.fieldcolors permanent in splunk default.
I need to place charting.chart.fieldcolors in splunk XML for every dashboard can we have it as default without manually inserting by making changes in CSS</p>",,0,0,,2021-9-22 17:02:06,,2021-9-22 17:02:06,,,,,16134665.0,,1,0,xml|customization|splunk,16,4
1091,259547,69288768,Splunk - How to extract two fileds distinct count one field by the other field?,"<p>I have such events:</p>
<pre><code>Id&quot;:&quot;123456&quot;,&quot;string&quot;,&quot;groupId&quot;:&quot;AB123&quot;}]
</code></pre>
<p>I want to extract the fields <code>Id</code>, i.e. 123456 and <code>groupId</code>, i.e. <code>AB123</code>.</p>
<p>I tried this:</p>
<pre><code>query 
| rex field=_raw &quot;Id\&quot;:\&quot;(?&lt;Id&gt;\d+)\&quot;.+groupId\W+(?&lt;groupId&gt;\w+)&quot;
| timechart partial=f span=10m dc(Id) by groupId
</code></pre>
<p>It did not count anything.</p>
<p>What did I do wrong?</p>",69290709.0,1,1,,2021-9-22 17:26:11,,2021-9-22 20:36:01,2021-9-22 20:36:01,,11572712.0,,11572712.0,,1,0,regex|splunk|splunk-query,20,6
1092,259548,69293737,splunk sort events by size column in the log event,"<p>Using below query to get the list of all messages having &quot;large partition&quot; keyword.</p>
<pre><code>index=&quot;*-mycass-db&quot; &quot;large partition&quot;
</code></pre>
<p>Gets me tons of below events, want to find events in the descending order of the size of the table (100.803MiB in this example), am ok with getting the event with the biggest size. How to sort events by size in this message? Is it possible at all? Sorry I am not much familiar with Splunk queries.</p>
<pre><code>WARN  [CompactionExecutor:111575] 2021-09-22 19:49:47,738  BigTableWriter.java:211 - Writing large partition keyspacename/tablename:xxxxxxx-yyyyy-zzzz-b6d4-1f4d3893e104:DOMAINDATA:REALTIME_EVENT_DATA (100.803MiB) to sstable /data/cassandra/data/keyspacename/tablename-aaaaaaaaaaaaaaabbbbbbbbbb/mc-17858-big-Data.db
host = myhost.mydomain source = /data/cassandra/logs/system.logsourcetype = cassandra:cluster:system
</code></pre>",69300720.0,1,0,,2021-9-23 03:42:16,,2021-9-23 13:22:43,2021-9-23 04:03:30,,3904501.0,,3904501.0,,1,3,splunk,34,7
1093,259549,69294371,Extract data from splunk,"<p>I have a Post query where I want to extract request payload or parameters and print a table. In the query, I am trying to extract the user_search <strong>name</strong> field</p>
<p>I have written a Splunk query but it is not working for me</p>
<pre><code>&quot;Parameters: {\&quot;user_search\&quot;=&gt;{\&quot;name\&quot;=&gt;*&quot;  | rex  field=_raw &quot;/\&quot;user_search\&quot;=&gt;{\&quot;name\&quot;=&gt;/(?&lt;result&gt;.*)&quot;  | table result
</code></pre>
<p><strong>Splunk Data</strong></p>
<pre><code>I, [2021-09-23T00:46:31.172197 #44154]  INFO -- : [651235bf-7ad5-4a2e-a3b8-7737a3af9fc3]   Parameters: {&quot;user_search&quot;=&gt;{&quot;name&quot;=&gt;&quot;aniket&quot;, &quot;has_primary_phone&quot;=&gt;&quot;false&quot;, &quot;query_params&quot;=&gt;{&quot;searchString&quot;=&gt;&quot;&quot;, &quot;start&quot;=&gt;&quot;0&quot;, &quot;filters&quot;=&gt;[&quot;&quot;]}}}
host = qa-1132-lx02source = /src/project.logsourcetype = data:log

I, [2021-09-23T00:48:31.162197 #44154]  INFO -- : [651235bf-7ad5-4a2e-a3b8-7737a3af9fc3]   Parameters: {&quot;user_search&quot;=&gt;{&quot;name&quot;=&gt;&quot;shivam&quot;, &quot;has_primary_phone&quot;=&gt;&quot;false&quot;, &quot;query_params&quot;=&gt;{&quot;searchString&quot;=&gt;&quot;&quot;, &quot;start&quot;=&gt;&quot;0&quot;, &quot;filters&quot;=&gt;[&quot;&quot;]}}}
host = qa-1132-lx02source = /src/project.logsourcetype = data:log

I, [2021-09-23T00:52:27.171197 #44154]  INFO -- : [651235bf-7ad5-4a2e-a3b8-7737a3af9fc3]   Parameters: {&quot;user_search&quot;=&gt;{&quot;name&quot;=&gt;&quot;tiwari&quot;, &quot;has_primary_phone&quot;=&gt;&quot;false&quot;, &quot;query_params&quot;=&gt;{&quot;searchString&quot;=&gt;&quot;&quot;, &quot;start&quot;=&gt;&quot;0&quot;, &quot;filters&quot;=&gt;[&quot;&quot;]}}}
host = qa-1132-lx02source = /src/project.logsourcetype = data:log
</code></pre>
<p>I have 2 questions</p>
<ol>
<li>How to write a splunk query to extract request payload in post query</li>
<li>In my above query I am not not sure what I am doing wrong. I would really appreciate if someone has any suggestion.</li>
</ol>",69304276.0,1,0,,2021-9-23 05:15:48,,2021-9-27 16:03:29,2021-9-23 05:40:24,,5787849.0,,5787849.0,,1,0,splunk|splunk-query,45,7
1094,259550,69310472,Alert in splunk based on remediation condition,"<p>I am trying to create an alert in splunk such that if there is a expression &quot;Error occured due to connection&quot; present in logs and if this is not remediated automatically after the 5 min it should generate an alert.</p>
<p>Here remediation can be if the &quot;Error occured due to connection&quot; doesnot occur in next five minutes after the alert is generated, it means issue is fixed.
Is this possible? pls guide.</p>",,2,0,,2021-9-24 06:27:28,,2021-9-28 23:54:16,,,,,9186499.0,,1,0,amazon-web-services|monitoring|amazon-eks|splunk|splunk-query,27,6
1095,259551,69322312,How do I transform array in search or elsewhere in dashboard,"<p>I have a search that is working fine</p>
<pre><code>index=event_db environment=prod release = 2020150015 
| timechart count as Events
</code></pre>
<p>However, I'd like to modify this to search for any release in an array of releases. I'm aware of the &quot;in&quot; operator.</p>
<p>The catch is that the array of releases I've been provided (&quot;Releases&quot;) is formatted slightly differently like so:</p>
<pre><code>[ver2020.15.0015, ver2020.15.0016, ver2020.22.0019]  // in general, many more than 3!
</code></pre>
<p>Is there a way to use the in operator and some mapping to get
release in</p>
<pre><code>[2020150015, 2020150016, 2020220019] ?
</code></pre>
<p>Can this be put in the search?</p>
<p>This is part of a panel so if it's simpler I could have code elsewhere to convert <code>[ver2020.15.0015, ver2020.15.0016, ver2020.22.0019]</code> into <code>[2020150015, 2020150016, 2020220019]</code></p>
<p>However, as mentioned I'm a newbie so my knowledge of where to put code to transform an array is limited :)</p>
<p>I have a fieldset section and a panel with a query in it.</p>
<p>The &quot;Releases&quot; array is populated in the fieldset section as so:</p>
<pre><code>&lt;input type=&quot;text&quot; token=&quot;Releases&quot;&gt;
  &lt;label&gt;Release or Releases&lt;/label&gt;
  &lt;default&gt;*&lt;/default&gt;
&lt;/input&gt;
</code></pre>
<p>The user enters ver2020.15.0015 or perhaps ver2020.15.*.</p>
<p>I can't just have the user enter 2020150015 as the ver2020.15.0015 format is used elsewhere.</p>
<p>Perhaps there's a way to create new field <code>Releases_Alt</code> right after getting this?</p>
<p>Let me know of any other info I can provide. As I said, I'm new to Splunk so I'm still struggling with terminology.</p>",69339022.0,1,8,,2021-9-25 00:46:21,,2021-9-27 16:50:29,2021-9-27 16:50:29,,4418.0,,286641.0,,1,0,splunk|splunk-query|splunk-dashboard,36,7
1096,259552,69326096,Splunk search Formatting,"<p>my search query checks for the last 15m for each 5min interval
Sample query:</p>
<pre><code>index=XXXX sourcetype=XXX* env=XXX OR env=XXX &quot;Continuation timed out&quot;
| bucket _time span=5m 
| timechart span=5m count AS Devices 
| eval inc_severity=case('Device'&gt;=450, &quot;3&quot;) 
| eval support_group=case('Device'&gt;=450, &quot;XXXXX&quot;) 
| eval dedup_tag=case('Device'&gt;=450, &quot;XXXXXX&quot;) 
| eval corr_tag=case('Devices'&gt;=450, &quot;XXXXXX&quot;) 
| eval event_status=case('Device'&gt;=450, &quot;1&quot;) 
| eval service_condition=case('Device'&gt;=450, &quot;1&quot;) 
| table sev event dedup corr support_group service_condition _time Devices
| sort 3 - Devices
| sort _time
| where isnotnull('inc_severity')
| where 'Devices'&gt;450
</code></pre>
<p>based on above query my output is as follows</p>
<pre><code>sev  event  dedup   corr    support_group   service_condition   _time    Device
3     1     xxx     xxx          xxx              1               x       700
3     1     xxx     xxx          xxx              1               y       900
3     1    xxx     xxx          xxx              1               z       1000
</code></pre>
<hr />
<p>but what i am trying to get the output as follows</p>
<pre><code>sev    event    dedup   corr    support_group   service_condition   _time    Device
3      1         xxx   xxx      xxx               1                x,y,z   700,900,1000
</code></pre>",,0,0,,2021-9-25 12:21:57,,2021-9-30 11:48:34,2021-9-30 11:48:34,,12976099.0,,12976099.0,,1,0,splunk|splunk-query,15,4
1097,259553,69340156,Alerting in splunk for different thresholds,<p>Can we trigger alerts in splunk for different values of threshold. I mean if threshold value is 90 then it should trigger an alert for slack and if the threshold is some other value like 120 then it should trigger some email alert. Is such a condition possible in splunk using a single alert. If yes then how?</p>,,1,0,,2021-9-27 01:06:15,,2021-9-27 11:30:04,,,,,9186499.0,,1,0,amazon-web-services|monitoring|dashboard|splunk|splunk-query,25,6
1098,259554,69356894,Splunk - How to count occurrences of a string by an extracted field?,"<p>I have such log-entries:</p>
<p><em>First entry:</em></p>
<pre><code>&quot;abc&quot;,&quot;Id&quot;:&quot;XYZ12&quot;},{&quot;lat&quot;:55},{&quot;lat&quot;:45}{&quot;lat&quot;:59}
</code></pre>
<p><em>Second entry:</em></p>
<pre><code>&quot;abc&quot;,&quot;Id&quot;:&quot;YZA56&quot;},{&quot;lat&quot;:23},{&quot;lat&quot;:101}
</code></pre>
<p>What I now want to get is the number of occurences of string <code>&quot;lat&quot;</code> per <code>Id</code>.</p>
<p>So in the end I would like to get a statistics like this one:</p>
<pre><code>Id occurences
XYZ12 3
YZA56 2
</code></pre>
<p>How can I do this in Splunk? I would know how to get the <code>Id</code> and then count all the events by this <code>Id</code>. But I do not know what to do when I want to do the exercise upon..</p>
<p>Can someone support here?</p>",69363410.0,1,0,,2021-9-28 06:41:15,,2021-9-28 14:15:59,2021-9-28 09:00:34,,11572712.0,,11572712.0,,1,0,regex|splunk,99,10
1099,259555,69359575,Count and sum in splunk,"<p>I have this sets of data:</p>
<pre><code>name    fruit   location

mary    apple   east

ben pear    east

peter   pear    east

ben apple   north

ben mango   north

peter   mango   north

mary    orange  north

alice   pear    north

janet   pear    north

janet   mango   west

janet   mango   west

peter   mango   west

janet   pear    west
</code></pre>
<p>I want to get fields:
name, number of fruits sent to name, number of fruits sent to name in location</p>
<p>I tried:</p>
<blockquote>
<p>|stats sum(count) as scount_by_name by name</p>
<p>|stats count as count_by_namelocation (......filled with other formulas......) by name location</p>
<p>|Table count_by_namelocation scount_by_name</p>
</blockquote>
<p>But it does not work, scount_by_name is empty, what's the correct syntax for this?</p>",69364812.0,1,0,,2021-9-28 09:55:07,,2021-9-28 15:48:14,,,,,16783738.0,,1,1,splunk|splunk-query,96,9
1100,259556,69364859,How to configure default splunk-kubernetes-logging rules?,"<p>I'm trying to configure multiline logging in Splunk for Kubernetes pods (I want stack traces to show up in a single event, rather than be split in multiple events per line).</p>
<p>I've had some success by configuring our <a href=""https://github.com/splunk/splunk-connect-for-kubernetes/blob/develop/helm-chart/splunk-connect-for-kubernetes/charts/splunk-kubernetes-logging/values.yaml"" rel=""nofollow noreferrer"">splunk-kubernetes-logging values.yaml</a> like this:</p>
<pre class=""lang-yaml prettyprint-override""><code>my-app:
  from:
    pod: &quot;my-app-*&quot;
  multiline: 
    firstline: /^\d{4}\-\d{2}\-\d{2} \d{2}\:\d{2}\:\d{2}[\.\,]\d{3}/
</code></pre>
<p>That worked, but I don't want to have to specify a section in the yaml every time we deploy an app. So I was hoping there might be a way to have an overridable default, and to that end I tried this:</p>
<pre class=""lang-yaml prettyprint-override""><code>default-app:
  from:
    pod: &quot;*&quot;
  multiline: 
    firstline: /^\d{4}\-\d{2}\-\d{2} \d{2}\:\d{2}\:\d{2}[\.\,]\d{3}/
</code></pre>
<p>But that had no effect. I tried some other wildcards for the pod name (like <code>&quot;*-*&quot;</code>) but nothing has worked. I don't understand why some wildcards work (<code>&quot;my-app-*&quot;</code>) but others don't (<code>&quot;*&quot;</code>). Is there a way to achieve what I'm trying to achieve?</p>",,0,0,,2021-9-28 15:51:23,,2021-9-28 16:04:55,2021-9-28 16:04:55,,1755598.0,,546561.0,,1,0,kubernetes|splunk,24,5
1101,259557,69374604,"Rancher 2.6.0: error=""this plugin 'Fluent::Plugin::SplunkHecOutput' cannot handle arguments for <buffer ...> section""","<p>We are using the below tech stack
rancher 2.6.0
K8s cluster created using rancher: v1.21.5
Separate Splunk container: Splunk Enterprise (Version: 8.2.2)</p>
<p>I installed a logging chart and configured the output/cluster output to Splunk using its token, index, and source. however, the fluentd-configcheck pods in rancher are failing because of below error.</p>
<pre><code>fluentd -c /fluentd/etc/fluent.conf --dry-run
2021-09-29 08:10:12 +0000 [info]: parsing config file is succeeded path=&quot;/fluentd/etc/fluent.conf&quot;
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-mixin-config-placeholders' version '0.4.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-aws-elasticsearch-service' version '2.4.1'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-azure-storage-append-blob' version '0.2.1'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-cloudwatch-logs' version '0.14.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-concat' version '2.5.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-datadog' version '0.13.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-dedot_filter' version '1.0.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-detect-exceptions' version '0.0.13'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-elasticsearch' version '5.0.4'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-enhance-k8s-metadata' version '2.0.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-gcs' version '0.4.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-gelf-hs' version '1.0.8'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-geoip' version '1.3.2'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-grafana-loki' version '1.2.16'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-kafka' version '0.16.3'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-kinesis' version '3.4.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-kubernetes-metadata-filter' version '2.5.3'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-kubernetes-sumologic' version '2.0.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-label-router' version '0.2.8'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-logdna' version '0.4.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-logzio' version '0.0.21'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-multi-format-parser' version '1.0.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-newrelic' version '1.2.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-oss' version '0.0.2'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-parser-logfmt' version '0.0.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-prometheus' version '2.0.1'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-record-modifier' version '2.1.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-redis' version '0.3.5'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-remote-syslog' version '1.1'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-rewrite-tag-filter' version '2.4.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-s3' version '1.6.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-splunk-hec' version '1.2.5'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-sumologic_output' version '1.7.2'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-syslog_rfc5424' version '0.9.0.rc.7'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-tag-normaliser' version '0.1.1'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-throttle' version '0.0.5'
2021-09-29 08:10:12 +0000 [info]: gem 'fluent-plugin-webhdfs' version '1.4.0'
2021-09-29 08:10:12 +0000 [info]: gem 'fluentd' version '1.12.4'
2021-09-29 08:10:12 +0000 [info]: starting fluentd-1.12.4 as dry run mode ruby=&quot;2.7.3&quot;
2021-09-29 08:10:12 +0000 [error]: config error file=&quot;/fluentd/etc/fluent.conf&quot; error_class=Fluent::ConfigError error=&quot;this plugin 'Fluent::Plugin::SplunkHecOutput' cannot handle arguments for &lt;buffer ...&gt; section&quot; 
</code></pre>
<p>Any help will be appreciated...</p>
<p>Below the generated config files</p>
<pre><code>devnull.conf ---------
&lt;label @ERROR&gt;
&lt;match **&gt;
    @type null
    @id main-fluentd-error
&lt;/match&gt;
&lt;/label&gt;

&lt;match **&gt;
    @type null
    @id main-no-output
&lt;/match&gt;

---------------------------
fluent.conf -------------------------

# include other config files
@include /fluentd/etc/input.conf
@include /fluentd/etc/generated.conf
@include /fluentd/etc/devnull.conf
@include /fluentd/etc/fluentlog.conf

----------------------------------------
generated.conf 
&lt;source&gt;
  @type forward
  @id main_forward
  bind 0.0.0.0
  port 24240
&lt;/source&gt;
&lt;match **&gt;
  @type label_router
  @id main
  metrics false
  &lt;route&gt;
    @label @d1068204e7ff0cba866d5886a7a10f33
    metrics_labels {&quot;id&quot;:&quot;flow:cattle-logging-system:splunkflow&quot;}
    &lt;match&gt;
      namespaces cattle-logging-system
      negate false
    &lt;/match&gt;
  &lt;/route&gt;
&lt;/match&gt;
&lt;label @d1068204e7ff0cba866d5886a7a10f33&gt;
  &lt;match **&gt;
    @type splunk_hec
    @id flow:cattle-logging-system:splunkflow:output:cattle-logging-system:splunkoutput
    hec_host 172.27.1.119
    hec_port 8088
    hec_token 7376611a-e249-42a8-a57b-0a0bacfadc07
    index devopsci
    insecure_ssl true
    protocol http
    source index-devopsci
    &lt;buffer tag,time&gt;
      @type file
      chunk_limit_size 8MB
      path /buffers/flow:cattle-logging-system:splunkflow:output:cattle-logging-system:splunkoutput.*.buffer
      retry_forever true
      timekey 10m
      timekey_wait 10m
    &lt;/buffer&gt;
  &lt;/match&gt;
&lt;/label&gt;

------------------------------------------
input.conf ------------------

# Enable RPC endpoint (this allows to trigger config reload without restart)
&lt;system&gt;
  rpc_endpoint 127.0.0.1:24444
  log_level info
  workers 1
&lt;/system&gt;

# Prometheus monitoring
-----------------------------------------------
</code></pre>",,0,2,,2021-9-29 10:24:46,,2021-9-29 13:30:35,2021-9-29 13:30:35,,8437681.0,,8437681.0,,1,1,kubernetes|splunk|fluentd|rancher,152,8
1102,259558,69376301,How to monitor data traffic in WAN using splunk?,"<p>My task is actually for self learning, I know the basics of Splunk and how does it work, overall it needs logs then further analysis is future part.</p>
<p>Right now, I am working in a small company without specific firewall device, just the router is there, I am planning to keep real time track of internet speed + data usage by each hosts and similar things, overall say I want to monitor all the network usage in this small WAN environment.</p>
<p>My question is, as splunk need logs then how can I get the log of all the network flow when there is just a router but no external device to create logs, that can be feeded to splunk later?</p>
<p>I tried google but there I find only preset softwares that automatically capture logs and show analysis while I just need logs only.</p>",,1,0,,2021-9-29 12:22:06,,2021-10-4 13:23:22,,,,,17034121.0,,1,0,splunk|splunk-dashboard,33,6
1103,259559,69380033,Adding multiple expressions to single searchmatch in splunk query,"<p>I am trying to add two different expressions in the searchmatch in below query using timechart but its giving me error. Can any one help me out with the same</p>
<p>Its like the count2 should increase if either of expression i.e. Expr2 or Expr3 is seen.</p>
<p>count1 is coming correct and count2 is giving issue as incorrect</p>
<pre><code>index=&quot;abc&quot; sourcetype=&quot;kube:container:abc_app&quot; source=&quot;/var/log/containers/abc-env-*&quot; 
| timechart count(eval(searchmatch(&quot;Expr1&quot;))) as &quot;count1&quot;, count(eval(searchmatch(&quot;Expr2&quot; OR &quot;Expr3&quot;))) as &quot;count2&quot;
</code></pre>",69381836.0,2,1,,2021-9-29 16:14:40,,2021-10-4 13:21:11,2021-10-4 13:19:38,,4418.0,,9186499.0,,1,0,amazon-web-services|monitoring|amazon-eks|splunk|splunk-query,26,6
1104,259560,69385567,Splunk dashboard does not pick the correct timestamp to display data,"<p>I am streaming a list of data with the most recent timestamp, but the data is getting displayed in a different time.  For example:</p>
<pre><code>t=1632967410.582567 devicename=abc Ethernet.dst=### Ethernet.src=### Ethernet.type=65535
t=1632967410.582567 devicename=abc Ethernet.dst=### Ethernet.src=### Ethernet.type=65535
t=1632967410.582567 devicename=abc Ethernet.dst=### Ethernet.src=### Ethernet.type=65535
</code></pre>
<p>The Epoch conversion of the above timestamp (<code>t=1632967410.582567</code>) is <code>7:03:30.582 PM</code> but the data on the dashboard is displayed at time <code>5:19:01.000 PM</code></p>
<p>Background:</p>
<ul>
<li>Data is generated from a python script, the data is a list of events, and each event is printed to stdout</li>
<li>I have tried to include additional line breaks between each event, but it still streams it as a single chunk and displays it in a different timestamp</li>
<li>The version of SUF is 8.2.1 (build - ddff1c41e5cf)</li>
<li>The version of Splunk Enterprise is 8.1.2</li>
</ul>
<p>Can someone guide me on fixing this to print the streamed data in the correct timestamp?</p>",,0,1,,2021-9-30 03:01:04,,2021-9-30 03:01:04,,,,,7456351.0,,1,0,splunk|splunk-query|splunk-dashboard,17,4
1105,259561,69411374,Error while creating a search request using Splunk JS SDK,"<p>For this example:
<a href=""https://github.com/splunk/splunk-sdk-javascript/blob/master/examples/node/helloworld/search_normal.js"" rel=""nofollow noreferrer"">https://github.com/splunk/splunk-sdk-javascript/blob/master/examples/node/helloworld/search_normal.js</a></p>
<p>When I modify line 53.</p>
<pre><code>service.search(&quot;search index=_internal | head 3&quot;, {}, done);
</code></pre>
<p>To this new query</p>
<pre><code>&quot;index=comtech_np sourcetype=comtech_cdv_csv ENodeB=* &quot;PSAP Name&quot;=* &quot;Calling Party Number&quot;=* &quot;Call Start Time&quot;=* &quot;Call End Time&quot;=*&quot;
</code></pre>
<p>I get errors. How would I rewrite this query in node to execute.</p>
<p>The error:</p>
<pre><code>[SPLUNKD] Unknown search command 'index'.
C:\Users\shahrsu\test\s4.js:37
      console.log(&quot;Job SID: &quot;, job.sid);
</code></pre>
<p>When I modify query to:</p>
<pre><code>&quot;ENodeB=* &quot;PSAP Name&quot;=* &quot;Calling Party Number&quot;=* &quot;Call Start Time&quot;=* &quot;Call End Time&quot;=*&quot;
</code></pre>
<p>I get</p>
<pre><code>[SPLUNKD] Unknown search command 'ENodeB'.
C:\Users\shahrsu\test\s4.js:37
      console.log(&quot;Job SID: &quot;, job.sid);
</code></pre>",,0,0,,2021-10-1 19:55:52,,2021-10-4 13:17:35,2021-10-4 13:17:35,,4418.0,,11610246.0,,1,0,javascript|node.js|splunk|splunk-query|splunk-sdk,18,5
1106,259562,69412986,Splunk Concurrency Calculation,"<p>I have some data from logs in Splunk where I need to determine what other requests were running concurrently at the time of any single event.</p>
<p>Using the following query, I was able to have it return a column for the number of requests that ran at the same time within my start time and duration.</p>
<pre><code>index=&quot;sfdc&quot; source=&quot;sfdc_event_log://EventLog_SFDC_Production_eventlog_hourly&quot; EVENT_TYPE IN (API, RestAPI) RUN_TIME&gt;20000 
| eval endTime=_time 
| eval permitTimeInSecs=(RUN_TIME-20000)/1000 
| eval permitAcquiredTime=endTime-permitTimeInSecs
| eval dbTotalTime=DB_TOTAL_TIME/1000000
| concurrency start=permitAcquiredTime duration=permitTimeInSecs 
| table _time API_TYPE EVENT_TYPE ENTITY_NAME apimethod concurrency permitAcquiredTime permitTimeInSecs RUN_TIME CPU_TIME dbtotalTime REQUEST_ID USER_ID
| fieldformat dbTotalTime=round(dbTotalTime,0)
| rename permitAcquiredTime as &quot;Start Time&quot;, permitTimeInSecs as &quot;Concurrency Duration&quot;, concurrency as &quot;Concurrent Running Events&quot;, API_TYPE as &quot;API Type&quot;, EVENT_TYPE as &quot;Event Type&quot;, ENTITY_NAME as &quot;Entity Name&quot;, apimethod as &quot;API Method&quot;, RUN_TIME as &quot;Run Time&quot;, CPU_TIME as &quot;CPU Time&quot;, dbtotalTime as &quot;DB Total Time&quot;, REQUEST_ID as &quot;Request ID&quot;, USER_ID as &quot;User ID&quot;
| sort &quot;Concurrent Running Events&quot; desc
</code></pre>
<p><a href=""https://i.stack.imgur.com/6JZwR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6JZwR.png"" alt=""enter image description here"" /></a></p>
<p>I am now trying to investigate a single event in these results. For example, the top event says that at the time it ran, there were 108 concurrent requests running in the 20 second window of time.</p>
<p><strong>How can I identify those 108 events using this data?</strong></p>
<p>I imagine it would be querying the events that had a specific time frame range, but I am not sure if I need to check something like <code>_time  + - 10 seconds</code> to see what was running within the 20 second window?</p>
<p>Just need to understand the data behind this <code>108 events</code> a little more for this top example. My end goal here is to be able to add a drill-down to the dashboard so that when I click on the <code>108</code>, I can see those events that were running.</p>",,1,0,,2021-10-2 00:03:21,1.0,2021-10-11 19:17:00,2021-10-4 22:19:28,,2628921.0,,2628921.0,,1,2,splunk|splunk-query|splunk-calculation,56,7
1107,259563,69418942,Splunk submit button (submitButton) does not refresh dashboard if no inputs are changed,"<p>I have a dashboard with a submit button (submitButton). The search isn't run until the button is pressed which is exactly what I want (the search takes a long time). I don't want the search starting as the user changes the other dropdowns (time), environment (Prod vs. QA), etc.</p>
<p>HOWEVER, sometimes it would be nice to hit the submit button and perform the search again without changing any of the other fields (time, environment, etc.).  In this case the submit button does nothing! I can tell the underlying data has changed via searches, but the dashboard is not updated. Simply changing any of the fields, doing a search, and then changing them back and searching again solves the problem, but surely the submit button should just work w/o this workaround?</p>
<p>Thanks</p>",69442322.0,3,4,,2021-10-2 17:59:54,,2021-10-5 07:00:45,,,,,286641.0,,1,0,splunk|splunk-query|splunk-sdk|splunk-dashboard,96,7
1108,259564,69432047,Kubernetes Jobs are not Completing due to splunk dependency,"<p>We have the cron job which is running every night 11:30 PM. Now issue is job is not shutting down after completing execution. We have added splunk dependency in our application. The splunk listener doesnt allow to shut down the job. The dependency which we used</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;dependency&gt;
    &lt;groupId&gt;com.splunk.logging&lt;/groupId&gt;
    &lt;artifactId&gt;splunk-library-javalogging&lt;/artifactId&gt;
    &lt;version&gt;1.7.3&lt;/version&gt;
    &lt;scope&gt;runtime&lt;/scope&gt;
&lt;/dependency&gt;.
</code></pre>
<p>We have added splunk appender in logback.xml to check logs in splunk</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;appender name=&quot;SPLUNK&quot; class=&quot;com.splunk.logging.HttpEventCollectorLogbackAppender&quot;&gt;
    &lt;url&gt;https://operations-hec.splunk.tescocloud.com&lt;/url&gt;
    &lt;index&gt;${SPLUNK_INDEX}&lt;/index&gt;
    &lt;source&gt;${SPLUNK_SOURCE}&lt;/source&gt;
    &lt;sourcetype&gt;${SPLUNK_SOURCE_TYPE}&lt;/sourcetype&gt;
    &lt;token&gt;${SPLUNK_TOKEN}&lt;/token&gt;
    &lt;batch_size_count&gt;${SPLUNK_BATCH_SIZE}&lt;/batch_size_count&gt;
    &lt;batch_interval&gt;${SPLUNK_BATCH_BATCH_INTERVALMS}&lt;/batch_interval&gt;
    &lt;disableCertificateValidation&gt;true&lt;/disableCertificateValidation&gt;
    &lt;retries_on_error&gt;3&lt;/retries_on_error&gt;
    &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt;
        &lt;pattern&gt;[%mdc] %msg&lt;/pattern&gt;
    &lt;/layout&gt;
    &lt;messageFormat&gt;json&lt;/messageFormat&gt;
&lt;/appender&gt;

&lt;if condition='&quot;pte&quot;.equals(property(&quot;ENVIRONMENT&quot;))'&gt;
    &lt;then&gt;
        &lt;root level=&quot;WARN&quot;&gt;
            &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt;
        &lt;/root&gt;
    &lt;/then&gt;
    &lt;else&gt;
        &lt;root level=&quot;INFO&quot;&gt;
            &lt;appender-ref ref=&quot;SPLUNK&quot;/&gt;
            &lt;appender-ref ref=&quot;RollingFile&quot;/&gt;
            &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt;
        &lt;/root&gt;
    &lt;/else&gt;
&lt;/if&gt;

</code></pre>
<p>In main class we have added below code to stop the job</p>
<pre class=""lang-java prettyprint-override""><code>@SpringBootApplication
@EnableFeignClients
public class MyBatchJob {

    public static void main(String[] args) {
        var context = SpringApplication.run(MyBatchJob.class, args);

        Optional.of(getILoggerFactory()).filter(LoggerContext.class::isInstance).map(LoggerContext.class::cast)
                .ifPresent(LoggerContext::stop);

        context.close();
    }
}
</code></pre>
<p>Jobs executions are completing but in Kubernetes the status of the job is Running.
<a href=""https://i.stack.imgur.com/iXOOu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iXOOu.png"" alt=""enter image description here"" /></a></p>
<p>But The job status in Kubernetes is in Running status.
<a href=""https://i.stack.imgur.com/CQNlm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CQNlm.png"" alt=""enter image description here"" /></a></p>
<p>There is a Splunk listener who is listening while writing logs to the console, the listener is still open after the job completion, it is still writing to the console. Please help me how to shut down the listener so that the job will be on completed status.</p>",,0,0,,2021-10-4 07:03:01,,2021-10-4 13:32:43,2021-10-4 13:32:43,,3815362.0,,3815362.0,,1,0,spring-boot|kubernetes|cron|microservices|splunk,36,6
1109,259565,69434562,monitoring kafka service on splunk obersvability,"<p>I wanted to monitor kafka service on splunk observability.Can someone help the procedure to do the same.
I have done below things.
1.I installed otel agent on the linux server.
2.Started the otel service.
3. updated the agent_config.yml file for kafka below.but not able to see anything for kafka on splunk.</p>
<h1>Default configuration file for the Linux (deb/rpm) and Windows MSI collector pac</h1>
<pre><code>kages

# If the collector is installed without the Linux/Windows installer script, the following
# environment variables are required to be manually defined or configured below:
# - SPLUNK_ACCESS_TOKEN: The Splunk access token to authenticate requests
# - SPLUNK_API_URL: The Splunk API URL, e.g. https://api.us0.signalfx.com
# - SPLUNK_BUNDLE_DIR: The path to the Smart Agent bundle, e.g. /usr/lib/splunk-otel-collector/agent-bundle
# - SPLUNK_COLLECTD_DIR: The path to the collectd config directory for the Smart Agent, e.g. /usr/lib/splunk-otel-collector/agent-bundle/run/collectd
# - SPLUNK_HEC_TOKEN: The Splunk HEC authentication token
# - SPLUNK_HEC_URL: The Splunk HEC endpoint URL, e.g. https://ingest.us0.signalfx`enter code here`.com/v1/log
# - SPLUNK_INGEST_URL: The Splunk ingest URL, e.g. https://ingest.us0.signalfx.com
# - SPLUNK_TRACE_URL: The Splunk trace endpoint URL, e.g. https://ingest.us0.signalfx.com/v2/trace

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
  http_forwarder:
    ingress:
      endpoint: 0.0.0.0:6060
    egress:
      endpoint: &quot;${SPLUNK_API_URL}&quot;
      # Use instead when sending to gateway
      #endpoint: &quot;${SPLUNK_GATEWAY_URL}&quot;
  smartagent:
    bundleDir: &quot;${SPLUNK_BUNDLE_DIR}&quot;
    collectd:
      configDir: &quot;${SPLUNK_COLLECTD_DIR}&quot;
  zpages:
    #endpoint: 0.0.0.0:55679
  memory_ballast:
    # In general, the ballast should be set to 1/3 of the collector's memory, the limit
    # should be 90% of the collector's memory.
    # The simplest way to specify the ballast size is set the value of SPLUNK_BALLAST_SIZE_MIB env variable.
    size_mib: ${SPLUNK_BALLAST_SIZE_MIB}

receivers:
  fluentforward:
    endpoint: 127.0.0.1:8006
  hostmetrics:
    collection_interval: 10s
    scrapers:
      cpu:
      disk:
      filesystem:
      memory:
      network:
      # System load average metrics https://en.wikipedia.org/wiki/Load_(computing)
      load:
      # Paging/Swap space utilization and I/O metrics
      paging:
      # Aggregated system process count metrics
      processes:
      # System processes metrics, disabled by default
      # process:
  jaeger:
    protocols:
      grpc:
        endpoint: 0.0.0.0:14250
      thrift_binary:
        endpoint: 0.0.0.0:6832
      thrift_compact:
        endpoint: 0.0.0.0:6831
      thrift_http:
        endpoint: 0.0.0.0:14268
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:55681
  # This section is used to collect the OpenTelemetry Collector metrics
  # Even if just a Splunk APM customer, these metrics are included
  prometheus/internal:
    config:
      scrape_configs:
      - job_name: 'otel-collector'
        scrape_interval: 10s
        static_configs:
        - targets: ['0.0.0.0:8888']
        metric_relabel_configs:
          - source_labels: [ __name__ ]
            regex: '.*grpc_io.*'
            action: drop
  smartagent/signalfx-forwarder:
    type: signalfx-forwarder
    listenAddress: 0.0.0.0:9080
  signalfx:
    endpoint: 0.0.0.0:9943
  zipkin:
    endpoint: 0.0.0.0:9411
  smartagent/kafka:
    type: collectd/kafka
  smartagent/haproxy:
    type: haproxy
  smartagent/mysql:
    type: collectd/mysql
  smartagent/docker:
    type: docker-container-stats
processors:
  batch:
  # Enabling the memory_limiter is strongly recommended for every pipeline.
  # Configuration is based on the amount of memory allocated to the collector.
  # For more information about memory limiter, see
  # https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiter/README.md
  memory_limiter:
    check_interval: 2s
    limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}
  # detect if the collector is running on a cloud system
  # important for creating unique cloud provider dimensions
  resourcedetection:
    detectors: [system, gce, ecs, ec2, azure]
    override: false

  # Same as above but overrides resource attributes set by receivers
  resourcedetection/internal:
    detectors: [system, gce, ecs, ec2, azure]
    override: true

  # Optional: The following processor can be used to add a default &quot;deployment.environment&quot; attribute to the logs and
  # traces when it's not populated by instrumentation libraries.
  # If enabled, make sure to enable this processor in the pipeline below.
  #resource/add_environment:
    #attributes:
      #- action: insert
        #value: staging/production/...
        #key: deployment.environment

exporters:
  # Traces
  sapm:
    access_token: &quot;${SPLUNK_ACCESS_TOKEN}&quot;
    endpoint: &quot;${SPLUNK_TRACE_URL}&quot;
  # Metrics + Events
  signalfx:
    access_token: &quot;${SPLUNK_ACCESS_TOKEN}&quot;
    api_url: &quot;${SPLUNK_API_URL}&quot;
    ingest_url: &quot;${SPLUNK_INGEST_URL}&quot;
    # Use instead when sending to gateway
    #api_url: http://${SPLUNK_GATEWAY_URL}:6060
    #ingest_url: http://${SPLUNK_GATEWAY_URL}:9943
    sync_host_metadata: true
    correlation:
  # Logs
  splunk_hec:
    token: &quot;${SPLUNK_HEC_TOKEN}&quot;
    endpoint: &quot;${SPLUNK_HEC_URL}&quot;
    source: &quot;otel&quot;
    sourcetype: &quot;otel&quot;
  # Send to gateway
  otlp:
    endpoint: &quot;${SPLUNK_GATEWAY_URL}:4317&quot;
    tls:
      insecure: true
  # Debug
  logging:
    loglevel: debug

service:
  extensions: [health_check, http_forwarder, zpages, memory_ballast]
  pipelines:
    traces:
      receivers: [jaeger, otlp, smartagent/signalfx-forwarder, zipkin]
      processors:
      - memory_limiter
      - batch
      - resourcedetection
      #- resource/add_environment
      exporters: [sapm, signalfx]
      # Use instead when sending to gateway
      #exporters: [otlp, signalfx]
    metrics:
      receivers: [hostmetrics, otlp, signalfx, smartagent/signalfx-forwarder]
      processors: [memory_limiter, batch, resourcedetection]
      exporters: [signalfx]
      # Use instead when sending to gateway
      #exporters: [otlp]
    metrics/internal:
      receivers: [prometheus/internal]
      processors: [memory_limiter, batch, resourcedetection/internal]
      exporters: [signalfx]
      # Use instead when sending to gateway
      #exporters: [otlp]
    logs/signalfx:
      receivers: [signalfx]
      processors: [memory_limiter, batch]
      exporters: [signalfx]
      # Use instead when sending to gateway
      #exporters: [otlp]
    logs:
      receivers: [fluentforward, otlp]
      processors:
      - memory_limiter
      - batch
      - resourcedetection
      #- resource/add_environment
      exporters: [splunk_hec]
      # Use instead when sending to gateway
      #exporters: [otlp]
</code></pre>",,0,1,,2021-10-4 10:36:29,,2021-10-4 10:36:29,,,,,17028759.0,,1,0,apache-kafka|splunk,31,5
1110,259566,69462028,Count count in splunk,"<p>Can you do double counting in Splunk via time_span?
I want to count the number of hits of number of fruits sold in an hour.</p>
<p>My code:</p>
<blockquote>
<p>|bucket _time span=1h |eventstats count as count_in_an_hour by fruit
time |stats count as count_count by fruit |table fruit count
count_count  |sort count_count count</p>
</blockquote>
<p>I can run this with a bit of data; but because I have a huge number of data, it's taking very long and taking up a lot of space resulting in &quot;not enough space error&quot;.</p>
<p>My sample set of data,</p>
<pre><code>name    fruit   location time

mary    apple   east 5.10

ben pear    east 6.10

peter   pear    east 5.50

ben apple   north 7.10

ben mango   north 7.40

peter   mango   north 5.30

mary    orange  north 7.20

alice   pear    north 7.20

janet   pear    north 7.20

janet   mango   west 6.30

janet   mango   west 5.50

peter   mango   west 4.20

janet   pear    west 5.50
</code></pre>",69465818.0,1,0,,2021-10-6 08:11:36,,2021-10-6 12:35:10,,,,,16783738.0,,1,0,splunk|splunk-query,36,6
1111,259567,69463082,Splunk rex expression to remove comma if present in json file,"<p>I have stuck in a small issue where I need to remove last character &quot;,&quot; ( if present) from JSON log file. I am using it in Splunk.</p>
<p>It seems simple and I was hoping my regex will work but its not working.
My Attempts :</p>
<pre><code>1. s/\(,$\)?//g
2. s/,$//g
3. s/\(.*\),/\1/
 
</code></pre>
<p>FYI: My json file is nested, along with removing last character, I am removing some header and footers from this file and breaking the 1 event in multiple. Due to event break it has , at the end of each event.
For better understanding can refer this link which I posted on Splunk Community fourm
<a href=""https://community.splunk.com/t5/Getting-Data-In/Updated-Help-in-event-break-for-json-file/td-p/569676"" rel=""nofollow noreferrer"">https://community.splunk.com/t5/Getting-Data-In/Updated-Help-in-event-break-for-json-file/td-p/569676</a></p>
<p><a href=""https://i.stack.imgur.com/FRiCv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FRiCv.png"" alt=""enter image description here"" /></a></p>",,2,0,,2021-10-6 09:25:29,,2021-10-6 15:02:44,,,,,7001109.0,,1,0,regex|sed|splunk|rex,29,5
1112,259568,69469837,Maximum match lines in transforms. Conf,"<p>How many match condition statements can be in transforms.conf of splunk. I see that with more than 700 lines of match conditions the data is not getting ingested and it is not getting recognised or processed, but with less than 650 lines of match cases data gets ingested.</p>",,0,3,,2021-10-6 17:00:12,,2021-10-7 04:13:46,2021-10-7 04:13:46,,3296786.0,,3296786.0,,1,0,splunk|splunk-dashboard,19,5
1113,259569,69473330,Splunk search to show top non-compliance controls in Dashboard,"<p>I am very new to splunk and am creating a dashboard to show top non-compliances. For the below data, I want to display top non-compliant controls (example output also mentioned below)</p>
<p>Could anyone please let me know how can I write a search query for the same?</p>
<p>Thanks in advance.</p>
<pre><code>Event_ID: abc1
Compliance_result: Non-Compliant
Eval_results: {
    required_tags: {
        compliance: Compliant 
        }
    encryption_enabled:{
        compliance: Non-Compliant
        }
    public_access:{
        compliance: Compliant
        }
    policy_enabled:{
        compliance: Compliant
        }
}


Event_ID: abc2
Compliance_result: Non-Compliant
Eval_results: {
    required_tags: {
        compliance: Compliant 
        }
    encryption_enabled:{
        compliance: Non-Compliant
        }
    public_access:{
        compliance: Non-Compliant
        }
    policy_enabled:{
        compliance: Compliant
        }
}
</code></pre>
<p>Generate Table in the below format -</p>
<pre><code>Top Non Compliance controls:

public_access - 2

encryption_enabled - 1
</code></pre>",,0,0,,2021-10-6 22:25:24,,2021-10-7 00:27:36,2021-10-7 00:27:36,,4418.0,,14090665.0,,1,0,splunk|splunk-query|splunk-dashboard,19,5
1114,259570,69485944,where vs sort: Which is more efficient?,"<p>Which is more efficient - where or sort?
I have a huge size of data and efficiency matters to me. Should I do 'where' first, or 'sort' first?</p>
<blockquote>
<p>|where count&gt;100
|sort count fruits</p>
</blockquote>
<p>OR</p>
<blockquote>
<p>|sort count fruits
|where count&gt;100</p>
</blockquote>",,2,0,,2021-10-7 18:12:50,,2021-10-7 19:23:05,,,,,16783738.0,,1,0,splunk|splunk-query|splunk-formula,23,5
1115,259571,69534413,Time difference column is empty when using SPLUNK query,"<p>I have two columns start time and finish time in the below format. I need to calculate the total time taken and add the difference as a new column.</p>
<p>Splunk Query used:</p>
<pre><code>| rename per_stage_info{}.starttime as starttime, per_stage_info{}.finishtime as finishtime 
| eval st=strptime(starttime, &quot;%Y/%m/%d %H:%M:%S&quot;) 
| eval ft=strptime(finishtime, &quot;%Y/%m/%d %H:%M:%S&quot;) 
| table per_stage_info{}.stage_name,starttime,finishtime, ft, st,ft-st
</code></pre>
<p>starttime   values</p>
<pre><code>2021/10/11 08:41:40
2021/10/11 08:48:07
2021/10/11 08:55:09
2021/10/11 08:41:40
</code></pre>
<p>finishtime values</p>
<pre><code>2021/10/11 08:48:00
2021/10/11 08:51:38
2021/10/11 08:55:14
2021/10/11 08:55:14
</code></pre>
<p>ft after conversion</p>
<pre><code>1633942080.000000
1633942298.000000
1633942514.000000
1633942514.000000
</code></pre>
<p>st after conversion</p>
<pre><code>1633941700.000000
1633942087.000000
1633942509.000000
1633941700.000000
</code></pre>
<p>ft-st column is empty</p>
<p>Why am I not seeing the calculated time difference in ft-st column? I tried multiple other ways but the column is empty.</p>",,1,0,,2021-10-12 02:28:30,,2021-10-12 19:24:55,2021-10-12 19:24:55,,4418.0,,17130945.0,,1,0,splunk,20,5
1116,259572,69543754,Splunk Dashboard shows differente events each refresh,"<p>I got this issue, I have a dashboard that gets refresh every minute, so I count the total of events and in that events the total of approved transactions to show as below:</p>
<pre><code>560 approved about of 1560 Transactions.
</code></pre>
<p>The issue is when the dashboard gets refresh instead of keep or increase the number it gets decrease as below.</p>
<pre><code>535 approved about of 1560 Transactions.
</code></pre>
<p>After another refresh, it shows the correct data again.</p>
<pre><code>560 approved about of 1560 Transactions.
</code></pre>
<p>I already increase the resources with more CPU, I do not know what can I do to solve this issue?</p>",,0,1,,2021-10-12 16:08:38,,2021-10-16 11:27:34,2021-10-16 11:27:34,,1041085.0,,16798583.0,,1,0,splunk|splunk-dashboard,20,5
1117,259573,69571787,Splunk: schedule an alert with two different frequencies (without overlapping),"<p>I have one Splunk alert which should run infrequent at night and more frequent at day.</p>
<pre><code>00:00 - 06:00 every 30 minutes
*/30 0-6 * * *
At every 30th minute past every hour from 0 through 6.
</code></pre>
<pre><code>08:00 - 22:00 every 10 minutes
*/10 8-22 * * *
At every 10th minute past every hour from 8 through 22.
</code></pre>
<p>Can I mix them using one cron expression?</p>
<p>Or do I have to clone the alert and as a trade-off everything is redudant (except the cron expression) then?</p>",,1,0,,2021-10-14 13:41:27,,2021-10-14 15:49:36,,,,,4874883.0,,1,0,cron|splunk,16,4
1118,259574,69601494,How to migrate entire index data from one Splunk server to another Splunk server,"<p>I have a Splunk server with index data for 650k events. I want to migrate the entire data from one instance to another new instance.
I tried using a migration script with data field -27D@d but I can only migrate 50k data.
-27D@d is the point from where initial data is available.
Can you please help me here?
Here's the code :</p>
<pre><code>import splunklib.client as client
import splunklib.results as results
import json
import requests

send_string = &quot;&quot;
service=client.connect(host=&quot;host1&quot;, port=8089,  username=&quot;admin&quot;, password=&quot;xxxx&quot;)
rr = results.ResultsReader(service.jobs.export('search index=my_index latest=-27D@d' ))
for result in rr:
    if isinstance(result, results.Message):
        continue
    elif isinstance(result, dict):
        final = dict(result)
        data = final['_raw']
        send_string = json.dumps({&quot;event&quot; : data,&quot;source&quot; : &quot;test&quot;},ensure_ascii=False).encode('utf8')
    url='http://host2:8088/services/collector'
    authHeader = {'Authorization': 'Splunk 5fbxxxx'}
    #Send data to Splunk
    response = requests.post(url, headers=authHeader, data=send_string, verify=False)
    if response.status_code == 200:
        print(&quot;Successfully pushed the data to Splunk source&quot;)
    else:
        print(&quot;Failed to push the data to Splunk source&quot;)
</code></pre>",69615797.0,1,3,,2021-10-17 05:13:34,,2021-10-18 12:26:22,,,,,6683853.0,,1,0,python|splunk|splunk-query|splunk-sdk,62,7
1119,259575,69605959,Extract Url from access Log in Splunk,"<p>This is the sample accessLog which is coming up in the Splunk ui.</p>
<ol>
<li><p><code>{&quot;timestamp&quot;:&quot;2021-10-17T15:03:56,763Z&quot;,&quot;level&quot;:&quot;INFO&quot;,&quot;thread&quot;:&quot;reactor-http-epolpl-20&quot;,&quot;message&quot;:&quot;method=GET, uri=/api/v1/hello1, status=200, duration=1, &quot;logger&quot;:&quot;reactor.netty.http.server.AccessLog&quot;}</code></p>
</li>
<li><p><code>{&quot;timestamp&quot;:&quot;2021-10-17T15:03:56,763Z&quot;,&quot;level&quot;:&quot;INFO&quot;,&quot;thread&quot;:&quot;reactor-http-epolpl-20&quot;,&quot;message&quot;:&quot;method=GET, uri=/api/v1/dummy1, status=200, duration=1, &quot;logger&quot;:&quot;reactor.netty.http.server.AccessLog&quot;}</code></p>
</li>
</ol>
<p>I want to extract the url and make a count for all the API's like how many times an API is hitted from the uri part(uri=/api/v1/dummy1)</p>
<pre><code>(index=dummy OR index=dummy1) source=*dummy-service*  logger=reactor.netty.http.server.AccessLog 
| rex field=message &quot;(?&lt;url&gt;uri.[\/api\/v1\/hello]+)&quot; 
| chart count by url
</code></pre>
<p>But it's not giving URL in a proper format. I tried various regex, but couldn't get the proper URL count.</p>
<p>I wanted to use this query to show the API counts in the Splunk dashboard.</p>",69606960.0,1,0,,2021-10-17 15:41:54,,2021-10-20 00:05:54,2021-10-20 00:05:54,,4418.0,,9045697.0,,1,0,extract|splunk|splunk-query|splunk-dashboard,40,6
1120,259576,69628766,"Splunk queries: unsuccessful logins or logins with accounts locked, logins with OCONUS IP","<p>I am using Splunk (7.3.3) and I am having tremendous difficulties trying to create a dashboard that can show (or 'report') the following information:</p>
<ol>
<li>unsuccessful admin logins</li>
<li>unsuccessful admin logins after duty hours (WINDOWS, ALL HOURS RIGHT NOW)</li>
<li>admin logins from OCONUS IPs</li>
<li>admin logon with account locked</li>
<li>attempts to logon with expired password</li>
<li>unsuccessful attempts to bypass login or logins not enforcing PKI, multifactor, and or modified authentication enforcement</li>
<li>system time outs</li>
<li>system memory spikes</li>
<li>system network traffic spikes</li>
<li>system errors</li>
</ol>
<p>I feel like the majority of these would be common things that people want to use to track these type of issues for their applications and was wondering if anybody would be available to share queries they have used in Splunk 7.3.3.</p>
<p>For simpler stuff such as Windows logons (event code) I am having success using the following query:
<code>index=windows EventCode=4624 | stats count BY TargetUserName</code> - I also pipe in some AND NOTs to prune out bad logs that I am not interested in but took them out for sake of query</p>
<p>For Windows Admin Logins.. I am creating a report that runs query <code>index=* source=&quot;*WinEventLog:Security&quot; EventCode=4720 OR (EventCode=4732 Administrators)</code> and add the report to the dashboard.</p>",69632945.0,1,0,,2021-10-19 09:57:19,,2021-10-19 14:42:43,,,,,3558514.0,,1,1,splunk|splunk-query,31,6
1121,259577,69629769,Splunk: How to use a variable in colorpalette expression in SimpleXML?,"<p>Is it possible to use a variable inside a color palette expression using only SimpleXML in Splunk? I have the following field:</p>
<h2>myField:</h2>
<pre><code>mySearch | eval myField = 100
</code></pre>
<p>In Splunk, I have a table. The table returns rows with just numbers (e.g 16,123,644 etc.). Changing the color for these rows based on the value works like this:</p>
<h2>Color palette:</h2>
<pre><code>&lt;format type=&quot;color&quot; field=&quot;sourceField&quot;&gt;
    &lt;colorPalette type=&quot;expression&quot;&gt;if (value &gt; 100 ,&quot;#df5065&quot;,&quot;#00FF00&quot;)&lt;/colorPalette&gt;
&lt;/format&gt;
</code></pre>
<p>If sourceField is greater than 100, the row with that value is colored RED. Any other value will color the row GREEN. I want to adjust the above piece of code to include the variable <strong>myField</strong> so that I can change the color based on the variable. I have tried the following:</p>
<pre><code>&lt;format type=&quot;color&quot; field=&quot;sourceField&quot;&gt;
    &lt;colorPalette type=&quot;expression&quot;&gt;if (value &gt; myField ,&quot;#df5065&quot;,&quot;#00FF00&quot;)&lt;/colorPalette&gt;
&lt;/format&gt;

&lt;format type=&quot;color&quot; field=&quot;sourceField&quot;&gt;
    &lt;colorPalette type=&quot;expression&quot;&gt;if (value &gt; $myField$ ,&quot;#df5065&quot;,&quot;#00FF00&quot;)&lt;/colorPalette&gt;
&lt;/format&gt;

&lt;format type=&quot;color&quot; field=&quot;sourceField&quot;&gt;
    &lt;colorPalette type=&quot;expression&quot;&gt;if (value &gt; 'myField' ,&quot;#df5065&quot;,&quot;#00FF00&quot;)&lt;/colorPalette&gt;
&lt;/format&gt;

&lt;format type=&quot;color&quot; field=&quot;sourceField&quot;&gt;
    &lt;colorPalette type=&quot;expression&quot;&gt;if (value &gt; (myField) ,&quot;#df5065&quot;,&quot;#00FF00&quot;)&lt;/colorPalette&gt;
&lt;/format&gt;
</code></pre>
<p>But none of the above work.</p>
<p>Is it possible to include variables in the above color palette expression and if so, how do I do it? Thanks in advance.</p>",69646403.0,2,0,,2021-10-19 11:14:55,,2021-10-20 12:57:46,2021-10-19 12:03:35,,11138845.0,,2862028.0,,1,1,xml|splunk|color-palette,39,6
1122,259578,69666789,Splunk strptime returning NaN,"<p>I have a eval on a dashboard that used to work but it stopped and I havent been able to figure out why.</p>
<p>On the dashboard im taking the _time and turning it into a human readable string using <code>strftime(_time, &quot;%m/%d/%Y %H:%M:%S %Z&quot;)</code> and that works great. The problem comes in when I try to convert it back later for making a link to a search.</p>
<p>For example:</p>
<pre><code>&lt;eval token=&quot;endTimestamp&quot;&gt;relative_time(strptime($row.Timestamp$, &quot;%m/%d/%Y %H:%M:%S %Z&quot;), &quot;+30m&quot;)&lt;/eval&gt;
</code></pre>
<p>Used to work and return the unix time that I added 30m to, but now <code>strptime</code> just returns NaN but this is the right format. I've checked out all the Splunk docs and everything looks right but it still is broke.</p>
<p>Any idea what I could be doing wrong?</p>
<p>Here is the snippet from my field row im making:</p>
<pre><code>&lt;condition field=&quot;Search&quot;&gt;
            &lt;eval token=&quot;startTimestamp&quot;&gt;$row.Timestamp$&lt;/eval&gt;
            &lt;eval token=&quot;endTimestamp&quot;&gt;relative_time(strptime($row.Timestamp$, &quot;%m/%d/%Y %H:%M:%S %Z&quot;), &quot;+30m&quot;)&lt;/eval&gt;
            &lt;eval token=&quot;corKey&quot;&gt;$row.Correlation Key$&lt;/eval&gt;
            &lt;link target=&quot;_blank&quot;&gt;search?q=(index=### OR index=###) earliest=$startTimestamp$ latest=$endTimestamp$ correlationKey=$corKey$&lt;/link&gt;
&lt;/condition&gt;
</code></pre>
<p>I have taken out everything but the $row.Timestamp$ and that returns something like <code>10/03/2021 07:41:27 PDT</code> which is the format that I put into it, I just cant do the reverse. I have copied and pasted the format from the <code>strftime</code> and still no luck converting it back so I can do math on it.</p>
<p>Any suggestions?</p>",,1,0,,2021-10-21 18:03:57,,2021-10-22 20:01:22,,,,,3302683.0,,1,1,dashboard|splunk|splunk-query,23,5
1123,259579,69707314,"Splunk: Return One or True from a search, use that result in another search","<p>In Splunk, I am looking for logs that say &quot;started with profile: [profile name]&quot; and retrieve the profile name from found events. Then I want to use the profile name to look for other events (from a different source) and if one error or more are found, I would like to let it count as one found error, per platform.</p>
<p>To make things more clear I have the following search query (<strong>query one</strong>):</p>
<pre><code>index=&quot;myIndex&quot; &quot;started with profile&quot; BD_L*
| table _raw, platform, RUNID
| eval Platform=case(searchmatch(&quot;LINUX&quot;),&quot;LINUX&quot;,searchmatch(&quot;AIX&quot;),&quot;AIX&quot;,searchmatch(&quot;DB2&quot;),&quot;DB2&quot;, searchmatch(&quot;SQL&quot;),&quot;SQL&quot;, searchmatch(&quot;WEBSPHERE&quot;),&quot;WEBSPHERE&quot;, searchmatch(&quot;SYBASE&quot;),&quot;SYBASE&quot;, searchmatch(&quot;WINDOWS&quot;),&quot;WINDOWS&quot;, true(),&quot;ZLINUX&quot;) 
| stats count by Platform
| rename count AS &quot;Amount&quot;
</code></pre>
<p>The events found from above query contains the following (raw) :</p>
<pre><code>Discovery run, 2021101306351355 started with profile BD_L2_Windows
</code></pre>
<p>The above query will return a list of events containing the raw data above and will result in the following table. This is a table with the amount of Discovery runs per platform:</p>
<p><a href=""https://i.stack.imgur.com/49sBc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/49sBc.png"" alt=""enter image description here"" /></a></p>
<p>Using the following piece of code I can extract RUNID from the events. RUNID is what I need to use in a second search when looking for errors:</p>
<pre><code>| rex &quot;Discovery run, (?&lt;RUNID&gt;.+) started with profile&quot;
</code></pre>
<p>Using RUNID I can look for errors (<strong>query two</strong>):</p>
<pre><code>index=&quot;myIndex&quot; source=&quot;/*/RUNID/*&quot; CASE(&quot;ERROR&quot;) CTJT* 
| dedup _raw   
| stats  count
| rename count AS &quot;Amount&quot;
</code></pre>
<p>Now, I am looking for a way to combine the above two queries into one and count the amount of platforms that have at least one error. So lets say we have the following simulation:</p>
<ul>
<li>Two runs (one Windows and one Linux)</li>
<li>Windows run has 0 errors (none found in query 2)</li>
<li>Linux has 6 errors (found in query 2)</li>
</ul>
<p>This should result in the following results:</p>
<pre><code>Platform     |     Amount
Linux        |          1
</code></pre>
<p>I need to find some way to return <strong>true</strong> or maybe <strong>one</strong> from query 2 and use that in query 1 to group the results, but I am unable to due to lack of experience. I have not yet found anything similair to my question and hope anyone here can help me out.</p>
<p>So far I can think of the following but this still shows me the same results as in the table shown above (counts of discovery runs per platform instead of counts of platforms that have at least one error):</p>
<pre><code>index=&quot;myIndex&quot; &quot;started with profile&quot; BD_L* 
| table _raw, platform, RUNID 
| eval Platform=case(searchmatch(&quot;LINUX&quot;),&quot;LINUX&quot;,searchmatch(&quot;AIX&quot;),&quot;AIX&quot;,searchmatch(&quot;DB2&quot;),&quot;DB2&quot;, searchmatch(&quot;SQL&quot;),&quot;SQL&quot;, searchmatch(&quot;WEBSPHERE&quot;),&quot;WEBSPHERE&quot;, searchmatch(&quot;SYBASE&quot;),&quot;SYBASE&quot;, searchmatch(&quot;WINDOWS&quot;),&quot;WINDOWS&quot;, true(),&quot;ZLINUX&quot;) 
| join type=left RUNID
    [ search index=&quot;myIndex&quot; source=&quot;/*/RUNID/*&quot; CASE(&quot;ERROR&quot;) CTJT*
        | dedup _raw
        | stats count 
    ]
| stats count by Platform
</code></pre>
<p>How to solve this? Thanks in advance.</p>
<p><strong>EDIT:</strong></p>
<p>As per request, data samples:</p>
<p>Query 1:</p>
<pre><code>2021-10-25 22:01:10,065 ProcessFlowManager [RMI TCP Connection(20)-127.0.0.1]  INFO processflowmgr.ProcessFlowManagerImpl - Discovery run, 2021102522011000 started with profile BD_L2_Windows
</code></pre>
<p>Query 2:</p>
<pre><code>2021-10-25 22:02:11,537 DiscoverManager [DiscoverWorker-47] 2021102522011000#SessionSensor-XX.XXX.XXX.XX-[135,445] ERROR cdb.TivoliStdMsgLogger - CTJTD3028E Sensor SessionSensor encountered an error, Seed: XX.XXX.XXX.XX:[135, 445], Run ID: 2021102522011000.
</code></pre>
<p><strong>EDIT 2:</strong></p>
<p>I have tried the latest answer and got the following result:</p>
<p><a href=""https://i.stack.imgur.com/lhfbk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lhfbk.png"" alt=""enter image description here"" /></a></p>
<p>Where as I am expecting a list of Platforms and the amount of errors that are related to the platform. Something like:</p>
<pre><code>Platform | Amount (of errors)
zLinux | 2
Windows | 4
</code></pre>",69738787.0,2,0,,2021-10-25 11:32:07,,2021-10-27 14:16:26,2021-10-27 07:08:32,,2862028.0,,2862028.0,,1,3,splunk,73,8
1124,259580,69727917,Using Splunk rex to extract String from logs,"<p>From splunk logs,how can I get a count of all those methods whose <strong>Time taken is &gt; 10ms?</strong><br>
Splunk logs which look some thing like this :</p>
<blockquote>
<p>c.s.m.c.advice.ExecutionTimeAdvice       : &lt;&gt;
relationId = aa12 | Method Name = methodA() Time taken  is = 0ms</p>
<p>c.s.m.c.advice.ExecutionTimeAdvice       : &lt;&gt;
relationId = ab12 | Method Name = methodA(). Time taken is = 15ms</p>
<p>c.s.m.c.advice.ExecutionTimeAdvice       : &lt;&gt;
relationId = ab12 | Method Name = methodB(). Time taken is = 1ms</p>
</blockquote>",,1,0,,2021-10-26 18:02:03,,2021-10-27 15:12:50,2021-10-26 18:15:36,,12256384.0,,7585812.0,,1,2,splunk,50,7
1125,259581,69728790,Splunk : Extracting the elements from JSON structure as separate fields,"<p>In Splunk, I'm trying to extract the key value pairs inside that &quot;tags&quot; element of the JSON structure so each one of the become a separate column so I can search through them.
for example :</p>
<pre><code>| spath data | rename data.tags.EmailAddress AS Email
</code></pre>
<p>This does not help though and Email field comes as empty.I'm trying to do this for all the tags. Any thoughts/pointers?</p>
<pre><code>{
    &quot;timestamp&quot;: &quot;2021-10-26T18:23:05.180707Z&quot;,
    &quot;data&quot;: {
        &quot;tags&quot;: [
            {
                &quot;key&quot;: &quot;Email&quot;,
                &quot;value&quot;: &quot;john.doe@example.com&quot;
            },
            {
                &quot;key&quot;: &quot;ProjectCode&quot;,
                &quot;value&quot;: &quot;ABCD&quot;
            },
            {
                &quot;key&quot;: &quot;Owner&quot;,
                &quot;value&quot;: &quot;John Doe&quot;
            }
        ]
    },
    &quot;field1&quot;: &quot;random1&quot;,
    &quot;field2&quot;: &quot;random2&quot;
}
</code></pre>",69732138.0,1,0,,2021-10-26 19:18:32,,2021-10-27 02:43:22,,,,,255562.0,,1,0,splunk|splunk-query,43,6
1126,259582,69735449,What is splunk's log file collect technology,"<p>I'm using Splunk and Splunk forwarder (UF) for log collecting.</p>
<p>I have questions that &quot;How to Splunk collect log file?&quot;, &quot;What is data collect technic?&quot;</p>
<p>So, I tried to find answer and found a few informations from Splunk doc, .conf's ppt, etc.</p>
<p>But, In Splunk document and .conf's document only explain that How to Splunkd work and Processor work</p>
<p>I just want know that which is log file collect tech.</p>
<p>Does Splunkd use <code>tail</code> command for collect log files? or that uses other commands?</p>",,1,0,,2021-10-27 08:43:12,,2021-10-28 15:23:13,2021-10-28 15:23:13,,4157124.0,,16296256.0,,1,-1,splunk|tail|logfile|data-collection,23,5
1127,259583,69744959,Annotating flag emojis to 3166 country codes in splunk,"<p>Given a bunch of <code>countrycode</code> / <code>percent</code> results, decorate a pie chart with the flag of each country.</p>",,1,0,,2021-10-27 20:16:51,,2021-10-27 20:16:51,,,,,1130377.0,,1,0,splunk|splunk-query,12,4
1128,259584,69747790,Avoid duplicate usage of timechart in one Splunk query,"<p>sI have a splunk query which works fine current. The Splunk query is as<br />
<code>index=my-index sourcetype=my-type | timechart span=2h perc95(time) as m95 | eval test1=200 | eval test2=400 | timechart span=2h avg(m95) as mm95, avg(test1) as &quot;TEST_1, min(test2) as &quot;TEST_2&quot;</code></p>
<p>If I only want to use <code>timechart span=2h</code> one time in the splunk query, but the query results should be the same. How can I modify my query? Thanks.</p>",,0,3,,2021-10-28 03:03:59,,2021-10-28 17:28:08,2021-10-28 17:28:08,,15788148.0,,15788148.0,,1,0,splunk|splunk-query|splunk-dashboard,20,5
1129,259585,69749324,"Splunk - Table, print number of rows per selected field","<p>I am executing a Splunk query as below</p>
<pre class=""lang-sql prettyprint-override""><code>myusername response_status=&quot;401&quot; | 
  table website website_url user_name transaction_name user | 
    dedup website transaction_name
</code></pre>
<p>I am getting output with filtered unique rows with <code>website</code> and <code>transaction_name</code></p>
<p>How can I get the count or number of rows for each <code>website</code> ?</p>",,2,0,,2021-10-28 06:39:32,,2021-10-28 20:39:32,,,,,1293013.0,,1,0,splunk|splunk-query,23,5
1130,259586,69792540,Search for specific patterns in Splunk cloud platform,"<p>I am new to splunk and am trying to perform incident analysis of a compromised domain controller security event logs. I am using the free trial version of splunk cloud platform and ingested the csv data and let splunk automatically create the indexes. The attack is a brute force attack and it seems that some malicious user tried to find the members of a remote desktop user group in the AD.</p>
<p><a href=""https://i.stack.imgur.com/eIbyA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eIbyA.png"" alt=""Splunk Search events results"" /></a>
I am attaching a screenshot of my current splunk search result and the further filter that I want to use [ highlighted in black ]. As you will see in the current search result, it is listing many such events, but I am only interested in certain events, where under the Subject section, the account names have values other than a given set of names (say ID4$, Admin, Harvester etc.). How can that be achieved?</p>",,2,1,,2021-11-1 04:10:20,,2021-11-1 12:45:04,,,,,807362.0,,1,0,splunk,20,5
1131,259587,69803012,Forward ECS logs to both splunk and cloudwatch,<p>Is there a way to forward ECS logs(EC2) to both Splunk and cloudwatch?</p>,,0,0,,2021-11-1 21:32:43,,2021-11-1 21:32:43,,,,,7046342.0,,1,0,amazon-ecs|amazon-cloudwatch|splunk|fluentd,16,4
1132,259588,69822642,Splunk: Not all field values are extracted for long JSON files,"<p>I am trying to ingest long JSON files into my Splunk index, where a record could contain more than 10000 characters. To prevent long records from getting truncated, I added a <code>TRUNCATE=0</code> into my <code>props.conf</code>, and the entire record was ingested into the index. All events are forwarded and stored in the index, but I'm having problems with fields that appear towards the end of the JSON records.</p>
<p>I'm currently testing with 2 files:</p>
<ul>
<li>File A has 382 records, of which 166 are long records.</li>
<li>File B has 252 records, of which all are long records.</li>
</ul>
<p>All 634 events are returned with a simple search of the index, and I can see all fields in each event, regardless of how long the event is.</p>
<p>However, not all fields are extracted and directly searchable. For example, one of the fields is called <code>name</code>, and it appears towards the end of each JSON record. On the <code>Interesting fields</code> pane, under <code>name</code>, it shows only a count of 216 events from File A, and none of the remaining 166 + 252 long events in Files A and B. This is the same for other fields that appear towards the end of each JSON record, but fields towards the beginning of the record show all 634 events.</p>
<p>If I negate the 216 events, then these fields do not appear on the Fields pane at all.</p>
<p>Also, while I'm not able to directly search for <code>name=&lt;name in File B&gt;</code>, I can still select the field from the event and <code>add to search</code>, and all 252 events would be returned.</p>
<p>I'm not sure why these fields are not properly extracted even though they did not appear to be truncated. How can I extract them properly?</p>",,1,0,,2021-11-3 09:41:32,,2021-12-3 03:03:46,,,,,270043.0,,1,0,json|splunk|indexer,30,5
1133,259589,69826758,Create Splunk Chart/Table with start and end logs to check Success/Failures,"<p>I have logs in <code>splunk</code> that capture information related to which file was part of the execution (Different execution captures the same log with different file name) as below</p>
<pre><code>Processing file : test_1.txt
Processing file : test_2.txt
Processing file : test_3.txt
</code></pre>
<p>Another log that captures info related to if the execution succeeded</p>
<pre><code>Processed file successfully : test_1.txt 
</code></pre>
<p>I want to make a splunk table or chart as below.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>File Name</th>
<th>Success</th>
<th>Failure</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021-01-01</td>
<td>test_1.txt</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2021-01-02</td>
<td>test_2.txt</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>2021-01-03</td>
<td>test_3.txt</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
</div>
<p>Later, when re-processing the failed tasks/files, if they are successful and we have a success log for those files, the above table should be updated with a new date (See text_3.txt file date and Success/Failure updated in below table)</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>File Name</th>
<th>Success</th>
<th>Failure</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021-01-01</td>
<td>test_1.txt</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>2021-01-02</td>
<td>test_2.txt</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>2021-01-04</td>
<td>test_3.txt</td>
<td>Yes</td>
<td>No</td>
</tr>
</tbody>
</table>
</div>
<p>Is this possible in Splunk?</p>",69829694.0,1,0,,2021-11-3 14:37:14,,2021-11-3 18:26:19,,,,,4186589.0,,1,0,logging|splunk|splunk-query,31,5
1134,259590,69843787,splunk chart doesn't show all site data,"<p>from checkbox value, if i choose multiple sites, i would like to show all sites separate line chart for avgtrackout time. now the problem if i choose multiple sites, it only show one line chart by coming all sites avgtrackout value.</p>
<p>query: MicronSite IN($site$) index=mtparam sourcetype=CommandTimesByArea | rex field=_raw &quot;Fabwide:AvgTotalTrackoutTime\s+(?\d+)&quot; | timechart span=12h avg(AvgTotalTrackoutTime) aligntime=@d+7h avg(command_time)</p>
<p>For example: from check box list i choose &quot;F10N&quot; and &quot;F10W&quot;. but in chart, only show one line by combing those two site avgtotaltrackout time values and show one chart. i would like to show separate avgtotal trackout time value for two separate chart</p>
<p>please help to suggest for this issue</p>",,0,1,,2021-11-4 17:59:21,,2021-11-4 17:59:21,,,,,17330400.0,,1,0,splunk-query|splunk-calculation,13,4
1135,259591,69895159,Splunk issue with Telegram Alert Action: Can not configure the action,"<p>I installed Telegram Alert Action app (<a href=""https://splunkbase.splunk.com/app/3703/"" rel=""nofollow noreferrer"">https://splunkbase.splunk.com/app/3703/</a>) for my SearchHead server (Splunk Enterprise 8.0.6) successfully. But when i add Telegram Alert action for the alerts, i can not see any its configurations as attached image:
<a href=""https://i.stack.imgur.com/5wWWR.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/5wWWR.png</a></p>
<p>Could any one tell me what is this issue?
Thanks very much!</p>",69945250.0,2,0,,2021-11-9 08:48:13,,2021-11-15 04:13:07,2021-11-10 03:38:43,,17365377.0,,17365377.0,,1,0,telegram-bot|splunk,55,6
1136,259592,69896820,Extract string in square bracket with regex,"<p>my logs in splunk like:</p>
<pre><code>[ A=xaxxxxx ] [ B=weea case ] [ C=another example 0 ]
</code></pre>
<p>How can I get only the string in square bracket after &quot;=&quot;</p>
<p>like so: <code>xaxxxxx</code> ; <code>weea case</code> ; <code>another example 0</code></p>
<p>my rex is: <code>rex field=_raw &quot;(?&lt;New_Field&gt;\[\sC=(.*?)\s\])&quot;</code></p>
<p>it extract all, include square bracket [...].</p>",69896866.0,3,2,,2021-11-9 10:50:42,,2021-11-22 17:25:10,2021-11-9 10:53:08,,3832970.0,,14697290.0,,1,2,regex|splunk,42,7
1137,259593,69922395,Splunk : Spath searching the JSON array,"<p>I have below two JSON events where under &quot;appliedConditionalAccessPolicies&quot;, in one event policy1 has results =failure and policy2 has results=notApplied. In the other event the values are reversed.</p>
<p>Now I'm trying to get the event where the policy1 has the status=&quot;failure&quot;, it gives both the events</p>
<pre><code>index=test
| spath path=&quot;appliedConditionalAccessPolicies{}&quot; | search &quot;appliedConditionalAccessPolicies{}.displayName&quot;=&quot;policy1&quot; &quot;appliedConditionalAccessPolicies{}.result&quot;=&quot;failure&quot;
</code></pre>
<p>It looks like Its searching within all the elements in the array.
How can I ensure It searches both the conditions on each element of the array and return the event which has the element satisfying both the conditions.</p>
<p>Events :</p>
<pre><code> appDisplayName: App1
   appId: aaaa-1111-111aeff-aad222221111
   appliedConditionalAccessPolicies: [ 
     { 
       displayName: policy1
       enforcedGrantControls: [
         Block
       ]
       enforcedSessionControls: [
         SignInFrequency
         ContinuousAccessEvaluation
       ]
       id: f111113-111-400c-a251-2123bbe4233e1
       result: failure
     }
     { [-]
       displayName: policy2
       enforcedGrantControls: [ [-]
         Block
       ]
       enforcedSessionControls: [ [-]
       ]
       id: sdsds-8c92-45ef-sdsds-c0b2e006d39b
       result: notApplied
     }
   ]
   
   appDisplayName: App1
   appId: aaaa-1111-111aeff-aad222221111
   appliedConditionalAccessPolicies: [ 
     { 
       displayName: policy1
       enforcedGrantControls: [
         Block
       ]
       enforcedSessionControls: [
         SignInFrequency
         ContinuousAccessEvaluation
       ]
       id: f111113-111-400c-a251-2123bbe4233e1
       result: notApplied
     }
     { [-]
       displayName: policy2
       enforcedGrantControls: [ [-]
         Block
       ]
       enforcedSessionControls: [ [-]
       ]
       id: sdsds-8c92-45ef-sdsds-c0b2e006d39b
       result: failure
     }
   ]
</code></pre>",69935544.0,1,0,,2021-11-11 02:04:20,,2021-11-16 02:25:21,,,,,255562.0,,1,0,splunk|splunk-query,67,7
1138,259594,69922757,jmeter login test splunk on RHEL 8,"<p>I am new to jmeter and need to do a login test for my splunk vm on rhel 8 using my windows server 2022 to run jmeter, but I got unauthorized or bad request errors.</p>
<p>I tried to record and use all the variables involved, but could not understand the value cval.</p>
<p><a href=""https://i.stack.imgur.com/jc4KR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jc4KR.png"" alt=""enter image description here"" /></a></p>
<p>if I exclude those fields and only use username and password, I get unauthorized error, even when I have added a HTTP Authorization Manager</p>
<p>Any help to solve this will be greatly appreciated.</p>",,1,0,,2021-11-11 03:14:39,,2021-11-15 00:38:06,2021-11-15 00:38:06,,5191371.0,,17382486.0,,1,0,jmeter|splunk,22,5
1139,259595,69948917,Using a group by or something similar using Splunk rex,"<p>If we have data like this in the splunk logs -</p>
<pre><code> DepId EmpName
 100    Jon
 100    Mike
 100    Tony
 200    Mary
 200    Jim
</code></pre>
<p>Is there a way to display the records with only one line for the repeating DepIds.</p>
<p>So for example,something like below&quot;</p>
<pre><code>DepId   EmpName
100     Jon
        Mike
        Tony
200     Mary
        Jim.
</code></pre>",,1,1,,2021-11-12 20:51:09,,2021-11-15 12:12:47,2021-11-15 12:12:47,,4418.0,,7585812.0,,1,0,splunk,15,4
1140,259596,69974233,Pushing logs from python to Splunk,"<p>How can we push Python Application logs to Splunk. what are the prerequisite.Should we first convert out logs in structured format(key-value based) before sending to Splunk?</p>
<p>There is a package splunk_handler for pushing logs to splunk but i could not found any documentation with practical working example.I just have gitbub page for this package but that does not  have any practical implementation.</p>
<p>Please help on this</p>",69974859.0,1,0,,2021-11-15 12:12:14,,2021-11-15 13:06:56,,,,,10871102.0,,1,0,python|python-3.x|splunk|python-logging|splunk-sdk,41,6
1141,259597,69979427,"Splunk - ""grandparent from process and parent process""","<p>In my data I have process and parent process. As an example, I can get process id and parent process id for &quot;calc.exe&quot;. Now I also want to be able to get &quot;grandparent&quot;, ie the parent process for calc.exe's parent process.</p>
<pre><code>index=data process=calc.exe
| table process_id parent_process_id grand_parent_id
</code></pre>
<p>Since I am new to SPL, I have a bit of a hard time understanding how to design a query.</p>",,2,1,,2021-11-15 18:50:20,,2021-11-19 16:44:44,2021-11-19 16:44:44,,4418.0,,17282852.0,,1,0,splunk|spl,25,5
1142,259598,69981364,Splunk - Create customized query for Splunk dashboard based on Input selection,"<p>I'm creating a Dashboard in Splunk.
It has one dropdown menu to select App-name(App1 or App2), another drop-down to select log_type (Detailed and App_specific), and a Search panel to show output of search query.</p>
<p>For instance,</p>
<ol>
<li>If user selects App1 and log_type as app_specific, then the Panel should result for the query:</li>
</ol>
<p><code>index=App1 &quot;taskExecutor-1&quot; | sort -_time | table msg</code></p>
<p>For App1, selecting app_specific should add &quot;taskExecutor-1&quot; to the query.</p>
<ol start=""2"">
<li>If user selects App2 and log_type as app_specific, then the Panel should result for the query:</li>
</ol>
<p><code>index=App2 &quot;ool-44-thread-1&quot; | sort -_time | table msg</code></p>
<p>For App2, selecting app_specific should add &quot;ool-44-thread-1&quot; to the query.</p>
<ol start=""3"">
<li>
<ol>
<li>If user selects App1 and log_type as Detailed, then the Panel should result for the query:</li>
</ol>
</li>
</ol>
<p><code>index=App1 | sort -_time | table msg</code></p>
<p>Selecting Detailed should not anything to the query. Or we can say, an empty value.</p>
<p>How can I customize the query to accommodate such behavior in Splunk? Is there any any if/else or case functionality in Splunk that can help achieve this behavior?</p>",,1,0,,2021-11-15 21:55:28,,2021-11-16 15:04:54,,,,,2769790.0,,1,0,splunk|splunk-query|splunk-dashboard,39,6
1143,259599,69989329,Eval command vs eval expression,"<p>It seems that there is a difference between eval <em>command</em> and eval <em>expression</em>:</p>
<p><em>Eval command:</em></p>
<pre><code>eval velocity=distance/time
</code></pre>
<p><em>Eval expression:</em></p>
<pre><code>stats count(eval(status=404)) AS status_count
</code></pre>
<p>Notice that in the case of the expression <code>eval</code> is used as a function with parentheses around the arguments. I found documentation for the eval command: <a href=""https://docs.splunk.com/Documentation/Splunk/8.2.3/SearchReference/Eval"" rel=""nofollow noreferrer"">https://docs.splunk.com/Documentation/Splunk/8.2.3/SearchReference/Eval</a></p>
<p>Is there documentation for the eval expression?</p>",69990198.0,1,0,,2021-11-16 12:39:43,,2021-11-16 13:40:21,,,,,480894.0,,1,1,splunk,21,5
1144,259600,69992042,How to aggregate or join two JSON datasets in Splunk?,"<p>I try to build up an overview some process steps of my application.
I generate two JSON documents</p>
<pre><code>{ 
  &quot;requestID&quot;: &quot;abc-123&quot;,
  &quot;username&quot;: &quot;ringo&quot;,
}

</code></pre>
<p>and</p>
<pre><code>{  
  &quot;requestID&quot;: &quot;abc-123&quot;,
  &quot;favoriteCar&quot;: &quot;Lada&quot;
}
</code></pre>
<p>ok, now I have also other entries like these:</p>
<ul>
<li>abc-456 / paul / Fiat</li>
<li>bcd-987 / george / Talbot</li>
</ul>
<p>and so on ... linked by the requestID</p>
<p>Now I want to do a table that shows me:</p>
<pre><code>ID       |  Username    |     Car
---------|--------------|---------------
abc-123  | ringo        | Lada
abc-456  | paul         | Fiat
bcd-987  | george       | Talbot
</code></pre>
<p>So my question is: How can I do these aggregation?</p>
<p>Kind regards</p>
<p>Markus</p>",69993915.0,1,0,,2021-11-16 15:38:13,,2021-11-17 02:00:09,2021-11-16 16:18:19,,2473539.0,,2473539.0,,1,0,splunk|splunk-query,23,5
1145,259601,69994671,REGEX not working- Filter the Splunk results,"<p>I have Splunk results in following format:</p>
<pre><code>2021-11-13 01:02:50.127 ERROR 23 --- [ taskExecutor-2] c.c.p.r.service.RedisService             : The Redis Cache had no record for key: null Returning empty list.

2021-10-22 21:11:51.996 ERROR 22 --- [ taskExecutor-1] c.c.p.r.service.SftpService           : Could not delete file: /-/XYZ.FILE - 4: Failure

2021-10-22 02:05:14.426 ERROR 22 --- [ taskExecutor-1] c.c.p.r.service.SftpService           : Could not delete file: /-/XYZ.FILE - 4: Failure
</code></pre>
<p>I want to create a Visualization in the following format:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Error Message</th>
<th style=""text-align: center;"">Time</th>
<th style=""text-align: right;"">Error code</th>
<th style=""text-align: left;"">TaskExecutor Number</th>
<th style=""text-align: center;"">Service name</th>
<th style=""text-align: center;"">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">The Redis Cache had no record for key: null Returning empty list.</td>
<td style=""text-align: center;"">2021-11-13 01:02:50.127</td>
<td style=""text-align: right;"">23</td>
<td style=""text-align: left;"">taskExecutor-2</td>
<td style=""text-align: center;"">c.c.p.r.service.RedisService</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">Could not delete file: /-/XYZ.FILE - 4: Failure</td>
<td style=""text-align: center;"">2021-10-22 21:11:51.996</td>
<td style=""text-align: right;"">22</td>
<td style=""text-align: left;"">taskExecutor-1</td>
<td style=""text-align: center;"">c.c.p.r.service. SftpService</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: left;"">Could not delete file: /-/XYZ.FILE - 4: Failure</td>
<td style=""text-align: center;"">2021-10-22 02:05:14.426</td>
<td style=""text-align: right;"">22</td>
<td style=""text-align: left;"">taskExecutor-1</td>
<td style=""text-align: center;"">c.c.p.r.service. SftpService</td>
<td style=""text-align: center;"">2</td>
</tr>
</tbody>
</table>
</div>
<p>The <strong>count variable</strong> is based on the &quot;Error Message&quot; only. Since &quot;Could not delete file: /-/XYZ.FILE - 4: Failure&quot; appeared twice, hence the count is set to 2. As the logs grow, and this message occurrence increase, this count should increase too.</p>
<p>I tried using erex and substring from Splunk but kinda failed miserably!</p>
<p>Here is the query I tried:</p>
<pre><code>index=my_index &quot;ERROR * ---&quot; &quot;taskExecutor-*&quot; | rex &quot;^(?&lt;Time&gt;\S+\s+\S+)\s+\S+\s+(?&lt;Error_Code&gt;\d+)[^\]]+\]\s+(?&lt;Service_Name&gt;\S+)\:\s+(?&lt;Error_Message&gt;.+)&quot;
| table Error_Message Time Error_Code Service_Name 
| eventstats count as Count by Error_Message Error_Code Service_Name
</code></pre>
<p>Seems like there is a problem with the REGEX: <a href=""https://regex101.com/r/smWKM8/2"" rel=""nofollow noreferrer"">https://regex101.com/r/smWKM8/2</a></p>
<p>Any help on how to fix this REGEX would be appreciated.</p>
<p>Thanks</p>",,1,0,,2021-11-16 18:56:09,,2021-11-17 14:16:37,2021-11-16 21:37:50,,2769790.0,,2769790.0,,1,0,java|regex|splunk|splunk-query|splunk-formula,39,6
1146,259602,69996785,Is the below syntax correct in terraform resource block to filter the logs when exporting from GCP to splunk?,"<p>filter = &quot;logName:&quot;/logs/cloudaudit.googleapis.com&quot; OR logName:&quot;/logs/compute.googleapis.com&quot;&quot;</p>
<p>My intention is to filter our logs and send only vpc flow logs and audit logs.</p>",,0,0,,2021-11-16 22:27:20,,2021-11-16 22:27:20,,,,,16816065.0,,1,-2,logging|terraform|splunk|terraform-provider-gcp,13,4
1147,259603,70073913,Syslog-NG: 2 logs from the same source written differently,"<p>I have 2 sets of logs.  Each is going to their own syslog server.  But the source of the logs is the same - a palo alto prisma vpn.</p>
<p>For whatever reason, Syslog-Server A (the oldest source) writes the logs like this (in bold):</p>
<p>Nov 22 15:08:03 <strong>34</strong> 456</p>
<p>But my newest Syslog Server, B, writes the logs like this (in bold):</p>
<p>Nov 22 15:08:03 <strong>34.0.0.1</strong> 456</p>
<p>This is a problem.  Because, on each syslog-ng server, we have a Splunk Universal Forwarder that forwards  the logs to an index.  We use the Palo Tech Add-On to parse the data.</p>
<p>It appears the TA is expected to parse the incoming data with this field:</p>
<p>Nov 22 15:08:03 <strong>34</strong> 456</p>
<p>Any other way breaks parsing.  I have my new syslog-ng file (for the new server - B) written as below:</p>
<pre><code>@version:3.31
@include &quot;scl.conf&quot;

options {
    flush_lines (0);
    time_reopen (10);
    log_fifo_size (1000);
    chain_hostnames (off);
    use_dns (no);  #was yes
    use_fqdn (no); #was yes
    create_dirs (no);
    keep_hostname (no); #was yes
};

source vpn_encrypted_log_traffic {
  network(
    ip(0.0.0.0)
    port(6514)
    transport(&quot;tls&quot;)
    tls(
      cert-file(&quot;/etc/syslog-ng/certs/prv.cer&quot;)
      key-file(&quot;/etc/syslog-ng/certs/prv.key&quot;)
      peer_verify(optional-untrusted)
    )
  );
};

destination prisma{ file(&quot;/directory/log.log&quot;) create_dir(yes) ); }

log { source(vpn_encrypted_log_traffic); destination(prisma); };
</code></pre>
<p>And the old syslog server (A) just has this:</p>
<pre><code>@version:3.5
@include &quot;scl.conf&quot;

options {
    time-reap (30);
    keep_hostname (no); #was yes
};

source vpn_encrypted_log_traffic {
  network(
    ip(0.0.0.0)
    port(6514)
    transport(&quot;tls&quot;)
    tls(
      cert-file(&quot;/etc/syslog-ng/certs/prv.cer&quot;)
      key-file(&quot;/etc/syslog-ng/certs/prv.key&quot;)
      peer_verify(optional-untrusted)
    )
  );
};

destination prisma{ file(&quot;/directory/log.log&quot;) create_dir(yes) ); }

log { source(vpn_encrypted_log_traffic); destination(prisma); };
</code></pre>
<p>I can only think the problem exists in Prisma.  But the configs look 1-to-1 to me.</p>",,1,2,,2021-11-23 00:10:41,,2021-11-28 14:15:47,,,,,10746813.0,,1,-1,splunk|syslog-ng,33,5
1148,259604,70076222,How to classify dashboards in a list under one application in splunk,<p>I have a multiple dashboards and I want to list them to come under one specific application. How to classify this dashboards ?</p>,,1,0,,2021-11-23 06:20:55,,2021-11-27 20:04:13,,,,,17485405.0,,1,0,splunk-dashboard,8,3
1149,259605,70076867,how to write splunk query for xml,"<pre><code>&lt;!DOCTYPE EmployeeInventory SYSTEM 'EmployeeInventory.dtd'&gt;&lt;EmployeeInventory version=&quot;2.0&quot;&gt;&lt;ProductInventoryInfo&gt;&lt;Product&gt;7781105882846&lt;/Product&gt;&lt;EmployeeID&gt;12151&lt;/EmployeeID&gt;&lt;Quantity&gt;28&lt;/Quantity&gt;&lt;CenterID&gt;167551&lt;/CenterID&gt;&lt;/ProductInventoryInfo&gt;&lt;/EmployeeInventory&gt;

    &lt;!DOCTYPE EmployeeInventory SYSTEM 'EmployeeInventory.dtd'&gt;&lt;EmployeeInventory version=&quot;2.0&quot;&gt;&lt;ProductInventoryInfo&gt;&lt;Product&gt;1781305782846&lt;/Product&gt;&lt;EmployeeID&gt;12152&lt;/EmployeeID&gt;&lt;Quantity&gt;18&lt;/Quantity&gt;&lt;CenterID&gt;167552&lt;/CenterID&gt;&lt;/ProductInventoryInfo&gt;&lt;/EmployeeInventory&gt;
</code></pre>
<p>How to write splunk query from above splunk log which will fetch table like this .</p>
<pre><code>Product         EmployeeID      Quantity   CenterID
7781105882846   12151               28      167551
1781305782846   12152               18      167552
</code></pre>",,1,0,,2021-11-23 07:26:42,,2021-11-23 14:46:01,,,,,739115.0,,1,-1,splunk,16,4
1150,259606,70103163,Install R-project on splunk,"<p>I am trying  to install  <a href=""https://splunkbase.splunk.com/app/3339/"" rel=""nofollow noreferrer"">'R Analytics'</a> in <a href=""https://www.splunk.com/"" rel=""nofollow noreferrer"">splunk</a> (I am using Windows 10 (64-bit) operating system).</p>
<p>I installed the 'R Analytics' app in splunk and 'OpenCPU' package in R. Unfortunately Run button in Script Editor does not work. I also found that 'runrdo' command is also unknow but 'runrdo' is in 'commandsconf' in splunk. How to fix this? I add some photo to ensure every thing is installed correctly.</p>
<ol>
<li>First install 'OpenCPU' package and run 'opencpu::ocpu_start_server()'</li>
</ol>
<p><a href=""https://i.stack.imgur.com/oivv6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oivv6.jpg"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>install 'R Analytics' (r-analytics_101.tgz) <a href=""https://www.splunk.com/"" rel=""nofollow noreferrer"">splunk</a></li>
</ol>
<p><a href=""https://i.stack.imgur.com/Gl2Ul.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Gl2Ul.jpg"" alt=""enter image description here"" /></a></p>
<p>The 'R Project' appeared as follows shown:</p>
<p><a href=""https://i.stack.imgur.com/KOeoy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KOeoy.jpg"" alt=""enter image description here"" /></a></p>
<p>Now I am trying to use script editor</p>
<p><a href=""https://i.stack.imgur.com/y9dki.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y9dki.jpg"" alt=""enter image description here"" /></a></p>
<p>I can see the splunk can send request to Rstudio (Show installed packages correctly)</p>
<p><a href=""https://i.stack.imgur.com/aepwH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aepwH.jpg"" alt=""enter image description here"" /></a></p>
<p>But unfortunately RUN button does not work.</p>
<p><a href=""https://i.stack.imgur.com/1Jsz2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1Jsz2.jpg"" alt=""enter image description here"" /></a></p>
<p>When I want to use 'runrdo':</p>
<p><a href=""https://i.stack.imgur.com/0eXvz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0eXvz.jpg"" alt=""enter image description here"" /></a></p>
<p>But it is in  'commandsconf'</p>
<p><a href=""https://i.stack.imgur.com/HRIlQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HRIlQ.jpg"" alt=""enter image description here"" /></a></p>
<p>Sample code from <a href=""https://community.splunk.com/t5/Getting-Data-In/connecting-Rest-API-from-R/m-p/204018"" rel=""nofollow noreferrer"">https://community.splunk.com/t5/Getting-Data-In/connecting-Rest-API-from-R/m-p/204018</a> worked for me with R version 4.1.1 (2021-08-10) and Splunk version 8.2.3.</p>
<p>I found there is not wrong with installation. I think the problem is on the difference between python 2.7 and python 3.7. The Splunk R-app is too old and written in python 2.7. The most recent version of Splunk is using python 3.7. I installed the Splunk 7.3.9(that use python 2.7), <a href=""https://download.splunk.com/products/splunk/releases/7.3.9/windows/splunk-7.3.9-39a78bf1bc5b-x64-release.msi"" rel=""nofollow noreferrer"">https://download.splunk.com/products/splunk/releases/7.3.9/windows/splunk-7.3.9-39a78bf1bc5b-x64-release.msi</a>, and all features except The Run button works fine as follows shown:</p>
<p><a href=""https://i.stack.imgur.com/HrOnV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HrOnV.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Iai7M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Iai7M.png"" alt=""enter image description here"" /></a></p>",,0,1,,2021-11-24 21:28:19,,2021-12-5 05:14:27,2021-12-5 05:14:27,,7192318.0,,7192318.0,,1,5,r|splunk,68,7
1151,259607,70157842,Integrate Mirth Connect with Splunk,"<p>I want to push the logs that we generate in the Mirth to Splunk, I am not able to find if there is some direct way in which mirth logged errors can be pushed to Splunk dashboards. I am open to indirect but efficient ways of doing this if direct integration is not possible.</p>",,1,0,,2021-11-29 16:18:17,,2021-11-30 18:59:58,,,,,6766368.0,,1,0,javascript|splunk|mirth,19,5
1152,259608,70161734,MAXROWS LIMIT AND OFFSET,"<p>I need to pull up 2100000 rows on data base.</p>
<p>I used maxrows=0</p>
<p>I also used LIMIT 1000000 and OFFSET 0 to get the first 1000000 but I'm getting an error.</p>
<p>How can I pull up the first 1000000?</p>
<p>ORDER BY audit_id ASC NULLS LAST
LIMIT 1000000
OFFSET 0</p>",,0,0,,2021-11-29 21:49:49,,2021-11-29 21:49:49,,,,,17163727.0,,1,-1,sql|splunk-query,13,4
1153,259609,70163093,"SQL query, offset and limit","<p>I'm using splunk. Postgres SQL.</p>
<p>I have 2.1M rows to pull up in ASC order.</p>
<p>This is my standard query that is working:</p>
<pre><code>WHERE we_student_id = 5678
ORDER BY audit.b_created_date DESC NULLS LAST
</code></pre>
<p>then I usually use: (if data is more than 1-2M. I split them into batches)</p>
<pre><code>FETCH FIRST 500000 ROWS ONLY
OFFSET 500000 ROWS FETCH NEXT 500000 ROWS ONLY
</code></pre>
<p>this time, my client requested to extract them by ASC order based on audited id not audit_created_date.</p>
<p>I used:</p>
<pre><code>WHERE student_id = 5678
ORDER BY audit.audited_id ASC NULLS LAST
</code></pre>
<p>==========</p>
<p>I tried to pull up the first 500k.</p>
<p>I used:</p>
<pre><code>*ORDER BY ASC NULLS LAST
LIMIT 500000 OFFSET 0* 
</code></pre>
<p>The result is just 100k.</p>
<p>I tried to put maxrows=0 before my select statement with the same query</p>
<pre><code>*ORDER BY ASC NULLS LAST
LIMIT 500000 OFFSET 0*
</code></pre>
<p>but I'm getting an error: <code>canceling statement due to user request.</code></p>
<p>I tried this query to get the first 400k instead of 500k and removed the OFFSET 0. And I'm still using <code>maxrows=0</code> before my select statement</p>
<pre><code>*ORDER BY ASC NULLS LAST
LIMIT 400000*
</code></pre>
<p>There's a result 400k.</p>
<p>When I tried to extract the next 400k, I queried</p>
<pre><code>*LIMIT 400000 OFFSET 400000*
</code></pre>
<p>I encountered the error again: <code>canceling statement due to user request.</code></p>
<p>Usually, I can pull up 2M rows on Database. I usually use &quot;FETCH FIRST 1000000&quot; Then offset the other batch. My usual query on DB is
<code>ORDER BY DESC NULLS LAST</code> and use FETCH first and OFFSET</p>
<p>But this time, my client wants to get the data by ASC order.</p>
<p>I tried <code>FETCH FIRST 400000 ROWS ONLY</code> query and there's a 400k result. but whenever I increase the number to 500000, I get this error: <code>canceling statement due to user request.</code></p>
<p>I usually use <code>maxrows=0</code> because Splunk only shows the first 100k rows. Most of my data are 1-2 Million.</p>
<p>This error only happened when the client requested the reports by ASC order.</p>
<p>I just want to pull up the 2.1M rows on the database and I don't know how to pull it up by ASC order. I don't know if I'm using OFFSET and LIMIT correctly.</p>",,0,4,,2021-11-30 01:03:04,,2021-11-30 18:58:19,2021-11-30 18:58:19,,4418.0,,17163727.0,,1,0,sql|postgresql|splunk|splunk-query|splunk-dbconnect,47,6
1154,259610,70195933,Splunk : How to figure out replication Factor,"<p>If this sound silly to you I apologise in advance, I am new to splunk and did udemy course but can't figure out this.</p>
<pre><code>If I check my indexes.conf file in cluster master I get repFator=0
#
# By default none of the indexes are replicated.
#
repFactor = 0
</code></pre>
<p>but if I check https://:8089/services/cluster/config</p>
<p>I see replication factor :</p>
<pre><code>replication_factor  2
</code></pre>
<p>So I am confused whether my data is getting replicated,
I have two indexes in a cluster</p>",,2,0,,2021-12-2 08:03:59,,2021-12-2 13:19:39,,,,,9616621.0,,1,2,splunk|splunk-query|splunk-dashboard,12,5
1155,259611,70204059,"Match a word, but ignore occurences in certain phrases","<p>I think I'm looking for a negative lookbehind regex, but I can't figure out the syntax.</p>
<p>I want to match a word in a single message field, but only when the word is not preceded by certain other words. The word <em>may</em> appear multiple times in the same message field -- as long as it's stand-alone somewhere, it should match.</p>
<p>Specifically, we have an alert to watch the logs for timeouts, and one of the match criteria is when the word &quot;timeout&quot; is part of the logged message. The trouble is that the message field also contains the stack trace, and the logs also contain many error events which are not timeouts. The stack trace often includes the word <code>timeout</code> as an argument to one or more methods for many different error conditions. These produce false positives (often, 50% of the alert results are not real timeouts). As arguments, they also specify the data type, such as <code>TimeSpan timeout</code> or <code>Int32 timeout</code> which is the type of multi-word &quot;phrase&quot; I'm trying to exclude/ignore.</p>
<p>These examples are slightly modified for simplicity.</p>
<p>An event that <em>should</em> match (the word &quot;timeout&quot; is stand-alone, even though it also exists as part of the unwanted &quot;TimeSpan timeout&quot; phrase later in the same message field):</p>
<pre><code>Message: System.TimeoutException: The request channel timed out. Increase the timeout value.
StackTrace: 
at System.ServiceModel.Request(Message message, TimeSpan timeout)
at ...(etc - many more lines of stacked calls)...
</code></pre>
<p>An example of log data that should <em>not</em> match, the word &quot;timeout&quot; doesn't exist stand-alone:</p>
<pre><code>Message: System.EndpointNotFoundException: There was no endpoint listening at https://xxxxx.
StackTrace:
at System.ServiceModel.Pool.EstablishConnection(TimeSpan timeout)
at ...(etc)...
</code></pre>
<p>I thought this criteria would work, but it does not, it excludes <em>real</em> timeouts (like the first example) when those errors <em>also</em> have a stack trace with the <code>TimeSpan timeout</code> argument:</p>
<pre><code>&quot;timeout&quot; NOT &quot;TimeSpan timeout&quot;
</code></pre>
<p>After I started thinking about regex, I tried a query along these lines but it appears to still match the <code>TimeSpan timeout</code> portions. I'm pretty new to Splunk and I've never used lookbehind regex so I suspect I just screwed up the details here. My thinking with the first &quot;timeout&quot; is to just quickly discard anything that doesn't mention &quot;timeout&quot; at all, then I thought the negative lookback regex would discard anything but events where &quot;timeout&quot; appears stand-alone:</p>
<pre><code>index=foo &quot;timed out&quot; OR &quot;timeout&quot; | regex _raw=&quot;(?&lt;!TimeSpan )timeout&quot;
</code></pre>
<p>Edit: The regex by itself seems to be correct, see <a href=""https://regex101.com/r/rZxkvm/1"" rel=""nofollow noreferrer"">https://regex101.com/r/rZxkvm/1</a> ... so the problem is how I'm using it within a Splunk query.</p>
<p>(Please note I'm not interested in completely different criteria -- I realize in these examples I could search for something like <code>*TimeoutException</code> which is a common-enough naming pattern, but we have some systems for which the approach I've described is the only way to reliably identify real timeout errors and exclude false-positives.)</p>",,1,6,,2021-12-2 17:36:17,,2021-12-2 18:15:17,2021-12-2 17:55:40,,152997.0,,152997.0,,1,1,regex|splunk,29,6
1156,259612,70210276,Send Google Home data to Splunk,"<p>I'm trying to send data generated by a Google Home mesh network to a Splunk instance. I'd especially like to capture which devices are connected to which points throughout the day. This information is available in the app, but does not seem to be able to be streamed to a centralized logging platform. Is there a way I'm not seeing?</p>",,0,0,,2021-12-3 06:23:12,,2021-12-3 06:23:12,,,,,17576604.0,,1,1,splunk|google-home,10,4
